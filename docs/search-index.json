[
  {
    "id": 1,
    "title": "Graphical Summaries",
    "url": "GraphicalSummaries.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Graphical Summaries There are many ways to display data. The fundamental idea is that the graphical depiction of data should communicate the truth the data has to offer about the situation of interest. Histograms 1 Quantitative Variable Overview Great for showing the distribution of data for a single quantitative variable when the sample size is large. Dotplots are a good alternative for smaller sample sizes. Gives a good feel for the mean and standard deviation of the data. R Instructions Base R ggplot2 plotly To make a histogram in R use the function: hist(object) object must be quantitative data. R refers to this as a â€œnumeric vector.â€ Usually this will be the column of a dataset accessed with the $ sign by hist(dataSetName$columnName). Type ?hist in your R Console to open the help file in R. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic histogram hist An R function â€œhistâ€ used to create a histogram. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the hist function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Change Color hist(airquality$Temp,Â  This code was explained in the first example code. col=â€œskyblueâ€ col= allows us to specify the color of the plot using a named color. The name of the color must be placed in quotations. Type â€œcolors()â€ in R to see color options. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Titles hist(airquality$Temp This part was explained in the first example code. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. col=â€œskyblueâ€col= allows us to specify the color of the plot using a named color. The name of the color must be placed in quotations. Type â€œcolors()â€ in R to see color options. ,Â  A comma must always be used to separate additional commands. xlab=â€œTemperatureâ€ xlab= stands for â€œx label.â€ Use it to specify the text to print on the plot under the x-axis. The desired text must always be in quotations. ,Â  A comma must always be used to separate additional commands. main=â€œLa Guardia Airport Daily Mean Temperaturesâ€ main= lets us specify the â€œmainâ€ title to be placed above the plot. The desired text must always be placed in quotations. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. To make a histogram in R using the ggplot approach, first ensure library(ggplot2) is loaded. Then, ggplot(data, aes(x=column)) + Â Â geom_histogram() data is the name of your dataset. column is a column of data from your dataset that is quantitative. The aesthetic helper function aes(x= ) is how you tell the gpplot to make the x-axis become your column of data. The geometry helper function geom_histogram() causes the ggplot to become a histogram. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic Histogram ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp â€œx=â€ declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_histogram() The â€œgeom_histogram()â€ function causes the ggplot to become a histogram. There are many other â€œgeom_â€ functions that could be used. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Change Bin Width and Color ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp â€œx=â€ declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_histogram( The â€œgeom_histogram()â€ function causes the ggplot to become a histogram. There are many other â€œgeom_â€ functions that could be used. binwidth=5,Â  The â€œbinwidthâ€ command controls the width of the bars in the histogram. fill=â€œskyblueâ€, The â€œfillâ€ command controls the color of the insides of each bar. color=â€œblackâ€ The â€œcolorâ€ command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Titles ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp â€œx=â€ declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_histogram( The â€œgeom_histogram()â€ function causes the ggplot to become a histogram. There are many other â€œgeom_â€ functions that could be used. binwidth=5,Â  The â€œbinwidthâ€ command controls the width of the bars in the histogram. fill=â€œskyblueâ€, The â€œfillâ€ command controls the color of the insides of each bar. color=â€œblackâ€ The â€œcolorâ€ command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  labs( The â€œlabsâ€ function is used to add labels to the plot, like a main title, x-label and y-label. title=â€œLa Guardia Airport Daily Mean Temperatureâ€,Â  The â€œtitle=â€ command allows you to control the main title at the top of the graphic. x=â€œTemperatureâ€,Â  The â€œx=â€ command allows you to control the x-label of the graphic. y=â€œNumber of Daysâ€ The â€œy=â€ command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Gallery See some ideas from past studentsâ€¦ Show/Hide Gallery Hover to see code. Copy the code into R. Play with it. Modify it to create your own graph. ggplot(airquality, aes(x = Temp)) + geom_histogram(binwidth=5, fill = \"skyblue\", color = \"black\") + labs(title = \"La Guardia Airport Daily Mean Temperature\", Â Â  x = \"Temperature\", y = \"Number of Days\") + theme_bw() + theme(plot.title = element_text(size = 14, face = \"bold\", hjust = .5), Â Â  panel.grid.major = element_line(color = \"grey80\")) ggplot(airquality) + geom_histogram(aes(x = Temp, fill = Temp > median(Temp)), color = \"black\", binwidth = 2) + labs(title = \"La Guardia Airport Daily Mean Temperature\", Â Â  x = \"Temperature\", y = \"Number of Days\") + theme_bw() + theme(plot.title = element_text(size = 14, face = \"bold\", hjust = .5), Â Â  panel.grid.major = element_line(color = \"grey80\"), Â Â  legend.position = \"none\") + geom_vline(xintercept = 72, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_vline(xintercept = 85, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 71, y = 10, label = \"Q1\") + geom_text(x = 86, y = 10, label = \"Q3\") ggplot(airquality, aes(x = Temp)) + geom_histogram(aes(fill = Temp > median(Temp)), color = \"black\", binwidth = 2) + labs(title = \"La Guardia Airport Daily Mean Temperature\", Â Â  x = \"Temperature\", y = \"Number of Days\", fill = \"Temperature\") + theme_bw() + theme(plot.title = element_text(size = 10, face = \"bold\", hjust = .5), Â Â  panel.grid.major = element_line(color = \"grey80\")) + scale_fill_manual(values = c(\"steelblue2\",\"brown1\"),labels=c(\"< 79 Degrees\",\"> 79 Degrees\")) + geom_vline(xintercept = 72, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_vline(xintercept = 85, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 71, y = 10, label = \"Q1\") + geom_text(x = 86, y = 10, label = \"Q3\") ggplot(airquality, aes(x = Temp)) + geom_histogram(aes(fill = Temp > median(Temp)), color = \"black\", binwidth = 3) + labs(title = \"Temperature of La Guardia Airport by Month\", Â Â  x = \"Temperature\", y = \"Number of Days\", fill = \"Temperature\") + theme_bw() + theme(plot.title = element_text(size = 16, face = \"bold\", hjust = .5), Â Â  panel.grid.major = element_line(color = \"grey80\")) + scale_fill_manual(values = c(\"steelblue2\",\"brown1\"),labels=c(\"< 79 Degrees\",\"> 79 Degrees\")) + facet_grid(~Month) + geom_vline(xintercept = 77.88, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 69, y = 10, label = \"Mean = \\n 77.88\") To make a histogram in plotly first load library(plotly) Then, use the function: plot_ly(dataName, x=~columnName, type=\"histogram\") dataName is the name of a data set columnName must be the name of a column of quantitative data. R refers to this as a â€œnumeric vector.â€ type=\"histogram\" tells the plot_ly(â€¦) function to create a histogram. Visit plotly.com/r/histograms for more details. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic histogram plot_ly An R function â€œplot_lyâ€ from library(plotly) used to create any plotly plot. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality,Â  â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. x= The x= allows us to declare which column of the data set will become the x-axis of the histogram. ~Temp, Â  â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. The ~ is required before column names inside all plot_ly(â€¦) commands. type=â€œhistogramâ€ This option tells the plot_ly(â€¦) function what â€œtypeâ€ of graph to make. In this case, a histogram. )Closing parenthsis for the plot_ly function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Change Color plot_ly(airquality, x=~Temp, type=â€œhistogramâ€,Â  This code was explained in the first example code. marker=list( this â€œlist(â€¦)â€ of options that will be specified will effect the bars of the histogram. color = â€œskyblueâ€,Â  this will change the color of the bars to skyblue. line = list(,Â  this opens a list of options to specify for the â€œlinesâ€ around the â€œmarkers.â€ color = â€œdarkgrayâ€,Â  this will change the color of the lines around the bars to darkgray. width = 2 this will change the width of the lines around the bars to 2 pixels. Too really see what this does, change it to something crazy like 10. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Titles plot_ly(airquality$Temp, type=â€œhistogramâ€,Â  This code was explained in the first example code. marker=list( this â€œlist(â€ of options that will be specified will effect the bars of the histogram. color = â€œskyblueâ€,Â  this will change the color of the bars to skyblue. line = list(,Â  this opens a list of options to specify for the â€œlinesâ€ around the â€œmarkers.â€ color = â€œdarkgrayâ€,Â  this will change the color of the lines around the bars to darkgray. width = 10 this will change the width of the lines around the bars to 10 pixels, which is rather large really. Using a width=2 is probably better. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. Â %>% The pipe operator passes the completed plot_ly(â€¦) code into the layout(â€¦) function. layout( The layout(â€¦) function is used for specifying details about the axes and their labels. title=â€œLa Guardia Airport Daily Mean Temperaturesâ€ This declares a main title for the top of the graph. xaxis=list( This declares a list of options to be specified for the xaxis. The same can be done for the yaxis(â€¦). title=â€œTemperature in Degrees Fâ€ This declares a title underneath the x-axis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Explanation Histograms group data that are close to each other into â€œbinsâ€ (the vertical bars in the plot). The height of a bin is determined by the number of data points that are contained within the bin. For example, if we group together all the sections of the book of scripture known as the Doctrine and Covenants that occurred in a given year (Jan.Â 1st - Dec.Â 31st) then we get the following counts. Year Number of Sections 1823 1 1824 0 1825 0 1826 0 1827 0 1828 1 1829 16 1830 19 1831 37 1832 16 1833 12 1834 5 1835 3 1836 4 1837 1 1838 8 1839 3 1840 0 1841 3 1842 2 1843 ",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Graphical Summaries There are many ways to display data.", "The fundamental idea is that the graphical depiction of data should communicate the truth the data has to offer about the situation of interest.", "Histograms 1 Quantitative Variable Overview Great for showing the distribution of data for a single quantitative variable when the sample size is large.", "Dotplots are a good alternative for smaller sample sizes.", "Gives a good feel for the mean and standard deviation of the data.", "R Instructions Base R ggplot2 plotly To make a histogram in R use the function: hist(object) object must be quantitative data.", "R refers to this as a â€œnumeric vector.â€ Usually this will be the column of a dataset accessed with the $ sign by hist(dataSetName$columnName).", "Type ?hist in your R Console to open the help file in R.", "Example Code Hover your mouse over the example codes to learn more.", "Click on them to see what they create.", "Basic histogram hist An R function â€œhistâ€ used to create a histogram.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset.", "Type â€œView(airquality)â€ in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset.", ")Closing parenthsis for the hist function.", "Â Â Â Â Press Enter to run the code.", "Â â€¦Â  Click to View Output.", "Change Color hist(airquality$Temp,Â  This code was explained in the first example code.", "col=â€œskyblueâ€ col= allows us to specify the color of the plot using a named color.", "The name of the color must be placed in quotations.", "Type â€œcolors()â€ in R to see color options.", ") Functions always end with a closing parenthesis.", "Â Â Â Â Press Enter to run the code.", "Â â€¦Â  Click to View Output.", "Add Titles hist(airquality$Temp This part was explained in the first example code.", ",Â  The comma allows us to specify optional commands to the function.", "The space after the comma is not required.", "It just looks nice.", "col=â€œskyblueâ€col= allows us to specify the color of the plot using a named color.", "The name of the color must be placed in quotations.", "Type â€œcolors()â€ in R to see color options.", ",Â  A comma must always be used to separate additional commands.", "xlab=â€œTemperatureâ€ xlab= stands for â€œx label.â€ Use it to specify the text to print on the plot under the x-axis.", "The desired text must always be in quotations.", ",Â  A comma must always be used to separate additional commands.", "main=â€œLa Guardia Airport Daily Mean Temperaturesâ€ main= lets us specify the â€œmainâ€ title to be placed above the plot.", "The desired text must always be placed in quotations.", ") Functions must always end with a closing parenthesis.", "Â Â Â Â Press Enter to run the code.", "Â â€¦Â  Click to View Output.", "To make a histogram in R using the ggplot approach, first ensure library(ggplot2) is loaded.", "Then, ggplot(data, aes(x=column)) + Â Â geom_histogram() data is the name of your dataset.", "column is a column of data from your dataset that is quantitative.", "The aesthetic helper function aes(x= ) is how you tell the gpplot to make the x-axis become your column of data.", "The geometry helper function geom_histogram() causes the ggplot to become a histogram.", "Example Code Hover your mouse over the example codes to learn more.", "Click on them to see what they create."],
    "type": "page",
    "page_title": "Graphical Summaries"
  },
  {
    "id": 2,
    "title": "ğŸ“ Histograms",
    "url": "GraphicalSummaries.html#histograms",
    "content": "Histograms 1 Quantitative Variable Overview Great for showing the distribution of data for a single quantitative variable when the sample size is large. Dotplots are a good alternative for smaller sample sizes. Gives a good feel for the mean and standard deviation of the data. R Instructions Base R ggplot2 plotly To make a histogram in R use the function: hist(object) object must be quantitative data. R refers to this as a â€œnumeric vector.â€ Usually this will be the column of a dataset accessed with the $ sign by hist(dataSetName$columnName). Type ?hist in your R Console to open the help file in R. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic histogram hist An R function â€œhistâ€ used to create a histogram. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the hist function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Change Color hist(airquality$Temp,Â  This code was explained in the first example code. col=â€œskyblueâ€ col= allows us to specify the color of the plot using a named color. The name of the color must be placed in quotations. Type â€œcolors()â€ in R to see color options. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Titles hist(airquality$Temp This part was explained in the first example code. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. col=â€œskyblueâ€col= allows us to specify the color of the plot using a named color. The name of the color must be placed in quotations. Type â€œcolors()â€ in R to see color options. ,Â  A comma must always be used to separate additional commands. xlab=â€œTemperatureâ€ xlab= stands for â€œx label.â€ Use it to specify the text to print on the plot under the x-axis. The desired text must always be in quotations. ,Â  A comma must always be used to separate additional commands. main=â€œLa Guardia Airport Daily Mean Temperaturesâ€ main= lets us specify the â€œmainâ€ title to be placed above the plot. The desired text must always be placed in quotations. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. To make a histogram in R using the ggplot approach, first ensure library(ggplot2) is loaded. Then, ggplot(data, aes(x=column)) + Â Â geom_histogram() data is the name of your dataset. column is a column of data from your dataset that is quantitative. The aesthetic helper function aes(x= ) is how you tell the gpplot to make the x-axis become your column of data. The geometry helper function geom_histogram() causes the ggplot to become a histogram. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic Histogram ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp â€œx=â€ declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_histogram() The â€œgeom_histogram()â€ function causes the ggplot to become a histogram. There are many other â€œgeom_â€ functions that could be used. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Change Bin Width and Color ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp â€œx=â€ declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_histogram( The â€œgeom_histogram()â€ function causes the ggplot to become a histogram. There are many other â€œgeom_â€ functions that could be used. binwidth=5,Â  The â€œbinwidthâ€ command controls the width of the bars in the histogram. fill=â€œskyblueâ€, The â€œfillâ€ command controls the color of the insides of each bar. color=â€œblackâ€ The â€œcolorâ€ command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Titles ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp â€œx=â€ declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_histogram( The â€œgeom_histogram()â€ function causes the ggplot to become a histogram. There are many other â€œgeom_â€ functions that could be used. binwidth=5,Â  The â€œbinwidthâ€ command controls the width of the bars in the histogram. fill=â€œskyblueâ€, The â€œfillâ€ command controls the color of the insides of each bar. color=â€œblackâ€ The â€œcolorâ€ command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  labs( The â€œlabsâ€ function is used to add labels to the plot, like a main title, x-label and y-label. title=â€œLa Guardia Airport Daily Mean Temperatureâ€,Â  The â€œtitle=â€ command allows you to control the main title at the top of the graphic. x=â€œTemperatureâ€,Â  The â€œx=â€ command allows you to control the x-label of the graphic. y=â€œNumber of Daysâ€ The â€œy=â€ command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Gallery See some ideas from past studentsâ€¦ Show/Hide Gallery Hover to see code. Copy the code into R. Play with it. Modify it to create your own graph. ggplot(airquality, aes(x = Temp)) + geom_histogram(binwidth=5, fill = \"skyblue\", color = \"black\") + labs(title = \"La Guardia Airport Daily Mean Temperature\", Â Â  x = \"Temperature\", y = \"Number of Days\") + theme_bw() + theme(plot.title = element_text(size = 14, face = \"bold\", hjust = .5), Â Â  panel.grid.major = element_line(color = \"grey80\")) ggplot(airquality) + geom_histogram(aes(x = Temp, fill = Temp > median(Temp)), color = \"black\", binwidth = 2) + labs(title = \"La Guardia Airport Daily Mean Temperature\", Â Â  x = \"Temperature\", y = \"Number of Days\") + theme_bw() + theme(plot.title = element_text(size = 14, face = \"bold\", hjust = .5), Â Â  panel.grid.major = element_line(color = \"grey80\"), Â Â  legend.position = \"none\") + geom_vline(xintercept = 72, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_vline(xintercept = 85, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 71, y = 10, label = \"Q1\") + geom_text(x = 86, y = 10, label = \"Q3\") ggplot(airquality, aes(x = Temp)) + geom_histogram(aes(fill = Temp > median(Temp)), color = \"black\", binwidth = 2) + labs(title = \"La Guardia Airport Daily Mean Temperature\", Â Â  x = \"Temperature\", y = \"Number of Days\", fill = \"Temperature\") + theme_bw() + theme(plot.title = element_text(size = 10, face = \"bold\", hjust = .5), Â Â  panel.grid.major = element_line(color = \"grey80\")) + scale_fill_manual(values = c(\"steelblue2\",\"brown1\"),labels=c(\"< 79 Degrees\",\"> 79 Degrees\")) + geom_vline(xintercept = 72, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_vline(xintercept = 85, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 71, y = 10, label = \"Q1\") + geom_text(x = 86, y = 10, label = \"Q3\") ggplot(airquality, aes(x = Temp)) + geom_histogram(aes(fill = Temp > median(Temp)), color = \"black\", binwidth = 3) + labs(title = \"Temperature of La Guardia Airport by Month\", Â Â  x = \"Temperature\", y = \"Number of Days\", fill = \"Temperature\") + theme_bw() + theme(plot.title = element_text(size = 16, face = \"bold\", hjust = .5), Â Â  panel.grid.major = element_line(color = \"grey80\")) + scale_fill_manual(values = c(\"steelblue2\",\"brown1\"),labels=c(\"< 79 Degrees\",\"> 79 Degrees\")) + facet_grid(~Month) + geom_vline(xintercept = 77.88, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 69, y = 10, label = \"Mean = \\n 77.88\") To make a histogram in plotly first load library(plotly) Then, use the function: plot_ly(dataName, x=~columnName, type=\"histogram\") dataName is the name of a data set columnName must be the name of a column of quantitative data. R refers to this as a â€œnumeric vector.â€ type=\"histogram\" tells the plot_ly(â€¦) function to create a histogram. Visit plotly.com/r/histograms for more details. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic histogram plot_ly An R function â€œplot_lyâ€ from library(plotly) used to create any plotly plot. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality,Â  â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. x= The x= allows us to declare which column of the data set will become the x-axis of the histogram. ~Temp, Â  â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. The ~ is required before column names inside all plot_ly(â€¦) commands. type=â€œhistogramâ€ This option tells the plot_ly(â€¦) function what â€œtypeâ€ of graph to make. In this case, a histogram. )Closing parenthsis for the plot_ly function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Change Color plot_ly(airquality, x=~Temp, type=â€œhistogramâ€,Â  This code was explained in the first example code. marker=list( this â€œlist(â€¦)â€ of options that will be specified will effect the bars of the histogram. color = â€œskyblueâ€,Â  this will change the color of the bars to skyblue. line = list(,Â  this opens a list of options to specify for the â€œlinesâ€ around the â€œmarkers.â€ color = â€œdarkgrayâ€,Â  this will change the color of the lines around the bars to darkgray. width = 2 this will change the width of the lines around the bars to 2 pixels. Too really see what this does, change it to something crazy like 10. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Titles plot_ly(airquality$Temp, type=â€œhistogramâ€,Â  This code was explained in the first example code. marker=list( this â€œlist(â€ of options that will be specified will effect the bars of the histogram. color = â€œskyblueâ€,Â  this will change the color of the bars to skyblue. line = list(,Â  this opens a list of options to specify for the â€œlinesâ€ around the â€œmarkers.â€ color = â€œdarkgrayâ€,Â  this will change the color of the lines around the bars to darkgray. width = 10 this will change the width of the lines around the bars to 10 pixels, which is rather large really. Using a width=2 is probably better. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. Â %>% The pipe operator passes the completed plot_ly(â€¦) code into the layout(â€¦) function. layout( The layout(â€¦) function is used for specifying details about the axes and their labels. title=â€œLa Guardia Airport Daily Mean Temperaturesâ€ This declares a main title for the top of the graph. xaxis=list( This declares a list of options to be specified for the xaxis. The same can be done for the yaxis(â€¦). title=â€œTemperature in Degrees Fâ€ This declares a title underneath the x-axis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output.",
    "sentences": ["1 Quantitative Variable", "Overview Great for showing the distribution of data for a single quantitative variable when the sample size is large.", "Dotplots are a good alternative for smaller sample sizes.", "Gives a good feel for the mean and standard deviation of the data.", "R Instructions Base R ggplot2 plotly To make a histogram in R use the function: hist(object) object must be quantitative data.", "R refers to this as a â€œnumeric vector.â€ Usually this will be the column of a dataset accessed with the $ sign by hist(dataSetName$columnName).", "Type ?hist in your R Console to open the help file in R.", "Example Code Hover your mouse over the example codes to learn more.", "Click on them to see what they create.", "Basic histogram hist An R function â€œhistâ€ used to create a histogram."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Histograms"
  },
  {
    "id": 3,
    "title": "ğŸ“ Overview",
    "url": "GraphicalSummaries.html#overview",
    "content": "Overview Great for showing the distribution of data for a single quantitative variable when the sample size is large. Dotplots are a good alternative for smaller sample sizes. Gives a good feel for the mean and standard deviation of the data.",
    "sentences": ["Great for showing the distribution of data for a single quantitative variable when the sample size is large.", "Dotplots are a good alternative for smaller sample sizes.", "Gives a good feel for the mean and standard deviation of the data."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 4,
    "title": "ğŸ“ R Instructions",
    "url": "GraphicalSummaries.html#rinstructions",
    "content": "R Instructions Base R ggplot2 plotly To make a histogram in R use the function: hist(object) object must be quantitative data. R refers to this as a â€œnumeric vector.â€ Usually this will be the column of a dataset accessed with the $ sign by hist(dataSetName$columnName). Type ?hist in your R Console to open the help file in R. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic histogram hist An R function â€œhistâ€ used to create a histogram. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the hist function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Change Color hist(airquality$Temp,Â  This code was explained in the first example code. col=â€œskyblueâ€ col= allows us to specify the color of the plot using a named color. The name of the color must be placed in quotations. Type â€œcolors()â€ in R to see color options. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Titles hist(airquality$Temp This part was explained in the first example code. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. col=â€œskyblueâ€col= allows us to specify the color of the plot using a named color. The name of the color must be placed in quotations. Type â€œcolors()â€ in R to see color options. ,Â  A comma must always be used to separate additional commands. xlab=â€œTemperatureâ€ xlab= stands for â€œx label.â€ Use it to specify the text to print on the plot under the x-axis. The desired text must always be in quotations. ,Â  A comma must always be used to separate additional commands. main=â€œLa Guardia Airport Daily Mean Temperaturesâ€ main= lets us specify the â€œmainâ€ title to be placed above the plot. The desired text must always be placed in quotations. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. To make a histogram in R using the ggplot approach, first ensure library(ggplot2) is loaded. Then, ggplot(data, aes(x=column)) + Â Â geom_histogram() data is the name of your dataset. column is a column of data from your dataset that is quantitative. The aesthetic helper function aes(x= ) is how you tell the gpplot to make the x-axis become your column of data. The geometry helper function geom_histogram() causes the ggplot to become a histogram. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic Histogram ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp â€œx=â€ declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_histogram() The â€œgeom_histogram()â€ function causes the ggplot to become a histogram. There are many other â€œgeom_â€ functions that could be used. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Change Bin Width and Color ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp â€œx=â€ declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_histogram( The â€œgeom_histogram()â€ function causes the ggplot to become a histogram. There are many other â€œgeom_â€ functions that could be used. binwidth=5,Â  The â€œbinwidthâ€ command controls the width of the bars in the histogram. fill=â€œskyblueâ€, The â€œfillâ€ command controls the color of the insides of each bar. color=â€œblackâ€ The â€œcolorâ€ command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Titles ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp â€œx=â€ declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_histogram( The â€œgeom_histogram()â€ function causes the ggplot to become a histogram. There are many other â€œgeom_â€ functions that could be used. binwidth=5,Â  The â€œbinwidthâ€ command controls the width of the bars in the histogram. fill=â€œskyblueâ€, The â€œfillâ€ command controls the color of the insides of each bar. color=â€œblackâ€ The â€œcolorâ€ command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  labs( The â€œlabsâ€ function is used to add labels to the plot, like a main title, x-label and y-label. title=â€œLa Guardia Airport Daily Mean Temperatureâ€,Â  The â€œtitle=â€ command allows you to control the main title at the top of the graphic. x=â€œTemperatureâ€,Â  The â€œx=â€ command allows you to control the x-label of the graphic. y=â€œNumber of Daysâ€ The â€œy=â€ command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Gallery See some ideas from past studentsâ€¦ Show/Hide Gallery Hover to see code. Copy the code into R. Play with it. Modify it to create your own graph. ggplot(airquality, aes(x = Temp)) + geom_histogram(binwidth=5, fill = \"skyblue\", color = \"black\") + labs(title = \"La Guardia Airport Daily Mean Temperature\", Â Â  x = \"Temperature\", y = \"Number of Days\") + theme_bw() + theme(plot.title = element_text(size = 14, face = \"bold\", hjust = .5), Â Â  panel.grid.major = element_line(color = \"grey80\")) ggplot(airquality) + geom_histogram(aes(x = Temp, fill = Temp > median(Temp)), color = \"black\", binwidth = 2) + labs(title = \"La Guardia Airport Daily Mean Temperature\", Â Â  x = \"Temperature\", y = \"Number of Days\") + theme_bw() + theme(plot.title = element_text(size = 14, face = \"bold\", hjust = .5), Â Â  panel.grid.major = element_line(color = \"grey80\"), Â Â  legend.position = \"none\") + geom_vline(xintercept = 72, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_vline(xintercept = 85, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 71, y = 10, label = \"Q1\") + geom_text(x = 86, y = 10, label = \"Q3\") ggplot(airquality, aes(x = Temp)) + geom_histogram(aes(fill = Temp > median(Temp)), color = \"black\", binwidth = 2) + labs(title = \"La Guardia Airport Daily Mean Temperature\", Â Â  x = \"Temperature\", y = \"Number of Days\", fill = \"Temperature\") + theme_bw() + theme(plot.title = element_text(size = 10, face = \"bold\", hjust = .5), Â Â  panel.grid.major = element_line(color = \"grey80\")) + scale_fill_manual(values = c(\"steelblue2\",\"brown1\"),labels=c(\"< 79 Degrees\",\"> 79 Degrees\")) + geom_vline(xintercept = 72, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_vline(xintercept = 85, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 71, y = 10, label = \"Q1\") + geom_text(x = 86, y = 10, label = \"Q3\") ggplot(airquality, aes(x = Temp)) + geom_histogram(aes(fill = Temp > median(Temp)), color = \"black\", binwidth = 3) + labs(title = \"Temperature of La Guardia Airport by Month\", Â Â  x = \"Temperature\", y = \"Number of Days\", fill = \"Temperature\") + theme_bw() + theme(plot.title = element_text(size = 16, face = \"bold\", hjust = .5), Â Â  panel.grid.major = element_line(color = \"grey80\")) + scale_fill_manual(values = c(\"steelblue2\",\"brown1\"),labels=c(\"< 79 Degrees\",\"> 79 Degrees\")) + facet_grid(~Month) + geom_vline(xintercept = 77.88, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 69, y = 10, label = \"Mean = \\n 77.88\") To make a histogram in plotly first load library(plotly) Then, use the function: plot_ly(dataName, x=~columnName, type=\"histogram\") dataName is the name of a data set columnName must be the name of a column of quantitative data. R refers to this as a â€œnumeric vector.â€ type=\"histogram\" tells the plot_ly(â€¦) function to create a histogram. Visit plotly.com/r/histograms for more details. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic histogram plot_ly An R function â€œplot_lyâ€ from library(plotly) used to create any plotly plot. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality,Â  â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. x= The x= allows us to declare which column of the data set will become the x-axis of the histogram. ~Temp, Â  â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. The ~ is required before column names inside all plot_ly(â€¦) commands. type=â€œhistogramâ€ This option tells the plot_ly(â€¦) function what â€œtypeâ€ of graph to make. In this case, a histogram. )Closing parenthsis for the plot_ly function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Change Color plot_ly(airquality, x=~Temp, type=â€œhistogramâ€,Â  This code was explained in the first example code. marker=list( this â€œlist(â€¦)â€ of options that will be specified will effect the bars of the histogram. color = â€œskyblueâ€,Â  this will change the color of the bars to skyblue. line = list(,Â  this opens a list of options to specify for the â€œlinesâ€ around the â€œmarkers.â€ color = â€œdarkgrayâ€,Â  this will change the color of the lines around the bars to darkgray. width = 2 this will change the width of the lines around the bars to 2 pixels. Too really see what this does, change it to something crazy like 10. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Titles plot_ly(airquality$Temp, type=â€œhistogramâ€,Â  This code was explained in the first example code. marker=list( this â€œlist(â€ of options that will be specified will effect the bars of the histogram. color = â€œskyblueâ€,Â  this will change the color of the bars to skyblue. line = list(,Â  this opens a list of options to specify for the â€œlinesâ€ around the â€œmarkers.â€ color = â€œdarkgrayâ€,Â  this will change the color of the lines around the bars to darkgray. width = 10 this will change the width of the lines around the bars to 10 pixels, which is rather large really. Using a width=2 is probably better. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. Â %>% The pipe operator passes the completed plot_ly(â€¦) code into the layout(â€¦) function. layout( The layout(â€¦) function is used for specifying details about the axes and their labels. title=â€œLa Guardia Airport Daily Mean Temperaturesâ€ This declares a main title for the top of the graph. xaxis=list( This declares a list of options to be specified for the xaxis. The same can be done for the yaxis(â€¦). title=â€œTemperature in Degrees Fâ€ This declares a title underneath the x-axis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output.",
    "sentences": ["Base R ggplot2 plotly To make a histogram in R use the function: hist(object) object must be quantitative data.", "R refers to this as a â€œnumeric vector.â€ Usually this will be the column of a dataset accessed with the $ sign by hist(dataSetName$columnName).", "Type ?hist in your R Console to open the help file in R.", "Example Code Hover your mouse over the example codes to learn more.", "Click on them to see what they create.", "Basic histogram hist An R function â€œhistâ€ used to create a histogram.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset.", "Type â€œView(airquality)â€ in R to see it."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 5,
    "title": "ğŸ“ Explanation",
    "url": "GraphicalSummaries.html#explanation",
    "content": "Explanation Histograms group data that are close to each other into â€œbinsâ€ (the vertical bars in the plot). The height of a bin is determined by the number of data points that are contained within the bin. For example, if we group together all the sections of the book of scripture known as the Doctrine and Covenants that occurred in a given year (Jan.Â 1st - Dec.Â 31st) then we get the following counts. Year Number of Sections 1823 1 1824 0 1825 0 1826 0 1827 0 1828 1 1829 16 1830 19 1831 37 1832 16 1833 12 1834 5 1835 3 1836 4 1837 1 1838 8 1839 3 1840 0 1841 3 1842 2 1843 4 1844 1 1845 0 1846 0 1847 1 *Note that Section 138 occurred in 1918 and is removed from this example. In this example, each â€œbinâ€ spans 365 days (Jan.Â 1 - Dec.Â 31 of each year). Since â€œdatesâ€ can be used as quantitative data, it makes sense to make a histogram of these data. (Remember, histograms are only for quantitative data.) Notice in the bins above that the left edge of the bin is on the year the data corresponds with. The right edge of the bin lands on the following year. For example, the first bin has left edge on 1823 and right edge on 1824. Since there was one revelation in 1823, this bin has a height of 1. The bin that has 1831 on the left and 1832 on the right shows that 37 revelations occurred in 1831. It is powerful to notice the amount of revelations occurring around 1830, the year the Church of Jesus Christ of Latter-day Saints was organized.",
    "sentences": ["Histograms group data that are close to each other into â€œbinsâ€ (the vertical bars in the plot).", "The height of a bin is determined by the number of data points that are contained within the bin.", "For example, if we group together all the sections of the book of scripture known as the Doctrine and Covenants that occurred in a given year (Jan.Â 1st - Dec.Â 31st) then we get the following counts.", "Year Number of Sections 1823 1 1824 0 1825 0 1826 0 1827 0 1828 1 1829 16 1830 19 1831 37 1832 16 1833 12 1834 5 1835 3 1836 4 1837 1 1838 8 1839 3 1840 0 1841 3 1842 2 1843 4 1844 1 1845 0 1846 0 1847 1 *Note that Section 138 occurred in 1918 and is removed from this example.", "In this example, each â€œbinâ€ spans 365 days (Jan.Â 1 - Dec.Â 31 of each year).", "Since â€œdatesâ€ can be used as quantitative data, it makes sense to make a histogram of these data.", "(Remember, histograms are only for quantitative data.) Notice in the bins above that the left edge of the bin is on the year the data corresponds with.", "The right edge of the bin lands on the following year.", "For example, the first bin has left edge on 1823 and right edge on 1824.", "Since there was one revelation in 1823, this bin has a height of 1."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Explanation"
  },
  {
    "id": 6,
    "title": "ğŸ“ Boxplots",
    "url": "GraphicalSummaries.html#boxplots",
    "content": "Boxplots 1 Quantitative Variable | 2+ Groups Overview Graphical depiction of the five-number summary. Great for comparing the distributions of data across several groups or categories. Provides a quick visual understanding of the location of the median as well as the range of the data. Can be useful in showing outliers. Sample size should be larger than at least five, or computing the five-number summary is not very meaningful. Side-by-side dotplots are a good alternative for smaller sample sizes. R Instructions Base R ggplot2 plotly To make a boxplot in R use the function: boxplot(object) To make side-by-side boxplots: boxplot(object ~ group, data=NameOfYourData, ...) object must be quantitative data. R refers to this as a â€œnumeric vector.â€ group must be qualitative data. R refers to this as either a â€œcharacter vectorâ€ or a â€œfactor.â€ However, a â€œnumeric vectorâ€ can also act as a qualitative variable. NameOfYourData is the name of the dataset containing object and group. implies there are many other options that can be given to the boxplot() function. Type ?boxplot in your R Console for more details. Example Code Basic Single Boxplot boxplot An R function â€œboxplotâ€ used to create boxplots. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. More Usefulâ€¦ Basic Side-by-Side Boxplot boxplot An R function â€œboxplotâ€ used to create boxplots. ( Parenthesis to begin the function. Must touch the last letter of the function. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. Â ~Â  The ~ is used to tell R that you want one boxplot of the quantitative variable (â€œTempâ€) for each group found in the qualitative variable (â€œMonthâ€). Month â€œMonthâ€ is a qualitative variable (in this case a â€œnumeric vectorâ€ defining months by 5, 6, 7, 8, and 9) from the â€œairqualityâ€ dataset. ,Â The â€œ,â€ is required to start specifying additional commands for the â€œboxplot()â€ function. data=airquality data= is used to tell R that the â€œTempâ€ and â€œMonthâ€ variables are located in the airquality dataset. Without this, R will not know where to find â€œTempâ€ and â€œMonthâ€ and the command will give an error. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Names under each Box boxplot An R function â€œboxplotâ€ used to create boxplots. ( Parenthesis to begin the function. Must touch the last letter of the function. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. Â ~Â  The ~ is used to tell R that you want one boxplot of the quantitative variable (â€œTempâ€) for each group found in the qualitative variable (â€œMonthâ€). Month â€œMonthâ€ is a qualitative variable (in this case a â€œnumeric vectorâ€ defining months by 5, 6, 7, 8, and 9) from the â€œairqualityâ€ dataset. ,Â The â€œ,â€ is required to start specifying additional commands for the â€œboxplot()â€ function. data=airquality data= is used to tell R that the â€œTempâ€ and â€œMonthâ€ variables are located in the airquality dataset. Without this, R will not know where to find â€œTempâ€ and â€œMonthâ€ and the command will give an error. ,Â The â€œ,â€ is required to start specifying additional commands for the â€œboxplot()â€ function. names=c(â€œMayâ€,â€œJuneâ€,â€œJulyâ€,â€œAugâ€,â€œSepâ€) names= is used to tell R what labels to place on the x-axis below each boxplot. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Color and Labels boxplot(Temp ~ Month, data=airquality This code was explained in the previous example code. ,Â  The comma is used to separate each additional command to a function. xlab=â€œMonth of the Yearâ€ xlab= stands for â€œx label.â€ Use it to specify the text to print on the plot under the x-axis. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. ylab=â€œTemperatureâ€ ylab= stands for â€œy label.â€ Use it to specify the text to print on the plot next to the y-axis. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. main=â€œLa Guardia Airport Daily Temperaturesâ€ main= stands for the â€œmain labelâ€ of the plot, which is placed at the top center of the plot. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. col=â€œwheatâ€ col= stands for the â€œcolorâ€ of the plot. The color name â€œwheatâ€ is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. To make a boxplot in R using the ggplot approach, first ensure library(ggplot2) is loaded. Then, ggplot(data, aes(x=groupsColumn, y=dataColumn) + Â Â geom_boxplot() data is the name of your dataset. groupsColumn is a column of data from your dataset that is qualitative and represents the groups that should each have a boxplot. dataColumn is a column of data from your dataset that is quantitative. The aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your groupsColumn of data, the y-axis become your dataColumn. Note if groupsColumn is not a factor, use factor(groupsColumn) instead. The geometry helper function geom_boxplot() causes the ggplot to become a boxplot. Example Code Basic Single Boxplot ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the y-axis should become. y=Temp â€œy=â€ declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_boxplot() The â€œgeom_boxplot()â€ function causes the ggplot to become a boxplot. There are many other â€œgeom_â€ functions that could be used. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Side-by-side Boxplot and Color Change ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=factor(Month),Â  â€œx=â€ declares which variable will become the x-axis of the graphic. Since Month is â€œnumericâ€ we must use â€œfactor(Month)â€ instead of just â€œMonthâ€. y=Temp â€œy=â€ declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_boxplot( The â€œgeom_boxplot()â€ function causes the ggplot to become a boxplot. There are many other â€œgeom_â€ functions that could be used. fill=â€œskyblueâ€,Â  The â€œfillâ€ command controls the color of the insides of each box in the boxplot. color=â€œblackâ€ The â€œcolorâ€ command controls the color of the edges of each box. )Closing parenthsis for the geom_boxplot function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Labels ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=factor(Month),Â  â€œx=â€ declares which variable will become the x-axis of the graphic. Since Month is â€œnumericâ€ we must use â€œfactor(Month)â€ instead of just â€œMonthâ€. y=Temp â€œy=â€ declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_boxplot( The â€œgeom_histogram()â€ function causes the ggplot to become a histogram. There are many other â€œgeom_â€ functions that could be used. fill=â€œskyblueâ€,Â  The â€œfillâ€ command controls the color of the insides of each box. color=â€œblackâ€ The â€œcolorâ€ command controls the color of the edges of each box. )Closing parenthsis for the geom_boxplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  labs( The â€œlabsâ€ function is used to add labels to the plot, like a main title, x-label and y-label. title=â€œLa Guardia Airport Daily Mean Temperatureâ€,Â  The â€œtitle=â€ command allows you to control the main title at the top of the graphic. x=â€œMonth of the Yearâ€,Â  The â€œx=â€ command allows you to control the x-label of the graphic. y=â€œDaily Mean Temperatureâ€ The â€œy=â€ command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Gallery See what past students have doneâ€¦ Click to view. Hover to see code. ggplot(data = mtcars, aes(x = as.factor(cyl), y = mpg, fill=as.factor(cyl))) + geom_boxplot() + stat_summary(fun.y = mean, geom = \"errorbar\", aes(ymax = ..y.., ymin = ..y..), Â Â  width = .75, linetype = \"dashed\", color=\"firebrick\") + theme_light() + theme(panel.grid.major=element_blank()) + scale_fill_brewer(palette=\"Dark2\") + geom_jitter(width=0.1, height=0) + labs(title = \"Miles Per Gallon Based on Cylinders\", Â Â  x=\"Number of Cylinders\", Â Â  fill=\"Cylinders\", Â Â  y=\"Miles Per Gallon\") ggplot(data = ToothGrowth, aes(x = as.factor(dose), y = len, fill=as.factor(dose))) + geom_boxplot( ) + facet_wrap(~supp) + theme_bw() + scale_fill_manual(values=c(\"#999999\", \"#E69F00\", \"#56B4E9\")) + geom_jitter(width=0.1, height=0) + labs(title = \"Tooth Length Based on Doses Â Â  According to Supplement Type\", Â Â  fill=\"Doses\", Â Â  x=\"Dosage Amount(mg)\", Â Â  y=\"Tooth Length\" ) To make a histogram in plotly first load library(plotly) Then, use the function: plot_ly(dataName, y=~columnNameY, x=~columnNameX, type=\"box\") dataName is the name of a data set columnNameY must be the name of a column of quantitative data. R refers to this as a â€œnumeric vector.â€ This will become the y-axis of the plot. columnNameX must be the name of a column of qualitative data. This will provide the â€œgroupsâ€ forming each individual box in the boxplot. type=\"box\" tells the plot_ly(â€¦) function to create a boxplot. Visit plotly.com/r/box-plots for more details. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic Boxplot plot_ly An R function â€œplot_lyâ€ from library(plotly) used to create any plotly plot. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality,Â  â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. y= The y= allows us to declare which column of the data set will become the y-axis of the boxplot. In other words, the quantitative data we are interested in studying for each group. ~Temp, Â  â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. The ~ is required before column names inside all plot_ly(â€¦) commands. x= The x= allows us to declare which column of the data set will become the x-axis of the boxplot. In other words, the â€œgroupsâ€ forming each separate box in the boxplot. ~as.factor(Month), Â  since â€œMonthâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset we have to change it to a â€œfactorâ€ which forces R to treat it as a qualitative (groups) variable. The ~ is required before column names inside all plot_ly(â€¦) commands. type=â€œboxâ€ This option tells the plot_ly(â€¦) function what â€œtypeâ€ of graph to make. In this case, a boxplot. )Closing parenthsis for the plot_ly function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Change Color plot_ly(airquality, y=~Temp, x=~as.factor(Month), type=â€œboxâ€,Â  This code was explained in the first example code. fillcolor=â€œskyblueâ€,Â  this changes the fill color of the boxes in the boxplot to the color specified, in this case â€œskyblue.â€ line=list(color=â€œdarkgrayâ€, width=3),Â  this â€œlist(â€¦)â€ of options that will be specified will effect the edges of the boxes in the boxplot. We are changing their color to â€œdarkgrayâ€ and their width to 3 pixels wide. marker=list( this â€œlist(â€¦)â€ of options that will be specified will effect the outlying dots shown in the boxplots beyond the â€œfencesâ€ of each box. color = â€œorangeâ€,Â  this will change the color of the dots to orange. line = list(,Â  this opens a list of options to specify for the â€œlinesâ€ around the â€œmarkers.â€ color = â€œredâ€,Â  this will change the color of the lines around the outlier dots to red. width = 1 this will change the width of the lines around the outlier dots to 1 pixel. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Titles plot_ly(airquality, y=~Temp, x=~as.factor(Month), type=â€œboxâ€, fillcolor=â€œskyblueâ€, line=list(color=â€œdarkgrayâ€, width=3), marker = list(color=â€œorangeâ€, line = list(color=â€œredâ€, width=1)))Â  This code was explained in the above example code. %>% the pipe operator sends the completed plot_ly(â€¦) code into the layout function. layout( The layout(â€¦) function is used for specifying details about the axes and their labels. title=â€œLa Guardia Airport Daily Mean Temperaturesâ€ This declares a main title for the top of the graph. xaxis=list( This declares a list of options to be specified for the xaxis. The same can be done for the yaxis(â€¦). title=â€œMonth of the Yearâ€ This declares a title underneath the x-axis. ),Â  Functions always end with a closing parenthesis. yaxis=list( This declares a list of options to be specified for the y-axis. title=â€œTemperature in Degrees Fâ€ This declares a title beside the y-axis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output.",
    "sentences": ["1 Quantitative Variable | 2+ Groups", "Overview Graphical depiction of the five-number summary.", "Great for comparing the distributions of data across several groups or categories.", "Provides a quick visual understanding of the location of the median as well as the range of the data.", "Can be useful in showing outliers.", "Sample size should be larger than at least five, or computing the five-number summary is not very meaningful.", "Side-by-side dotplots are a good alternative for smaller sample sizes.", "R Instructions Base R ggplot2 plotly To make a boxplot in R use the function: boxplot(object) To make side-by-side boxplots: boxplot(object ~ group, data=NameOfYourData, ...) object must be quantitative data.", "R refers to this as a â€œnumeric vector.â€ group must be qualitative data.", "R refers to this as either a â€œcharacter vectorâ€ or a â€œfactor.â€ However, a â€œnumeric vectorâ€ can also act as a qualitative variable."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Boxplots"
  },
  {
    "id": 7,
    "title": "ğŸ“ Overview",
    "url": "GraphicalSummaries.html#overview",
    "content": "Overview Graphical depiction of the five-number summary. Great for comparing the distributions of data across several groups or categories. Provides a quick visual understanding of the location of the median as well as the range of the data. Can be useful in showing outliers. Sample size should be larger than at least five, or computing the five-number summary is not very meaningful. Side-by-side dotplots are a good alternative for smaller sample sizes.",
    "sentences": ["Graphical depiction of the five-number summary.", "Great for comparing the distributions of data across several groups or categories.", "Provides a quick visual understanding of the location of the median as well as the range of the data.", "Can be useful in showing outliers.", "Sample size should be larger than at least five, or computing the five-number summary is not very meaningful.", "Side-by-side dotplots are a good alternative for smaller sample sizes."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 8,
    "title": "ğŸ“ R Instructions",
    "url": "GraphicalSummaries.html#rinstructions",
    "content": "R Instructions Base R ggplot2 plotly To make a boxplot in R use the function: boxplot(object) To make side-by-side boxplots: boxplot(object ~ group, data=NameOfYourData, ...) object must be quantitative data. R refers to this as a â€œnumeric vector.â€ group must be qualitative data. R refers to this as either a â€œcharacter vectorâ€ or a â€œfactor.â€ However, a â€œnumeric vectorâ€ can also act as a qualitative variable. NameOfYourData is the name of the dataset containing object and group. implies there are many other options that can be given to the boxplot() function. Type ?boxplot in your R Console for more details. Example Code Basic Single Boxplot boxplot An R function â€œboxplotâ€ used to create boxplots. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. More Usefulâ€¦ Basic Side-by-Side Boxplot boxplot An R function â€œboxplotâ€ used to create boxplots. ( Parenthesis to begin the function. Must touch the last letter of the function. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. Â ~Â  The ~ is used to tell R that you want one boxplot of the quantitative variable (â€œTempâ€) for each group found in the qualitative variable (â€œMonthâ€). Month â€œMonthâ€ is a qualitative variable (in this case a â€œnumeric vectorâ€ defining months by 5, 6, 7, 8, and 9) from the â€œairqualityâ€ dataset. ,Â The â€œ,â€ is required to start specifying additional commands for the â€œboxplot()â€ function. data=airquality data= is used to tell R that the â€œTempâ€ and â€œMonthâ€ variables are located in the airquality dataset. Without this, R will not know where to find â€œTempâ€ and â€œMonthâ€ and the command will give an error. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Names under each Box boxplot An R function â€œboxplotâ€ used to create boxplots. ( Parenthesis to begin the function. Must touch the last letter of the function. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. Â ~Â  The ~ is used to tell R that you want one boxplot of the quantitative variable (â€œTempâ€) for each group found in the qualitative variable (â€œMonthâ€). Month â€œMonthâ€ is a qualitative variable (in this case a â€œnumeric vectorâ€ defining months by 5, 6, 7, 8, and 9) from the â€œairqualityâ€ dataset. ,Â The â€œ,â€ is required to start specifying additional commands for the â€œboxplot()â€ function. data=airquality data= is used to tell R that the â€œTempâ€ and â€œMonthâ€ variables are located in the airquality dataset. Without this, R will not know where to find â€œTempâ€ and â€œMonthâ€ and the command will give an error. ,Â The â€œ,â€ is required to start specifying additional commands for the â€œboxplot()â€ function. names=c(â€œMayâ€,â€œJuneâ€,â€œJulyâ€,â€œAugâ€,â€œSepâ€) names= is used to tell R what labels to place on the x-axis below each boxplot. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Color and Labels boxplot(Temp ~ Month, data=airquality This code was explained in the previous example code. ,Â  The comma is used to separate each additional command to a function. xlab=â€œMonth of the Yearâ€ xlab= stands for â€œx label.â€ Use it to specify the text to print on the plot under the x-axis. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. ylab=â€œTemperatureâ€ ylab= stands for â€œy label.â€ Use it to specify the text to print on the plot next to the y-axis. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. main=â€œLa Guardia Airport Daily Temperaturesâ€ main= stands for the â€œmain labelâ€ of the plot, which is placed at the top center of the plot. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. col=â€œwheatâ€ col= stands for the â€œcolorâ€ of the plot. The color name â€œwheatâ€ is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. To make a boxplot in R using the ggplot approach, first ensure library(ggplot2) is loaded. Then, ggplot(data, aes(x=groupsColumn, y=dataColumn) + Â Â geom_boxplot() data is the name of your dataset. groupsColumn is a column of data from your dataset that is qualitative and represents the groups that should each have a boxplot. dataColumn is a column of data from your dataset that is quantitative. The aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your groupsColumn of data, the y-axis become your dataColumn. Note if groupsColumn is not a factor, use factor(groupsColumn) instead. The geometry helper function geom_boxplot() causes the ggplot to become a boxplot. Example Code Basic Single Boxplot ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the y-axis should become. y=Temp â€œy=â€ declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_boxplot() The â€œgeom_boxplot()â€ function causes the ggplot to become a boxplot. There are many other â€œgeom_â€ functions that could be used. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Side-by-side Boxplot and Color Change ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=factor(Month),Â  â€œx=â€ declares which variable will become the x-axis of the graphic. Since Month is â€œnumericâ€ we must use â€œfactor(Month)â€ instead of just â€œMonthâ€. y=Temp â€œy=â€ declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_boxplot( The â€œgeom_boxplot()â€ function causes the ggplot to become a boxplot. There are many other â€œgeom_â€ functions that could be used. fill=â€œskyblueâ€,Â  The â€œfillâ€ command controls the color of the insides of each box in the boxplot. color=â€œblackâ€ The â€œcolorâ€ command controls the color of the edges of each box. )Closing parenthsis for the geom_boxplot function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Labels ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=factor(Month),Â  â€œx=â€ declares which variable will become the x-axis of the graphic. Since Month is â€œnumericâ€ we must use â€œfactor(Month)â€ instead of just â€œMonthâ€. y=Temp â€œy=â€ declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_boxplot( The â€œgeom_histogram()â€ function causes the ggplot to become a histogram. There are many other â€œgeom_â€ functions that could be used. fill=â€œskyblueâ€,Â  The â€œfillâ€ command controls the color of the insides of each box. color=â€œblackâ€ The â€œcolorâ€ command controls the color of the edges of each box. )Closing parenthsis for the geom_boxplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  labs( The â€œlabsâ€ function is used to add labels to the plot, like a main title, x-label and y-label. title=â€œLa Guardia Airport Daily Mean Temperatureâ€,Â  The â€œtitle=â€ command allows you to control the main title at the top of the graphic. x=â€œMonth of the Yearâ€,Â  The â€œx=â€ command allows you to control the x-label of the graphic. y=â€œDaily Mean Temperatureâ€ The â€œy=â€ command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Gallery See what past students have doneâ€¦ Click to view. Hover to see code. ggplot(data = mtcars, aes(x = as.factor(cyl), y = mpg, fill=as.factor(cyl))) + geom_boxplot() + stat_summary(fun.y = mean, geom = \"errorbar\", aes(ymax = ..y.., ymin = ..y..), Â Â  width = .75, linetype = \"dashed\", color=\"firebrick\") + theme_light() + theme(panel.grid.major=element_blank()) + scale_fill_brewer(palette=\"Dark2\") + geom_jitter(width=0.1, height=0) + labs(title = \"Miles Per Gallon Based on Cylinders\", Â Â  x=\"Number of Cylinders\", Â Â  fill=\"Cylinders\", Â Â  y=\"Miles Per Gallon\") ggplot(data = ToothGrowth, aes(x = as.factor(dose), y = len, fill=as.factor(dose))) + geom_boxplot( ) + facet_wrap(~supp) + theme_bw() + scale_fill_manual(values=c(\"#999999\", \"#E69F00\", \"#56B4E9\")) + geom_jitter(width=0.1, height=0) + labs(title = \"Tooth Length Based on Doses Â Â  According to Supplement Type\", Â Â  fill=\"Doses\", Â Â  x=\"Dosage Amount(mg)\", Â Â  y=\"Tooth Length\" ) To make a histogram in plotly first load library(plotly) Then, use the function: plot_ly(dataName, y=~columnNameY, x=~columnNameX, type=\"box\") dataName is the name of a data set columnNameY must be the name of a column of quantitative data. R refers to this as a â€œnumeric vector.â€ This will become the y-axis of the plot. columnNameX must be the name of a column of qualitative data. This will provide the â€œgroupsâ€ forming each individual box in the boxplot. type=\"box\" tells the plot_ly(â€¦) function to create a boxplot. Visit plotly.com/r/box-plots for more details. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic Boxplot plot_ly An R function â€œplot_lyâ€ from library(plotly) used to create any plotly plot. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality,Â  â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. y= The y= allows us to declare which column of the data set will become the y-axis of the boxplot. In other words, the quantitative data we are interested in studying for each group. ~Temp, Â  â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. The ~ is required before column names inside all plot_ly(â€¦) commands. x= The x= allows us to declare which column of the data set will become the x-axis of the boxplot. In other words, the â€œgroupsâ€ forming each separate box in the boxplot. ~as.factor(Month), Â  since â€œMonthâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset we have to change it to a â€œfactorâ€ which forces R to treat it as a qualitative (groups) variable. The ~ is required before column names inside all plot_ly(â€¦) commands. type=â€œboxâ€ This option tells the plot_ly(â€¦) function what â€œtypeâ€ of graph to make. In this case, a boxplot. )Closing parenthsis for the plot_ly function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Change Color plot_ly(airquality, y=~Temp, x=~as.factor(Month), type=â€œboxâ€,Â  This code was explained in the first example code. fillcolor=â€œskyblueâ€,Â  this changes the fill color of the boxes in the boxplot to the color specified, in this case â€œskyblue.â€ line=list(color=â€œdarkgrayâ€, width=3),Â  this â€œlist(â€¦)â€ of options that will be specified will effect the edges of the boxes in the boxplot. We are changing their color to â€œdarkgrayâ€ and their width to 3 pixels wide. marker=list( this â€œlist(â€¦)â€ of options that will be specified will effect the outlying dots shown in the boxplots beyond the â€œfencesâ€ of each box. color = â€œorangeâ€,Â  this will change the color of the dots to orange. line = list(,Â  this opens a list of options to specify for the â€œlinesâ€ around the â€œmarkers.â€ color = â€œredâ€,Â  this will change the color of the lines around the outlier dots to red. width = 1 this will change the width of the lines around the outlier dots to 1 pixel. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Add Titles plot_ly(airquality, y=~Temp, x=~as.factor(Month), type=â€œboxâ€, fillcolor=â€œskyblueâ€, line=list(color=â€œdarkgrayâ€, width=3), marker = list(color=â€œorangeâ€, line = list(color=â€œredâ€, width=1)))Â  This code was explained in the above example code. %>% the pipe operator sends the completed plot_ly(â€¦) code into the layout function. layout( The layout(â€¦) function is used for specifying details about the axes and their labels. title=â€œLa Guardia Airport Daily Mean Temperaturesâ€ This declares a main title for the top of the graph. xaxis=list( This declares a list of options to be specified for the xaxis. The same can be done for the yaxis(â€¦). title=â€œMonth of the Yearâ€ This declares a title underneath the x-axis. ),Â  Functions always end with a closing parenthesis. yaxis=list( This declares a list of options to be specified for the y-axis. title=â€œTemperature in Degrees Fâ€ This declares a title beside the y-axis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output.",
    "sentences": ["Base R ggplot2 plotly To make a boxplot in R use the function: boxplot(object) To make side-by-side boxplots: boxplot(object ~ group, data=NameOfYourData, ...) object must be quantitative data.", "R refers to this as a â€œnumeric vector.â€ group must be qualitative data.", "R refers to this as either a â€œcharacter vectorâ€ or a â€œfactor.â€ However, a â€œnumeric vectorâ€ can also act as a qualitative variable.", "NameOfYourData is the name of the dataset containing object and group.", "implies there are many other options that can be given to the boxplot() function.", "Type ?boxplot in your R Console for more details.", "Example Code Basic Single Boxplot boxplot An R function â€œboxplotâ€ used to create boxplots.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 9,
    "title": "ğŸ“ Explanation",
    "url": "GraphicalSummaries.html#explanation",
    "content": "Explanation Understanding how a boxplot is created is the best way to understand what the boxplot shows. How Boxplots are Made The five-number summary is computed. A box is drawn with one edge located at the first quartile and the opposite edge located at the third quartile. This box is then divided into two boxes by placing another line inside the box at the location of the median. The maximum value and minimum value are marked on the plot. Whiskers are drawn from the first quartile out towards the minimum and from the third quartile out towards the maximum. If the minimum or maximum is too far away, then the whisker is ended early. Any points beyond the line ending the whisker are marked on the plot as dots. This helps identify possible outliers in the data.",
    "sentences": ["Understanding how a boxplot is created is the best way to understand what the boxplot shows.", "How Boxplots are Made The five-number summary is computed.", "A box is drawn with one edge located at the first quartile and the opposite edge located at the third quartile.", "This box is then divided into two boxes by placing another line inside the box at the location of the median.", "The maximum value and minimum value are marked on the plot.", "Whiskers are drawn from the first quartile out towards the minimum and from the third quartile out towards the maximum.", "If the minimum or maximum is too far away, then the whisker is ended early.", "Any points beyond the line ending the whisker are marked on the plot as dots.", "This helps identify possible outliers in the data."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Explanation"
  },
  {
    "id": 10,
    "title": "ğŸ“ Dot Plots",
    "url": "GraphicalSummaries.html#dotplots",
    "content": "Dot Plots 1 Quantitative Variable | 2+ Groups Overview Depicts the actual values of each data point. Best for small sample sizes or for datasets where there are lots of repeated values. Histograms or boxplots are better alternatives for large sample sizes when there are few repeated values. Great for comparing the distribution of data across several groups or categories. R Instructions Base R ggplot2 plotly To make a dot plot in Base R use the code: stripchart(object) For side-by-side dotplots: stripchart(object ~ group, data=NameOfYourData) object must be a quantitative (or ordinal) variable, what R refers to as a â€œnumeric vector.â€ group is a qualitative variable, which in R can be either a â€œcharacter vectorâ€ or a â€œfactor.â€ NameOfYourData is the name of the dataset containing object and group. Example Code stripchart An R function â€œstripchartâ€ used to create a dot plot. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. ,Â The â€œ,â€ is required to start specifying additional commands for the function. method=â€œstackâ€method= allows us to choose from the options â€œoverplotâ€, â€œjitterâ€, and â€œstackâ€. The â€œstackâ€ option stacks mutliple points that occur at the same location on top of each other. You can try the code yourself to see what â€œoverplotâ€ and â€œjitterâ€ do. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. stripchart An R function â€œstripchartâ€ used to create dot plots. ( Parenthesis to begin the function. Must touch the last letter of the function. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. Â ~Â  The ~ is used to tell R that you want a dot plot of the quantitative variable (â€œTempâ€) for each group found in the qualitative variable (â€œMonthâ€). Month â€œMonthâ€ is a qualitative variable (in this case a â€œnumeric vectorâ€ defining months by 5, 6, 7, 8, and 9) from the â€œairqualityâ€ dataset. ,Â The â€œ,â€ is required to start specifying additional commands for the function. data=airquality data= is used to tell R that the â€œTempâ€ and â€œMonthâ€ variables are located in the airquality dataset. Without this, R will not know where to find â€œTempâ€ and â€œMonthâ€ and the command will give an error. ,Â The â€œ,â€ is required to start specifying additional commands for the function. method=â€œstackâ€method= allows us to choose from the options â€œoverplotâ€, â€œjitterâ€, and â€œstackâ€. The â€œstackâ€ option stacks mutliple points that occur at the same location on top of each other. You can try the code yourself to see what â€œoverplotâ€ and â€œjitterâ€ do. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. stripchart(Temp ~ Month This part of the code was explained already in the example code directly above this one. ,Â The â€œ,â€ is required to start specifying additional commands for the function. data=airquality data= is used to tell R that the â€œTempâ€ and â€œMonthâ€ variables are located in the airquality dataset. Without this, R will not know where to find â€œTempâ€ and â€œMonthâ€ and the command will give an error. ,Â The â€œ,â€ is required to start specifying additional commands for the function. method=â€œstackâ€method= allows us to choose from the options â€œoverplotâ€, â€œjitterâ€, and â€œstackâ€. The â€œstackâ€ option stacks mutliple points that occur at the same location on top of each other. You can try the code yourself to see what â€œoverplotâ€ and â€œjitterâ€ do. ,Â  The comma is used to separate each additional command to a function. ylab=â€œMonth of the Yearâ€ ylab= stands for â€œy label.â€ Use it to specify the text to print on the plot next to the y-axis. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. xlab=â€œTemperatureâ€ xlab= stands for â€œx label.â€ Use it to specify the text to print on the plot below the x-axis. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. main=â€œLa Guardia Airport Daily Temperaturesâ€ main= stands for the â€œmain labelâ€ of the plot, which is placed at the top center of the plot. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. col=â€œsiennaâ€ col= stands for the â€œcolorâ€ of the plot. The color name â€œsiennaâ€ is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. ,Â  The comma is used to separate each additional command to a function. pch=16 pch= stands for the â€œplotting characterâ€ of the plot. This plot uses the filled circle (option 16) as the plotting character. The options are 0, 1, 2, â€¦, 25. Type ?pch in the R Console, and scroll down the help file half way to see what each option does. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. To make a dot plot in R using the ggplot approach, first ensure: library(ggplot2) is loaded. Then, ggplot(data, aes(x=groupsColumn, y=dataColumn) + Â Â geom_dotplot() data is the name of your dataset. groupsColumn is a column of data from your dataset that is qualitative and represents the groups that should each have a boxplot. dataColumn is a column of data from your dataset that is quantitative. The aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your groupsColumn of data, the y-axis become your dataColumn. Note if groupsColumn is not a factor, use factor(groupsColumn) instead. The geometry helper function geom_dotplot() causes the ggplot to become a dot plot. Example Code Click to view. Hover to learn. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the y-axis should become. x=Temp â€œx=â€ declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_dotplot() The â€œgeom_dotplot()â€ function causes the ggplot to become a dot plot. There are many other â€œgeom_â€ functions that could be used. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=factor(Month),Â  â€œx=â€ declares which variable will become the x-axis of the graphic. Use factor(Month) to change â€œMonthâ€, which is numeric, into categories. y=Temp â€œy=â€ declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_dotplot( The â€œgeom_dotplot()â€ function causes the ggplot to become a dot plot. There are many other â€œgeom_â€ functions that could be used. binaxis = â€œyâ€,Â  This tells the function that the y=Temp statement should be used as the quantitative data. stackdir = â€œupâ€,Â  This causes the dots to be stacked on top of each other. position = â€œdodgeâ€,Â  This causes the dots to not overalap, i.e., â€œdodge each other.â€ dotsize = 0.75,Â  Controls the size of the dots. You can make them larger with numbers greater than 1 and smaller with numbers less than 1. binwidth = 0.5 Controls how the dots are grouped, similar to the bins in a histogram. )Closing parenthsis for the geom_dotplot function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=factor(Month),Â  â€œx=â€ declares which variable will become the x-axis of the graphic. Use factor(Month) to change â€œMonthâ€, which is numeric, into categories. y=Temp â€œy=â€ declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  coord_flip( ) The â€œcoord_flip()â€ function causes the ggplot to reverse the axes when drawing the plot. However, all commands must be given as if the plot were to be drawn without coord_flip(), then coord_flip() is applied. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_dotplot( The â€œgeom_dotplot()â€ function causes the ggplot to become a dot plot. There are many other â€œgeom_â€ functions that could be used. binaxis = â€œyâ€,Â  This tells the function that the y=Temp statement should be used as the quantitative data. stackdir = â€œupâ€,Â  This causes the dots to be stacked on top of each other. position = â€œdodgeâ€,Â  This causes the dots to not overalap, i.e., â€œdodge each other.â€ dotsize = 0.75,Â  Controls the size of the dots. You can make them larger with numbers greater than 1 and smaller with numbers less than 1. binwidth = 0.5 Controls how the dots are grouped, similar to the bins in a histogram. )Closing parenthsis for the geom_dotplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  labs( The â€œlabsâ€ function is used to add labels to the plot, like a main title, x-label and y-label. title=â€œLa Guardia Airport Daily Mean Temperatureâ€,Â  The â€œtitle=â€ command allows you to control the main title at the top of the graphic. x=â€œMonth of the Yearâ€,Â  The â€œx=â€ command allows you to control the x-label of the graphic. y=â€œDaily Mean Temperatureâ€ The â€œy=â€ command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Not yet available.",
    "sentences": ["1 Quantitative Variable | 2+ Groups", "Overview Depicts the actual values of each data point.", "Best for small sample sizes or for datasets where there are lots of repeated values.", "Histograms or boxplots are better alternatives for large sample sizes when there are few repeated values.", "Great for comparing the distribution of data across several groups or categories.", "R Instructions Base R ggplot2 plotly To make a dot plot in Base R use the code: stripchart(object) For side-by-side dotplots: stripchart(object ~ group, data=NameOfYourData) object must be a quantitative (or ordinal) variable, what R refers to as a â€œnumeric vector.â€ group is a qualitative variable, which in R can be either a â€œcharacter vectorâ€ or a â€œfactor.â€ NameOfYourData is the name of the dataset containing object and group.", "Example Code stripchart An R function â€œstripchartâ€ used to create a dot plot.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Dot Plots"
  },
  {
    "id": 11,
    "title": "ğŸ“ Overview",
    "url": "GraphicalSummaries.html#overview",
    "content": "Overview Depicts the actual values of each data point. Best for small sample sizes or for datasets where there are lots of repeated values. Histograms or boxplots are better alternatives for large sample sizes when there are few repeated values. Great for comparing the distribution of data across several groups or categories.",
    "sentences": ["Depicts the actual values of each data point.", "Best for small sample sizes or for datasets where there are lots of repeated values.", "Histograms or boxplots are better alternatives for large sample sizes when there are few repeated values.", "Great for comparing the distribution of data across several groups or categories."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 12,
    "title": "ğŸ“ R Instructions",
    "url": "GraphicalSummaries.html#rinstructions",
    "content": "R Instructions Base R ggplot2 plotly To make a dot plot in Base R use the code: stripchart(object) For side-by-side dotplots: stripchart(object ~ group, data=NameOfYourData) object must be a quantitative (or ordinal) variable, what R refers to as a â€œnumeric vector.â€ group is a qualitative variable, which in R can be either a â€œcharacter vectorâ€ or a â€œfactor.â€ NameOfYourData is the name of the dataset containing object and group. Example Code stripchart An R function â€œstripchartâ€ used to create a dot plot. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. ,Â The â€œ,â€ is required to start specifying additional commands for the function. method=â€œstackâ€method= allows us to choose from the options â€œoverplotâ€, â€œjitterâ€, and â€œstackâ€. The â€œstackâ€ option stacks mutliple points that occur at the same location on top of each other. You can try the code yourself to see what â€œoverplotâ€ and â€œjitterâ€ do. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. stripchart An R function â€œstripchartâ€ used to create dot plots. ( Parenthesis to begin the function. Must touch the last letter of the function. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. Â ~Â  The ~ is used to tell R that you want a dot plot of the quantitative variable (â€œTempâ€) for each group found in the qualitative variable (â€œMonthâ€). Month â€œMonthâ€ is a qualitative variable (in this case a â€œnumeric vectorâ€ defining months by 5, 6, 7, 8, and 9) from the â€œairqualityâ€ dataset. ,Â The â€œ,â€ is required to start specifying additional commands for the function. data=airquality data= is used to tell R that the â€œTempâ€ and â€œMonthâ€ variables are located in the airquality dataset. Without this, R will not know where to find â€œTempâ€ and â€œMonthâ€ and the command will give an error. ,Â The â€œ,â€ is required to start specifying additional commands for the function. method=â€œstackâ€method= allows us to choose from the options â€œoverplotâ€, â€œjitterâ€, and â€œstackâ€. The â€œstackâ€ option stacks mutliple points that occur at the same location on top of each other. You can try the code yourself to see what â€œoverplotâ€ and â€œjitterâ€ do. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. stripchart(Temp ~ Month This part of the code was explained already in the example code directly above this one. ,Â The â€œ,â€ is required to start specifying additional commands for the function. data=airquality data= is used to tell R that the â€œTempâ€ and â€œMonthâ€ variables are located in the airquality dataset. Without this, R will not know where to find â€œTempâ€ and â€œMonthâ€ and the command will give an error. ,Â The â€œ,â€ is required to start specifying additional commands for the function. method=â€œstackâ€method= allows us to choose from the options â€œoverplotâ€, â€œjitterâ€, and â€œstackâ€. The â€œstackâ€ option stacks mutliple points that occur at the same location on top of each other. You can try the code yourself to see what â€œoverplotâ€ and â€œjitterâ€ do. ,Â  The comma is used to separate each additional command to a function. ylab=â€œMonth of the Yearâ€ ylab= stands for â€œy label.â€ Use it to specify the text to print on the plot next to the y-axis. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. xlab=â€œTemperatureâ€ xlab= stands for â€œx label.â€ Use it to specify the text to print on the plot below the x-axis. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. main=â€œLa Guardia Airport Daily Temperaturesâ€ main= stands for the â€œmain labelâ€ of the plot, which is placed at the top center of the plot. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. col=â€œsiennaâ€ col= stands for the â€œcolorâ€ of the plot. The color name â€œsiennaâ€ is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. ,Â  The comma is used to separate each additional command to a function. pch=16 pch= stands for the â€œplotting characterâ€ of the plot. This plot uses the filled circle (option 16) as the plotting character. The options are 0, 1, 2, â€¦, 25. Type ?pch in the R Console, and scroll down the help file half way to see what each option does. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. To make a dot plot in R using the ggplot approach, first ensure: library(ggplot2) is loaded. Then, ggplot(data, aes(x=groupsColumn, y=dataColumn) + Â Â geom_dotplot() data is the name of your dataset. groupsColumn is a column of data from your dataset that is qualitative and represents the groups that should each have a boxplot. dataColumn is a column of data from your dataset that is quantitative. The aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your groupsColumn of data, the y-axis become your dataColumn. Note if groupsColumn is not a factor, use factor(groupsColumn) instead. The geometry helper function geom_dotplot() causes the ggplot to become a dot plot. Example Code Click to view. Hover to learn. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the y-axis should become. x=Temp â€œx=â€ declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_dotplot() The â€œgeom_dotplot()â€ function causes the ggplot to become a dot plot. There are many other â€œgeom_â€ functions that could be used. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=factor(Month),Â  â€œx=â€ declares which variable will become the x-axis of the graphic. Use factor(Month) to change â€œMonthâ€, which is numeric, into categories. y=Temp â€œy=â€ declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_dotplot( The â€œgeom_dotplot()â€ function causes the ggplot to become a dot plot. There are many other â€œgeom_â€ functions that could be used. binaxis = â€œyâ€,Â  This tells the function that the y=Temp statement should be used as the quantitative data. stackdir = â€œupâ€,Â  This causes the dots to be stacked on top of each other. position = â€œdodgeâ€,Â  This causes the dots to not overalap, i.e., â€œdodge each other.â€ dotsize = 0.75,Â  Controls the size of the dots. You can make them larger with numbers greater than 1 and smaller with numbers less than 1. binwidth = 0.5 Controls how the dots are grouped, similar to the bins in a histogram. )Closing parenthsis for the geom_dotplot function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=factor(Month),Â  â€œx=â€ declares which variable will become the x-axis of the graphic. Use factor(Month) to change â€œMonthâ€, which is numeric, into categories. y=Temp â€œy=â€ declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  coord_flip( ) The â€œcoord_flip()â€ function causes the ggplot to reverse the axes when drawing the plot. However, all commands must be given as if the plot were to be drawn without coord_flip(), then coord_flip() is applied. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_dotplot( The â€œgeom_dotplot()â€ function causes the ggplot to become a dot plot. There are many other â€œgeom_â€ functions that could be used. binaxis = â€œyâ€,Â  This tells the function that the y=Temp statement should be used as the quantitative data. stackdir = â€œupâ€,Â  This causes the dots to be stacked on top of each other. position = â€œdodgeâ€,Â  This causes the dots to not overalap, i.e., â€œdodge each other.â€ dotsize = 0.75,Â  Controls the size of the dots. You can make them larger with numbers greater than 1 and smaller with numbers less than 1. binwidth = 0.5 Controls how the dots are grouped, similar to the bins in a histogram. )Closing parenthsis for the geom_dotplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  labs( The â€œlabsâ€ function is used to add labels to the plot, like a main title, x-label and y-label. title=â€œLa Guardia Airport Daily Mean Temperatureâ€,Â  The â€œtitle=â€ command allows you to control the main title at the top of the graphic. x=â€œMonth of the Yearâ€,Â  The â€œx=â€ command allows you to control the x-label of the graphic. y=â€œDaily Mean Temperatureâ€ The â€œy=â€ command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Not yet available.",
    "sentences": ["Base R ggplot2 plotly To make a dot plot in Base R use the code: stripchart(object) For side-by-side dotplots: stripchart(object ~ group, data=NameOfYourData) object must be a quantitative (or ordinal) variable, what R refers to as a â€œnumeric vector.â€ group is a qualitative variable, which in R can be either a â€œcharacter vectorâ€ or a â€œfactor.â€ NameOfYourData is the name of the dataset containing object and group.", "Example Code stripchart An R function â€œstripchartâ€ used to create a dot plot.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset.", "Type â€œView(airquality)â€ in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset.", ",Â The â€œ,â€ is required to start specifying additional commands for the function.", "method=â€œstackâ€method= allows us to choose from the options â€œoverplotâ€, â€œjitterâ€, and â€œstackâ€."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 13,
    "title": "ğŸ“ Scatterplots",
    "url": "GraphicalSummaries.html#scatterplots",
    "content": "Scatterplots 2 Quantitative Variables Overview Depicts the actual values of the data points, which are \\((x,y)\\) pairs. Works well for small or large sample sizes. Visualizes well the correlation between the two variables. Should be used in linear regression contexts whenever possible. R Instructions Base R ggplot2 plotly To make a scatterplot in R use the code: plot(y ~ x, data=NameOfYourData) y is the quantitative response variable, i.e., â€œnumeric vector.â€ x is the quantitative explanatory variable, i.e., â€œnumeric vector.â€ NameOfYourData is the name of the dataset containing y and x. Note: plot(object) where object is a â€œnumeric vectorâ€ will create a time series plot, which is sometimes useful. Example Code plot An R function â€œplotâ€ used to create a scatterplot, or in this case a time series plot because only one quantitative variable is being supplied to the function. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. ,Â The â€œ,â€ is required to start specifying additional commands for the function. type=â€œlâ€type= allows us to choose from the options â€œpâ€ for points, â€œlâ€ for lines, and â€œbâ€ for both. There are also other options that could be chosen, typeÂ  ?plot in the R Console to learn about them. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. plot An R function â€œplotâ€ used to create a scatterplot. ( Parenthesis to begin the function. Must touch the last letter of the function. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset that is being used as the response variable (y-axis) for this plot. Â ~Â  The ~ is used to tell R that you want a scatterplot with the quantitative variable â€œTempâ€ on the y-axis and the qauntitative variable â€œMonthâ€ on the x-axis. Wind â€œWindâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset that is being used as the explanatory variable (x-axis) for this plot. ,Â The â€œ,â€ is required to start specifying additional commands for the function. data=airquality data= is used to tell R that the â€œTempâ€ and â€œMonthâ€ variables are located in the airquality dataset. Without this, R will not know where to find â€œTempâ€ and â€œMonthâ€ and the command will give an error. ,Â The â€œ,â€ is required to start specifying additional commands for the function. pch=8 pch= stands for the â€œplotting characterâ€ of the plot. This plot uses the star shape (option 8) as the plotting character. The options are 0, 1, 2, â€¦, 25. Type ?pch in the R Console, and scroll down the help file half way to see what each option does. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. plot(Temp ~ Wind This part of the code was explained already in the example code directly above this one. ,Â The â€œ,â€ is required to start specifying additional commands for the function. data=airquality data= is used to tell R that the â€œTempâ€ and â€œMonthâ€ variables are located in the airquality dataset. Without this, R will not know where to find â€œTempâ€ and â€œMonthâ€ and the command will give an error. ,Â The â€œ,â€ is required to start specifying additional commands for the function. xlab=â€œDaily Wind Speed (mph)â€ xlab= stands for â€œx label.â€ Use it to specify the text to print on the plot below the x-axis. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. ylab=â€œTemperatureâ€ ylab= stands for â€œy label.â€ Use it to specify the text to print on the plot next to the y-axis. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. main=â€œLa Guardia Airport (May - Sep)â€ main= stands for the â€œmain labelâ€ of the plot, which is placed at the top center of the plot. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. col=â€œivory3â€ col= stands for the â€œcolorâ€ of the plot. The color name â€œivory3â€ is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. ,Â  The comma is used to separate each additional command to a function. pch=18 pch= stands for the â€œplotting characterâ€ of the plot. This plot uses the filled diamond (option 18) as the plotting character. The options are 0, 1, 2, â€¦, 25. Type ?pch in the R Console, and scroll down the help file half way to see what each option does. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. pch Options To make a scatterplot in R using the ggplot approach, first ensure: library(ggplot2) is loaded. Then, ggplot(data, aes(x=dataColumn1, y=dataColumn2) + Â Â geom_point() data is the name of your dataset. dataColumn1 is a column of data from your dataset that is quantitative and will be used as the explanatory variable. dataColumn2 is a column of data from your dataset that is quantitative and will be used as the response variable. The aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your dataColumn1 of data, the y-axis become your dataColumn2. The geometry helper function geom_point() causes the ggplot to become a scatterplot. Example Code Click to view. Hover to learn. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Wind,Â  â€œx=â€ declares which variable will become the x-axis of the graphic, the explanatory variable. y=Temp â€œy=â€ declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_point( The â€œgeom_point()â€ function causes the ggplot to become a scatterplot. There are many other â€œgeom_â€ functions that could be used. )Closing parenthsis for the geom_point function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Wind,Â  â€œx=â€ declares which variable will become the x-axis of the graphic, the explanatory variable. y=Temp â€œy=â€ declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_point( The â€œgeom_point()â€ function causes the ggplot to become a scatterplot. There are many other â€œgeom_â€ functions that could be used. color = â€œivory3â€,Â  Controls the color of the dots. pch = 18 Controls the type of plotting character to be used in the plot. )Closing parenthsis for the geom_point function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  labs( The â€œlabsâ€ function is used to add labels to the plot, like a main title, x-label and y-label. title=â€œLa Guardia Airport (May - Sep)â€,Â  The â€œtitle=â€ command allows you to control the main title at the top of the graphic. x=â€œDaily Average Wind Speed (mph)â€,Â  The â€œx=â€ command allows you to control the x-label of the graphic. y=â€œDaily Mean Temperatureâ€ The â€œy=â€ command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â theme_bw()Changes the â€œthemeâ€ or look of the plot to â€œblackâ€ and â€œwhiteâ€. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. To make a scatterplot in R using the plotly approach, first ensure: library(plotly) is loaded. Then, plot_ly(data, x= ~dataColumn1, y= ~dataColumn2) data is the name of your dataset. dataColumn1 is a column of data from your dataset that is quantitative and will be used as the explanatory variable. dataColumn2 is a column of data from your dataset that is quantitative and will be used as the response variable. Example Code plot_ly(airquality, x= ~Wind, y= ~Temp) plot_ly(KidsFeet, x= ~length, y= ~width, color= ~sex, size= ~birthmonth, text= ~paste(\"Name:\", name, \"\\n\", \"Birth-Month:\", birthmonth), colors=c(\"skyblue\",\"hotpink\")) %>% layout(title=\"KidsFeet dataset\", xaxis=list(title=\"Length of the longer foot in cm\"), yaxis=list(title=\"Width of the longer foot in cm\"))",
    "sentences": ["2 Quantitative Variables", "Overview Depicts the actual values of the data points, which are \\((x,y)\\) pairs.", "Works well for small or large sample sizes.", "Visualizes well the correlation between the two variables.", "Should be used in linear regression contexts whenever possible.", "R Instructions Base R ggplot2 plotly To make a scatterplot in R use the code: plot(y ~ x, data=NameOfYourData) y is the quantitative response variable, i.e., â€œnumeric vector.â€ x is the quantitative explanatory variable, i.e., â€œnumeric vector.â€ NameOfYourData is the name of the dataset containing y and x.", "Note: plot(object) where object is a â€œnumeric vectorâ€ will create a time series plot, which is sometimes useful.", "Example Code plot An R function â€œplotâ€ used to create a scatterplot, or in this case a time series plot because only one quantitative variable is being supplied to the function.", "( Parenthesis to begin the function.", "Must touch the last letter of the function."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Scatterplots"
  },
  {
    "id": 14,
    "title": "ğŸ“ Overview",
    "url": "GraphicalSummaries.html#overview",
    "content": "Overview Depicts the actual values of the data points, which are \\((x,y)\\) pairs. Works well for small or large sample sizes. Visualizes well the correlation between the two variables. Should be used in linear regression contexts whenever possible.",
    "sentences": ["Depicts the actual values of the data points, which are \\((x,y)\\) pairs.", "Works well for small or large sample sizes.", "Visualizes well the correlation between the two variables.", "Should be used in linear regression contexts whenever possible."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 15,
    "title": "ğŸ“ R Instructions",
    "url": "GraphicalSummaries.html#rinstructions",
    "content": "R Instructions Base R ggplot2 plotly To make a scatterplot in R use the code: plot(y ~ x, data=NameOfYourData) y is the quantitative response variable, i.e., â€œnumeric vector.â€ x is the quantitative explanatory variable, i.e., â€œnumeric vector.â€ NameOfYourData is the name of the dataset containing y and x. Note: plot(object) where object is a â€œnumeric vectorâ€ will create a time series plot, which is sometimes useful. Example Code plot An R function â€œplotâ€ used to create a scatterplot, or in this case a time series plot because only one quantitative variable is being supplied to the function. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. ,Â The â€œ,â€ is required to start specifying additional commands for the function. type=â€œlâ€type= allows us to choose from the options â€œpâ€ for points, â€œlâ€ for lines, and â€œbâ€ for both. There are also other options that could be chosen, typeÂ  ?plot in the R Console to learn about them. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. plot An R function â€œplotâ€ used to create a scatterplot. ( Parenthesis to begin the function. Must touch the last letter of the function. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset that is being used as the response variable (y-axis) for this plot. Â ~Â  The ~ is used to tell R that you want a scatterplot with the quantitative variable â€œTempâ€ on the y-axis and the qauntitative variable â€œMonthâ€ on the x-axis. Wind â€œWindâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset that is being used as the explanatory variable (x-axis) for this plot. ,Â The â€œ,â€ is required to start specifying additional commands for the function. data=airquality data= is used to tell R that the â€œTempâ€ and â€œMonthâ€ variables are located in the airquality dataset. Without this, R will not know where to find â€œTempâ€ and â€œMonthâ€ and the command will give an error. ,Â The â€œ,â€ is required to start specifying additional commands for the function. pch=8 pch= stands for the â€œplotting characterâ€ of the plot. This plot uses the star shape (option 8) as the plotting character. The options are 0, 1, 2, â€¦, 25. Type ?pch in the R Console, and scroll down the help file half way to see what each option does. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. plot(Temp ~ Wind This part of the code was explained already in the example code directly above this one. ,Â The â€œ,â€ is required to start specifying additional commands for the function. data=airquality data= is used to tell R that the â€œTempâ€ and â€œMonthâ€ variables are located in the airquality dataset. Without this, R will not know where to find â€œTempâ€ and â€œMonthâ€ and the command will give an error. ,Â The â€œ,â€ is required to start specifying additional commands for the function. xlab=â€œDaily Wind Speed (mph)â€ xlab= stands for â€œx label.â€ Use it to specify the text to print on the plot below the x-axis. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. ylab=â€œTemperatureâ€ ylab= stands for â€œy label.â€ Use it to specify the text to print on the plot next to the y-axis. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. main=â€œLa Guardia Airport (May - Sep)â€ main= stands for the â€œmain labelâ€ of the plot, which is placed at the top center of the plot. The desired text must always be contained in quotes. ,Â  The comma is used to separate each additional command to a function. col=â€œivory3â€ col= stands for the â€œcolorâ€ of the plot. The color name â€œivory3â€ is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. ,Â  The comma is used to separate each additional command to a function. pch=18 pch= stands for the â€œplotting characterâ€ of the plot. This plot uses the filled diamond (option 18) as the plotting character. The options are 0, 1, 2, â€¦, 25. Type ?pch in the R Console, and scroll down the help file half way to see what each option does. ) Functions always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. pch Options To make a scatterplot in R using the ggplot approach, first ensure: library(ggplot2) is loaded. Then, ggplot(data, aes(x=dataColumn1, y=dataColumn2) + Â Â geom_point() data is the name of your dataset. dataColumn1 is a column of data from your dataset that is quantitative and will be used as the explanatory variable. dataColumn2 is a column of data from your dataset that is quantitative and will be used as the response variable. The aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your dataColumn1 of data, the y-axis become your dataColumn2. The geometry helper function geom_point() causes the ggplot to become a scatterplot. Example Code Click to view. Hover to learn. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Wind,Â  â€œx=â€ declares which variable will become the x-axis of the graphic, the explanatory variable. y=Temp â€œy=â€ declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_point( The â€œgeom_point()â€ function causes the ggplot to become a scatterplot. There are many other â€œgeom_â€ functions that could be used. )Closing parenthsis for the geom_point function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Wind,Â  â€œx=â€ declares which variable will become the x-axis of the graphic, the explanatory variable. y=Temp â€œy=â€ declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_point( The â€œgeom_point()â€ function causes the ggplot to become a scatterplot. There are many other â€œgeom_â€ functions that could be used. color = â€œivory3â€,Â  Controls the color of the dots. pch = 18 Controls the type of plotting character to be used in the plot. )Closing parenthsis for the geom_point function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  labs( The â€œlabsâ€ function is used to add labels to the plot, like a main title, x-label and y-label. title=â€œLa Guardia Airport (May - Sep)â€,Â  The â€œtitle=â€ command allows you to control the main title at the top of the graphic. x=â€œDaily Average Wind Speed (mph)â€,Â  The â€œx=â€ command allows you to control the x-label of the graphic. y=â€œDaily Mean Temperatureâ€ The â€œy=â€ command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â theme_bw()Changes the â€œthemeâ€ or look of the plot to â€œblackâ€ and â€œwhiteâ€. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. To make a scatterplot in R using the plotly approach, first ensure: library(plotly) is loaded. Then, plot_ly(data, x= ~dataColumn1, y= ~dataColumn2) data is the name of your dataset. dataColumn1 is a column of data from your dataset that is quantitative and will be used as the explanatory variable. dataColumn2 is a column of data from your dataset that is quantitative and will be used as the response variable. Example Code plot_ly(airquality, x= ~Wind, y= ~Temp) plot_ly(KidsFeet, x= ~length, y= ~width, color= ~sex, size= ~birthmonth, text= ~paste(\"Name:\", name, \"\\n\", \"Birth-Month:\", birthmonth), colors=c(\"skyblue\",\"hotpink\")) %>% layout(title=\"KidsFeet dataset\", xaxis=list(title=\"Length of the longer foot in cm\"), yaxis=list(title=\"Width of the longer foot in cm\"))",
    "sentences": ["Base R ggplot2 plotly To make a scatterplot in R use the code: plot(y ~ x, data=NameOfYourData) y is the quantitative response variable, i.e., â€œnumeric vector.â€ x is the quantitative explanatory variable, i.e., â€œnumeric vector.â€ NameOfYourData is the name of the dataset containing y and x.", "Note: plot(object) where object is a â€œnumeric vectorâ€ will create a time series plot, which is sometimes useful.", "Example Code plot An R function â€œplotâ€ used to create a scatterplot, or in this case a time series plot because only one quantitative variable is being supplied to the function.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset.", "Type â€œView(airquality)â€ in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset.", ",Â The â€œ,â€ is required to start specifying additional commands for the function."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 16,
    "title": "ğŸ“ Bar Charts",
    "url": "GraphicalSummaries.html#barcharts",
    "content": "Bar Charts 1 (or 2) Qualitative Variable(s) Overview Depicts the number of occurrances for each category, or level, of the qualitative variable. Similar to a histogram, but there is no natural way to order the bars. Thus the white-space between each bar. It is called a Pareto chart if the bars are ordered from tallest to shortest. Clustered and stacked bar charts are often used to display information for two qualitative variables simultaneously. R Instructions Base R ggplot2 plotly To make a bar chart in R use the code: barplot(heights) heights must be a â€œnumeric vectorâ€ that contains the heights for each bar that will be drawn in the plot. Note: both the c() and table() functions can be used to specify the heights. The example codes below demonstrate. Example Code Using the c() function. barplot barplot is an R function used to create a bar chart. ( Parenthesis to begin the barplot function. Must touch the last letter of the function. c c is an R function used to concatenate a list of values together into a â€œvector.â€ It is being used here to specify the heights of the 4 bars in the bar plot. ( Parenthesis to begin the c function. Must touch the last letter of the function. 10,5,28,3 This list of numbers will be joined together into a single â€œvector.â€ There is no limit on the number of entries that can be put into such a list. )Closing parenthsis for the c() function. ,Â The â€œ,â€ is required to start specifying additional commands for the barplot function. col=â€œgray24â€ col= stands for the â€œcolorâ€ of the plot. The color name â€œgray24â€ is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. )Closing parenthsis for the barplot function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. barplot barplot is an R function used to create a bar chart. ( Parenthesis to begin the barplot function. Must touch the last letter of the function. c c is an R function used to concatenate a list of values together into a â€œvector.â€ It is being used here to specify the heights of the 4 bars in the bar plot. ( Parenthesis to begin the c function. Must touch the last letter of the function. Pigs=10,Cats=5,Dogs=28,Roosters=3 This named list of numbers will be joined together into a single â€œvector.â€ There is no limit on the number of entries that can be put into such a list. Notice how the names show up as the labels for each bin in the bar chart. )Closing parenthsis for the c() function. ,Â The â€œ,â€ is required to start specifying additional commands for the barplot function. col=â€œgray44â€ col= stands for the â€œcolorâ€ of the plot. The color name â€œgray44â€ is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. )Closing parenthsis for the barplot function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. barplot( barplot is an R function used to create a bar chart. rbind( rbind stands for â€œrow bindâ€ and is a function that joins together different c() vectors to make them become rows of a table. `Farm 1`=c(Pigs=10,Cats=5,Dogs=28,Roosters=3) Notice how this c() vector of named values is being named â€œFarm 1.â€ The tick marks ` ` are required to specify a name of a vector that has a space in it. If the name was just Farm1 (without a space) then the tick marks would not be needed. Since `Farm 1` is the first vector in the rbind() function, it will become the first row of the resulting table that rbind() will create. ,Â The â€œ,â€ is required to specify additional c() vectors for the rbind() function. `Farm 2`=c(Pigs=15,Cats=3,Dogs=8,Roosters=1) Notice how this c() vector of named values is being named â€œFarm 2.â€ It will become the second row of the table created by rbind(). )Closing parenthsis for the rbind() function. ,Â The â€œ,â€ is required to specify additional commands for the barplot function. col=c(â€œgray84â€,â€œgray44â€) col= stands for the â€œcolorâ€ of the plot. Here two colors: â€œgray84â€ and â€œgray44â€ are being passed to the col= option by using the c() function. Notice how these two colors are used in the resulting bar chart. ,Â The â€œ,â€ is required to specify additional commands for the barplot function. beside=TRUE beside= can be set to either TRUE or FALSE. When it is TRUE, the bars are clustered side-by-side. When it is set to FALSE, the bars are stacked on top of each other. Typically, beside=TRUE is preferred. ,Â The â€œ,â€ is required to specify additional commands for the barplot function. legend.text=TRUE legend.text=TRUE allows for the legend to be placed on the barplot. )Closing parenthsis for the barplot function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Using the table() function. barplot barplot is an R function used to create a bar chart. ( Parenthesis to begin the function. Must touch the last letter of the function. table table is an R function used to tabulate how many times each value occurs in a given dataset. It is being used here to specify the heights of the bars in the bar chart. ( Parenthesis to begin the function. Must touch the last letter of the function. mtcars â€œmtcarsâ€ is a dataset. Type â€œView(mtcars)â€ in R to see it. $ The $ allows us to access any variable from the mtcars dataset. cyl â€œcylâ€ is a qualitative variable (in this case actually a numeric vector acting as a qualitative variable) from the â€œmtcarsâ€ dataset. It represents the number of cylinders the vehicleâ€™s engine has. )Closing parenthsis for the table() function. ,Â The â€œ,â€ is required to start specifying additional commands for the barplot function. col=â€œcornsilkâ€ col= stands for the â€œcolorâ€ of the plot. The color name â€œcornsilkâ€ is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. )Closing parenthsis for the barplot function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. barplot( barplot is an R function used to create a bar chart. table( table is an R function used to tabulate how many times each pair of values occurs in a given dataset. It is being used here to specify the heights of the bars in this clustered bar chart. mtcars$am â€œmtcarsâ€ is a dataset and the $ sign is being used to access the â€œamâ€ variable from that dataset. Note that â€œamâ€ is being used as a qualitative variable, but is actually a numeric vector acting as a qualitative variable. It denotes whether the vehicle is an automatic (0) or manual (1) transmission. ,Â The â€œ,â€ is required to specify additional variables for the table() function. mtcars$cyl â€œmtcarsâ€ is a dataset and the $ sign is being used to access the â€œcylâ€ variable from that dataset. The â€œcylâ€ variable gives the cylinders of the vehicleâ€™s engine as either 4, 6, or 8. So even though it is numeric, it can be used as a qualitative variable. )Closing parenthsis for the table() function. ,Â The â€œ,â€ is required to start specifying additional commands for the barplot function. beside=TRUEbeside= is an optional command to the barplot() function. When TRUE, the bars are placed next to each other. When FALSE, the bars are stacked on top of each other. ,Â The â€œ,â€ is required to specify additional commands for the barplot function. col=c(â€œfirebrickâ€,â€œsnow1â€) col= stands for the â€œcolorâ€ of the plot. The colors of â€œfirebrickâ€ and â€œsnow1â€ are being passed to the col= option using the c() function. ,Â The â€œ,â€ is required to specify additional commands for the barplot function. legend.text=TRUE legend.text=TRUE allows for the legend to be placed on the barplot. ,Â The â€œ,â€ is required to specify additional commands for the barplot function. xlab=â€œCylindersâ€ xlab= stands for â€œx label.â€ Use it to specify the text to print on the plot below the x-axis. The desired text must always be contained in quotes. )Closing parenthsis for the barplot function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. To make a bar chart in R using the ggplot approach, first ensure: library(ggplot2) is loaded. Then, ggplot(data, aes(x=groupsColumn, y=countsColumn) + Â Â geom_bar() data is the name of your dataset. groupsColumn is a column of data from your dataset that is qualitative and represents the groups that should each have a bar in the barplot. countsColumn is a column of data from your dataset that contains the counts of how many times each group has been observed. The aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your groupsColumn of data, the y-axis become your countsColumn. Note if groupsColumn is not a factor, use factor(groupsColumn) instead. The geometry helper function geom_bar() causes the ggplot to become a bar chart. Example Code Manually building the counts data. FarmAnimals <- data.frame(animal = c(â€œpigsâ€,â€œcatsâ€,â€œdogsâ€,â€œRoostersâ€), count = c(10,5,28,3)) This code creates a data set manually called FarmAnimals using the data.frame() function. Notice that there are two columns in this dataset: â€œanimalâ€ and â€œcountâ€. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. FarmAnimals â€œFarmAnimalsâ€ is a dataset we just created. Type â€œView(FarmAnimals)â€ in R after running the above code to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. animal,Â  Declares which variable will become the x-axis of the graphic, the explanatory variable. count,Â  Declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_col( ) The â€œgeom_col()â€ function is being used here instead of â€œgeom_bar()â€ because this is a very simple bar chart for just one groups column. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. FarmAnimals <- data.frame(animal = c(â€œpigsâ€,â€œpigsâ€,â€œcatsâ€,â€œcatsâ€,â€œdogsâ€,â€œdogsâ€,â€œRoostersâ€,â€œRoostersâ€), count = c(6,4,2,3,18,10,2,1), farm = c(â€œfarm1â€,â€œfarm2â€,â€œfarm1â€,â€œfarm2â€,â€œfarm1â€,â€œfarm2â€,â€œfarm1â€,â€œfarm2â€)) This code creates a data set manually called FarmAnimals using the data.frame() function. Notice that there are two columns in this dataset: â€œanimalâ€ and â€œcountâ€. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. FarmAnimals â€œFarmAnimalsâ€ is a dataset we just created. Type â€œView(FarmAnimals)â€ in R after running the above code to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x = animal,Â  Declares which variable will become the x-axis of the graphic, the explanatory variable. y = count,Â  Declares which variable will become the y-axis of the graphic. fill = farm,Â  Declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_bar( The â€œgeom_barâ€ function tells the ggplot() to become a bar chart. stat = â€œidentityâ€,Â Tells the ggplot to use the counts as listed in the counts column. position = â€œdodgeâ€,Â Causes the bars in the barchart to be side-by-side rather than stacked. color = â€œblackâ€,Â Controls the colors of the borders of the bars in the plot. )Closing parenthsis for the geom_bar() function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Using an existing dataset directly. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. mtcars â€œmtcarsâ€ is a dataset in R. Type â€œView(mtcars)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x = factor(cyl) Declares which variable will become the x-axis of the graphic. Use factor(columnName) when the column consists of numbers to turn it into groups. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_bar( The â€œgeom_barâ€ function tells the ggplot() to become a bar chart. fill = â€œcornsilkâ€,Â Controls the colors of the insides of the bars in the plot. color = â€œblackâ€ Controls the colors of the borders of the bars in the plot. )Closing parenthsis for the geom_bar() function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. mtcars â€œmtcarsâ€ is a dataset in R. Type â€œView(mtcars)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x = factor(cyl),Â  Declares which variable will become the x-axis of the graphic. Use factor(columnName) when the column consists of numbers to turn it into groups. fill = factor(am),Â  Declares which variable will become the x-axis of the graphic. Use factor(columnName) when the column consists of numbers to turn it into groups. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_bar( The â€œgeom_barâ€ function tells the ggplot() to become a bar chart. position = â€œdodgeâ€,Â Causes the bars to be side-by-side instead of stacked. color = â€œblackâ€ Controls the colors of the borders of the bars in the plot. )Closing parenthsis for the geom_bar() function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  labs(x=â€œCylindersâ€) The â€œlabsâ€ function is being used to add a title to the x-axis only. title=â€œmain titleâ€ and y=â€œy titleâ€ could also be used. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Not yet available.",
    "sentences": ["1 (or 2) Qualitative Variable(s)", "Overview Depicts the number of occurrances for each category, or level, of the qualitative variable.", "Similar to a histogram, but there is no natural way to order the bars.", "Thus the white-space between each bar.", "It is called a Pareto chart if the bars are ordered from tallest to shortest.", "Clustered and stacked bar charts are often used to display information for two qualitative variables simultaneously.", "R Instructions Base R ggplot2 plotly To make a bar chart in R use the code: barplot(heights) heights must be a â€œnumeric vectorâ€ that contains the heights for each bar that will be drawn in the plot.", "Note: both the c() and table() functions can be used to specify the heights.", "The example codes below demonstrate.", "Example Code Using the c() function."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Bar Charts"
  },
  {
    "id": 17,
    "title": "ğŸ“ Overview",
    "url": "GraphicalSummaries.html#overview",
    "content": "Overview Depicts the number of occurrances for each category, or level, of the qualitative variable. Similar to a histogram, but there is no natural way to order the bars. Thus the white-space between each bar. It is called a Pareto chart if the bars are ordered from tallest to shortest. Clustered and stacked bar charts are often used to display information for two qualitative variables simultaneously.",
    "sentences": ["Depicts the number of occurrances for each category, or level, of the qualitative variable.", "Similar to a histogram, but there is no natural way to order the bars.", "Thus the white-space between each bar.", "It is called a Pareto chart if the bars are ordered from tallest to shortest.", "Clustered and stacked bar charts are often used to display information for two qualitative variables simultaneously."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 18,
    "title": "ğŸ“ R Instructions",
    "url": "GraphicalSummaries.html#rinstructions",
    "content": "R Instructions Base R ggplot2 plotly To make a bar chart in R use the code: barplot(heights) heights must be a â€œnumeric vectorâ€ that contains the heights for each bar that will be drawn in the plot. Note: both the c() and table() functions can be used to specify the heights. The example codes below demonstrate. Example Code Using the c() function. barplot barplot is an R function used to create a bar chart. ( Parenthesis to begin the barplot function. Must touch the last letter of the function. c c is an R function used to concatenate a list of values together into a â€œvector.â€ It is being used here to specify the heights of the 4 bars in the bar plot. ( Parenthesis to begin the c function. Must touch the last letter of the function. 10,5,28,3 This list of numbers will be joined together into a single â€œvector.â€ There is no limit on the number of entries that can be put into such a list. )Closing parenthsis for the c() function. ,Â The â€œ,â€ is required to start specifying additional commands for the barplot function. col=â€œgray24â€ col= stands for the â€œcolorâ€ of the plot. The color name â€œgray24â€ is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. )Closing parenthsis for the barplot function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. barplot barplot is an R function used to create a bar chart. ( Parenthesis to begin the barplot function. Must touch the last letter of the function. c c is an R function used to concatenate a list of values together into a â€œvector.â€ It is being used here to specify the heights of the 4 bars in the bar plot. ( Parenthesis to begin the c function. Must touch the last letter of the function. Pigs=10,Cats=5,Dogs=28,Roosters=3 This named list of numbers will be joined together into a single â€œvector.â€ There is no limit on the number of entries that can be put into such a list. Notice how the names show up as the labels for each bin in the bar chart. )Closing parenthsis for the c() function. ,Â The â€œ,â€ is required to start specifying additional commands for the barplot function. col=â€œgray44â€ col= stands for the â€œcolorâ€ of the plot. The color name â€œgray44â€ is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. )Closing parenthsis for the barplot function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. barplot( barplot is an R function used to create a bar chart. rbind( rbind stands for â€œrow bindâ€ and is a function that joins together different c() vectors to make them become rows of a table. `Farm 1`=c(Pigs=10,Cats=5,Dogs=28,Roosters=3) Notice how this c() vector of named values is being named â€œFarm 1.â€ The tick marks ` ` are required to specify a name of a vector that has a space in it. If the name was just Farm1 (without a space) then the tick marks would not be needed. Since `Farm 1` is the first vector in the rbind() function, it will become the first row of the resulting table that rbind() will create. ,Â The â€œ,â€ is required to specify additional c() vectors for the rbind() function. `Farm 2`=c(Pigs=15,Cats=3,Dogs=8,Roosters=1) Notice how this c() vector of named values is being named â€œFarm 2.â€ It will become the second row of the table created by rbind(). )Closing parenthsis for the rbind() function. ,Â The â€œ,â€ is required to specify additional commands for the barplot function. col=c(â€œgray84â€,â€œgray44â€) col= stands for the â€œcolorâ€ of the plot. Here two colors: â€œgray84â€ and â€œgray44â€ are being passed to the col= option by using the c() function. Notice how these two colors are used in the resulting bar chart. ,Â The â€œ,â€ is required to specify additional commands for the barplot function. beside=TRUE beside= can be set to either TRUE or FALSE. When it is TRUE, the bars are clustered side-by-side. When it is set to FALSE, the bars are stacked on top of each other. Typically, beside=TRUE is preferred. ,Â The â€œ,â€ is required to specify additional commands for the barplot function. legend.text=TRUE legend.text=TRUE allows for the legend to be placed on the barplot. )Closing parenthsis for the barplot function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Using the table() function. barplot barplot is an R function used to create a bar chart. ( Parenthesis to begin the function. Must touch the last letter of the function. table table is an R function used to tabulate how many times each value occurs in a given dataset. It is being used here to specify the heights of the bars in the bar chart. ( Parenthesis to begin the function. Must touch the last letter of the function. mtcars â€œmtcarsâ€ is a dataset. Type â€œView(mtcars)â€ in R to see it. $ The $ allows us to access any variable from the mtcars dataset. cyl â€œcylâ€ is a qualitative variable (in this case actually a numeric vector acting as a qualitative variable) from the â€œmtcarsâ€ dataset. It represents the number of cylinders the vehicleâ€™s engine has. )Closing parenthsis for the table() function. ,Â The â€œ,â€ is required to start specifying additional commands for the barplot function. col=â€œcornsilkâ€ col= stands for the â€œcolorâ€ of the plot. The color name â€œcornsilkâ€ is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. )Closing parenthsis for the barplot function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. barplot( barplot is an R function used to create a bar chart. table( table is an R function used to tabulate how many times each pair of values occurs in a given dataset. It is being used here to specify the heights of the bars in this clustered bar chart. mtcars$am â€œmtcarsâ€ is a dataset and the $ sign is being used to access the â€œamâ€ variable from that dataset. Note that â€œamâ€ is being used as a qualitative variable, but is actually a numeric vector acting as a qualitative variable. It denotes whether the vehicle is an automatic (0) or manual (1) transmission. ,Â The â€œ,â€ is required to specify additional variables for the table() function. mtcars$cyl â€œmtcarsâ€ is a dataset and the $ sign is being used to access the â€œcylâ€ variable from that dataset. The â€œcylâ€ variable gives the cylinders of the vehicleâ€™s engine as either 4, 6, or 8. So even though it is numeric, it can be used as a qualitative variable. )Closing parenthsis for the table() function. ,Â The â€œ,â€ is required to start specifying additional commands for the barplot function. beside=TRUEbeside= is an optional command to the barplot() function. When TRUE, the bars are placed next to each other. When FALSE, the bars are stacked on top of each other. ,Â The â€œ,â€ is required to specify additional commands for the barplot function. col=c(â€œfirebrickâ€,â€œsnow1â€) col= stands for the â€œcolorâ€ of the plot. The colors of â€œfirebrickâ€ and â€œsnow1â€ are being passed to the col= option using the c() function. ,Â The â€œ,â€ is required to specify additional commands for the barplot function. legend.text=TRUE legend.text=TRUE allows for the legend to be placed on the barplot. ,Â The â€œ,â€ is required to specify additional commands for the barplot function. xlab=â€œCylindersâ€ xlab= stands for â€œx label.â€ Use it to specify the text to print on the plot below the x-axis. The desired text must always be contained in quotes. )Closing parenthsis for the barplot function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. To make a bar chart in R using the ggplot approach, first ensure: library(ggplot2) is loaded. Then, ggplot(data, aes(x=groupsColumn, y=countsColumn) + Â Â geom_bar() data is the name of your dataset. groupsColumn is a column of data from your dataset that is qualitative and represents the groups that should each have a bar in the barplot. countsColumn is a column of data from your dataset that contains the counts of how many times each group has been observed. The aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your groupsColumn of data, the y-axis become your countsColumn. Note if groupsColumn is not a factor, use factor(groupsColumn) instead. The geometry helper function geom_bar() causes the ggplot to become a bar chart. Example Code Manually building the counts data. FarmAnimals <- data.frame(animal = c(â€œpigsâ€,â€œcatsâ€,â€œdogsâ€,â€œRoostersâ€), count = c(10,5,28,3)) This code creates a data set manually called FarmAnimals using the data.frame() function. Notice that there are two columns in this dataset: â€œanimalâ€ and â€œcountâ€. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. FarmAnimals â€œFarmAnimalsâ€ is a dataset we just created. Type â€œView(FarmAnimals)â€ in R after running the above code to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. animal,Â  Declares which variable will become the x-axis of the graphic, the explanatory variable. count,Â  Declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_col( ) The â€œgeom_col()â€ function is being used here instead of â€œgeom_bar()â€ because this is a very simple bar chart for just one groups column. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. FarmAnimals <- data.frame(animal = c(â€œpigsâ€,â€œpigsâ€,â€œcatsâ€,â€œcatsâ€,â€œdogsâ€,â€œdogsâ€,â€œRoostersâ€,â€œRoostersâ€), count = c(6,4,2,3,18,10,2,1), farm = c(â€œfarm1â€,â€œfarm2â€,â€œfarm1â€,â€œfarm2â€,â€œfarm1â€,â€œfarm2â€,â€œfarm1â€,â€œfarm2â€)) This code creates a data set manually called FarmAnimals using the data.frame() function. Notice that there are two columns in this dataset: â€œanimalâ€ and â€œcountâ€. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. FarmAnimals â€œFarmAnimalsâ€ is a dataset we just created. Type â€œView(FarmAnimals)â€ in R after running the above code to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x = animal,Â  Declares which variable will become the x-axis of the graphic, the explanatory variable. y = count,Â  Declares which variable will become the y-axis of the graphic. fill = farm,Â  Declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_bar( The â€œgeom_barâ€ function tells the ggplot() to become a bar chart. stat = â€œidentityâ€,Â Tells the ggplot to use the counts as listed in the counts column. position = â€œdodgeâ€,Â Causes the bars in the barchart to be side-by-side rather than stacked. color = â€œblackâ€,Â Controls the colors of the borders of the bars in the plot. )Closing parenthsis for the geom_bar() function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Using an existing dataset directly. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. mtcars â€œmtcarsâ€ is a dataset in R. Type â€œView(mtcars)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x = factor(cyl) Declares which variable will become the x-axis of the graphic. Use factor(columnName) when the column consists of numbers to turn it into groups. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_bar( The â€œgeom_barâ€ function tells the ggplot() to become a bar chart. fill = â€œcornsilkâ€,Â Controls the colors of the insides of the bars in the plot. color = â€œblackâ€ Controls the colors of the borders of the bars in the plot. )Closing parenthsis for the geom_bar() function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. mtcars â€œmtcarsâ€ is a dataset in R. Type â€œView(mtcars)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x = factor(cyl),Â  Declares which variable will become the x-axis of the graphic. Use factor(columnName) when the column consists of numbers to turn it into groups. fill = factor(am),Â  Declares which variable will become the x-axis of the graphic. Use factor(columnName) when the column consists of numbers to turn it into groups. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_bar( The â€œgeom_barâ€ function tells the ggplot() to become a bar chart. position = â€œdodgeâ€,Â Causes the bars to be side-by-side instead of stacked. color = â€œblackâ€ Controls the colors of the borders of the bars in the plot. )Closing parenthsis for the geom_bar() function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  labs(x=â€œCylindersâ€) The â€œlabsâ€ function is being used to add a title to the x-axis only. title=â€œmain titleâ€ and y=â€œy titleâ€ could also be used. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Not yet available.",
    "sentences": ["Base R ggplot2 plotly To make a bar chart in R use the code: barplot(heights) heights must be a â€œnumeric vectorâ€ that contains the heights for each bar that will be drawn in the plot.", "Note: both the c() and table() functions can be used to specify the heights.", "The example codes below demonstrate.", "Example Code Using the c() function.", "barplot barplot is an R function used to create a bar chart.", "( Parenthesis to begin the barplot function.", "Must touch the last letter of the function.", "c c is an R function used to concatenate a list of values together into a â€œvector.â€ It is being used here to specify the heights of the 4 bars in the bar plot.", "( Parenthesis to begin the c function.", "Must touch the last letter of the function."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 19,
    "title": "ğŸ“ Custom Plots",
    "url": "GraphicalSummaries.html#customplots",
    "content": "Custom Plots Creativity Required Overview Sometimes no standard plot sufficiently describes the data. In these cases, the only guideline is the one stated originally, â€œthe graphical depiction of data should communicate the truth the data has to offer about the situation of interest.â€ R Examples You should add links to examples you find of interesting plots made in R. Here is the R Code for the graphic to the left: plot(density(CO2$uptake[CO2$Type==\"Quebec\"]), main=\"\", col='skyblue4', xlab=\"\", ylab=\"\", xaxt='n', yaxt='n') lines(density(CO2$uptake[CO2$Type==\"Mississippi\"]), col='firebrick')",
    "sentences": ["Creativity Required", "Overview Sometimes no standard plot sufficiently describes the data.", "In these cases, the only guideline is the one stated originally, â€œthe graphical depiction of data should communicate the truth the data has to offer about the situation of interest.â€", "R Examples You should add links to examples you find of interesting plots made in R.", "Here is the R Code for the graphic to the left: plot(density(CO2$uptake[CO2$Type==\"Quebec\"]), main=\"\", col='skyblue4', xlab=\"\", ylab=\"\", xaxt='n', yaxt='n') lines(density(CO2$uptake[CO2$Type==\"Mississippi\"]), col='firebrick')"],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Custom Plots"
  },
  {
    "id": 20,
    "title": "ğŸ“ Overview",
    "url": "GraphicalSummaries.html#overview",
    "content": "Overview Sometimes no standard plot sufficiently describes the data. In these cases, the only guideline is the one stated originally, â€œthe graphical depiction of data should communicate the truth the data has to offer about the situation of interest.â€",
    "sentences": ["Sometimes no standard plot sufficiently describes the data.", "In these cases, the only guideline is the one stated originally, â€œthe graphical depiction of data should communicate the truth the data has to offer about the situation of interest.â€"],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 21,
    "title": "ğŸ“ R Examples",
    "url": "GraphicalSummaries.html#rexamples",
    "content": "R Examples You should add links to examples you find of interesting plots made in R. Here is the R Code for the graphic to the left: plot(density(CO2$uptake[CO2$Type==\"Quebec\"]), main=\"\", col='skyblue4', xlab=\"\", ylab=\"\", xaxt='n', yaxt='n') lines(density(CO2$uptake[CO2$Type==\"Mississippi\"]), col='firebrick')",
    "sentences": ["You should add links to examples you find of interesting plots made in R.", "Here is the R Code for the graphic to the left: plot(density(CO2$uptake[CO2$Type==\"Quebec\"]), main=\"\", col='skyblue4', xlab=\"\", ylab=\"\", xaxt='n', yaxt='n') lines(density(CO2$uptake[CO2$Type==\"Mississippi\"]), col='firebrick')"],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "R Examples"
  },
  {
    "id": 22,
    "title": "Statistics Notebook",
    "url": "index.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Statistics Notebook This page, and all pages of this notebook, are meant to be customized to become a useful Guide to Statistical Analysis in R for your current and future self. Sections Paigeâ€™s Notes : Notes taken throughout different statistic based classes. R Help : R Studio commands, structuring tips, and tricks. Describing Data : R Studio commands for graphically and numerically describing data. Making Inference : Deep dives into the different statistical tests. Analyses : Statistical analyses that utilizes the tools of collecting, analyzing, and interpreting data to result in informed decision-making. Search Bar <input id=\"search-input\" class=\"search-input\" type=\"search\" placeholder=\"Search notes, sections, examplesâ€¦\" autocomplete=\"off\" /> <button class=\"search-btn\" id=\"search-go\" title=\"Search\">Search<\/button> <button class=\"clear-btn\" id=\"search-clear\" title=\"Clear\">Clear<\/button> <span id=\"search-count\">Type to search.<\/span> <span>Tip: use â†‘/â†“ then Enter to open<\/span> Table of Contents These are the statistical tools used to explore and interpret different data types. One Quantitative Response Variable Y Graphics Y is a single quantitative variable of interest. This would be like â€œheightsâ€ of BYU-Idaho students. Test(s) used for this category: One Sample T-test Paired Sample T-test Wilcoxon Signed- Rank Test Used for one quantitative variable. Pretty simple tbh. Questions that this answers: How long are 4th graderâ€™s feet? What is the average length of feet in the KidsFeet dataset? These are the best graphics to use: Examples : What is the mean temperature at Airport? (One Sample) can be done with histogram or dot plot! plot_ly(airquality, x=~Temp, type=\"histogram\", marker=list(color=\"skyblue\", line=list(color=\"darkgray\", width=10))) %>% layout(title=\"La Guardia Airport Daily Mean Temperatures\", xaxis=list(title=\"Temperature in Degrees F\")) stripchart(airquality$Temp, method=\"stack\", xlab= \"Temperature (F)\", main=\"La Guardia Airport Daily Mean Temperature\") You can change the method to â€œoverplotâ€, â€œjitterâ€, or â€œstackâ€ What is the median temperature of La Guardia Airport? (One Sample) boxplot (airquality$Temp, ylab=\"Temperature (F)\", main=\"La Guardia Airport Temperature Median\") Tests One Sample t Test A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable. Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average? Is human body temperature really 98.6&deg; F on average? Do I spend less than $3 a day,on average, purchasing snacks? Requirements This test is only appropriate when both of the following are satisfied. The sample is representative of the population. (Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal. This is a safe assumption when either: the population data can be assumed to be normally distributed using a Q-Q Plot OR the size of the sample (n) that was taken from the population is large (at least n > 30, but â€œlargeâ€ really depends on how badly the data is skewed). If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population. If the requirements are not met, then that doesnâ€™t mean the results of the test are necessarily bad, but there is no guarantee that they are good. Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits R Instructions Console Help Command: ?t.test() t.test(NameOfYourData$Y, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set, like mtcars or KidsFeet. Y must be a â€œnumericâ€ vector of quantitative data. YourNull is the numeric value from your null hypothesis for \\(\\mu\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(NameOfYourData$Y) Example Code Hover your mouse over the example codes to learn more. t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. mtcars â€˜mtcarsâ€™ is a dataset. Type â€˜View(mtcars)â€™ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,Â  â€˜mpgâ€™ is Y, a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,Â  The numeric value from the null hypothesis is 20 meaning \\(\\mu=20\\). alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1âˆ’Î±. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. One Sample t-test EXPLANATION. data: mtcars$mpg EXPLANATION. t = 0.08506, EXPLANATION. Â df = 31, EXPLANATION. Â p-value = 0.9328 EXPLANATION. alternative hypothesis: true mean is not equal to 20 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â 17.91768 EXPLANATION. Â 22.26357 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION. Â 20.09062 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. mtcars â€˜mtcarsâ€™ is a dataset. Type â€˜View(mtcars)â€™ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg) â€˜mpgâ€™ is a quantitative variable (numeric vector) from the mtcars dataset. Â Click to Show OutputÂ  Click to View Output. ## [1] 20 18 Explanation When we want to check if a claim about the average of a group(population mean \\(\\mu\\)) is true, we often use a test called the â€œone sample t testâ€. This test works well when we can assume that the data follows a normal pattern and that weâ€™ve picked our sample randomly from the bigger group weâ€™re interested in. In the figure below, the null hypothesis \\(H_0: \\mu = \\mu_0\\) is represented by the normal distribution (gray) centered at \\(\\mu_0\\). - Note that \\(\\mu_0\\) is just some specified number. - This shows how the null hypothesis represents the assumption about the center of the distribution of the data. After a hypothesis (null) is established and an alternative hypothesis similarly declared, a simple random sample of data of size \\(n\\) is obtained from the population of interest. - In the plot above, this is depicted by the points (blue dots) which are centered around their sample mean \\(\\bar{x}\\). Above the points (blue dots) is another bell-shaped curve (blue dashed line). This curve shows that the alternative hypothesis might fit the data better than the null hypothesis. The role of the one sample t test is to measure the probability of a sample mean being as extreme or more extreme from the hypothesized value of \\(\\mu_0\\) than the one observed assuming the null hypothesis is true. - This probability is of course the p-value of the test. - This works because the sampling distribution of the sample mean has been assumed to be normal. In this case, the distribution of the test statistic t, \\[ t = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\] is known to follow a t distribution with \\(n-1\\) degrees of freedom. *The mathematics that provide this result are phenominal! You can consult any advanced statistical textbook for the details. The p-value of the one sample t test represents the probability that the test statistic \\(t\\) is as extreme or more extreme than the one observed according to a t-distribution with \\(n-1\\) degrees of freedom. If the probability (the p-value) is close enough to zero (smaller than \\(\\alpha\\)), then it is determined that the most plausible hypothesis is the alternative hypothesis, and thus the null is â€œrejectedâ€ in favor of the alternative. Paired Samples t Test The paired samples t test is used when a value is hypothesized for the popluation mean of the differences, \\(\\mu_d\\), obtained from paired observations. Overview Questions The Paired Samples t Test can be used to answer questions like: From pre-test to post-test is there an improvement on average in the subjects? How much taller are husbands than their wives, on average? Do hospital patients that are carefully matched together according to reason for being in the hospital, age, gender, ethnicity, height, and weight show increased stay times in the hospital when infected with a nosocomial infection compared to those who were not infected? Requirements The test is only appropriate when both of the following are satisfied. The sample of differences is representative of the population differences. The sampling distribution of the sample mean of the differences \\(\\bar{d}\\) (\\(\\bar{x}\\) of the differences) can be assumed to be normal. (This second requirement can be assumed to be satisfied when (a) the differences themselves can be assumed to be normal from a Q-Q Plot, or (b) when the sample size \\(n\\) of the differences is large.) Hypotheses Math Code $$ H_0: \\mu_d = 0 $$ $$ H_a: \\mu_d \\neq 0 $$ \\(H_0: \\mu_d = \\text{some number, but typically 0}\\)\\(H_a: \\mu_d \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepPaired studentPaired R Instructions Console Help Command: ?t.test() Option 1: t.test(NameOfYourData$Y1, NameOfYourData$Y2, paired = TRUE, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set like sleep or mtcars or KidsFeet. Y1 must be a â€œnumericâ€ vector that represents the quantitative data from the first sample of data. Y2 must be a â€œnumericâ€ vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y1 - Y2) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the â€œgroup1â€ data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the â€œgroup2â€ data from the sleep data set t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. sleep2$extra,Â  A numeric vector that represents the hours of extra sleep that the group had with drug 2. sleep1$extra,Â  A numeric vector that represents the hours of extra sleep that the same group had with drug 1. paired=TRUE,Â  Indicates that this is a paired t-Test. This will cause the subtraction of sleep2$extra - sleep1$extra to be performed to obtain the paired differences. To cause the subtraction to occur in the other order, reverse the order sleep1$extra, sleep2$extra occur in the t.test(â€¦) function. mu = 0,Â  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\). Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. Paired t-test EXPLANATION. data: sleep2$extra and sleep1$extra EXPLANATION. t = 4.0621, EXPLANATION. Â df = 9, EXPLANATION. Â p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â 0.7001142 EXPLANATION. Â 2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of the differences EXPLANATION. Â  1.58 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. sleep2$extra The hours of extra sleep that the group had with drug 2. Â -Â  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. ) Closing parenthesis for qqPlot(â€¦) function. Â Click to Show OutputÂ  Click to View Output. ## [1] 9 5 Option 2: Compute the differences yourself instead of using paired=TRUE. differences = NameOfYourData$Y1 - NameOfYourData$Y2 t.test(differences, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a â€œnumericâ€ vector that represents the quantitative data from the first sample of data. Y2 must be a â€œnumericâ€ vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. differences are the resulting differences obtained from subtracting Y1 - Y2. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(differences) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the â€œgroup1â€ data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the â€œgroup2â€ data from the sleep data set differencesÂ <-Â  Saved the computed differences to an object called â€˜differencesâ€™. sleep2$extra The hours of extra sleep that t",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Statistics Notebook This page, and all pages of this notebook, are meant to be customized to become a useful Guide to Statistical Analysis in R for your current and future self.", "Sections Paigeâ€™s Notes : Notes taken throughout different statistic based classes.", "R Help : R Studio commands, structuring tips, and tricks.", "Describing Data : R Studio commands for graphically and numerically describing data.", "Making Inference : Deep dives into the different statistical tests.", "Analyses : Statistical analyses that utilizes the tools of collecting, analyzing, and interpreting data to result in informed decision-making.", "Search Bar <input id=\"search-input\" class=\"search-input\" type=\"search\" placeholder=\"Search notes, sections, examplesâ€¦\" autocomplete=\"off\" /> <button class=\"search-btn\" id=\"search-go\" title=\"Search\">Search<\/button> <button class=\"clear-btn\" id=\"search-clear\" title=\"Clear\">Clear<\/button> <span id=\"search-count\">Type to search.<\/span> <span>Tip: use â†‘/â†“ then Enter to open<\/span> Table of Contents These are the statistical tools used to explore and interpret different data types.", "One Quantitative Response Variable Y Graphics Y is a single quantitative variable of interest.", "This would be like â€œheightsâ€ of BYU-Idaho students.", "Test(s) used for this category: One Sample T-test Paired Sample T-test Wilcoxon Signed- Rank Test Used for one quantitative variable.", "Pretty simple tbh.", "Questions that this answers: How long are 4th graderâ€™s feet?", "What is the average length of feet in the KidsFeet dataset?", "These are the best graphics to use: Examples : What is the mean temperature at Airport?", "(One Sample) can be done with histogram or dot plot!", "plot_ly(airquality, x=~Temp, type=\"histogram\", marker=list(color=\"skyblue\", line=list(color=\"darkgray\", width=10))) %>% layout(title=\"La Guardia Airport Daily Mean Temperatures\", xaxis=list(title=\"Temperature in Degrees F\")) stripchart(airquality$Temp, method=\"stack\", xlab= \"Temperature (F)\", main=\"La Guardia Airport Daily Mean Temperature\") You can change the method to â€œoverplotâ€, â€œjitterâ€, or â€œstackâ€ What is the median temperature of La Guardia Airport?", "(One Sample) boxplot (airquality$Temp, ylab=\"Temperature (F)\", main=\"La Guardia Airport Temperature Median\") Tests One Sample t Test A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable.", "Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average?", "Is human body temperature really 98.6&deg; F on average?", "Do I spend less than $3 a day,on average, purchasing snacks?", "Requirements This test is only appropriate when both of the following are satisfied.", "The sample is representative of the population.", "(Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal.", "This is a safe assumption when either: the population data can be assumed to be normally distributed using a Q-Q Plot OR the size of the sample (n) that was taken from the population is large (at least n > 30, but â€œlargeâ€ really depends on how badly the data is skewed).", "If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population.", "If the requirements are not met, then that doesnâ€™t mean the results of the test are necessarily bad, but there is no guarantee that they are good.", "Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits R Instructions Console Help Command: ?t.test() t.test(NameOfYourData$Y, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set, like mtcars or KidsFeet.", "Y must be a â€œnumericâ€ vector of quantitative data.", "YourNull is the numeric value from your null hypothesis for \\(\\mu\\).", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Testing Assumptions library(car) qqPlot(NameOfYourData$Y) Example Code Hover your mouse over the example codes to learn more.", "t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests.", "mtcars â€˜mtcarsâ€™ is a dataset.", "Type â€˜View(mtcars)â€™ in R to view the dataset.", "$ The $ allows us to access any variable from the mtcars dataset.", "mpg,Â  â€˜mpgâ€™ is Y, a quantitative variable (numeric vector) from the mtcars dataset.", "mu = 20,Â  The numeric value from the null hypothesis is 20 meaning \\(\\mu=20\\).", "alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu\\neq20\\).", "conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1âˆ’Î±.", "Â Â Â Â Press Enter to run the code if you have typed it in yourself.", "You can also click here to view the output.", "Â Click to Show OutputÂ  Click to View Output.", "One Sample t-test EXPLANATION.", "data: mtcars$mpg EXPLANATION.", "t = 0.08506, EXPLANATION.", "Â df = 31, EXPLANATION.", "Â p-value = 0.9328 EXPLANATION.", "alternative hypothesis: true mean is not equal to 20 EXPLANATION."],
    "type": "page",
    "page_title": "Statistics Notebook"
  },
  {
    "id": 23,
    "title": "ğŸ“ Sections",
    "url": "index.html#sections",
    "content": "Sections Paigeâ€™s Notes : Notes taken throughout different statistic based classes. R Help : R Studio commands, structuring tips, and tricks. Describing Data : R Studio commands for graphically and numerically describing data. Making Inference : Deep dives into the different statistical tests. Analyses : Statistical analyses that utilizes the tools of collecting, analyzing, and interpreting data to result in informed decision-making.",
    "sentences": ["Paigeâ€™s Notes : Notes taken throughout different statistic based classes.", "R Help : R Studio commands, structuring tips, and tricks.", "Describing Data : R Studio commands for graphically and numerically describing data.", "Making Inference : Deep dives into the different statistical tests.", "Analyses : Statistical analyses that utilizes the tools of collecting, analyzing, and interpreting data to result in informed decision-making."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Sections"
  },
  {
    "id": 24,
    "title": "ğŸ“ Search Bar",
    "url": "index.html#searchbar",
    "content": "Search Bar <input id=\"search-input\" class=\"search-input\" type=\"search\" placeholder=\"Search notes, sections, examplesâ€¦\" autocomplete=\"off\" /> <button class=\"search-btn\" id=\"search-go\" title=\"Search\">Search<\/button> <button class=\"clear-btn\" id=\"search-clear\" title=\"Clear\">Clear<\/button> <span id=\"search-count\">Type to search.<\/span> <span>Tip: use â†‘/â†“ then Enter to open<\/span>",
    "sentences": "<input id=\"search-input\" class=\"search-input\" type=\"search\" placeholder=\"Search notes, sections, examplesâ€¦\" autocomplete=\"off\" /> <button class=\"search-btn\" id=\"search-go\" title=\"Search\">Search<\/button> <button class=\"clear-btn\" id=\"search-clear\" title=\"Clear\">Clear<\/button> <span id=\"search-count\">Type to search.<\/span> <span>Tip: use â†‘/â†“ then Enter to open<\/span>",
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Search Bar"
  },
  {
    "id": 25,
    "title": "ğŸ“ One Quantitative Response Variable Y",
    "url": "index.html#onequantitativeresponsevariabley",
    "content": "One Quantitative Response Variable Y Graphics Y is a single quantitative variable of interest. This would be like â€œheightsâ€ of BYU-Idaho students. Test(s) used for this category: One Sample T-test Paired Sample T-test Wilcoxon Signed- Rank Test Used for one quantitative variable. Pretty simple tbh. Questions that this answers: How long are 4th graderâ€™s feet? What is the average length of feet in the KidsFeet dataset? These are the best graphics to use: Examples : What is the mean temperature at Airport? (One Sample) can be done with histogram or dot plot! plot_ly(airquality, x=~Temp, type=\"histogram\", marker=list(color=\"skyblue\", line=list(color=\"darkgray\", width=10))) %>% layout(title=\"La Guardia Airport Daily Mean Temperatures\", xaxis=list(title=\"Temperature in Degrees F\")) stripchart(airquality$Temp, method=\"stack\", xlab= \"Temperature (F)\", main=\"La Guardia Airport Daily Mean Temperature\") You can change the method to â€œoverplotâ€, â€œjitterâ€, or â€œstackâ€ What is the median temperature of La Guardia Airport? (One Sample) boxplot (airquality$Temp, ylab=\"Temperature (F)\", main=\"La Guardia Airport Temperature Median\")",
    "sentences": ["Graphics Y is a single quantitative variable of interest.", "This would be like â€œheightsâ€ of BYU-Idaho students.", "Test(s) used for this category: One Sample T-test Paired Sample T-test Wilcoxon Signed- Rank Test Used for one quantitative variable.", "Pretty simple tbh.", "Questions that this answers: How long are 4th graderâ€™s feet?", "What is the average length of feet in the KidsFeet dataset?", "These are the best graphics to use: Examples : What is the mean temperature at Airport?", "(One Sample) can be done with histogram or dot plot!", "plot_ly(airquality, x=~Temp, type=\"histogram\", marker=list(color=\"skyblue\", line=list(color=\"darkgray\", width=10))) %>% layout(title=\"La Guardia Airport Daily Mean Temperatures\", xaxis=list(title=\"Temperature in Degrees F\")) stripchart(airquality$Temp, method=\"stack\", xlab= \"Temperature (F)\", main=\"La Guardia Airport Daily Mean Temperature\") You can change the method to â€œoverplotâ€, â€œjitterâ€, or â€œstackâ€ What is the median temperature of La Guardia Airport?", "(One Sample) boxplot (airquality$Temp, ylab=\"Temperature (F)\", main=\"La Guardia Airport Temperature Median\")"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "One Quantitative Response Variable Y"
  },
  {
    "id": 26,
    "title": "ğŸ“ Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single quantitative variable of interest. This would be like â€œheightsâ€ of BYU-Idaho students. Test(s) used for this category: One Sample T-test Paired Sample T-test Wilcoxon Signed- Rank Test Used for one quantitative variable. Pretty simple tbh. Questions that this answers: How long are 4th graderâ€™s feet? What is the average length of feet in the KidsFeet dataset? These are the best graphics to use: Examples : What is the mean temperature at Airport? (One Sample) can be done with histogram or dot plot! plot_ly(airquality, x=~Temp, type=\"histogram\", marker=list(color=\"skyblue\", line=list(color=\"darkgray\", width=10))) %>% layout(title=\"La Guardia Airport Daily Mean Temperatures\", xaxis=list(title=\"Temperature in Degrees F\")) stripchart(airquality$Temp, method=\"stack\", xlab= \"Temperature (F)\", main=\"La Guardia Airport Daily Mean Temperature\") You can change the method to â€œoverplotâ€, â€œjitterâ€, or â€œstackâ€ What is the median temperature of La Guardia Airport? (One Sample) boxplot (airquality$Temp, ylab=\"Temperature (F)\", main=\"La Guardia Airport Temperature Median\")",
    "sentences": ["Y is a single quantitative variable of interest.", "This would be like â€œheightsâ€ of BYU-Idaho students.", "Test(s) used for this category: One Sample T-test Paired Sample T-test Wilcoxon Signed- Rank Test Used for one quantitative variable.", "Pretty simple tbh.", "Questions that this answers: How long are 4th graderâ€™s feet?", "What is the average length of feet in the KidsFeet dataset?", "These are the best graphics to use: Examples : What is the mean temperature at Airport?", "(One Sample) can be done with histogram or dot plot!", "plot_ly(airquality, x=~Temp, type=\"histogram\", marker=list(color=\"skyblue\", line=list(color=\"darkgray\", width=10))) %>% layout(title=\"La Guardia Airport Daily Mean Temperatures\", xaxis=list(title=\"Temperature in Degrees F\")) stripchart(airquality$Temp, method=\"stack\", xlab= \"Temperature (F)\", main=\"La Guardia Airport Daily Mean Temperature\") You can change the method to â€œoverplotâ€, â€œjitterâ€, or â€œstackâ€ What is the median temperature of La Guardia Airport?", "(One Sample) boxplot (airquality$Temp, ylab=\"Temperature (F)\", main=\"La Guardia Airport Temperature Median\")"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 27,
    "title": "ğŸ“ Tests",
    "url": "index.html#tests",
    "content": "Tests One Sample t Test A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable. Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average? Is human body temperature really 98.6&deg; F on average? Do I spend less than $3 a day,on average, purchasing snacks? Requirements This test is only appropriate when both of the following are satisfied. The sample is representative of the population. (Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal. This is a safe assumption when either: the population data can be assumed to be normally distributed using a Q-Q Plot OR the size of the sample (n) that was taken from the population is large (at least n > 30, but â€œlargeâ€ really depends on how badly the data is skewed). If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population. If the requirements are not met, then that doesnâ€™t mean the results of the test are necessarily bad, but there is no guarantee that they are good. Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits R Instructions Console Help Command: ?t.test() t.test(NameOfYourData$Y, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set, like mtcars or KidsFeet. Y must be a â€œnumericâ€ vector of quantitative data. YourNull is the numeric value from your null hypothesis for \\(\\mu\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(NameOfYourData$Y) Example Code Hover your mouse over the example codes to learn more. t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. mtcars â€˜mtcarsâ€™ is a dataset. Type â€˜View(mtcars)â€™ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,Â  â€˜mpgâ€™ is Y, a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,Â  The numeric value from the null hypothesis is 20 meaning \\(\\mu=20\\). alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1âˆ’Î±. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. One Sample t-test EXPLANATION. data: mtcars$mpg EXPLANATION. t = 0.08506, EXPLANATION. Â df = 31, EXPLANATION. Â p-value = 0.9328 EXPLANATION. alternative hypothesis: true mean is not equal to 20 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â 17.91768 EXPLANATION. Â 22.26357 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION. Â 20.09062 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. mtcars â€˜mtcarsâ€™ is a dataset. Type â€˜View(mtcars)â€™ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg) â€˜mpgâ€™ is a quantitative variable (numeric vector) from the mtcars dataset. Â Click to Show OutputÂ  Click to View Output. ## [1] 20 18 Explanation When we want to check if a claim about the average of a group(population mean \\(\\mu\\)) is true, we often use a test called the â€œone sample t testâ€. This test works well when we can assume that the data follows a normal pattern and that weâ€™ve picked our sample randomly from the bigger group weâ€™re interested in. In the figure below, the null hypothesis \\(H_0: \\mu = \\mu_0\\) is represented by the normal distribution (gray) centered at \\(\\mu_0\\). - Note that \\(\\mu_0\\) is just some specified number. - This shows how the null hypothesis represents the assumption about the center of the distribution of the data. After a hypothesis (null) is established and an alternative hypothesis similarly declared, a simple random sample of data of size \\(n\\) is obtained from the population of interest. - In the plot above, this is depicted by the points (blue dots) which are centered around their sample mean \\(\\bar{x}\\). Above the points (blue dots) is another bell-shaped curve (blue dashed line). This curve shows that the alternative hypothesis might fit the data better than the null hypothesis. The role of the one sample t test is to measure the probability of a sample mean being as extreme or more extreme from the hypothesized value of \\(\\mu_0\\) than the one observed assuming the null hypothesis is true. - This probability is of course the p-value of the test. - This works because the sampling distribution of the sample mean has been assumed to be normal. In this case, the distribution of the test statistic t, \\[ t = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\] is known to follow a t distribution with \\(n-1\\) degrees of freedom. *The mathematics that provide this result are phenominal! You can consult any advanced statistical textbook for the details. The p-value of the one sample t test represents the probability that the test statistic \\(t\\) is as extreme or more extreme than the one observed according to a t-distribution with \\(n-1\\) degrees of freedom. If the probability (the p-value) is close enough to zero (smaller than \\(\\alpha\\)), then it is determined that the most plausible hypothesis is the alternative hypothesis, and thus the null is â€œrejectedâ€ in favor of the alternative. Paired Samples t Test The paired samples t test is used when a value is hypothesized for the popluation mean of the differences, \\(\\mu_d\\), obtained from paired observations. Overview Questions The Paired Samples t Test can be used to answer questions like: From pre-test to post-test is there an improvement on average in the subjects? How much taller are husbands than their wives, on average? Do hospital patients that are carefully matched together according to reason for being in the hospital, age, gender, ethnicity, height, and weight show increased stay times in the hospital when infected with a nosocomial infection compared to those who were not infected? Requirements The test is only appropriate when both of the following are satisfied. The sample of differences is representative of the population differences. The sampling distribution of the sample mean of the differences \\(\\bar{d}\\) (\\(\\bar{x}\\) of the differences) can be assumed to be normal. (This second requirement can be assumed to be satisfied when (a) the differences themselves can be assumed to be normal from a Q-Q Plot, or (b) when the sample size \\(n\\) of the differences is large.) Hypotheses Math Code $$ H_0: \\mu_d = 0 $$ $$ H_a: \\mu_d \\neq 0 $$ \\(H_0: \\mu_d = \\text{some number, but typically 0}\\)\\(H_a: \\mu_d \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepPaired studentPaired R Instructions Console Help Command: ?t.test() Option 1: t.test(NameOfYourData$Y1, NameOfYourData$Y2, paired = TRUE, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set like sleep or mtcars or KidsFeet. Y1 must be a â€œnumericâ€ vector that represents the quantitative data from the first sample of data. Y2 must be a â€œnumericâ€ vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y1 - Y2) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the â€œgroup1â€ data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the â€œgroup2â€ data from the sleep data set t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. sleep2$extra,Â  A numeric vector that represents the hours of extra sleep that the group had with drug 2. sleep1$extra,Â  A numeric vector that represents the hours of extra sleep that the same group had with drug 1. paired=TRUE,Â  Indicates that this is a paired t-Test. This will cause the subtraction of sleep2$extra - sleep1$extra to be performed to obtain the paired differences. To cause the subtraction to occur in the other order, reverse the order sleep1$extra, sleep2$extra occur in the t.test(â€¦) function. mu = 0,Â  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\). Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. Paired t-test EXPLANATION. data: sleep2$extra and sleep1$extra EXPLANATION. t = 4.0621, EXPLANATION. Â df = 9, EXPLANATION. Â p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â 0.7001142 EXPLANATION. Â 2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of the differences EXPLANATION. Â  1.58 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. sleep2$extra The hours of extra sleep that the group had with drug 2. Â -Â  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. ) Closing parenthesis for qqPlot(â€¦) function. Â Click to Show OutputÂ  Click to View Output. ## [1] 9 5 Option 2: Compute the differences yourself instead of using paired=TRUE. differences = NameOfYourData$Y1 - NameOfYourData$Y2 t.test(differences, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a â€œnumericâ€ vector that represents the quantitative data from the first sample of data. Y2 must be a â€œnumericâ€ vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. differences are the resulting differences obtained from subtracting Y1 - Y2. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(differences) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the â€œgroup1â€ data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the â€œgroup2â€ data from the sleep data set differencesÂ <-Â  Saved the computed differences to an object called â€˜differencesâ€™. sleep2$extra The hours of extra sleep that the group had with drug 2. Â -Â  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. differences,Â  â€˜differencesâ€™ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2. mu = 0,Â  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\). Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. One Sample t-test EXPLANATION. data: differences EXPLANATION. t = 4.0621, EXPLANATION. Â df = 9, EXPLANATION. Â p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â 0.7001142 EXPLANATION. Â 2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION. Â  1.58 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. differences) â€˜differencesâ€™ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2. Â Click to Show OutputÂ  Click to View Output. ## [1] 9 5 Explanation The paired samples t test considers the single mean of all the differences from the paired values. Thus, the paired samples t test essentially becomes a one sample t test on the differences between paired observations. Hence the requirement is that the sampling distribution of the sample mean of the differences, \\(\\bar{d}\\), can be assumed to be normally distributed. (It is also required that the obtained differences represent a simple random sample of the full population of possible differences.) The paired samples t test is similar to the independent samples t test scenario, except that there is extra information that allows values from one sample to be paired with a value from the other sample. This pairing of values allows for a more direct analysis of the change or difference individuals experience between the two samples. The points in the plot below demonstrate how points are paired together, and the only thing of interest are the differences between the paired points. Wilcoxon Signed-Rank Test For testing hypotheses about the value of the median of: one sample of quantitative data or one set of differences from paired data. Box plots + Dot plots are great to show off this kind of data (because it shows the median!) Median line closer to the bottom of the box plot = Right Skewed Median line is closer to the top of the box plot = Left Skewed Overview The nonparametric equivalent of the paired-samples t test as well as the one-sample t test. Best for smaller sample sizes where the distribution of the data is not normal The t test is more appropriate when the data is normal or when the sample size is large. While the test will work in most scenarios it suffers slightly when ties (repeated values) are present in the data. If many ties are present in the data, the test is not appropriate. If only a few ties are present, the test is still appropriate. This test is similar to the paired-samples t test and the one-sample t test, but it doesnâ€™t assume the data follows a normal distribution (nonparametric equivalent) It works best when: - You have a small number of samples - Your data doesnâ€™t follow a normal distribution ** The t test is better when your data is normal or when you have a lot of samples.This test usually works fine, but it can have problems if you have many repeated values in your data. If there are just a few repeated values, itâ€™s still okay to use. Hypotheses Originally created to test hypotheses about the value of the median works as well for the mean when the distribution of the data is symmetrical. One Sample of Data Math Code $$ H_0: \\text{Median} = \\text{(Some Number)} $$ $$ H_a: \\text{Median} \\neq \\text{(Same Number)} $$ \\(H_0: \\text{Median} = \\text{(Some Number)}\\) \\(H_a: \\text{Median} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{(Some Number)}\\) Paired Samples of Data Math Code $$ H_0: \\text{median of differences} = 0 $$ $$ H_a: \\text{median of differences} \\neq 0 $$ \\(H_0: \\text{median of differences} = 0\\) \\(H_a: \\text{median of differences} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ 0\\) Examples: sleep, CornHeights R Instructions Console Help Command: ?wilcox.test() Paired Data wilcox.test(Y1, Y2, mu = YourNull, alternative = YourAlternative, paired = TRUE, conf.level = 0.95) Y1 must be a â€œnumericâ€ vector. One set of measurements from the pair. Y2 also a â€œnumericâ€ vector. Other set of measurements from the pair. YourNull is the numeric value from your null hypothesis for the median of differences from the paired data. Usually zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests. sleep$extra[sleep$group==1],Â  The hours of extra sleep that the group had with drug 2. sleep$extra[sleep$group==2],Â  The hours of extra sleep that the same group had with drug 1. mu = 0,Â  The numeric value from the null hypothesis for the median of differences from the paired data is 0 meaning the null hypothesis is \\(\\text{median of differences} = 0\\). paired=TRUE,Â  This command forces a â€œpairedâ€ samples test to be performed. alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\text{median of differences} \\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\). Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Wilcoxon signed rank test with continuity correction The phrase â€œwith continuity correctionâ€ implies that instead of using the â€œexactâ€ distribution of the test statistic a â€œnormal approximationâ€ was used instead to compute the p-value. Further, a small correction was made to allow for the change from the â€œdiscreteâ€ exact distribution to the â€œcontinuous normal distributionâ€ when calculating the p-value. data: sleep$extra[sleep$group == 1] and sleep$extra[sleep$group == 2] This statement of the output just reminds you of the code you used to perform the test. The important thing is recognizing that the first group listed is â€œGroup 1â€ and the second group listed is â€œGroup 2.â€ This is especially important when using alternative hypotheses of â€œlessâ€ or â€œgreaterâ€ as the order is always â€œGroup 1â€ is â€œlessâ€ than â€œGroup 2â€ or â€œGroup 1â€ is â€œgreaterâ€ than â€œGroup 2.â€ V = 0, This is the test statistic of the test, i.e., the sum of the ranks from the positive group minus the minimum sum of ranks possible. Â p-value = 0.009091 This is the p-value of the test. If no warning is displayed when the test is run, then this is the â€œexactâ€ p-value from the non-parametric Wilcoxon Test Statistic distribution. Sometimes a message will appear stating â€œCannot compute exact p-value with tiesâ€ or other similar messages. In those cases, the p-value is still considered valid even though it is obtained through a normal approximation to the exact distribution. alternative hypothesis: true location shift is not equal to 0 This reports that the alternative hypothesis was â€œtwo-sided.â€ If the alternative had been â€œlessâ€ or â€œgreaterâ€ the wording would change accordingly. One Sample wilcox.test(object, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object must be a â€œnumericâ€ vector. YourNull is the numeric value from your null hypothesis for the median (even though it says â€œmuâ€). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests. mtcars â€˜mtcarsâ€™ is a dataset. Type â€˜View(mtcars)â€™ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,Â  â€˜mpgâ€™ is a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,Â  The numeric value from the null hypothesis is 20 meaning \\(\\mu = 20\\). alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1âˆ’Î±. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Wilcoxon signed rank test with continuity correction This reports on the type of test performed. The phrase â€œwith continuity correctionâ€ implies the normal approximation was used when calculating the p-value of the test. data: mtcars$mpg This print-out reminds us that the mpg column of the mtcars data was used as â€œYâ€ in the test. V = 249, The test statistic of the test. Â p-value = 0.7863 The p-value of the test. alternative hypothesis: true location is not equal to 20 The words â€œnot equalâ€ tell us this was a two-sided test. Had it been a one-sided test, either the word â€œlessâ€ or the word â€œgreaterâ€ would have appeared instead of â€œnot equal.â€ Explanation In many cases it is of interest to perform a hypothesis test about the location of the center of a distribution of data. The Wilcoxon Signed Rank Test allows a nonparametric approach to doing this. The Wilcoxon Signed-Rank Test covers two important scenarios: One sample of data from a population. (Not very common.) The differences obtained from paired data. (Very common.) The Wilcoxon methods are most easily explained through examples, beginning with the paired data for which the method was originally created. ** Scroll down for the One Sample Example if that is what you are really interested in. However, it is still recommended that you read the paired data example first. Paired Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon. Background Height differences â€œbetween cross- and self- fertilized corn plants of the same pairâ€ were collected. The experiment hypothesized that the center of the distribution of the height differences would be zero, with the alternative being that the center was not zero. The result of the data collection was 15 height differences: Differences: 14, 56, 60, 16, 6, 8, -48, 49, 24, 28, 29, 41, -67, 23, 75 Step 1 The first step of the Wilcoxon Signed Rank Test is to order the differences from smallest magnitude to largest magnitude. Negative signs are essentially ignored at this point and only magnitudes of the numbers matter. Sorted Differences: 6, 8, 14, 16, 23, 24, 28, 29, 41, -48, 49, 56, 60, -67, 75 Step 2 The next step is to rank the ordered values. Negative signs are attached to the ranks corresponding to negative numbers. Â  Differences: 6 8 14 16 23 24 28 29 41 -48 49 56 60 -67 75 Ranks: 1 2 3 4 5 6 7 8 9 -10 11 12 13 -14 15 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=15\\). Step 3 The ranks are then put into two groups. Negative Ranks Positive Ranks -10, -14 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15 Step 4 One of the groups is summed, usually the group with the fewest observations. Only the absolute values of the ranks are summed. Sum of Negative Ranks: \\(\\left|-10\\right| + \\left|-14\\right| = 24\\) The sum of the ranks becomes the test statistic of the Wilcoxon Test. The test statistic is sometimes called \\(W\\) or \\(V\\) or \\(U\\). Step 5 The \\(p\\)-value of the test is then obtained by computing the probability of the test statistic being as extreme or more extreme than the one obtained. This is done by first computing the probability of all possible values the test statistic could have obtained using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=15\\) ranks, the possible sums of ranks range from 0 to 120 and hit every integer in between, i.e., \\(1, 2, 3, \\ldots, 120\\). (Note, if summing the negative ranks these sums would technically all be negative.) To verify that \\(120\\) is the largest sum possible for \\(n=15\\) ranks, note that: \\(1+15 = 16\\), \\(2 + 14 = 16\\), \\(3+13 = 16\\), \\(4+12=16\\), \\(5+11=16\\), \\(6+10=16\\), \\(7+9=16\\), and finally that \\(8 = \\frac{16}{2}\\). Thus, there are 7 sums of 16 and one sum of \\(\\frac{16}{2}\\). This could be said in a mathematically equivalent way by stating there are \\(\\frac{14}{2}\\) sums of 16 and one sum of \\(\\frac{16}{2}\\). By multiplication this gives \\[ \\frac{14}{2}\\cdot\\frac{16}{1} + \\frac{1}{1}\\cdot\\frac{16}{2} = \\frac{14\\cdot16 + 1\\cdot16}{2} = \\frac{15\\cdot16}{2} = \\frac{n(n+1)}{2} = 120 \\] The probability of each sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 32,768 total different groups of ranks possible when there are \\(n=15\\) ranks.) For example, a sum of 1 can happen only one way, only the rank of 1 is in the group. A sum of 2 can also only happen 1 way. The sum of 3 however, can happen two ways: we could have the ranks of 1 and 2 in the group, or just the rank of 3 in the group. A similar counting technique is then implemented for each possible sum. After all the calculations are performed, the distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(24\\) (or its opposite of \\(120-24=96\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of (the absolute value of) negative ranks as extreme or more extreme than \\(24\\) is \\(p=0.04126\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would reject the null hypothesis that the center of the distribution of differences is zero. We conclude that the center of the distribution is greater than zero because the sum of negative ranks is much smaller than we expected under the zero center hypothesis (the null). Thus, there is sufficient evidence to conclude that the centers of the distributions of â€œcross- and self-fertilized corn plantsâ€ heights are not equal. One is greater than the other. Notice how the following dot plot shows that the differences are in favor of the cross-fertilized plants (the first group in the subtraction) being taller. This is true even though two self-fertilized plants were much taller than their cross-fertilized counterpart (the two negative differences). Comment If the distribution of differences is symmetric, then the hypotheses can be written as \\[ H_0: \\mu = 0 \\] \\[ H_a: \\mu \\neq 0 \\] If the distribution is skewed, then the hypotheses technically refer to the median instead of the mean and should be written as \\[ H_0: \\text{median} = 0 \\] \\[ H_a: \\text{median} \\neq 0 \\] One Sample Example {#one} The idea behind the one sample Wilcoxon Signed Rank test is nearly identical to the paired data. The only change is that the median must be subtracted from all observed values to obtain the differences. Note that the mean is equal to the median when data is symmetric. Background Suppose we are interested in testing to see if the median hourly wage of BYU-Idaho students during their off-track employment is equal to the minimum wage in Idaho, $7.25 an hour as of January 1st, 2015. Five randomly sampled hourly wages from BYU-Idaho Math 221B students provides the following data. Wages: $6.00, $9.00, $8.10, $18.00, $10.45 The differences are then obtained by subtracting the hypothesized value for the median (or mean if the data is symmetric) from all observations. Differences: -1.25, 1.75, 0.85, 10.75, 3.20 Note: from this point down, the wording of this example is identical to the paired data example (above) with the numbers changed to match \\(n=5\\). It is useful to continue reading to reinforce the idea of the Wilcoxon Signed Rank Test, but no new knowledge will be presented. Step 1 The first step of the Wilcoxon Signed Rank Test is to order the differences from smallest magnitude to largest magnitude. Negative signs are essentially ignored at this point and only magnitudes of the numbers matter. Sorted Differences: 0.85, -1.25, 1.75, 3.20, 10.75 Step 2 The next step is to rank the ordered values. Negative signs are attached to the ranks corresponding to negative numbers. Ranks: 1, -2, 3, 4, 5 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=5\\). Step 3 The ranks are then put into two groups. Negative Ranks Positive Ranks -2 1, 3, 4, 5 Step 4 One of the groups is summed, usually the group with the fewest observations. Sum of Negative Ranks: \\(\\left|-2\\right| = 2\\) Step 5 The \\(p\\)-value of the test is then obtained by computing the probabilities of all possible sums using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=5\\) ranks, the possible sums of ranks range from 0 to 15 and hit every integer in between, i.e., \\(1, 2, 3, \\ldots, 15\\). (Note, if summing the negative ranks these sums would technically all be negative.) To verify that \\(15\\) is the largest sum possible for \\(n=5\\) ranks, note that: \\(1+5 = 6\\), \\(2 + 4 = 6\\), and finally \\(3 = \\frac{6}{2}\\). Thus, there are 2 sums of 6 and one sum of \\(\\frac{6}{2}\\). This could be said in a mathematically equivalent way by stating there are \\(\\frac{4}{2}\\) sums of 6 and one sum of \\(\\frac{6}{2}\\). By multiplication this gives \\[ \\frac{4}{2}\\cdot\\frac{6}{1} + \\frac{1}{1}\\cdot\\frac{6}{2} = \\frac{4\\cdot6 + 1\\cdot6}{2} = \\frac{5\\cdot6}{2} = \\frac{n(n+1)}{2} = 15 \\] The probability of each sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 32 total different groups of ranks possible when there are \\(n=5\\) ranks.) For example, a sum of 1 can happen only one way, only the rank of 1 is in the group. A sum of 2 can also only happen 1 way. The sum of 3 however, can happen two ways: we could have the ranks of 1 and 2 in the group, or just the rank of 3 in the group. A similar counting technique is then implemented for each possible sum. After all the calculations are performed, the distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(2\\) (or its opposite of \\(15-2=13\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of negative ranks as extreme or more extreme than \\(-2\\) is \\(p=0.1875\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would fail to reject the null hypothesis that the center of the distribution of differences is zero. We will continue to assume the null hypothesis was true, that the median off-track hourly wage of BYU-Idaho students is the same as the Idaho minimum wage. Final Comment Notice that when the sample size is large the distribution of the test statistic can be approximated by a normal distribution. Most software applications use this approximation when the sample size is over \\(50\\), i.e., for \\(n\\geq50\\) because computing all the possible sums of ranks becomes incredibly time consuming. Thus, for large sample sizes the results will be almost identical whether the Wilcoxon Tests or a t Test is used. Quantitative Y | Categorical X (2 Groups) Graphics Y is a single quantitative variable of interest. This would be like â€œheightsâ€ of BYU-Idaho students. X is a qualitative (categorical) variable of interest like â€œgenderâ€ that has just two groups â€œAâ€ and â€œBâ€. So this logo represents situtations where we would want to compare heights of male (group A) and female (group B) students. Test(s) used for this category: Independent Samples T-test Analysis : High School Seniors t Test Wilcoxon Rank Sum (Mann- Whitney Test) Analysis : The Benefits of Word Recall Strategies Used for the comparison of two groups, Quantitative Y (ex. height, weight, money, distance) and Categorical X (ex. gender, birth month, phone number) Questions that this answers: How long are boys feet and how long are girls feet? Do boys (B) or girls (G) have longer feet, on average, in the KidsFeet dataset? These are the best graphics to use: Examples: How long are boys feet and how long are girls feet? (Independent Sample) Can use box or dot plot! plot_ly(KidsFeet, y=~length, x=~as.factor(sex),type=\"box\",color=~sex, colors=c(\"lightblue\",\"pink\")) %>% layout( title=\"4th Grader's Average Foot Size\", xaxis=list(title=\"Gender\"),yaxis=list(title=\"Length (cm)\")) ggplot(KidsFeet, aes(x=factor(sex),y=length, fill=sex)) + coord_flip() + geom_dotplot(binaxis=\"y\", stackdir=\"up\", position=\"dodge\", dotsize=0.45, binwidth=0.5) + scale_fill_manual(values=c(\"B\"=\"lightblue\",\"G\"=\"pink\")) + labs(title= \"Kennedy 4th Grader's Foot Size\", x=\"Length (cm)\", y=\"Gender\") Does the Meshed or Before approach have any positive benefits on memory recall when it comes to remembering content? (Wilcoxon Rank Sum Test) Dot plot over box plot shows the distribution and the individuals within each distribution Tests Independent Samples t Test The independent samples t test is used when a value is hypothesized for the difference between two (possibly) different population means, \\(\\mu_1 - \\mu_2\\). finds the mean of each data set, then subtracts those means for their difference two groups donâ€™t depend on each otherâ€™s results Overview Questions The Independent Samples t Test can be used to answer questions like: Are boys taller than girls on average? Do students who show up to class everyday get higher scores on average than those who donâ€™t? Do you take more steps on average on weekdays or on weekends? Requirements The test is only appropriate when both of the following are satisfied. Both samples are representative of the population. (Simple random samples are the best way to do this.) The sampling distribution of the difference of the sample means \\((\\bar{x}_1 - \\bar{x}_2)\\) can be assumed to be normal. This is a safe assumption when the sample size of each group is \\(30\\) or greater or when the population data from each group can be assumed to be normal with a Q-Q Plot. Hypotheses Math Code $$ H_0: \\mu_\\text{label 1} - \\mu_\\text{label 2} = 13.2 $$ $$ H_a: \\mu_\\text{label 1} - \\mu_\\text{label 2} \\neq 13.2 $$ \\(H_0: \\mu_1 - \\mu_2 = \\text{some number, but typically 0}\\) \\(H_a: \\mu_1 - \\mu_2 \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepInd student1 student2 R Instructions Console Help Command: ?t.test() There are two ways to perform the test. Option 1: t.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a â€œnumericâ€ vector from YourData that represents the data for both samples. X must be a â€œfactorâ€ or â€œcharacterâ€ vector from YourData that represents the group assignment for each observation. There can only be two groups specified in this column of data. YourNull is the numeric value from your null hypothesis for \\(\\mu_1-\\mu_2\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y ~ X, data=YourData) Example Code Hover your mouse over the example codes to learn more. t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. lengthÂ  â€˜lengthâ€™ is a quantitative variable (numeric vector). ~Â  â€˜~â€™ is the tilde symbol. sex,Â  â€˜sexâ€™ is a â€˜factorâ€™ or â€˜characterâ€™ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet,Â  â€˜KidsFeetâ€™ is a dataset in library(mosaic). Type View(KidsFeet) to view it. mu = 0,Â  The numeric value from the null hypothesis for Î¼1-Î¼2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo-sidedâ€ meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\) Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. Welch Two Sample t-test EXPLANATION. data: length by sex EXPLANATION. t = 1.9174, EXPLANATION. Â df = 36.275, EXPLANATION. Â p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â -0.04502067 EXPLANATION. Â 1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean in group B mean in group G EXPLANATION. Â  25.10500 EXPLANATION. Â  24.32105 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. lengthÂ  â€˜lengthâ€™ is a quantitative variable (numeric vector). ~Â  â€˜~â€™ is the tilde symbol. sex,Â  â€˜sexâ€™ is a â€œfactorâ€ or â€œcharacterâ€ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet) â€˜KidsFeetâ€™ is a dataset in library(mosaic). Type View(KidsFeet) to view it. Â Click to Show OutputÂ  Click to View Output. Option 2: t.test(NameOfYourData$Y1, NameOfYourData$Y2, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a â€œnumericâ€ vector that represents the quantitative data from the first sample. Y2 must be a â€œnumericâ€ vector that represents the quantitative data from the second sample. YourNull is the numeric value from your null hypothesis for the difference of \\(\\mu_1-\\mu_2\\). This is typically zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) par(mfrow=c(1,2)) qqPlot(NameOfYourData$Y1) qqPlot(NameOfYourData$Y2) Example Code Hover your mouse over the example codes to learn more. t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. KidsFeet$length[KidsFeet$sexÂ ==Â â€œBâ€],Â  A numeric vector that represents the quantitative data or the foot length for the first sample of data which in this case is the boys. KidsFeet$length[KidsFeet$sexÂ ==Â â€œGâ€],Â  A numeric vector that represents the quantitative data or the foot length for the second sample of data which in this case is the girls. mu = 0,Â  The numeric value from the null hypothesis for Î¼1-Î¼2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo-sidedâ€ meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\) Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Welch Two Sample t-test EXPLANATION. data: KidsFeet$length[KidsFeet$sex == â€œBâ€] and KidsFeet$length[KidsFeet$sex == â€œGâ€] EXPLANATION. t = 1.9174, EXPLANATION. Â df = 36.275, EXPLANATION. Â p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â -0.04502067 EXPLANATION. Â 1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean of x mean of y EXPLANATION. Â  25.10500 EXPLANATION. Â  24.32105 EXPLANATION. par( â€˜parâ€™ is a R function that can be used to set or query graphical parameters. mfrow=c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. KidsFeet$length[KidsFeet$sexÂ ==Â â€œBâ€]) A numeric vector that represents the quantitative data or the foot length for the first sample of data which in this case is the boys. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. KidsFeet$length[KidsFeet$sexÂ ==Â â€œGâ€]) A numeric vector that represents the quantitative data or the foot length for the second sample of data which in this case is the girls. Â â€¦Â  Click to View Output. ## [1] 9 8 ## [1] 18 7 Explanation The first figure below depicts the scenario where the difference in means of two separate normal distributions is non-zero. - In other words, the two distributions have different means, \\(\\mu_1\\) and \\(\\mu_2\\), respectively. - It is worth emphasizing that the values of \\(\\mu_1\\) and \\(\\mu_2\\) are unknown to the researcher. The only thing observed are two separate samples of data (blue dots) of sizes \\(n_1\\) and \\(n_2\\), respectively. For the scenario depicted, the null hypothesis that \\(H_0: \\mu_1 - \\mu_2 = 0\\) (i.e., that \\(\\mu_1=\\mu_2\\)) is rejected in favor of the alternative that \\(H_a: \\mu_1 - \\mu_2 \\neq 0\\) based on the sample data observed. - This dicision would be correct as the true difference in means, \\(\\mu_1-\\mu_2\\) is non-zero in this case. When the null hypothesis is true, that \\(H_0: \\mu_1 - \\mu_2 = 0\\) : - the test statistic \\(t\\) that is obtained by measuring the distance between the two sample means, \\(\\bar{x}_1-\\bar{x}_2\\) - appropriately standardizing the result follows a \\(t\\) distribution with degrees of freedom less than or equal to \\(n_1+n_2-2\\). - Thus, the \\(p\\)-value of the independent samples \\(t\\) test is obtained by using this \\(t\\) distribution to calculate the probability of a test statistic \\(t\\) being as extreme or more extreme than the one observed assuming the null hypothesis is true. \\[ t = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{s_1/n_1+s_2/n_2 }} \\] Wilcoxon Rank Sum (Mann-Whitney) Test For testing the equality of the medians of two (possibly different) distributions of a quantitative variable. Box plots + Dot plots are great to show off this kind of data (because it shows the median!) Median line closer to the bottom of the box plot = Right Skewed Median line is closer to the top of the box plot = Left Skewed Overview The nonparametric equivalent of the Independent Samples t Test. Can also be used when data is ordered (ordinal) but does not have an exact measurement. For example, first place, second place, and so on. The Independent Samples t Test is more appropriate when the distributions are normal, or when the sample size for each sample is large. The test is negatively affected when there are ties (repeated values) present in the data, but the results are still useful if there are relatively few ties. Hypotheses Originally designed to test for the equality of medians from two identically shaped distributions. Math Code $$ H_0: \\text{difference in medians} = 0 $$ $$ H_a: \\text{difference in medians} \\neq 0 $$ \\(H_0: \\text{difference in medians} = 0\\) \\(H_a: \\text{difference in medians} \\neq 0\\) However, the test also allows for the more general hypotheses that one distribution is stochastically greater than the other. \\(H_0: \\text{the distributions are stochastically equal}\\) \\(H_a: \\text{one distribution is stochastically greater than the other}\\) If these hypotheses are used, then the distributions do not have to be identically distributed. (Note: Menâ€™s heights are stochastically greater than womenâ€™s heights because men are generally taller than women, but not all men are taller than all women.) Examples: BugSpray, MoralIntegration R Instructions Console Help Command: ?wilcox.test() There are two ways to perform the test. Option 1: wilcox.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a â€œnumericâ€ vector from YourData that represents the data for both samples. X must be a â€œfactorâ€ or â€œcharacterâ€ vector from YourData that represents the group assignment for each observation. There can only be two groups specified in this column of data. YourNull is the numeric value from your null hypothesis for the difference in medians from the two groups. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests. lengthÂ  â€˜lengthâ€™ is a quantitative variable (numeric vector). ~Â  â€˜~â€™ is the tilde symbol. sex,Â  â€˜sexâ€™ is a â€˜factorâ€™ or â€˜characterâ€™ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet,Â  â€˜KidsFeetâ€™ is a dataset in library(mosaic). Type View(KidsFeet) to view it. mu = 0,Â  The numeric value from the null hypothesis for the difference in medians from the two groups is 0 meaning the null hypothesis is \\(\\text{difference in medians} = 0\\) alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo-sidedâ€ meaning the alternative hypothesis is \\(\\text{difference in medians} \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\) Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Wilcoxon rank sum test with continuity correction This states the test that was performed and that a normal approximation to the test statistic was used instead of the exact distribution. data: length by sex This states that the length column was split into the two groups found in the â€œsexâ€ column. Unfortunately, it forgets to remind us that the test used the KidsFeet data set. V = 252, The test statistic of the test. In this case, the sum of the ranks from the alphabetically first group minus the minimum sum of ranks possible. Â p-value = 0.0836 The p-value of the test. alternative hypothesis: true location shift is not equal to 0 This reports that the test used an alternative hypothesis of â€œnot equalâ€ (a two-sided test). Further, the phrase â€œtrue location shiftâ€ emphasizes that the Wilcoxon Rank Sum Test is testing to see if one distribution is shifted higher or lower than the other. Option 2: wilcox.test(object1, object2, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object1 must be a â€œnumericâ€ vector that represents the first sample of data. obejct2 must be a â€œnumericâ€ vector that represents the second sample of data. YourNull is the numeric value from your null hypothesis for the difference in medians from the two groups. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests. KidsFeet$length[KidsFeet$sexÂ ==Â â€œBâ€],Â  A numeric vector of foot length for the first sample of data or for the group of boys. KidsFeet$length[KidsFeet$sexÂ ==Â â€œGâ€],Â  A numeric vector of foot length for the second sample of data or for the group of girls. mu = 0,Â  The numeric value from the null hypothesis for the difference in medians from the two groups is 0 meaning the null hypothesis is \\(\\text{difference in medians} = 0\\) alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo-sidedâ€ meaning the alternative hypothesis is \\(\\text{difference in medians} \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\) Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Wilcoxon rank sum test with continuity correction This states the type of test performed. data: KidsFeet$length[KidsFeet$sex == â€œBâ€] and KidsFeet$length[KidsFeet$sex == â€œGâ€] Reminds you what data was used for the test and points out that the first set of data listed is â€œGroup 1â€ and the second set of data listed is â€œGroup 2.â€ V = 252, The test statistic of the test. Â p-value = 0.0836 The p-value of the test. alternative hypothesis: true location shift is not equal to 0 The alternative hypothesis of the test. The phrase â€œnot equalâ€ implies a â€œtwo-sidedâ€ alternative was used. Explanation In many cases it is of interest to perform a hypothesis test concerning the equality of the centers of two (possibly different) distributions. In other words, an independent samples test. The Wilcoxon Rank Sum Test allows a nonparametric approach to doing this. It is often considered the nonparametric equivalent of the independent samples t test. The method is most easily explained through an example. The theory behind it is very similar to the theory behind the Wilcoxon Signed-Rank Test. Independent Samples Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon. Background The percent of flies (bugs) killed from two different concentrations of a certain spray were recorded from 16 different trials, 8 trials per treatment concentration. The experiment hypothesized that the center of the distributions of the percent killed by either concentration were the same. In other words, that both treatments were equally effective. The alternative hypothesis was that the treatments differed in their effectiveness. Spray Concentration Percent Killed A 68, 68, 59, 72, 64, 67, 70, 74 B 60, 67, 61, 62, 67, 63, 56, 58 Step 1 The first step of the Wilcoxon Rank Sum Test is to order all the data from smallest magnitude to largest magnitude, while keeping track of the group. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Step 2 The next step is to rank the ordered values. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=16\\). Any ranks that are tied need to have the average rank assigned to each of those that are tied. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Rank 1 2 3 4 5 6 7 8 10 10 10 12.5 12.5 14 15 16 Step 3 The ranks are then returned to their original groups. Ranks of Spray A Ranks of Spray B 3, 8, 10, 12.5, 12.5, 14, 15, 16 1, 2, 4, 5, 6, 7, 10, 10 Step 4 The ranks are summed for one of the groups. (It does not matter which group.) Sum of Ranks for Spray A: \\[ 3+8+10+12.5+12.5+14+15+16 = 91 \\] Step 5 The \\(p\\)-value of the test is then obtained by computing the probabilities of all possible sums that one group can achieve using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=16\\) ranks, with just \\(8\\) of the ranks assigned to one group, the possible sums of ranks range from \\(36\\) to \\(100\\) and include every integer in between, i.e., \\(36, 37, 38, \\ldots, 100\\). Note that the smallest sum would be obtained if the ranks 1-8 were in the group. The largest sum would be obtained if the ranks 9-16 were in the group. The probability of each possible sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 12,870 total different sets of ranks possible when there are \\(n=16\\) ranks and \\(8\\) are assigned to one group.) The distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(91\\) (or its opposite of \\(136-91=45\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of negative ranks as extreme or more extreme than \\(91\\) is \\(p=0.01476\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would reject the null hypothesis that the difference in the center of the distributions is zero. We conclude that the Spray Concentration A is more effective at killing flies (bugs). Comment The hypotheses for the Wilcoxon Rank Sum Test are difficult to write out in simple mathematical statements. The test is often referred to as a test of medians, but goes deeper than this. Technically, it allows us to determine if one distribution is stochastically larger than another. In other words, if one distribution typically gives larger values than does another distribution. If the distributions are identically shaped and have the same spread, then this implies the medians (and means) are different. Thus, the hypotheses for the test can be written mathematically as \\[ H_0: \\text{difference in medians} = 0 \\] \\[ H_a: \\text{difference in medians} \\neq 0 \\] or even as \\[ H_0: \\mu_1-\\mu_2 = 0 \\] \\[ H_a: \\mu_1-\\mu_2 \\neq 0 \\] However, it is important to remember that the estimated difference in location parameters that results from the test does not provide a measurement on either of these things. However, the \\(p\\)-value can lead us to determine whether to reject, or fail to reject the null, whichever of the above hypotheses is used. As stated in the R help file for this test ?wilcox.test() â€œthe estimator for the difference in location parameters does not estimate the difference in medians (a common misconception) but rather the median of the difference between a sample from [the first population] and a sample from [the second population].â€ These are technical details that most people ignore without encountering too much difficulty. However, it does remind us that the t test is more easily interpreted whenever it is appropriate to use that test. Final Comment Notice that when the sample size is large the distribution of the test statistic can be approximated by a normal distribution. Most software applications use this approximation when the sample size is over \\(50\\), i.e., for \\(n\\geq50\\) because computing all the possible sums of ranks becomes incredibly time consuming. Thus, for large sample sizes the results will be almost identical whether the Wilcoxon Tests or a t Test is used. Quantitative Y | Categorical X (3+ Groups) Graphics Y is a single quantitative variable of interest, like â€œheightsâ€ of BYU-Idaho students. X is a categorical (qualitative) variable like which Math 221 you took, 221A, 221B, or 221C. In other words, X has three or more groups. So â€œClassrankâ€ could be X, with groups â€œFreshmanâ€, â€œSophomoreâ€, â€œJuniorâ€, and â€œSeniorâ€. Test(s) used for this category: One-Way ANOVA (means) Block Design Kruskal-Wallis Rank Sum Test (distributions) Analysis : College Studentâ€™s Food Perception Used comparing several groups based on means or distributions: How long are their feet based of width and gender? Are there certain months of the year that are associated with children having longer feet, on average, than others? Does a college studentâ€™s perception of food result in a change in their weight? (Kruskal-Wallis Rank Sum Test) # Read data food <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv\") # Data transformation and cleaning food_cleaned <- food %>% # Convert weight to numeric right away mutate(weight = as.numeric(weight)) %>% # Remove rows with NA weight or missing drink values filter(!is.na(weight) & !is.na(drink)) %>% # Create a new column `Food Perception Score` mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) # Create plots boxplot(weight ~ `Food Perception Score`, data=food_cleaned, col=\"lightgreen\", xlab=\"Food Perception Score\", main=\"Weight of College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=food_cleaned, pch=16, vertical=TRUE, add=TRUE, col=\"palegreen4\") Tests One-way ANOVA Each experimental unit is assigned to exactly one factor-level combination. Another way to say this is â€œone measurement per individualâ€ (no repeated measures) and â€œequal numbers of individuals per groupâ€. Overview An ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA can be assumed to be normally distributed when seen in a Q-Q Plot.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out in a Residuals versus fitted-values plot.) comparing means Hypotheses For a One-way ANOVA \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] Mathematical Model A typical model for a one-way ANOVA is of the form \\[ Y_{ij} = \\mu_i + \\epsilon_{ij} \\] where \\(\\mu_i\\) is the mean for level (group) \\(i\\), and \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) is the error term for each point \\(j\\) within level (group) \\(i\\). R Instructions Console Help Command: ?aov() myaov is some name you come up with to store the results of the aov() test. Y must be a â€œnumericâ€ vector of the quantitative response variable. X is a qualitative variable (should have class(X) equal to factor or character. If it does not, use as.factor(X) inside the aov(Y ~ as.factor(X),...) command. YourDataSet is the name of your data set. Perform the ANOVA myaov <- aov(Y ~ X, data=YourDataSet) summary(myaov) Diagnose ANOVA Assumptions par(mfrow=c(1,2)) plot(myaov, which=1:2) Example Code Hover your mouse over the example codes to learn more. Perform the ANOVA chick.aovÂ <-Â  Saves the results of the ANOVA test as an object named â€˜chick.aovâ€™. aov( â€˜aov()â€™ is a function in R used to perform the ANOVA. weight Y is â€˜weightâ€™, which is a numeric variable from the chickwts dataset. Â ~Â  â€˜~â€™ is the tilde symbol used to separate the Y and X in a model formula. feed, X is â€˜feedâ€™, which is a qualitative variable in the chickwts dataset, or more specifically, a factor with six levels: â€œcaseinâ€, â€œhorsebeanâ€, and so onâ€¦ Use str(chickwts) to see this. Â dataÂ =Â chickwts) â€˜chickwtsâ€™ is a dataset in R. summary( â€˜summary()â€™ shows the results of the ANOVA. chick.aov) â€˜chick.aovâ€™ is the name of the ANOVA. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to View OutputÂ  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## feed 5 231129 46226 15.37 5.94e-10 *** ## Residuals 65 195556 3009 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Diagnose the ANOVA par( â€˜parâ€™ is a R function that can be used to set or query graphical parameters. mfrowÂ =Â c(1,2)) The mfrow parameter controls â€œmultiple frames on a rowâ€. In this case, the c(1,2) specifies 1 row of 2 plots. This will cause the two diagnostic plots to be placed side-by-side. plot( â€˜plotâ€™ is a R function for the plotting of R objects. chick.aov, â€˜chick.aovâ€™ is the name of the ANOVA. Â whichÂ =Â 1:2) The which=1:2 selects â€œwhichâ€ of 6 available plots we want to have graphed. In this case, 1 shows the Residuals vs Fitted, and 2 shows the Normal QQ-plot. Both are needed to check the ANOVA assumptions. Â Click to View OutputÂ  Click to View Output. Explanation Analysis of variance (ANOVA) is often applied to the scenario of testing for the equality of three or more means from (possibly) separate normal distributions of data. The normality assumption is required. No matter the sample size. If the distributions are skewed then a nonparametric test should be applied instead of ANOVA. One-Way ANOVA One-way ANOVA is when a completely randomized design is used with a single factor of interest. A typical mathematical model for a one-way ANOVA is of the form \\[ Y_{ik} = \\mu_i + \\epsilon_{ik} \\quad (\\text{sometimes written}\\ Y_{ik} = \\mu + \\alpha_i + \\epsilon_{ik}) \\] where \\(\\mu_i\\) is the mean of each group (or level) \\(i\\) of a factor, and \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) is the error term. The plot below demonstrates what these symbols represent. Note that the notation \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) states that we are assuming the error term \\(\\epsilon_{ik}\\) is normally distributed with a mean of 0 and a standard deviation of \\(\\sigma\\). Hypotheses The aim of ANOVA is to determine which hypothesis is more plausible, that the means of the different distributions are all equal (the null), or that at least one group mean differs (the alternative). Mathematically, \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_m = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\quad \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\}. \\] In other words, the goal is to determine if it is more plausible that each of the \\(m\\) different samples (where each sample is of size \\(n\\)) came from the same normal distribution (this is what the null hypothesis claims) or that at least one of the samples (and possibly several or all) come from different normal distributions (this is what the alternative hypothesis claims). Visualizing the Hypotheses The first figure below demonstrates what a given scenario might look like when all \\(m=3\\) samples of data are from the same normal distribution. In this case, the null hypothesis \\(H_0\\) is true. Notice that the variability of the sample means is smaller than the variability of the points. The figure below shows what a given scenario might look like for \\(m=3\\) samples of data from three different normal distributions. In this case, the alternative hypothesis \\(H_a\\) is true. Notice that the variability of the sample means, i.e., \\((\\bar{x}_1,\\bar{x}_2,\\bar{x}_3)\\), is greater than the variability of the points. Explaining the Name The above plots are useful in understanding the mathematical details behind ANOVA and why it is called analysis of variance. Recall that variance is a measure of the spread of data. When data is very spread out, the variance is large. When the data is close together, the variance is small. ANOVA utilizes two important variances, the between groups variance and the within groups variance. Between groups varianceâ€“a measure of the variability in the sample means, the \\(\\bar{x}\\)â€™s. Within groups varianceâ€“a combined measure of the variability of the points within each sample. The plot below combines the information from the previous plots for ease of reference. It emphasizes the fact that when the null hypothesis is true, the points should have a large variance (be really spread out) while the sample means are relatively close together. On the other hand, when the points are relative close together within each sample and the sample means have a large variance (are really spread out) then the alternative hypothesis is true. This is the theory behind analysis of variance, or ANOVA. Calculating the Test Statistic, \\(F\\) The ratio of the â€œbetween groups variationâ€ to the â€œwithin groups variationâ€ provides the test statistic for ANOVA. Note that the test statistic of ANOVA is an \\(F\\) statistic. \\[ F = \\frac{\\text{Between groups variation}}{\\text{Within groups variation}} \\] It would be good to take a minute and review the \\(F\\) distribution. The \\(p\\)-value for ANOVA thus comes from an \\(F\\) distribution with parameters \\(p_1 = m-1\\) and \\(p_2 = n-m\\) where \\(m\\) is the number of samples and \\(n\\) is the total number of data points. A Deeper Look at Variance It is useful to take a few minutes and explain the word variance as well as mathematically define the terms â€œwithin group varianceâ€ and â€œbetween groups variance.â€ Variance is a statistical measure of the variability in data. The square root of the variance is called the standard deviation and is by far the more typical measure of spread. This is because standard deviation is easier to interpret. However, mathematically speaking, the variance is the more important measurement. As mentioned previously, the variance turns out to be the key to determining which hypothesis is the most plausible, \\(H_0\\) or \\(H_a\\), when several means are under consideration. There are two variances that are important for ANOVA, the â€œwithin groups varianceâ€ and the â€œbetween groups variance.â€ Recall that the formula for computing a sample variance is given by \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})^2}{n-1} \\quad\\leftarrow \\frac{\\text{sum of squares}}{\\text{degrees of freedom}} \\] This formula has a couple of important pieces that are so important they have been given special names. The \\(n-1\\) in the denominator of the formula is called the â€œdegrees of freedom.â€ The other important part of this formula is the \\(\\sum_{i=1}^n(x_i - \\bar{x})^2\\), which is called the â€œsum of squared errorsâ€ or sometimes just the â€œsum of squaresâ€ or â€œSSâ€ for short. Thus, the sample variance is calculated by computing a â€œsum of squaresâ€ and dividing this by the â€œdegrees of freedom.â€ It turns out that this general approach works for many different contexts. Specifically, it allows us to compute the â€œwithin groups varianceâ€ and the â€œbetween groups variance.â€ To introduce the mathematical definitions of these two variances, we need to introduce some new notation. Let \\(\\bar{y}_{i\\bullet}\\) represent the sample mean of group \\(i\\) for \\(i=1,\\ldots,m\\). Let \\(n_i\\) denote the sample size in group \\(i\\). Let \\(\\bar{y}_{\\bullet\\bullet}\\) represent the sample mean of all \\(n = n_1+n_2+\\cdots+n_m\\) data points. The mathematical calculations for each of these variances is given as follows. \\[ \\text{Between groups variance} = \\frac{\\sum_{i=1}^m (\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2}{m-1} \\leftarrow \\frac{\\text{Between groups sum of squares}}{\\text{Between groups degrees of freedom}} \\] \\[ \\text{Within groups variance} = \\frac{\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2}{n-m} \\leftarrow \\frac{\\text{Within groups sum of squares}}{\\text{Within groups degrees of freedom}} \\] A Fabricated Example The following table provides three samples of data: A, B, and C. These samples were randomly generated from normal distributions using a computer. The true means \\(\\mu_1, \\mu_2\\), and \\(\\mu_3\\) of the normal distributions are thus known, but withheld from you at this point of the example. A B C 13.15457 13.17463 16.66831 12.65225 12.16277 15.54719 13.73061 12.76905 16.63074 14.43471 13.38524 15.06726 13.79728 12.02690 15.57534 13.88599 13.24651 15.99915 12.77753 12.58386 15.58995 13.81536 12.64615 16.99429 13.03635 12.52055 15.47153 14.26062 14.03566 16.13330 An ANOVA will be performed with the sample data to determine which hypothesis is more plausible: \\[ H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\in \\{1,\\ldots,m\\} \\] To perform an ANOVA, we must compute the between groups variance and the within groups variance. This requires the Between groups sums of squares, within groups sums of squares, between groups degrees of freedom, and the within groups degrees of freedom. Note that to get the sums of squares, we first had to calculate \\(\\bar{y}_{1\\bullet}\\), \\(\\bar{y}_{2\\bullet}\\), \\(\\bar{y}_{3\\bullet}\\), and \\(\\bar{y}_{\\bullet\\bullet}\\) where the 1, 2, 3 corresponds to Samples A, B, and C, respectively. After some work, we find these values to be \\[ \\bar{y}_{1\\bullet} = 13.55, \\quad \\bar{y}_{2\\bullet} = 12.86 \\quad \\bar{y}_{3\\bullet} = 15.97 \\] and \\[ \\bar{y}_{\\bullet\\bullet} = \\frac{13.55+12.86+15.97}{3} = 14.13 \\] Using these values we can then compute the between groups sum of squares and the within groups sum of squares according to the formulas stated previously. This process is very tedious and will not be demonstrated. Only the results are shown in the following table which summarizes all the important information. Â  Degrees of Freedom Sum of Squares Variance F-value p-value Between groups 2 53.3 26.67 70.2 2e-11 Within groups 27 10.3 0.38 ANOVA Table In general, the ANOVA table is created by Â  Degrees of Freedom Sum of Squares Variance F-value p-value Between groups \\(m-1\\) \\(\\sum_{i=1}^m n_i(\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) \\(\\frac{\\text{Between groups variance}}{\\text{Within groups variance}}\\) \\(F\\)-distribution tail probability Within groups \\(n-m\\) \\(\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) ANOVA Assumptions{#residuals} The requirements for an analysis of variance (the assumptions of the test) are two-fold and concern only the error terms, the \\(\\epsilon_{ik}\\). The errors are normally distributed. The variance of the errors is constant. Both of these assumptions were stated in the mathematical model where we assumed that \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\). Checking ANOVA Assumptions To check that the ANOVA assumptions are satisfied, it is required to check the data in each group for normality using QQ-Plots. Also, the sample variance of each group must be relatively constant. The fastest way to check these two assumptions is by analyzing the residuals. An ANOVA residual is defined as the difference between the observed value of \\(y_{ik}\\) and the mean \\(\\bar{y}_{i\\bullet}\\). Mathematically, \\[ r_{ik} = y_{ik} - \\bar{y}_{i\\bullet} \\] One QQ-Plot of the residuals will provide the necessary evidence to decide if it is reasonable to assume that the error terms are normally distributed. Also, the constant variance can be checked visually by using what is known as a residuals versus fitted values plot. For the Fabricated Example above, the QQ-Plot and residuals versus fitted values plots show the two assumptions of ANOVA appear to be satisfied. Examples: chickwts (One-way) Block Design Repeated measures or other factors that group individuals into similar groups (blocks) are included in the study design. Overview A typical model for a block design is of the form \\[ Y_{lijk} = \\mu + B_l + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijlk} \\] where \\(\\mu\\) is the grand mean, \\(B_l\\) is the blocking factor, \\(\\alpha_i\\) is one factor with at least two levels, \\(\\beta_j\\) is another factor with at least two levels, \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors, and \\(\\epsilon_{ijlk} \\sim N(0,\\sigma^2)\\) is the error term. Only one block and one factor is required. Multiple blocks and multiple factors are allowed. It is not required to include interaction terms. The error term is always required. R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ Block+X1+X2+X1:X2, data=YourDataSet) Perform the test summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a â€œnumericâ€ vector of the quantitative response variable. Block is a qualitative variable that is not of direct interest, but is included in the model to account for variability in the data. It should have class(Block) equal to either factor or character. Use as.factor() if it does not. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. If it does not, use as.factor(X2). Note that factors C, D, and so on could also be added + to the model if desired. X1:X2 denotes the interaction of the factors X1 and X2. It is not required and should only be included if the interaction term is of interest. YourDataSet is the name of your data set. Examples: ChickWeight Kruskal-Wallis Rank Sum Test Allows for deciding if several samples come from the same population or if at least one sample comes from a different population. Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated. Can also be used when data is ordered (ordinal) but does not have an exact measurement. Best used when the distribution of the data is not normal. ANOVA is appropriate when the data is normal. comparing distributions Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, â€œis a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly. In many cases, this is practically equivalent to the mean of at least one population differing from the others.â€ Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable. g is a qualitative grouping variable defining which groups each value in x belongs to. It must either be a character vector or a factor vector. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. weight â€˜weightâ€™ is a numeric variable from the chickwts dataset that represents the quantatitive response variable. Â ~Â  â€˜~â€™ is the tilde symbol used to separate the left- and right-hand side in a model formula. feed, â€˜feedâ€™ is a qualitative grouping variable in the chickwts dataset. Â dataÂ =Â chickwts) â€˜chickwtsâ€™ is a dataset in R. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to View OutputÂ  Click to View Output. ## ## Kruskal-Wallis rank sum test ## ## data: weight by feed ## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07 Alternatively, you could use the following approach as well. kruskal.test(list(x, y, z)) x is a numeric vector of data values that represents the quantatitive response variable for the first group. y is a numeric vector of data values that represents the quantatitive response variable for the second group. z is a numeric vector of data values that represents the quantatitive response variable for the third group. Note that more than three vectors of data could be included inside of list(). Example Code Hover your mouse over the example codes to learn more. feed1 <- filter(chickwts, feed == â€œhorsebeanâ€) This splits out the first group of feed (horsebean) from the chickwts dataset. feed2 <- filter(chickwts, feed == â€œlinseedâ€) This splits out the second group of feed (linseed) from the chickwts dataset. feed3 <- filter(chickwts, feed == â€œsoybeanâ€) This splits out the third group of feed (soybean) from the chickwts dataset. feed4 <- filter(chickwts, feed == â€œsunflowerâ€) This splits out the fourth group of feed (sunflower) from the chickwts dataset. feed5 <- filter(chickwts, feed == â€œmeatmealâ€) This splits out the fifth group of feed (meatmeal) from the chickwts dataset. feed6 <- filter(chickwts, feed == â€œcaseinâ€) This splits out the sixth group of feed (casein) from the chickwts dataset. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. list(feed1$weight,feed2$weight,feed3$weight,feed4$weight,feed5$weight,feed6$weight)) A list of numeric vecotrs that represent the quantitative response variable for each group. Â Click to View OutputÂ  Click to View Output. Load library(tidyverse) to run this code in R. ## ## Kruskal-Wallis rank sum test ## ## data: list(feed1$weight, feed2$weight, feed3$weight, feed4$weight, feed5$weight, feed6$weight) ## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07 Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population. The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples. The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity. Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated. (Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\). Note that the notation \\(n_i\\) allows for each sample to be a different size. In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\). Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest. Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\). Award any tied values the average of the ranks of those values that are tied. In the bottle-cap data we have the following. Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample. Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\). In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\). In the bottle-cap data, \\(\\bar{R}_1 = \\frac{R_1}{n_1} = \\frac{24}{5} = 4.8\\) \\(\\bar{R}_2 = \\frac{R_2}{n_2} = \\frac{14}{3} = 4.667\\) \\(\\bar{R}_3 = \\frac{R_3}{n_3} = \\frac{40}{4} = 10.0\\) Step 5: If there are no ties, calculate the test statistic \\(H\\) by the formula \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] See the original article for the formula to use when there are ties present in the data. In the bottle-cap data, \\(H = 5.656\\). To understand the logic behind the test statistic \\(H\\), note that the null hypothesis assumes that each of the \\(C\\) samples are taken from the same population. Under this assumption, the ranks assigned to each sample should represent a uniform sample of the ranks \\(1,\\ldots,N\\). If the null hypothesis were true, we would expect the average of the ranks from each group, the \\(\\bar{R}_i\\), to be roughly about the same. More precisely, using the properties of the discrete uniform distribution (see wikipedia), we expect each \\(\\bar{R}_i\\) to be close to \\(\\frac{1}{2}(N+1)\\), which is the mean of the discrete uniform distribution. That \\(\\bar{R}_i\\) should be close to this value comes from the fact that the mean of a distribution of means is equal to the population mean. Further, the variance of \\(\\bar{R}_i\\) is given by the population variance, \\(\\frac{1}{12}(N^2-1)\\) in this case, divided by the sample size \\(n_i\\). Finally, the \\(\\frac{N-1}{N}\\) is a correction factor that will not be explained here. The full article explains the details about how a multivariate normal distribution is used to uncover the formula for \\(H\\). Thus, understanding \\(H\\) completely will require an advanced degree in statistical theory. However, the short story is that the distribution of the test statistic \\(H\\) is approximately a chi squared distribution with \\(C-1\\) degrees of freedom and has the form of a sum of values consisting of the (observed mean - expected mean)^2/(variance of the observed mean). Step 6: Calculate the p-value of \\(H\\) using a chi squared distribution with \\(C-1\\) degrees of freedom. \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] In the bottle-cap data, \\(p = 0.059\\), which represents the probability of \\(H\\) being as extreme or more extreme than the observed value of 5.656, assuming the null hypothesis is true. Examples: wages, pigweights Quantitative Y | Multiple Categorical X Graphics Y is a single quantitative variable of interest, like â€œheightsâ€ of BYU-Idaho students. X1 is a categorical (qualitative) variable like gender, with levels â€œboyâ€ and â€œgirl.â€ X2 is another categorical (qualitative) variable like â€œClassrankâ€ with levels â€œFreshmanâ€, â€œSophomoreâ€, and â€œJuniorâ€. Of course, both X1 and X2 can have as many levels as you want to work with, but must have at least two levels each. Test(s) used for this category: Two-Way ANOVA (means) Used to answer THREE questions: the effects of factor 1 on the quantitative variable the effects of factor 2 on the quantitative variable the interaction between the two factors best paired with numerical summary looking at mean Analysis : Two-way ANOVA: Final Beginning Algebra Grades Question Examples: Based on your p-values and these graphs, which of the following is a correct conclusion to reach? xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1))) ANSWER : Among fourth graders, right handed boys have longer feet on average than left handed boys while the opposite is true for girls. Does the amount a student participates in the course affect the average final Beginning Algebra grades? Does the semester in which a student takes Beginning Algebra affect their average final grade? Does a studentâ€™s participation level vary depending on the semester? (Alternatively, does the semester influence a studentâ€™s participation?) BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") BAFinalGrades$Semester <- factor(BAFinalGrades$Semester, levels = c(\"Fall\",\"Winter\",\"Spring\")) ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=1)) + geom_point(color=\"firebrick1\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Spring Semester Seems to Have Higher Final Grades\", x=\"Semester\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades$Participation <- factor(BAFinalGrades$Participation, levels = c(\"Low\",\"Moderate\",\"High\")) ggplot(BAFinalGrades, aes(x=Participation, y=FinalGrade, group=1)) + geom_point(color=\"darkred\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Participation Seems to Lead to Higher Grades\", x=\"Participation\", y=\"Beginning Algebra Final Grades\") + theme_minimal() ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=Participation, color=Participation)) + geom_point() + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Beginning Algebra Seems Better during the Spring Semester\", x=\"BYU-Idaho Semesters\", y=\"Beginning Algebra Final Grades\") + theme_minimal() Tests Two-way ANOVA Overview A two-way ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses. Writing out the hypotheses can be very involved depending on whether you use the official â€œeffects modelâ€ notation (very mathematically correct) or a simplified â€œmeans modelâ€ notation (which isnâ€™t very mathematically correct, but gets the idea across in an acceptable way). Means Model Effects Model The first set of hypotheses are a â€œone-wayâ€ set of hypotheses for the first factor of the ANOVA. Factor: X1 with say, levels \\(A\\) and \\(B\\). \\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a â€œone-wayâ€ set of hypotheses, but for the second factor of the ANOVA. Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\mu_C = \\mu_D = \\mu_E = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one}\\ i\\in\\{1=C,2=D,3=E\\} \\] The third set of hypotheses are the most interesting hypotheses in a two-way ANOVA. These are called the interaction hypotheses. They test to see if the levels of one of the factors, say \\(X1\\), impact \\(Y\\) differently for the differing levels of the other factor, \\(X2\\). The hypotheses read formally as \\[ H_0: \\text{The effect of the first factor on Y} \\\\ \\text{is the same for all levels of the second factor.} \\] \\[ H_a: \\text{The effect of the first factor on Y is not the same} \\\\ \\text{for all levels of the second factor.} \\] A mathematically correct way to state the two-way ANOVA model is with the equation \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] In this model, \\(\\mu\\) is the grand mean (which is the average Y-value ignoring all information contained in the factors); \\(\\alpha_i\\) is the first factor \\(X1\\) with levels \\(A\\) and \\(B\\) (though there could be more levels in \\(X1\\) depending on your data); \\(\\beta_j\\) is the second factor with levels \\(C\\), \\(D\\), and \\(E\\) (though there could be fewer or more levels to this factor depending on your data); \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors which has \\(2\\times3=6\\) (may differ for your data) levels; and \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) is the normally distributed error term for each point \\(k\\) found within level \\(i\\) of \\(X1\\) and level \\(j\\) of \\(X2\\). This model allows us to more formally state the hypotheses as First factor \\(X1\\) having say, levels \\(A\\) and \\(B\\). \\[ H_0: \\alpha_A = \\alpha_B = 0 \\] \\[ H_a: \\alpha_i \\neq 0 \\ \\text{for at least one}\\ i\\in\\{1=A,2=B\\} \\] Second factor \\(X2\\) with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\beta_C = \\beta_D = \\beta_E = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j\\in\\{1=C,2=D,3=E\\} \\] Does the effect of the first factor (\\(X1\\)) change for the different levels of the second factor (\\(X2\\))? In other words, is there an interaction between the two factors \\(X1\\) and \\(X2\\)? \\[ H_0: \\alpha\\beta_{ij} = 0 \\ \\text{for all } i,j \\] \\[ H_a: \\alpha\\beta_{ij} \\neq 0 \\ \\text{for at least one } i,j \\] R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ X1+X2+X1:X2, data=YourDataSet) Perform the ANOVA summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a â€œnumericâ€ vector of the quantitative response variable. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. Note that factors X3, X4, and so on could also be added + to the model if desired, but this would create a three-way, or four-way ANOVA model, and so on. X1:X2 denotes the interaction of the factors X1 and X2. It is not required, but is usually included. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. warp.aovÂ <-Â  Saves the results of the ANOVA test as an object named â€˜warp.aovâ€™. aov( â€˜aov()â€™ is a function in R used to perform the ANOVA. breaks \\(Y\\) is â€˜breaksâ€™, which is a numeric variable from the warpbreaks dataset. Â ~Â  â€˜~â€™ is the tilde symbol used to separate the left- and right-hand side in a model formula. woolÂ  The first factor \\(X1\\) is â€˜woolâ€™, which is a qualitative variable in the warpbreaks dataset. In this case, wool is a factor with two levels. Use str(warpbreaks) to see this. +Â tensionÂ  The second factor \\(X2\\) is â€˜tensionâ€™, which is another qualitative variable in the warpbreaks dataset. In this case, tension is a factor with three levels. Use str(warpbreaks) to see this. +Â wool:tension, The interaction of the two factors: wool and tension. Â dataÂ =Â warpbreaks) â€˜warpbreaksâ€™ is a dataset in R. summary( â€˜summary()â€™ shows the results of the ANOVA. warp.aov) â€˜warp.aovâ€™ is the name of the ANOVA. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to View OutputÂ  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## wool 1 451 450.7 3.765 0.058213 . ## tension 2 2034 1017.1 8.498 0.000693 *** ## wool:tension 2 1003 501.4 4.189 0.021044 * ## Residuals 48 5745 119.7 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 par( â€˜parâ€™ is a R function that can be used to set or query graphical parameters. mfrowÂ =Â c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. plot( â€˜plotâ€™ is a R function for the plotting of R objects. warp.aov, â€˜warp.aovâ€™ is the name of the ANOVA. Â whichÂ =Â 1:2) Will show the Residuals vs Fitted and the Normal QQ-plot to check the ANOVA assumptions. Â Click to View OutputÂ  Click to View Output. Explanation Hypotheses in Two-way ANOVA The hypotheses that can be tested in a two-way ANOVA that includes an interaction term are three-fold. Hypotheses about \\(\\alpha\\) where \\(\\alpha\\) has \\(m\\) levels. \\[ H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_m = 0 \\] \\[ H_a: \\alpha_i \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\} \\] Hypotheses about \\(\\beta\\) where \\(\\beta\\) has \\(q\\) levels. \\[ H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_q = 0 \\] \\[ H_a: \\beta_j \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,q\\} \\] Hypotheses about the interaction term \\(\\alpha\\beta\\). \\[ H_0: \\text{the effect of one factor is the same across all levels of the other factor} \\] \\[ H_a: \\text{the effect of one factor differs for at least one level of the other factor} \\] Expanding the ANOVA Model{#expanding} It turns out that more can be done with ANOVA than simply checking to see if the means of several groups differ. Reconsider the mathematical model of two-way ANOVA. \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] This model could be expanded to include any number of new terms in the model. The power of this approach is in the several questions (hypotheses) that can be posed to data simultaneously. What happens if your ANOVA fails both requirement tests? Example: BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") ## Rows: 204 Columns: 3 ## â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ## Delimiter: \",\" ## chr (2): Semester, Participation ## dbl (1): FinalGrade ## ## â„¹ Use `spec()` to retrieve the full column specification for this data. ## â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message. BAanova <- aov(FinalGrade ~ Semester + Participation + Semester:Participation, data=BAFinalGrades) par(mfrow=c(1,2)) plot(BAanova, which=1:2, pch=16) The residual plot shows that the constant (equal) variance assumption raises some concern as the points within each group have varying spread. The spread of each group seems to gets closer and closer as we go from left to right as well as some straying point from the more clustered groups. Thus, showing the variance of the residuals are not constant and do not accurately show a relationship between each variable. For the Q-Q plot, it shows that the residuals of the data is not quite normally distributed. There are visible points at the lower and upper ends of the graph deviating away form the diagonal line, which represents a normal distribution. Thus, showing us that the residuals of the data are not normal distributed. Although both graphs indicate that the data fails to meet both requirements, this primarily means our findings may be less reliable due to the abnormalities in the data. As a result, we should interpret the results cautiously as we proceed with our Two-Way ANOVA test! Examples: warpbreaks (Two-way), CO2 (three-way) Quantitative Y | Quantitative X Graphics Y is a single quantitative variable of interest, like â€œheightâ€. X is another single quantitative variable of interest, like â€œshoe-sizeâ€. This would imply we are using â€œshoe-sizeâ€ (X) to explain â€œheightâ€ (Y). Test(s) used for this category: Simple Linear Regression Analysis : Simple Linear Regression: BYU-Idaho Math Tutoring Used to find the relationship between one independent variable (predictor) and one dependent variable(outcome): Is there a relationship between the length of childrenâ€™s feet and the width of their foot? Example: Does the amount of tutoring a student receives correlate with their chapter exam scores? (Simple Linear Regression) ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams) Tests Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\). Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$\\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$$ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is â€œequal toâ€ the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to â€œGodâ€™s Lawâ€ or â€œNatural Lawâ€. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are â€œrandomâ€ and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)â€¦ \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced â€œy-hatâ€, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what followsâ€¦ \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, â€œthe change in the average y-value for a one unit change in the x-value.â€ It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, â€œthe average y-value when x is zero.â€ It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful. R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your lm() code into mylm name. lm( lm(â€¦) is an R function that stands for â€œLinear Modelâ€. It performs a linear regression analysis for Y ~ X. YÂ  Y is your quantitative response variable. It is the name of one of the columns in your data set. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X. X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value. Â data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X. ) Closing parenthesis for the lm(â€¦) function. summary(mylm) The summary command allows you to print the results of your linear regression that were previously saved in mylm name. Â Click to Show OutputÂ  Click to View Output. Example output from a regression. Hover each piece to learn more. Call: lm(formula = dist ~ speed, data = cars) This is simply a statement of your original lm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(â€¦). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min Â  -29.069 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -9.525 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -2.272 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -2.272 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-2.272) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  9.215 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  43.201 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  t value To learn more about the â€œt valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  Pr(>|t|) The â€œPrâ€ stands for â€œProbabilityâ€ and the â€œ(> |t|)â€ stands for â€œmore extreme than the observed t-valueâ€. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the â€œp-valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. (Intercept) This always says â€œInterceptâ€ for any lm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -17.5791 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  6.7584 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -2.601 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the intercept (-17.5791) by its standard error (6.7584). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of -17.5791 is -2.601 standard errors (6.7584) from zero, which is a fairly surprising distance as shown by the p-value. 0.0123 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). speed This is always the name of your X-variable in your lm(Y ~ X, â€¦). Â  3.9324 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.4155 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 9.464 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the slope (3.9324) by its standard error (0.4155). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from zero, which is a really surprising distance as shown by the smallness of the p-value. 1.49e-12 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE. Â 15.38 For this particular regression, the estimate of \\(\\sigma\\) is 15.38. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\). Â on 48 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 50 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 50-2 = 48. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO. Â 0.6511, In this particular regression, 65.11% of the variation in stopping distance dist is explained by the regression model using speed of the car. Â Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data. Â 0.6438 In this case, the value of 0.6438 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is found as the ratio of the MSR/MSE where MSR = SSR/(p-1) and MSE = SSE/(n-p) where n is the sample size and p is the number of parameters in the regression model. Â 89.57 This is the value of the F-statistic for the lm(dist ~ speed, data=cars) regression. Note that SSE = sum( cars.lm$res^2 ) = 11353.52 with n - p = 50 - 2 = 48 degrees of freedom for this data. Further, SSR = sum( (cars.lm$fit - mean(cars$dist))^2 ) = 21185.46 with p - 1 = 1 degree of freedom. So MSR = 21185.46 and MSE = 11353.52 / 48 = 236.5317. So MSR / MSE = 21185.46 / 236.5317 = 89.56711. Â on 1 and 48 DF, The 1 degree of freedom is the SSR degrees of freedom (p-1). The 48 is the SSE degrees of freedom (n-p). Â p-value: 1.49e-12 The p-value for an F-statistic is found by the code pf(89.56711, 1, 48, lower.tail=FALSE), which gives the probability of being more extreme than the observed F-statistic in an F distribution with 1 and 48 degrees of freedom. Check Assumptions 1, 2, 3, and 5 When making it in an r chunk make your height 3! { r, fig.height = 3 } par( The par(â€¦) command stands for â€œGraphical PARametersâ€. It allows you to control various aspects of graphics in Base R. mfrow= This stands for â€œmultiple frames filled by rowâ€, which means, put lots of plots on the same row, starting with the plot on the left, then working towards the right as more plots are created. c( The combine function c(â€¦) is used to specify how many rows and columns of graphics should be placed together. 1, This specifies that 1 row of graphics should be produced. 3 This states that 3 columns of graphics should be produced. ) Closing parenthesis for c(â€¦) function. ) Closing parenthesis for par(â€¦) function. plot( This version of plot(â€¦) will actually create several regression diagnostic plots by default. mylm, This is the name of an lm object that you created previously. which= This allows you to select â€œwhichâ€ regression diagnostic plots should be drawn. 1 Selecting 1, would give the residuals vs.Â fitted values plot only. : The colon allows you to select more than just one plot. 2 Selecting 2 also gives the Q-Q Plot of residuals. If you wanted to instead you could just use which=1 to get the residuals vs fitted values plot, then you could use qqPlot(mylm$residuals) to create a fancier Q-Q Plot of the residuals. ) Closing parenthesis for plot(â€¦) function. plot( This version of plot(â€¦) will be used to create a time-ordered plot of the residuals. The order of the residuals is the original order of the x-values in the original data set. If the original data set doesnâ€™t have an order, then this plot is not interesting. mylm The lm object that you created previously. $ This allows you to access various elements from the regression that was performed. residuals This grabs the residuals for each observation in the regression. ) Closing parenthesis for plot(â€¦) function. Â Click to Show OutputÂ  Click to View Output. Which regression assumption(s) does the residuals versus fitted values plot diagnose? Linear Relation Constance Variance Which regression assumption(s) does the Q-Q Plot of the residuals diagnose? Normal Errors Which regression assumption(s) does the Residuals versus Order plot diagnose? (Remember, this plot can only be created when the data was collected in a specific order.) Independent Errors Plotting the Regression Line Base R ggplot2 To add the regression line to a scatterplot use the abline(...) command: plot( The plot(â€¦) function is used to create a scatterplot with a y-axis (the vertical axis) and an x-axis (the horizontal axis). YÂ  This is the â€œresponse variableâ€ of your regression. The thing you are interested in predicting. This is the name of a â€œnumericâ€ column of data from the data set called YourDataSet. ~Â  The tilde â€œ~â€ is used to relate Y to X and can be found on the top-left key of your keyboard. X,Â  This is the explanatory variable of your regression. It is the name of a â€œnumericâ€ column of data from YourDataSet. data= The data= statement is used to specify the name of the data set where the columns of â€œXâ€ and â€œYâ€ are located. YourDataSet This is the name of your data set, like KidsFeet or cars or airquality. ) Closing parenthesis for plot(â€¦) function. abline( This stands for â€œaâ€ (intercept) â€œbâ€ (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm This is the name of an lm(â€¦) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(â€¦) function will locate these two values from within mylm and use them to add a line to your current plot(â€¦). ) Closing parenthesis for abline(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm) You can customize the look of the regression line with abline( This stands for â€œaâ€ (intercept) â€œbâ€ (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm, This is the name of an lm(â€¦) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(â€¦) function will locate these two values from within mylm and use them to add a line to your current plot(â€¦). lty= The lty= stands for â€œline typeâ€ and allows you to select between 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. 1, This creates a solid line. Remember, other options include: 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. lwd= The lwd= allows you to specify the width of the line. The default width is 1. Using lwd=2 would double the thickness, and so on. Any positive value is allowed. 1, Default line width. To make a thicker line, us 2 or 3â€¦ To make a thinner line, try 0.5, but 1 is already pretty thin. col= This allows you to specify the color of the line using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). â€œsomeColorâ€ Type colors() in R for options. ) Closing parenthesis for abline(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm, lty=1, lwd=1, col=\"firebrick\") You can add points to the regression withâ€¦ points( This is like plot(â€¦) but adds points to the current plot(â€¦) instead of creating a new plot. newYÂ  newY should be a column of values from some data set. Or, use points(newX, newY) to add a single point to a graph. ~Â  This links Y to X in the plot. newX,Â  newX should be a column of values from some data set. It should be the same length as newY. If just a single value, use points(newX, newY) instead. data=YourDataSet,Â  If newY and newX come from a dataset, then use data= to tell the points(â€¦) function what data set they come from. If newY and newX are just single values, then data= is not needed. col=â€œskyblueâ€, This allows you to specify the color of the points using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). pch=16 This allows you to specify the type of plotting symbol to be used for the points. Type ?pch and scroll half way down in the help file that appears to learn about other possible symbols. ) Closing parenthesis for points(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) points(7,40, pch=16, col=\"skyblue\", cex=2) text(7,40, \"New Dot\", pos=3, cex=0.5) points(dist ~ speed, data=filter(cars, mylm$res > 2), cex=.8, col=\"red\") abline(mylm, lty=1, lwd=1, col=\"firebrick\") To add the regression line to a scatterplot using the ggplot2 approach, first ensure: library(ggplot2) or library(tidyverse) is loaded. Then, use the geom_smooth(method = lm) command: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. YourDataSet,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. X, This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. Â y =Â  â€œy=Â â€ declares which variable will become the y-axis of the graphic. Y This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. + Here the + is used to add yet another layer to ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. There are several other methods that could be used here. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(â€¦) to place more complicated models on the plot. Â se = FALSE se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. ) Closing parenthesis for the geom_smooth() function. Â Click to Show OutputÂ  Click to View Output. ggplot(cars, aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = \"lm\", formula=y~x, se=FALSE) There are a number of ways to customize the appearance of the regression line: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. speed,Â  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. y =Â  â€œy=Â â€ declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. Â + Here the + is used to add yet another layer to ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(â€¦) to place more complicated models on the plot. Â se = FALSE, se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. Â size = 2, Use size = 2 to adjust the thickness of the line to size 2. Â color = â€œorangeâ€, Use color = â€œorangeâ€ to change the color of the line to orange. Â Â linetype = â€œdashedâ€ Use linetype = â€œdashedâ€ to change the solid line to a dashed line. Some linetype options include â€œdashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. ) Closing parenthesis for the geom_smooth() function. Â Click to Show OutputÂ  Click to View Output. In addition to customizing the regression line, you can customize the points, add points, add lines, and much more. ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. speed,Â  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. y =Â  â€œy=Â â€ declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point( geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. size = 1.5, Use size = 1.5 to change the size of the points. Â color = â€œskyblueâ€ Use color = â€œskyblueâ€ to change the color of the points to Brother Saundersâ€™ favorite color. Â alpha = 0.5 Use alpha = 0.5 to change the transparency of the points to 0.5. ) Closing parenthesis of geom_point() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in ways similar to lm(â€¦) to place more complicated models on the plot. Â se = FALSE, se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. Â color = â€œnavyâ€, Use color = â€œnavyâ€ to change the color of the line to navy blue. Â size = 1.5 Use size = 1.5 to adjust the thickness of the line to 1.5. ) Closing parenthesis of geom_smooth() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_hline( Use geom_hline() to add a horizontal line at a specified y-intercept. You can also use geom_vline(xintercept = some_number) to add a vertical line to the graph. yintercept = Use â€œyintercept =â€ to tell geom_hline() that you are going to declare a y intercept for the horizontal line. Â 75 75 is the value of the y-intercept. , color = â€œfirebrickâ€ Use color = â€œfirebrickâ€ to change the color of the horizontal line to firebrick red. , size = 1, Use size = 1 to adjust the thickness of the horizontal line to size 1. Â Â Â Â Â Â Â Â Â Â Â Â Â linetype = â€œlongdashâ€ Use linetype = â€œlongdashâ€ to change the solid line to a dashed line with longer dashes. Some linetype options include â€œdashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. , alpha = 0.5 Use alpha = 0.5 to change the transparency of the horizontal line to 0.5. ) Closing parenthesis of geom_hline function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_segment( geom_segment() allows you to add a line segment to ggplot() by using specified start and end points. x = â€œx =â€ tells geom_segment() that you are going to declare the x-coordinate for the starting point of the line segment. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the starting point of the line segment. Â y =â€œy =â€ tells geom_segment() that you are going to declare the y-coordinate for the starting point of the line segment. Â 75, 75 is a number on the y-axis of your graph. It is the y-coordinate of the starting point of the line segment. Â xend = â€œxend =â€ tells geom_segment() that you are going to declare the x-coordinate for the end point of the line segment. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the end point of the line segment. Â yend = â€œyend =â€ tells geom_segment() that you are going to declare the y-coordinate for the end point of the line segment. Â 38, 38 is a number on the y-axis of your graph. It is the y-coordinate of the end point of the line segment. Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â size = 1 Use size = 1 to adjust the thickness of the line segment. , color = â€œlightgrayâ€ Use color = â€œlightgrayâ€ to change the color of the line segment to light gray. , linetype = â€œlongdashâ€ Use *linetype = â€œlongdash* to change the solid line segment to a dashed one. Some linetype options includeâ€dashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. ) Closing parenthesis for geom_segment() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_point( geom_point() can also be used to add individual points to the graph. Simply declare the x and y coordinates of the point you want to plot. x = â€œx =â€ tells geom_point() that you are going to declare the x-coordinate for the point. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the point. Â y = â€œy =â€ tells geom_point() that you are going to declare the y-coordinate for the point. Â 75 75 is a number on the y-axis of your graph. It is the y-coordinate of the point. , size = 3 Use size = 3 to make the point stand out more. , color = â€œfirebrickâ€ Use color = â€œfirebrickâ€ to change the color of the point to firebrick red. ) Closing parenthesis of the geom_point() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_text( geom_text() allows you to add customized text anywhere on the graph. It is very similar to the base R equivalent, text(â€¦). x = â€œx =â€ tells geom_text() that you are going to declare the x-coordinate for the text. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the text. Â y = â€œy =â€ tells geom_text() that you are going to declare the y-coordinate for the text. Â 84, 84 is a number on the y-axis of your graph. It is the y-coordinate of the text. Â label = â€œlabel =â€ tells geom_text() that you are going to give it the label. Â â€œMy Point (14, 75)â€, â€œMy Point (14, 75)â€ is the text that will appear on the graph. Â Â Â Â Â Â Â Â Â Â Â Â color = â€œnavyâ€ Use color = â€œnavyâ€ to change the color of the text to navy blue. , size = 3 Use size = 3 to change the size of the text. ) Closing parenthesis of the geom_text() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â theme_minimal() Add a minimalistic theme to the graph. There are many other themes that you can try out. Â Click to Show OutputÂ  Click to View Output. ## `geom_smooth()` using formula = 'y ~ x' Accessing Parts of the Regression Finally, note that the mylm object contains the names(mylm) of mylm$coefficients Contains two values. The first is the estimated \\(y\\)-intercept. The second is the estimated slope. ## (Intercept) speed ## -17.579095 3.932409 mylm$residuals Contains the residuals from the regression in the same order as the actual dataset. ## 1 2 3 4 5 6 7 8 9 ## 3.849460 11.849460 -5.947766 12.052234 2.119825 -7.812584 -3.744993 4.255007 12.255007 ## 10 11 12 13 14 15 16 17 18 ## -8.677401 2.322599 -15.609810 -9.609810 -5.609810 -1.609810 -7.542219 0.457781 0.457781 ## 19 20 21 22 23 24 25 26 27 ## 12.457781 -11.474628 -1.474628 22.525372 42.525372 -21.407036 -15.407036 12.592964 -13.339445 ## 28 29 30 31 32 33 34 35 36 ## -5.339445 -17.271854 -9.271854 0.728146 -11.204263 2.795737 22.795737 30.795737 -21.136672 ## 37 38 39 40 41 42 43 44 45 ## -11.136672 10.863328 -29.069080 -13.069080 -9.069080 -5.069080 2.930920 -2.933898 -18.866307 ## 46 47 48 49 50 ## -6.798715 15.201285 16.201285 43.201285 4.268876 mylm$fitted.values The values of \\(\\hat{Y}\\) in the same order as the original dataset. ## 1 2 3 4 5 6 7 8 9 10 ## -1.849460 -1.849460 9.947766 9.947766 13.880175 17.812584 21.744993 21.744993 21.744993 25.677401 ## 11 12 13 14 15 16 17 18 19 20 ## 25.677401 29.609810 29.609810 29.609810 29.609810 33.542219 33.542219 33.542219 33.542219 37.474628 ## 21 22 23 24 25 26 27 28 29 30 ## 37.474628 37.474628 37.474628 41.407036 41.407036 41.407036 45.339445 45.339445 49.271854 49.271854 ## 31 32 33 34 35 36 37 38 39 40 ## 49.271854 53.204263 53.204263 53.204263 53.204263 57.136672 57.136672 57.136672 61.069080 61.069080 ## 41 42 43 44 45 46 47 48 49 50 ## 61.069080 61.069080 61.069080 68.933898 72.866307 76.798715 76.798715 76.798715 76.798715 80.731124 mylm$â€¦ several other things that will not be explained here. Making Predictions predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ) Closing parenthesis for the data.frame(â€¦) function. ) Closing parenthesis for the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12)) 1 Â  29.60981 The value given is the â€œfitted-valueâ€ or â€œpredicted-valueâ€ for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(â€¦) function. Â interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. â€œpredictionâ€ This specifies that a prediction interval will be included with the predicted value. A prediction interval gives you a 95% confidence interval that captures 95% of the data, or \\(Y_i\\) values for the specific \\(X\\)-value specified in the prediction. ) Closing parenthesis of the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"prediction\") Â  fit The â€œfitâ€ is the predicted value. Â  lwr The â€œlwrâ€ is the lower bound. Â  upr The â€œuprâ€ is the upper bound. 1 29.60981 In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance canâ€™t go negative) feet to 60.96915 feet. -1.749529 This is the lower bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us the stopping distance could be as quick as -1.749529 feet (or 0 because distance canâ€™t go negative). 60.96915 This is the upper bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us that the actual stopping distance could be as high as 60.96915 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(â€¦) function. Â interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. â€œconfidenceâ€ This specifies that a confidence interval for the prediction should be provided. This is of use whenever your interest is in just estimating the average y-value, not the actual y-values. ) Closing parenthesis of the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"confidence\") Â  fit The â€œfitâ€ is the predicted value. Â  lwr The â€œlwrâ€ is the lower bound. Â  upr The â€œuprâ€ is the upper bound. 1 29.60981 In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. 24.39514 This is the lower bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is greater than this value. 34.82448 This is the upper bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is less than this value. Finding Confidence Intervals for Model Parameters confint( The R function confint(â€¦) allows you to use an lm(â€¦) object to compute confidence intervals for one or more parameters (like \\(\\beta_0\\) or \\(\\beta_1\\)) in your model. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â level = â€œlevel =â€ tells the confint(â€¦) function that you are going to declare at what level of confidence you want the interval. The default is â€œlevel = 0.95.â€ If you want to find 95% confidence intervals for your parameters, then just run confint(mylm). Â someConfidenceLevel someConfidenceLevel is simply a confidence level you choose when you want something other than a 95% confidence interval. Some examples of appropriate levels include 0.90 and 0.99. ) Closing parenthesis for confint(..) function. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.90) Â  Â  5 % The lower bound of a 90% confidence interval occurs at the 5th percentile. This is because at 90% confidence, 10% is left in the tails, with 5% on each end. The upper bound will thus end at the 95th percentile, hence the 5% and 95% as the column names. Â  95 % The upper bound of a 90% confidence interval ends at the 95th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -28.914514 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -6.243676 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.235501 and 4.629317. 3.235501 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. 4.629317 This is the upper bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.95) Â  Â  2.5 % The lower bound of a 95% confidence interval occurs at the 2.5th percentile. This is because at 95% confidence, 5% is left in the tails, with 2.5% on each end. The upper bound will thus end at the 97.5th percentile, hence the 2.5% and 97.5% as the column names for the lower and upper bounds, respectively. Â  97.5 % The upper bound of a 95% confidence interval ends at the 97.5th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -31.167850 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -3.990340 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.096964 and 4.767853. 3.096964 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.096964 and 4.767853 4.767853 This is the upper bound of the confidence interval. In this case, you can be 95% confident that the slope lies between 3.096964 and 4.767853 Explanation Linear regression has a rich mathematical theory behind it. This is because it uses a mathematical function and a random error term to describe the regression relation between a response variable \\(Y\\) and an explanatory variable called \\(X\\). Expand each element below to learn more. Regression Cheat Sheet (Expand) Term Pronunciation Meaning Math R Code \\(Y_i\\) $Y_i$ â€œwhy-eyeâ€ The data \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)\\) $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$ YourDataSet$YourYvariable \\(\\hat{Y}_i\\) $\\hat{Y}_i$ â€œwhy-hat-eyeâ€ The fitted line \\(\\hat{Y}_i = b_0 + b_1 X_i\\) $\\hat{Y}_i = b_0 + b_1 X_i$ lmObject$fitted.values \\(E\\{Y_i\\}\\) $E\\{Y_i\\}$ â€œexpected value of why-eyeâ€ True mean y-value \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) $E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i$ <none> \\(\\beta_0\\) $\\beta_0$ â€œbeta-zeroâ€ True y-intercept <none> <none> \\(\\beta_1\\) $\\beta_1$ â€œbeta-oneâ€ True slope <none> <none> \\(b_0\\) $b_0$ â€œb-zeroâ€ Estimated y-intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) $b_0 = \\bar{Y} - b_1\\bar{X} b_0 <- mean(Y) - b_1*mean(X)$ \\(b_1\\) $b_1$ â€œb-oneâ€ Estimated slope \\(b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\) $b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})} {\\sum(X_i - \\bar{X})^2}$ b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) \\(\\epsilon_i\\) $\\epsilon_i$ â€œepsilon-eyeâ€ Distance of dot to true line \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) $\\epsilon_i = Y_i - E\\{Y_i\\}$ <none> \\(r_i\\) $r_i$ â€œr-eyeâ€ or â€œresidual-eyeâ€ Distance of dot to estimated line \\(r_i = Y_i - \\hat{Y}_i\\) $r_i = Y_i - \\hat{Y}_i$ lmObject$residuals \\(\\sigma^2\\) $\\sigma^2$ â€œsigma-squaredâ€ Variance of the \\(\\epsilon_i\\) \\(Var\\{\\epsilon_i\\} = \\sigma^2\\)$Var\\{\\epsilon_i\\} = \\sigma^2$ <none> \\(MSE\\) $MSE$ â€œmean squared errorâ€ Estimate of \\(\\sigma^2\\) \\(MSE = \\frac{SSE}{n-p}\\)$MSE = \\frac{SSE}{n-p}$ sum( lmObject$res^2 ) / (n - p) \\(SSE\\) $SSE$ â€œsum of squared errorâ€ (residuals) Measure of dotâ€™s total deviation from the line \\(SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\)$SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$ sum( lmObject$res^2 ) \\(SSR\\) $SSR$ â€œsum of squared regression errorâ€ Measure of lineâ€™s deviation from y-bar \\(SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\)$SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2$ sum( (lmObject$fit - mean(YourData$Y))^2 ) \\(SSTO\\) $SSTO$ â€œtotal sum of squaresâ€ Measure of total variation in Y \\(SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\)$SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2$ sum( (YourData$Y - mean(YourData$Y))^2 ) \\(R^2\\) $R^2$ â€œR-squaredâ€ Proportion of variation in Y explained by the regression \\(R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\)$R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}$ SSR/SSTO \\(r\\) $r$ â€œrâ€ Correlation between X and Y. \\(r = \\sqrt{R^2}\\)$r = \\sqrt{R^2}$ sqrt(R^2) \\(\\hat{Y}_h\\) $\\hat{Y}_h$ â€œwhy-hat-aitchâ€ Estimated mean y-value for some x-value called \\(X_h\\) \\(\\hat{Y}_h = b_0 + b_1 X_h\\)$\\hat{Y}_h = b_0 + b_1 X_h$ predict(lmObject, data.frame(XvarName=#)) \\(X_h\\) $X_h$ â€œex-aitchâ€ Some x-value, not necessarily one of the \\(X_i\\) values used in the regression \\(X_h =\\) some number$X_h = $ Xh = # Confidence Interval â€œconfidence intervalâ€ Estimated bounds at a certain level of confidence for a parameter \\(b_0 \\pm t^* \\cdot s_{b_0}\\)b_0 \\pm t^* \\cdot s_{b_0} or \\(b_1 \\pm t^* \\cdot s_{b_1}\\)b_1 \\pm t^* \\cdot s_{b_1} confint(mylm, level = someConfidenceLevel) Parameter Estimate \\(\\beta_0\\) \\(b_0\\) \\(\\beta_1\\) \\(b_1\\) \\(\\epsilon_i\\) \\(r_i\\) \\(\\sigma^2\\) \\(MSE\\) \\(\\sigma\\) \\(\\sqrt{MSE}\\), the Residual standard error The Mathematical Model (Expand) \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)â€¦ There are three main elements to the mathematical model of regression. Each of these three elements is pictured below in the â€œRegression Relation Diagram.â€ Study both the three bullet points and their visual representations in the plot below for a clearer understanding. The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read moreâ€¦) The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as â€œnatural lawâ€ or â€œGodâ€™s lawâ€. It is some true line that is unknown to us. The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line. The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\). Note: \\(E\\{Y\\}\\) is pronounced â€œthe expected value of yâ€ because, wellâ€¦ the mean is the typical, average, or â€œexpectedâ€ value. The dots, i.e., the regression relation plus an error term: \\(Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}} + \\underbrace{\\epsilon_i}_\\text{error term} \\quad \\text{where} \\ \\epsilon_i\\sim N(0,\\sigma^2)\\) (Read moreâ€¦) This is shown by the dots in the graph below. This is the data. In regression, the assumption is that the y-value for individual \\(i\\), denoted by \\(Y_i\\), was â€œcreatedâ€ by adding an error term \\(\\epsilon_i\\) to each individualâ€™s â€œexpectedâ€ value \\(\\beta_0 + \\beta_1 X_i\\). Note the â€œorder of creationâ€ would require first knowing an indivualâ€™s x-value, \\(X_i\\), then their expected value from the regression relation \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) and then adding their \\(\\epsilon_i\\) value to the result. The \\(\\epsilon_i\\) allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance \\(\\epsilon_i\\) from the line. Note: \\(Y_i\\) is pronounced â€œwhy-eyeâ€ because it is the y-value for individual \\(i\\). Sometimes also called â€œwhy-sub-eyeâ€ because \\(i\\) is in the subscript of \\(Y\\). The estimated line, i.e., the line we get from a sample of data. \\(\\underbrace{\\hat{Y}_i}_{\\substack{\\text{estimated mean} \\\\ \\text{y-value}}} = \\underbrace{b_0 + b_1 X_i}_\\text{estimated regression equation}\\) (Read moreâ€¦) The estimated line is shown by the solid line in the graph below. \\(\\hat{Y}\\) is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation \\(E\\{Y\\}\\). So \\(\\hat{Y}\\) is interpreted as the estimated average (or mean) \\(Y\\)-value for any given \\(X\\)-value. Thus, \\(b_0\\) is the estimated y-intercept and \\(b_1\\) is the estimated slope. The bâ€™s are sample statistics, like \\(\\bar{x}\\) and the \\(\\beta\\)â€™s are population parameters like \\(\\mu\\). The \\(b\\)â€™s estimate the \\(\\beta\\)â€™s. Note: \\(\\hat{Y}_i\\) is pronounced â€œwhy-hat-eyeâ€ and is known as the â€œestimated y-valueâ€ or â€œfitted y-valueâ€ because it is the y-value you get from \\(b_0 + b_1 X_i\\). It is always different from \\(Y_i\\) because dots are rarely if ever exactly on the estimated regression line. This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line). Something to ponder: The true line, when coupled with the error terms, â€œcreatesâ€ the data. The estimated (or fitted) line uses the sampled data to try to â€œre-createâ€ the true line. We could loosely call this the â€œorder of creationâ€ as shown by the following diagram. par(mfrow=c(1,3), mai=c(.2,.2,.4,.1)) plot(y ~ x, col=\"white\", main=\"A Law is Given\", yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, main=\"Data is Created\", xaxt='n', yaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, xaxt='n', yaxt='n', main=\"The Law is Estimated\") curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) A Law is Given Data is Created The Law is Estimated \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) \\(Y_i = E\\{Y_i\\} + \\epsilon_i\\) \\(\\hat{Y}_i = b_0 + b_1 X_i\\) The true line is the â€œlawâ€. The \\(Y_i\\) are created by adding \\(\\epsilon_i\\) to \\(E\\{Y_i\\}\\) where \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\). The law is estimated with \\(\\hat{Y}_i\\) which is given with lm(...). Click open the â€œCodeâ€ buttom below to the right to find code that runs a simulation demonstrating this â€œorder of creationâ€. ## Simulating Data from a Regression Model ## This R-chunk is meant to be played in your R Console. ## It allows you to explore how the various elements ## of the regression model combine together to \"create\" ## data and then use the data to \"re-create\" the line. set.seed(101) #Allows us to always get the same \"random\" sample #Change to a new number to get a new sample n <- 3 #set the sample size X_i <- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45. beta0 <- 3 #Our choice for the y-intercept. beta1 <- .1 #Our choice for the slope. sigma <- 12.5 #Our choice for the std. deviation of the error terms. epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data View(fabData) #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it. fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData. summary(fab.lm) #Summarize your model. plot(y ~ x, data=fabData) #Plot the data. abline(fab.lm) #Add the estimated regression line to your plot. # Now for something you can't do in real life... but since we created the data... abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). legend(\"topleft\", legend=c(\"True Line\", \"Estimated Line\"), lty=c(2,1), bty=\"n\") #Add a legend to your plot specifying which line is which. Interpreting the Model Parameters (Expand) \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted asâ€¦ The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model. If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\). The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\). It is often misunderstood to be the â€œaverage change in yâ€ or just â€œthe change in yâ€ but it is more correctly referred to as the â€œchange in the average yâ€. To better see this, consider the three graphics shown below. par(mfrow=c(1,3)) hist(mtcars$mpg, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Number of Vehicles\", xlab=\"Gas Mileage (mpg)\", col=\"skyblue\") boxplot(mpg ~ cyl, data=mtcars, border=\"skyblue\", boxwex=0.5, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Number of Cylinders of Engine (cyl)\") plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=\"skyblue\", main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Quarter Mile Time (qsec)\") abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=\"darkgray\") mtext(side=3, text=\"Automatic Transmissions Only (am==0)\", cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=\"gray\") The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09. The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg. The scatterplot on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value. Residuals and Errors (Expand) \\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true errorâ€¦ Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\). The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\). We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summaryâ€¦ Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\). \\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms. Keep in mind the idea that the errors \\(\\epsilon_i\\) â€œcreatedâ€ the data and that the residuals \\(r_i\\) are computed after using the data to â€œre-createâ€ the line. Residuals have many uses in regression analysis. They allow us to diagnose the regression assumptions, See the â€œAssumptionsâ€ section below for more details. estimate the regression relation, See the â€œEstimating the Model Parametersâ€ section below for more details. estimate the variance of the error terms, See the â€œEstimating the Model Varianceâ€ section below for more details. and assess the fit of the regression relation. See the â€œAssessing the Fit of a Regressionâ€ section below for more details. Assessing the Fit of a Regression (Expand) \\(R^2\\), SSTO, SSR, and SSEâ€¦ Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line. par(mfrow=c(1,3), mai=c(.1,.1,.5,.1)) set.seed(2) x <- runif(30,0,20) y1 <- 2 + 3.5*x + rnorm(30,0,2) y2 <- 2 + 3.5*x + rnorm(30,0,8) y3 <- 2 + 3.5*x + rnorm(30,0,27) plot(y1 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Excellent Fit\") abline(lm(y1 ~ x), col=\"gray\") plot(y2 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Good Fit\") abline(lm(y2 ~ x), col=\"gray\") plot(y3 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Poor Fit\") abline(lm(y3 ~ x), col=\"gray\") A common way to measure the fit of a regression is with correlation. While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\). (If you are a Math 325 student, just stick with correlation for now and skip on to the next section of this Explanation tab. If you are a Math 425 student, it is critical that you come to understand \\(R^2\\) deeply, so read on.) Before you can understand \\(R^2\\), you must understand three important â€œsums of squaresâ€. (Read more about sumsâ€¦) A sum is just a fancy word for adding things together. \\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand. So we use the symbol \\(\\Sigma\\) to denote the word â€œsumâ€. Further, we use a subscript \\(\\Sigma_{i=1}\\) to state what value the sum is beginning with, and a superscript \\(\\Sigma_{i=1}^6\\) to state the value we are ending at. This gives \\[ \\sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Test your knowledge, do you see why the answer is 6 to the sum below? \\[ \\sum_{i=1}^3 i = 6 \\] Computing sums in R is fairly easy. Type the following codes in your R Console. sum(1:6) #gives the answer of 21 sum(1:3) #gives the answer of 6 However, sums really become useful when used with a data set. Each row of a data set represents an â€œindividualâ€™sâ€ data. We can reference each individual with a row number. In the data below, individual 3, denoted by \\(i=3\\), has a speed of 7 and a dist of 4. pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3) Individual speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 To compute the sum of the speed column, use sum(speed). If we divided this sum by 6, we would get the mean of speed mean(speed). In fact, the two most used statistics mean(...) and sd(...) both use sums. Take a moment to review the formulas for mean and standard deviation. It is strongly recommended that you study the Explanation tab for both as well. Weâ€™ll wait. See you back here shortly. â€¦ Welcome back. Suppose we let X = speed and Y = dist. Then \\(X_3 = 7\\) and \\(Y_3 = 4\\) because we are accessing row 3 of both the \\(X\\) (or speed) column and \\(Y\\) (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, sum(speed) would be written mathematically as \\(\\sum_{i=1}^6 X_i\\) and sum(dist) would be written as \\(\\sum_{i=1}^6 Y_i\\). Sum of Squared Errors Sum of Squares Regression Total Sum of Squares \\(\\text{SSE} = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\) \\(\\text{SSR} = \\sum_{i=1}^n \\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\) \\(\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\\) Measures how much the residuals deviate from the line. Measures how much the regression line deviates from the average y-value. Measures how much the y-values deviate from the average y-value. Equals SSTO - SSR Equals SSTO - SSE Equals SSE + SSR sum( (Y - mylm$fit)^2 ) sum( (mylm$fit - mean(Y))^2 ) sum( (Y - mean(Y))^2 ) It is important to remember that SSE and SSR split up SSTO, so that \\[ \\text{SSTO} = \\text{SSE} + \\text{SSR} \\] This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works. The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called \\(R^2\\) (â€œr-squaredâ€). R-Squared (\\(R^2\\)) \\[ \\underbrace{R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}}_\\text{Interpretation: Proportion of variation in Y explained by the regression.} \\] The smallest \\(R^2\\) can be is zero, and the largest it can be is 1. This is because \\(SSR\\) must be between 0 and SSTO, inclusive. Residual Plots & Regression Assumptions (Expand) Residuals vs.Â fitted-values, Q-Q Plot of the residuals, and residuals vs.Â order plotsâ€¦ There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate. Each assumption is labeled in the regression equation below. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions. (Read moreâ€¦) As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not. Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3 The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(\\hat{Y}_i\\). The residuals are the \\(r_i\\). This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable. | Show Examples | (Read moreâ€¦) The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption. The linear relation is assumed to be satisfied if there are no apparent trends in the plot. The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values. The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance. set.seed(2) X <- rnorm(30,15,3) notLin <- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8)) notLin.lm <- lm(Y~X, data=notLin) set.seed(15) Lin <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) Lin.lm <- lm(Y~X, data=Lin) par(mfrow=c(3,3), mai=c(.25,.25,.25,.25), mgp=c(1,.75,0)) plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Not Linear\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(notLin.lm$fitted.values,notLin.lm$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Good: Linear, Constant Variance\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) set.seed(6) notCon <- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5)) notCon.lm <- lm(Y~X, data=notCon) LinO <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) LinO[1] <- LinO[1]^2 LinO.lm <- lm(Y~X, data=LinO) plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Unconstant Variance\", cex.main=0.95, yaxt='n', xaxt='n', col=\"firebrick\") polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) # plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Outliers\", cex.main=0.95) # abline(h=0) tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(Girth ~ Volume, data=trees[-31,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(Height ~ Volume, data=trees) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) abline(h=0) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals, f=.4) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(weight ~ repwt, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(weight ~ repht, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) abline(h=0) Q-Q Plot of the Residuals: Checks Assumption #2 The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption. | Show Examples | (Read moreâ€¦) There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot. Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) set.seed(123) tmp <- rnorm(100) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"skyblue\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Normal\", col=\"skyblue\") tmp <- Davis$weight qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Right-skewed\", breaks=15, col=\"firebrick\") par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- rbeta(100, 5,1) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Left-skewed\", breaks=seq(min(tmp),max(tmp), length.out=13), col=\"firebrick\") tmp <- rbeta(100,2,2) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Heavy-tailed\", col=\"firebrick\") Residuals versus Order Plot: Checks Assumption #5 When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated. | Show Examples | (Read moreâ€¦) Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Good: No Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: General Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ qsec, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: Interesting Patterns\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),]) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Bad: Obvious Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") Problems from Failed Assumptions There are various problems that can arise when certain of the regression assupmtions are not satisfied. Lack of Linearity When the linearity assumption is violated, pretty much everything we obtain from the regression summary is no longer meaningful. The y-intercept estimate can be drastically off from its actual true value. Important model information is lost by trying to use a simple slope term \\(\\beta_1\\) to describe the model with respect to \\(X\\). The residual standard error will be much higher than it otherwise would be because of curvature patterns in the data that the line cannot capture. Thus, R-squared will be lower than it otherwise should be. P-values can become non-significant, when in fact there is a strong pattern in the data, but that pattern just cannot be captured by a simple line. *Normality of the errors is often put into question as well when a simplified line is used to try to capture a more complicated curved model. The plot below demonstrate these difficulties. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 7.5 #True slope beta_2 <- -0.25 #True bend X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors Y_i <- beta_0 + beta_1*X_i + beta_2*X_i^2 + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Non-Linear Relation\") abline(mylm, col=\"gray\") #Add fitted line to plot curve(beta_0 + beta_1*x + beta_2*x^2, col=\"gray\", lty=2, add=TRUE) #Add True line to plot #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (True value:\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (True value:\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (True value:\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Non-normal Error Terms When the normality assumption for the error terms is violated, not all is lost. In fact, the estimate of the slope and intercept are still often fairly meaningful. However, it is unwise to put too much trust in the residual standard error as an estimate of the standard deviation \\(\\sigma\\). This is because the standard deviation in skewed distributions does not carry the same meaning it has in normal distributions. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rchisq(n, 1)*3 - 1 #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Unconstant Variance When variance of the error term changes across the regression, the regression approximates the â€œaverage varianceâ€ of the errors because the regression is still assuming the variance is constant across the regression. The estimates of the slope and intercept are still typically quite good, and can be used for interpretation. The residual standard error however should not be considered to be meaningful as it will be too large on one end of the regression and too small on the other end. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma + X_i) #normally distributed errors #with increasing variance Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Variance Varies (Non-Constant)\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 13.82 Slope 3.5 3.768 Sigma 2.5 13.02 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", round(mean(sigma + X_i), 2), \", mean)\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Normality Violated As silly as it sounds, if the only problem with the regression is the lack of normality of the error terms, it isnâ€™t all that big of a problem. Depending on how non-normal the residuals appear, there could be some skewing to the residual standard error, but otherwise, the slope and intercept are still interpretable and meaningful. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- runif(n, -sqrt(12*sigma^2)/2, sqrt(12*sigma^2)/2) #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 14.7 Slope 3.5 3.449 Sigma 2.5 2.317 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Independence Assumption Violated While the slope and intercept are often still meaningful when the independence assumption is violated, the residual standard error is unnecessarily large in this case. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, 2.5) + (1:n -n/2)*.5 #normal, but correlated errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Independence Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 16.47 Slope 3.5 3.296 Sigma 2.5 4.819 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Outliers Present While outliers do not violate any of the regression assumptions, they do pose substantial difficulties for the least squares regression estimates of the slope and intercept. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors epsilon_i[3] <- ifelse(X_i[3] < 10, runif(1,25,35), -runif(1,25,35)) #create outlier Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"An Outlier Present\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 13.18 Slope 3.5 3.447 Sigma 2.5 6.389 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) The material below this section is meant for Math 425 students only. Estimating the Model Parameters (Expand) How to get \\(b_0\\) and \\(b_1\\): least squares & maximum likelihoodâ€¦ There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical. The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas. Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 <- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\). When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\). Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i. \\label{exp} \\end{equation}\\] Least Squares{#leastSquares} To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\). \\[ Q = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2 \\] Then we use the function Q as if it were a function of \\(\\beta_0\\) and \\(\\beta_1\\). Ironically, the values of \\(Y\\) and \\(X\\) are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we donâ€™t know are the values of \\(\\beta_0\\) and \\(\\beta_1\\). This least squares applet is a good way to explore how various choices of the slope and intercept yield different values of the â€œsum of squared residualsâ€. But it turns out that there is one â€œbestâ€ choice of the slope and intercept that yields a â€œsmallestâ€ value of the â€œsum of squared residuals.â€ This best choice can actually be found using calculus by taking the partial derivatives of \\(Q\\) with respect to both \\(\\beta_0\\) and \\(\\beta_1\\). \\[ \\frac{\\partial Q}{\\partial \\beta_0} = -2\\sum (Y_i - \\beta_0 - \\beta_1X_i) \\] \\[ \\frac{\\partial Q}{\\partial \\beta_1} = -2\\sum X_i(Y_i-\\beta_0-\\beta_1X_i) \\] Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize \\(Q\\) for a given set of data. After all the calculations are completed we find the values of the parameter estimators \\(b_0\\) and \\(b_1\\) (of \\(\\beta_0\\) and \\(\\beta_1\\), respectively) are as stated previously. Maximum Likelihood{#mle} The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of \\(\\beta_0\\) and \\(\\beta_1\\) which minime the least squares \\(Q\\) function, we choose the values of \\(\\beta_0\\) and \\(\\beta_1\\) which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the \\(Y_i\\) for all observations \\(i=1,\\ldots,n\\). We can do this rather simply by using the assumption that the errors, \\(\\epsilon_i\\) are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if \\(f(Y_i)\\) denotes the probability density function for \\(Y_i\\), then the joint probability density for all \\(Y_i\\), \\(f(Y_1,\\ldots,Y_n)\\) is given by \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) \\] Since each \\(Y_i\\) is assumed to be normally distributed with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\) (see model (\\(\\ref{model}\\))) we have that \\[ f(Y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{Y_i-\\beta_0-\\beta_1X_i}{\\sigma}\\right)^2\\right]} \\] which provides the joint probability as \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] The likelihood function \\(L\\) is then given by consider the \\(Y_i\\) and \\(X_i\\) fixed and the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) as the variables in the function. \\[ L(\\beta_0,\\beta_1,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] Instead of taking partial derivatives of \\(L\\) directly (with respect to all parameters) we take the partial derivatives of the \\(\\log\\) of \\(L\\), which is easier to work with. In a similar, but more difficult calculation, to that of minimizing \\(Q\\), we obtain the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) which maximize the log of \\(L\\), and which therefore maximize \\(L\\). (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate \\(\\hat{\\sigma}^2\\) of \\(\\sigma^2\\). \\[ \\hat{\\sigma}^2 = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n} \\] Estimating the Model Variance (Expand) Estimating \\(\\sigma^2\\) with MSEâ€¦ As shown previously in the â€œEstimating Model Parametersâ€ section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation. Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\). This estimate turns out to be a biased estimator. This means that it is consistently wrong in its estimates of \\(\\sigma^2\\). If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong. This is bad. Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\). Without going into all the details, to â€œfixâ€ the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently. To find the correct degrees of freedom, we have to notice that the \\(\\hat{Y}_i\\) in the numerator of \\(\\widehat{\\sigma}^2\\) is defined by \\[\\begin{equation} \\widehat{Y}_i = b_0 + b_1X_i \\label{hatY} \\end{equation}\\] From this equation, we notice that two means, \\(\\bar{X}\\) and \\(\\bar{Y}\\), were estimated from the data in order to obtain \\(\\hat{Y}_i\\). (See the formulas for \\(b_0\\) and \\(b_1\\) above, and note how they use both \\(\\bar{X}\\) and \\(\\bar{Y}\\) in their calculation.) Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for \\(\\hat{\\sigma}^2\\) should be \\(n-2\\) instead of \\(n\\). Some incredibly long calculations will show that the â€œfixedâ€ estimator \\[\\begin{equation} s^2 = MSE = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n-2} \\quad \\text{(Unbiased Estimator of $\\sigma^2$)} \\end{equation}\\] is an unbiased estimator of \\(\\sigma^2\\). Here \\(MSE\\) stands for mean squared error, which is the most obvious name for a formula that squares the errors \\(Y_i-\\hat{Y}_i\\) then adds them up and divides by their degrees of freedom. Similarly, we call the numerator \\(\\sum(Y_i-\\hat{Y}_i)^2\\) the sum of the squared errors, denoted by \\(SSE\\). It is also important to note that the errors are often denoted by \\(r_i = Y_i-\\hat{Y}_i\\), the residuals. Putting this all together we get the following equivalent statements for \\(MSE\\). \\[\\begin{equation} s^2 = MSE = \\frac{SSE}{n-2} = \\frac{\\sum(Y_i-\\widehat{Y}_i)^2}{n-2} = \\frac{\\sum r_i^2}{n-2} \\end{equation}\\] As a final note, even though the expected value \\(E\\{MSE\\} = \\sigma^2\\), which shows \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\), it unfortunately isnâ€™t true that \\(\\sqrt{MSE}\\) is an unbiased estimator of \\(\\sigma\\). This presents a few problems later on, but these are minimal enough that we can overlook the issue and move forward. Transformations (Expand) \\(Y'\\), \\(X'\\), and returning to the original spaceâ€¦ Y transformations are denoted by y-prime, written \\(Y'\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\). \\[ Y' = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y' = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y' = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y' = \\log(Y)\\) lm(log(Y) ~ X) 0.25 \\(Y' = \\sqrt(\\sqrt(Y))\\) lm(sqrt(sqrt(Y)) ~ X) 0.5 \\(Y' = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y' = Y\\) lm(Y ~ X) 2 \\(Y' = Y^2\\) lm(Y^2 ~ X) Using â€œmaximum-likelihoodâ€ estimation, the Box-Cox procedure can actually automatically detect the â€œoptimalâ€ value of \\(\\lambda\\) to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise. Scatterplot Recognition Box-Cox Suggestion An Example Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\). set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Ya <- 1/sqrt(Y) #1/Y^2 Lam = -2 Yb <- 1/Y #1/Y Lam = -1 Yc <- exp(.02*Y) #log(Y) Lam = 0 Yd <- Y^2 #sqrt(Y) Lam = 0.5 Ye <- Y #Y Lam = 1 Yf <- sqrt(Y) #Y^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Ya ~ X, main=expression(paste(\"Use \", lambda == -2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ya^-2 ~ X)) curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yb ~ X, main=expression(paste(\"Use \", lambda == -1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yb^-1 ~ X)) curve(1/(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yc ~ X, main=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(log(Yc) ~ X)) curve(exp(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yd ~ X, main=expression(paste(\"Use \", lambda == 0.5)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(sqrt(Yd) ~ X)) curve((b[1] + b[2]*x)^2, add=TRUE, col=\"green\", lwd=2) plot(Ye ~ X, main=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ye ~ X)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yf ~ X, main=expression(paste(\"Use \", lambda == 2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yf^2 ~ X)) curve(sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try. par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0)) boxCox(lm(Ya ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -2)), line=.5) boxCox(lm(Yb ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -1)), line=.5) boxCox(lm(Yc ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), line=.5) boxCox(lm(Yd ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0.5)), line=.5) boxCox(lm(Ye ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), line=.5) boxCox(lm(Yf ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 2)), line=.5) An Example Suppose we were running a simple linear regression on the cars dataset. This would be done with the code cars.lm <- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesnâ€™t quite fit the data as well as we would hope. Instead, the data looks a little curved. cars.lm <-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") abline(cars.lm, col=\"gray\") Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful. library(car) boxCox(cars.lm) The output from the boxCox(...) function looks as follows. This plot tells use to use the \\(\\lambda = 0.5\\) transformation, so that \\(Y' = Y^0.5 = \\sqrt{Y}\\). (To see this yourself, click on the â€œBox-Cox Suggestionâ€ tab above, as well as on the â€œScatterplot Recognitionâ€ tab.) Now, a transformation regression is performed using sqrt(Y) in place of Y as follows: cars.lm.t <- lm(sqrt(dist) ~ speed, data=cars) summary(cars.lm.t) Â  Estimate Std. Error t value Pr(> (Intercept) 1.277 0.4844 2.636 0.01126 speed 0.3224 0.02978 10.83 1.773e-14 Then, \\[ \\widehat{Y}_i' = 1.277 + 0.3224 X_i \\] And replacing \\(\\hat{Y}_i' = \\sqrt{\\hat{Y}_i}\\) we have \\[ \\sqrt{\\widehat{Y}_i} = 1.277 + 0.3224 X_i \\] Solving for \\(\\hat{Y}_i\\) gives \\[ \\widehat{Y}_i = (1.277 + 0.3224 X_i)^2 \\] Which, using curve((1.277 + 0.3224*x)^2, add=TRUE) (see code for details) looks like this: plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") curve( (1.277 + 0.3224*x)^2, add=TRUE, col=\"firebrick\") X-Transformations X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them. The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated. The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation. set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Xa <- 1/sqrt(X) #1/X^2 Lam = -2 Xb <- 1/X #1/X Lam = -1 Xc <- exp(.02*X) #log(X) Lam = 0 Xd <- X^2 #sqrt(X) Lam = 0.5 Xe <- X #X Lam = 1 Xf <- sqrt(X) #X^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Y ~ Xa, main=expression(paste(\"Use \", X*minute == X^-2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xa^-2))) curve(b[1] + b[2]*x^-2, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xb, main=expression(paste(\"Use \", X*minute == X^-1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xb^-1))) curve(b[1] + b[2]*x^-1, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xc, main=expression(paste(\"Use \", X*minute == log(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ log(Xc))) curve(b[1] + b[2]*log(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xd, main=expression(paste(\"Use \", X*minute == sqrt(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ sqrt(Xd))) curve(b[1] + b[2]*sqrt(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xe, main=expression(paste(\"Use \", X*minute == X, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ Xe)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xf, main=expression(paste(\"Use \", X*minute == X^2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xf^2))) curve(b[1] + b[2]*x^2, add=TRUE, col=\"green\", lwd=2) Inference for the Model Parameters (Expand) t test formulas, sampling distributions, confidence intervals, and F testsâ€¦ When fitting the regression model given by the equation \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] to a sample of data, we typically test hypotheses about the parameters \\(\\beta_0\\), \\(\\beta_1\\), or both. Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_0 - \\overbrace{0}^\\text{a number}}{s_{b_0}}\\] This is the formula for the test statistic. It measures how far the estimated y-intercept \\(b_0\\) is from the null hypothesis for \\(\\beta_0\\) in units of â€œstandard errors of \\(b_0\\)â€. Thus the division by \\(s_{b_0}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0, it could be any number. \\(H_0: \\beta_1 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_1\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_1 - \\overbrace{0}^\\text{a number}}{s_{b_1}}\\] This is the formula for the test statistic. It measures how far the estimated slope \\(b_1\\) is from the null hypothesis for \\(\\beta_1\\) in units of â€œstandard errors of \\(b_1\\)â€. Thus the division by \\(s_{b_1}\\). Though the hypothesized value of \\(\\beta_1\\) is typically 0, it could be any number. Left-tailed p-value = pt(-abs(tvalue), degrees of freedom). Double it to get the two-sided p-value. In R, these values correspond to the output summary of an lm as follows. (Show Example) Consider the cars data in R. Suppose we used the regression model given by \\[ \\underbrace{Y_i}_\\text{Feet to Stop} = \\beta_0 + \\beta_1 \\underbrace{X_{i}}_\\text{mph} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\quad \\sim N(0,\\sigma^2) \\] to model the feet a vehicle (from the 1920â€™s) takes to stop when traveling at a certain speed (in miles per hour, mph) prior to stopping. When the regression is performed and summarized in R, it is always testing the following two hypotheses: \\[ H_0: \\beta_0 = 0 \\quad\\quad H_0: \\beta_1 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\quad\\quad H_a: \\beta_1 \\neq 0 \\] To perform the test of these hypotheses for the regression stated above, we would run the following codes in R. cars.lm <- lm(dist ~ speed, data=cars) pander(summary(cars.lm)$coefficients) These would produce summary output like the following, but the following output has been labeled with the math notation corresponding to each value. Letâ€™s emphasize what is happening in this summary output table. First, here is how the â€œt valueâ€ is calculated for the â€œ(Intercept)â€ in the summary table above. \\[ t = \\frac{b_0-0}{s_{b_0}} = \\frac{-17.58 - 0}{6.758} = -2.601 \\] Second, here is a visual representation of how the P-value, the â€œPr(>|t|)â€ as it is called in the summary table above, is calculated for this test statistic. (Click the graph to view an interactive applet showing this calculation.) Notice both ends of the t-distribution are being shaded to compute the P-value because the alternative hypothesis was \\(H_a: \\beta_0 \\neq 0\\). To compute the P-value in R, we use the â€œpercentile function for the t-distributionâ€ called pt( ). This function requires two things, the t-value and the degrees of freedom, in our case pt(-2.601, 48). Note the degrees of freedom (df) are 48 because the sample size is \\(n=50\\) and there are two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) in our regression model. Running this code in R gives: pt(-2.601, 48) = 0.00616 However, note that this value is only half of the actual P-value of 0.0123. To get the â€œtwo-sidedâ€ P-value (note that our alternative hypothesis used a \\(\\neq\\) symbol) we need to double this left-tailed P-value. 2*pt(-2.601, 48)) = 0.0123 Finally, note that the same procedure can be used to test hypotheses that use a value other than 0 in the null and alternative. For example, to test the hypotheses: \\[ H_0: \\beta_1 = 3 \\\\ H_a: \\beta_1 \\neq 3 \\] Use the t-formula \\[ t = \\frac{b_1 - 3}{s_{b_1}} = \\frac{\\overbrace{3.932}^{b_1} - \\overbrace{3}^{H_0}}{\\underbrace{0.4155}_{s_{b_1}}} = 2.243 \\] then the P-value is calculated in R by 2*pt(-abs(2.243), 48) = 0.0295495 To obtain confidence intervals in R use confint(mylm). Confidence Interval Formula Standard Error \\(\\beta_0\\) \\(b_0 \\pm\\) \\(t^*\\) This is called the â€œcritical valueâ€ and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\). Use qt(0.975, df) to get \\(t*\\) in R. \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_0\\). \\(s_{b_0}\\) The standard error of \\(b_0\\), denoted by \\(s_{b_0}\\) is provided in the regression summary output under the column header called â€œStd. Errorâ€ for the â€œ(Intercept)â€ row of the output. It is calculated using the formula shown below. \\[s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\] This is called the â€œestimated variance of \\(b_0\\)â€. Taking the square root of this number gives the â€œstandard error of \\(b_0\\)â€. \\(\\beta_1\\) \\(b_1 \\pm\\) \\(t^*\\) This is called the â€œcritical valueâ€ and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\) (sample size - number of parameters in the regression model). Use qt(0.975, df) to get \\(t*\\) in R \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_1\\). \\(s_{b_1}\\) The standard error of \\(b_1\\), denoted by \\(s_{b_1}\\) is provided in the regression summary output under the column header called â€œStd. It is calculated using the formula shown below. \\[s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\] This is called the â€œestimated variance of \\(b_1\\)â€. Taking the square root of this number gives the â€œstandard error of \\(b_1\\)â€. To be more exact, the types of inference we are interested in are the following. Determine if there is evidence of a meaningful linear relationship in the data. If \\(\\beta_1 = 0\\), then there is no relation between \\(X\\) and \\(E\\{Y\\}\\). Hence we might be interested in testing the hypotheses \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) is some hypothesized number. To provide a confidence interval for the true value of \\(\\beta_1\\). Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the sampling distribution of the estimate \\(b_1\\) of the parameter \\(\\beta_1\\). And, while we are at it, we may as well come to understand the sampling distribution of the estimate \\(b_0\\) of the parameter \\(\\beta_0\\). Review sampling distributions from Math 221. Since \\(b_1\\) is an estimate, it will vary from sample to sample, even though the truth, \\(\\beta_1\\), remains fixed. (The same holds for \\(b_0\\) and \\(\\beta_0\\).) It turns out that the sampling distribution of \\(b_1\\) (where the \\(X\\) values remain fixed from study to study) is normal with mean and variance: \\[ \\mu_{b_1} = \\beta_1 \\] \\[ \\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} \\] ## Simulation to Show relationship between Standard Errors ##----------------------------------------------- ## Edit anything in this area... n <- 100 #sample size Xstart <- 30 #lower-bound for x-axis Xstop <- 100 #upper-bound for x-axis beta_0 <- 2 #choice of true y-intercept beta_1 <- 3.5 #choice of true slope sigma <- 13.8 #choice of st. deviation of error terms ## End of Editable area. ##----------------------------------------------- # Create X, which will be used in the next R-chunk. X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) ## After playing this chunk, play the next chunk as well. To see that this is true, consider the regression model with values specified for each parameter as follows. \\[ Y_i = \\overbrace{\\beta_0}^{2} + \\overbrace{\\beta_1}^{3.5} X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\overbrace{\\sigma^2}^{\\sigma=13.8}) \\] Using the equations above for \\(\\mu_{b_1}\\) and \\(\\sigma^2_{b_1}\\) we obtain that the mean of the sampling distribution of \\(b_1\\) will be \\(\\mu_{b_1} = \\beta_1 = 3.5\\) Further, we see that the variance of the sampling distribution of \\(b_1\\) will be \\(\\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} = \\frac{13.8^2}{4.25\\times 10^{4}}\\) Taking the square root of the variance, the standard deviation of the sampling distribution of \\(b_1\\) will be \\(\\sigma_{b_1} = 0.067\\). Thatâ€™s very nice. But to really believe it, letâ€™s run a simulation ourselves. The â€œCodeâ€ below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of \\(b_1\\) and \\(b_0\\) for that regression. After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of \\(b_0\\) is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of \\(b_1\\) is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of \\(\\beta_0\\) or \\(\\beta_1\\), respectively. Also, the â€œStd. Errorâ€ of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of \\(\\sigma_{b_0}\\) and \\(\\sigma_{b_1}\\), respectively. N <- 5000 #number of times to pull a random sample storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N) for (i in 1:N){ Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model mylm <- lm(Y ~ X) storage_b0[i] <- coef(mylm)[1] storage_b1[i] <- coef(mylm)[2] storage_rmse[i] <- summary(mylm)$sigma } layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3)) Ystart <- 0 #min(0,min(Y)) Ystop <- 500 #max(max(Y), 0) Yrange <- Ystop - Ystart plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), ylim=c(Ystart, Ystop), pch=16, col=\"gray\", main=\"Regression Lines from many Samples (gray lines) \\n Plus Residual Standard Deviation Lines (green lines)\") text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1) text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1) text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1) for (i in 1:N){ abline(storage_b0[i], storage_b1[i], col=\"darkgray\") } abline(beta_0, beta_1, col=\"green\", lwd=3) abline(beta_0+sigma, beta_1, col=\"green\", lwd=2) abline(beta_0-sigma, beta_1, col=\"green\", lwd=2) abline(beta_0+2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0-2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0+3*sigma, beta_1, col=\"green\", lwd=.5) abline(beta_0-3*sigma, beta_1, col=\"green\", lwd=.5) par(mai=c(1,.6,.5,.01)) addnorm <- function(m,s, col=\"firebrick\"){ curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2) lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col) lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col) lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col) lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col) lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col) lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col) lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col) legend(\"topleft\", legend=paste(\"Std. Error = \", round(s,3)), cex=0.7, bty=\"n\") } h0 <- hist(storage_b0, col=\"skyblue3\", main=\"Sampling Distribution\\n Y-intercept\", xlab=expression(paste(\"Estimates of \", beta[0], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m0 <- mean(storage_b0) s0 <- sd(storage_b0) addnorm(m0,s0, col=\"green\") h1 <- hist(storage_b1, col=\"skyblue3\", main=\"Sampling Distribution\\n Slope\", xlab=expression(paste(\"Estimates of \", beta[1], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m1 <- mean(storage_b1) s1 <- sd(storage_b1) addnorm(m1,s1, col=\"green\") t Tests{#tTests} Using the information above about the sampling distributions of \\(b_1\\) and \\(b_0\\), an immediate choice of statistical test to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) can be zero, or any other value, is a t test given by \\[ t = \\frac{b_1 - \\beta_{10}}{s_{b_1}} \\] where \\(s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\). (You may want to review the section â€œEstimating the Model Varianceâ€ of this file to know where MSE came from.) With quite a bit of work it has been shown that \\(t\\) is distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. The nearly identical test statistic for testing \\[ H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_0 \\neq \\beta_{00} \\] is given by \\[ t = \\frac{b_0 - \\beta_{00}}{s_{b_0}} \\] where \\(s^2_{b_0} = MSE\\left[\\frac{1}{n}+\\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\). This version of \\(t\\) has also been shown to be distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. Confidence Intervals Creating a confidence interval for either \\(\\beta_1\\) or \\(\\beta_0\\) follows immediately from these results using the formulas \\[ b_1 \\pm t^*_{n-2}\\cdot s_{b_1} \\] \\[ b_0 \\pm t^*_{n-2}\\cdot s_{b_0} \\] where \\(t^*_{n-2}\\) is the critical value from a t distribution with \\(n-2\\) degrees of freedom corresponding to the chosen confidence level. F tests{#Ftests} Another way to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\quad\\quad \\text{or} \\quad\\quad H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\quad\\quad \\ \\ \\quad \\quad H_a: \\beta_0 \\neq \\beta_{00} \\] is with an \\(F\\) Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an \\(F\\) test is very general and can be used in many places that a t test cannot. In its most general form, the \\(F\\) test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that \\(H_0:\\beta_1=0\\) against the alternative that \\(H_a: \\beta_1\\neq 0\\), we are essentially comparing two models against each other. If \\(\\beta_1=0\\), then the corresponding model would be \\(E\\{Y_i\\} = \\beta_0\\). If \\(\\beta_1\\neq0\\), then the model remains \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the \\(F\\) Test, that the null model (reduced model) have fewer â€œfreeâ€ parameters than the alternative model (full model). To demonstrate what we mean by â€œfreeâ€ parameters, consider the following example. Say we wanted to test the hypothesis that \\(H_0:\\beta_1 = 2.5\\) against the alternative that \\(\\beta_1\\neq2.5\\). Then the null, or reduced model, would be \\(E\\{Y_i\\}=\\beta_0+2.5X_i\\). The alternative, or full model, would be \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). Thus, the null (reduced) model contains only one â€œfreeâ€ parameter because \\(\\beta_1\\) has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two â€œfreeâ€ parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model. Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing \\(H_0: \\beta_1=0\\) against \\(H_a:\\beta_1\\neq0\\) we have the partition \\[ \\underbrace{Y_i-\\bar{Y}}_{Total} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{Regression} + \\underbrace{Y_i-\\hat{Y}_i}_{Error} \\] The reason we use \\(\\bar{Y}\\) for the null model is that \\(\\bar{Y}\\) is the unbiased estimator of \\(\\beta_0\\) for the null model, \\(E\\{Y_i\\} = \\beta_0\\). Thus we would compute the following sums of squares: \\[ SSTO = \\sum(Y_i-\\bar{Y})^2 \\] \\[ SSR = \\sum(\\hat{Y}_i-\\bar{Y})^2 \\] \\[ SSE = \\sum(Y_i-\\hat{Y}_i)^2 \\] and note that \\(SSTO = SSR + SSE\\). Important to note is that \\(SSTO\\) uses the difference between the observations \\(Y_i\\) and the null (reduced) model. The \\(SSR\\) uses the diffences between the alternative (full) and null (reduced) model. The \\(SSE\\) uses the differences between the observations \\(Y_i\\) and the alternative (full) model. From these we could set up a General \\(F\\) table of the form Â  Sum Sq Df Mean Sq F Value Model Error \\(SSR\\) \\(df_R-df_F\\) \\(\\frac{SSR}{df_R-df_F}\\) \\(\\frac{SSR}{df_R-df_F}\\cdot\\frac{df_F}{SSE}\\) Residual Error \\(SSE\\) \\(df_F\\) \\(\\frac{SSE}{df_F}\\) Total Error \\(SSTO\\) \\(df_R\\) Prediction and Confidence Intervals for \\(\\hat{Y}_h\\) (Expand) predict(â€¦, interval=â€œpredictionâ€)â€¦ It is a common mistake to assume that averages (means) describe individuals. They do not. So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line. Interval R Code Math Equation When to Use Prediction predict(..., interval=\"prediction\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\text{Pred}\\ Y}\\) Predict an individualâ€™s value. Confidence predict(..., interval=\"confidence\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\hat{Y}}\\) Estimate location of the mean y-value. predict(mylm, data.frame(XvarName = number), interval=...) For example, consider this graph. Then click here to read about the graph. Notice the three dots above 15 mph in the graph. Each of these dots show a car that was going 15 mph when it applied the brakes. However, stopping distances of the three individual cars differ with one at 20 feet, one at 26 feet and one at 54 feet. The regression line represents the average stopping distance of cars. In this case, cars going 15 mph are estimated to have an average stopping distance of about 40 feet, as shown by the line. But individual vehicles, all going the same speed of 15 mph, varied from stopping distances of 20 feet up to 54 feet! So, to predict that a car going 15 mph will take 41.4 feet to stop, doesnâ€™t tell the whole story. Far more revealing is the complete statement, â€œCars going 15 mph are predicted to take anywhere from 10.2 to 72.6 feet to stop, with an average stopping distance of 41.4 feet.â€ This is called the â€œprediction intervalâ€ and is shown in the graph in blue. It is obtained in R with the codes: cars.lm <- lm(dist ~ speed, data=cars) predict(cars.lm, data.frame(speed=15), interval=\"prediction\") cars.lm <- lm(dist ~ speed, data=cars) pander(predict(cars.lm, data.frame(speed=15), interval=\"prediction\")) fit lwr upr 41.41 10.17 72.64 plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=\"firebrick2\", cex=1.5) cars.lm <- lm(dist ~ speed, data=cars) abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3)) abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2)) abline(v=15, lty=2, col=\"firebrick\") preds <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12) lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8)) lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8)) Now, for the details behind prediction intervals and confidence intervals. Letâ€™s begin by recalling some details (from the section â€œInference for the Model Parametersâ€) about the standard error of the y-intercept, \\(b_0\\). Recall that the y-intercept is the average y-value for the given x-value of \\(x=0\\). Recall further that the formula for the standard error of \\(b_0\\) is given by the formula \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] If we wanted to be more exact with this formula, we would write it as \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{(0-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice the addition of \\((0 - \\bar{X})^2\\) instead of just \\(\\bar{X}^2\\) in the numerator of the right-most part of the equation? This more complete statement obviously would reduce to just \\(\\bar{X}^2\\), but that is only because \\(X=0\\) when we are working with the y-intercept, \\(b_0\\). We could be working with other values of \\(X\\) than just zero. Letâ€™s take a quick detour and talk notation for a second. Typically, \\(X_i\\) and \\(Y_i\\) are used to denote the x-value and y-value of points that are contained in our data set. When we want to reference a point that wasnâ€™t within our original data set, we use the notation \\(X_h\\) and \\(Y_h\\). (The letter h is close to i, but different from i, so why not. There is really no other reason to use h.) Thus, \\(Y_h\\) is the y-value for the \\(X_h\\) x-value, neither of which were included in our original regression of \\(X_i\\)â€™s and \\(Y_i\\)â€™s. Now, back to the previous discussion. If \\(X_h = 0\\), then \\(\\hat{Y}_h\\) is the y-intercept, so \\(\\hat{Y}_h = b_0\\) when \\(X_h=0\\). So, we could write, \\[ s^2_{\\hat{Y}_h} = MSE\\left[\\frac{1}{n} + \\frac{(X_h-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice how the \\(b_0\\) in \\(s_{b_0}\\) was replaced with \\(\\hat{Y}_h\\) to get \\(s_{\\hat{Y}_h}\\) and the 0 in \\((0 - \\bar{X})^2\\) was replaced with \\(X_h\\) to get \\((X_h - \\bar{X})^2\\)? Interesting. We now have a formula that would give us the standard error of \\(\\hat{Y}_h\\) for any \\(X_h\\) value, not just \\(X_h = 0\\), or the y-intercept, \\(b_0\\). That is fantastic. It would look like this if plotted. Notice how the gray region is showing the standard error for each \\(\\hat{Y}_h\\) value? (It is technically showing the confidence interval for \\(E\\{Y_h\\}\\) at every possible \\(X_h\\) value, but that is just \\(\\hat{Y}_h \\pm t^* \\cdot s_{\\hat{Y}_h}\\).) ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + theme_bw() ## `geom_smooth()` using formula = 'y ~ x' Confidence Interval for \\(\\hat{Y}_h\\) \\[ \\hat{Y}_h \\pm t^* s_{\\hat{Y}_h} \\quad \\text{where} \\ s_{\\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] The confidence interval is a wonderful tool for estimating \\(E\\{Y_h\\}\\), the â€œtrueâ€ average y-value for a given x-value of \\(X_h\\). However, it is not valuable for predicting an individual dot, or \\(Y_h\\) value. Notice how few of the dots of the regression are actually contained within the confidence interval band in the plot? The confidence interval does not really predict where the dots will land, just where the average y-value is located for each x-value. Remember the 68-95-99.7 Rule of the normal distribution? If not, here is a link back to that concept in the Math 221 textbook. This rule states that roughly 95% of data, when normally distributed, will be between \\(z=-2\\) and \\(z=2\\) standard deviations from the mean. So, is going two â€œresidual standard errorsâ€ to both sides of the regression line enough to capture 95% of the data? The answer is, not quite. The reason for this is because our knowledge of where the true mean lies is uncertain. (Notice the confidence interval band shown in the plot.) However, adding two standard errors to the edges of the confidence band would get us in the right place. In other words, there are two sources of variability at play here, (1) our uncertaintity in where the regression line is sitting, and (2) the natural variability of the data points around the line. Thus, the â€œprediction intervalâ€ requires accounting for both of these sources of variability to produce the following equation. Prediction Interval for \\(Y_h\\) \\[ \\hat{Y}_h \\pm t^* s_{Pred \\hat{Y}_h} \\quad \\text{where} \\ s_{Pred \\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + 1 + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] This formula provides a useful band for identifying a region where we are 95% confident that a new observation for \\(Y_h\\) will land, given the value of \\(X_h\\). It looks as follows. Notice the prediction interval is much wider than the confidence interval. This is because data varies far more than do means. Prediction is for where the individual data points will land, confidence is for where the mean will land. cars.lm <- lm(dist ~ speed, data=cars) predy <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + geom_segment(aes(x=15, xend=15, y=predy[2], yend=predy[3]), lwd=4, color=rgb(.5,.7,.5,.01)) + geom_point(aes(x=15, y=predy[1]), cex=2, color=\"skyblue\", pch=15) + theme_bw() Lowess (and Loess) Curves (Expand) A non-parametric approach to estimating \\(E\\{Y_i\\}\\)â€¦ Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value. Using Base R # Select the relevant columns and remove NA values air2 <- airquality %>% dplyr::select(Temp, Ozone) %>% # Use dplyr's select function explicitly drop_na() # Drop rows with NA values in Temp or Ozone plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") lines(lowess(air2$Ozone, air2$Temp), col=\"firebrick\") Using ggplot2 # Your dataset (air2) should have been created earlier, ensuring no NAs air2 <- na.omit(airquality[c(\"Temp\", \"Ozone\")]) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=\"darkgray\") + # Add points with dark gray color geom_smooth(se=FALSE, method=\"loess\", method.args = list(degree=1)) + # Lowess curve, no standard error shading theme_bw() # Black and white theme ## `geom_smooth()` using formula = 'y ~ x' # Optionally, allow for predictions as well as the graph: # Uncomment the following lines if you want predictions as well: # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fitted, x=Ozone)) # Use fitted values from loess Advantages Disadvantages Quick. Good at ignoring outliers. Good at capturing the general pattern in the data. Good for making predictions within the scope of the data. No mathematical model. Not interpretable. No p-values. No adjusted R-squared. How it Works The Lowess curve localizes the regression model to a â€œneighborhoodâ€ of points, and then joins these localized regressions together into a smooth line. It minimizes the effect of outliers, and letâ€™s the data â€œspeak for itselfâ€. As a downside, it is not interpretable, and has no final way to write the model mathematically. All the same, it is a very powerful tool for identifying an appropriate model, or verifying the fit of a model, or making predictions when no reasonable model does an adequate job. Study this graphic and the explanations below to learn how it works. Recommendation: run the code in this â€œCodeâ€ chunk to the right in your Console, and flip through the resulting graphics. X <- cars$speed Y <- cars$dist X <- X[!is.na(X) & !is.na(Y)] Y <- Y[!is.na(X) & !is.na(Y)] f <- 1/2 n <- length(X) lfit <- rep(NA,n) for (xh in 1:n){ xdists <- X - X[xh] nn <- floor(n*f) r <- sort(abs(xdists))[nn] xdists.nbrhd <- which(abs(xdists) < r) w <- rep(0, length(xdists)) w[xdists.nbrhd] <- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w), col=rgb(.2,.2,.2,.3), cex=1.5, yaxt='n', xaxt='n', xlab=\"\", ylab=\"\") points(Y[xh] ~ X[xh], pch=16, col=\"orange\") lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2)) cat(\"\\n\\n\") readline(prompt=paste0(\"Center point is point #\", xh, \"... Press [enter] to continue...\")) MADnotThereYet <- TRUE count <- 0 while(MADnotThereYet){ readline(prompt=paste0(\"\\n Adjusting line to account for outliers in the y-direction... Press [enter] to continue...\")) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"wheat\", add=TRUE) MAD <- median(abs(lmc$res)) resm <- lmc$res/(6*MAD) resm[resm>1] <- 1 bisq <- (1-resm^2)^2 w <- w*bisq obs <- coef(lmc) lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) count <- count + 1 if ( (sum(abs(obs-lmc$coef))<.1) | (count > 3)) MADnotThereYet <- FALSE } curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"green\", add=TRUE) points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=\"green\") readline(prompt=paste0(\"\\n Use final line to get fitted value for this point... Press [enter] to continue to next point...\")) lfit[xh] <- predict(lmc, data.frame(X=X[xh])) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") if (xh == n){ readline(prompt=paste0(\"\\n Press [enter] to see actual Lowess curve...\")) lines(lowess(X,Y, f=f), col=\"firebrick\") legend(\"topleft\", bty=\"n\", legend=\"Actual lowess Curve using lowess(...)\", col=\"firebrick\", lty=1) } } Select a fraction of the data to use for the â€œneighborhoodâ€ of points (shown in blue in the graph above). The lowess function in R uses â€œf=2/3â€ and the loess function uses â€œspan=0.75â€ for this value, which selects the nearest two-thirds or 75% of the data, respectively, depending on which function you use. For this example, we set the fraction of points at 50%. Both functions can be set to whatever you want. Pick any point in the regression, eventually selecting all points one at a time. The selected point becomes the â€œcenterâ€ of a â€œneighborhoodâ€ of points surrounding it. In this example, the center point is in orange, and the neighboring points are in blue. Use the points within the neighborhood to fit a regression line. However, make the regression depend most on points closest to â€œcenterâ€ and least on points furthest from â€œcenter.â€ This is called a weighted regression. Weights are decided according to what is called the tricubic weight function, so that the weight \\(w\\) given to point \\(j\\) of the neighborhood of points is defined by \\[ w_j = \\left(1- \\left( \\frac{|X_c - X_j|}{\\max_k |X_c - X_k|}\\right)^3\\right)^3 \\] where \\(X_c\\) is the x-value of the â€œcenterâ€ dot and \\(X_j\\) is the x-value of any other dot in the neighborhood. The fitted-value of \\(\\hat{Y}_c\\) is obtained for the center point \\(X_c\\) of the current regression. This point is used as the Lowess (or Loess) curveâ€™s value at that particular x-value. Well, almost. Itâ€™s a first guess at where this value will end up, but thereâ€™s a little more to the algorithm before we are done. Initial guesses for each of these fitted values are obtained for each point in the regression. Now each local regression for each neighborhood is re-run a few times in such a way the the effect of outliers is minimized. The final line for each neighborhood is obtained by the following steps. Compute all residuals for points in the neighborhood of the current regression, denoted by \\(r_i\\). Then compute the MAD, median absolute deviation, of the residuals \\(MAD = \\text{median} (|r_1|, |r_2|, \\ldots)\\). Divide all residuals by 6 times the MAD: \\(u_i = r_i/(6\\cdot MAD)\\) (If \\(r_i > 6\\cdot MAD\\) then set \\(u_i = 0\\).) Compute what are called bisquare weights using the formula: \\(b_i = (1 - u_i^2)^2\\) Perform a regression using the weights \\(w_i = w_i b_i\\) Repeat the above process with the new weights \\(w_i\\) until the weights stop changing very much. The final fitted values for each \\(X\\)-value in the regression are obtained from the final regression line for each neighborhood. These fitted values make up the Lowess (or loess) curve. Note that the default of the loess function in R is to use quadratic regressions in each neighborhood instead of linear regressions. This can be controlled with the loess option of â€œdegree=2â€ (quadratic fits) or â€œdegree = 1â€. In the lowess function only a linear regression in each neighborhood is allowed. Examples: bodyweight, cars Quantitative Y | Multiple X Graphics Y is a single quantitative variable of interest, like height. While we could use an X1 of â€œshoe-sizeâ€ to explain height, we might also want to use a second x-variable, X2, like â€œgenderâ€ to help explain height. Further x-variables could also be used. Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a studentâ€™s final exam grade? How do factors like age, weight, exercise level, and diet impact a personâ€™s blood pressure? Examples: Is one Navy base more prone to heatwaves than the other? The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH) Tests Multiple Linear Regression Multiple regression is a way to understand how different factors affect something weâ€™re interested in. Itâ€™s like having more than one clue to solve a mystery. Hereâ€™s what you need to know: We can use several different pieces of information (called variables) to predict an outcome. There are many ways to combine these pieces of information. We start with simple combinations and build up to more complex ones, kind of like building with Lego blocks. This helps us make better predictions and understand complex situations more clearly. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model is a bit like using the same ingredient twice in a recipe, but in different ways. It uses the same factor (\\(X_i^2\\)) in two different forms: Once in its regular form (\\(\\beta_1 X_i\\)) Once squared (\\(\\beta_2 X_i^2\\)) This squared part is what we call the â€œquadraticâ€ term. It helps us describe relationships that arenâ€™t just straight lines, but have curves to them. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and â€œsteepnessâ€ of the Model: negative values face down, positive values face up; large values imply â€œsteeperâ€ parabolas and low values imply â€œflatterâ€ parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)â€™s explanation. An Example Using the airquality data set, we run the following â€œquadraticâ€ regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our â€œquadraticâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. Temp Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These â€œEstimatesâ€ can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(Month^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of Month from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, ) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will â€œopen downâ€ (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be â€œmonth zero.â€ Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the â€œcubicâ€ term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the â€œbase slope coefficientâ€ and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following â€œcubicâ€ regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our â€œcubicâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. uptake Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^3) \\(X_{i}^3\\), where the function I(â€¦) protects the cubing of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will â€œopen upâ€. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The â€œtwo-linesâ€ model is a way to compare two different groups in statistics using numbers. Hereâ€™s how it works: It uses two types of numbers: A regular number that can be any value (\\(X_{1i}\\)) A special number thatâ€™s either 0 or 1 (called an â€œindicator variableâ€, \\(X_{2i}\\)) The special number (\\(X_{2i}\\)) helps us turn group information (like â€œGroup Aâ€ or â€œGroup Bâ€) into numbers we can use in calculations. This model creates two separate lines on a graph: One line for Group A One line for Group B By using this method, we can easily see and compare how the two groups are different from each other. Think of it like having two recipes for the same dish, but with slightly different ingredients. This model helps us see how those small changes affect the final result. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the â€œbase-lineâ€ of the model, the â€œGroup 0â€ line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the â€œbase-lineâ€ line. \\(\\beta_3\\) Called the â€œinteractionâ€ term. Controls the change in the slope for the second line in the model as compared to the slope of the â€œbase-lineâ€ line. An Example Using the mtcars data set, we run the following â€œtwo-linesâ€ regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our â€œtwo-linesâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. mpg Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) (beta zero): This is like the starting point. Itâ€™s estimated by something called the â€œInterceptâ€. \\(\\beta_1\\) (beta one): This shows how one thing changes when another thing (called â€œqsecâ€) changes. Itâ€™s estimated to be 1.439. \\(\\beta_2\\) (beta two): This shows how something else (called â€œamâ€) affects the overall result. Itâ€™s estimated to be -14.51. \\(\\beta_3\\) (beta three): This shows how â€œqsecâ€ and â€œamâ€ work together to affect the result. Itâ€™s estimated to be 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called â€œ3Dâ€ regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to â€œbendâ€. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the â€œTempâ€ direction is estimated to be 2.659 and the slope in the â€œMonthâ€ direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction termâ€™s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the â€œslopesâ€ in each direction to change, creating a â€œcurvedâ€ surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called â€œHDâ€, or â€œHigh Dimensionalâ€, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isnâ€™t really even possible to â€œmentally connectâ€ with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)â€™s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the â€œWindâ€ direction is estimated to be -3.334. The slope in the â€œTempâ€ direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the â€œSolar.Râ€ direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here. R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the â€œOverviewâ€ section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the â€œcolumn (c) bindâ€ function and it joins together things as columns. Res =Â  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,Â  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals. Â panel=panel.smooth,Â  This places a lowess smoothing line on each scatterplot. col =Â  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(â€œcolor1â€,â€œcolor2â€, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for â€œlinear model.â€ Y Y must be a â€œnumericâ€ vector of the quantitative response variable. Â ~Â  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats â€œnumericâ€ variables as quantitative and â€œcharacterâ€ or â€œfactorâ€ variables as qualitative. R will automatcially recode qualitative variables to become â€œnumericâ€ variables using a 0,1 encoding. See the Explanation tab for details. Â + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details. Â + â€¦, * ... emphasizes that as many explanatory variables as are desired can be included in the model. Â data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(â€¦) function displays the results of an lm(â€¦) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(â€¦) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(â€¦). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min Â  -4.3818 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -2.2696 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.1344 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  1.7058 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  5.8752 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  t value To learn more about the â€œt valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  Pr(>|t|) The â€œPrâ€ stands for â€œProbabilityâ€ and the â€œ(> |t|)â€ stands for â€œmore extreme than the observed t-valueâ€. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the â€œp-valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. (Intercept) This always says â€œInterceptâ€ for any lm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero. Â  2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the intercept (26.6248479) by its standard error (2.1829432). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a â€œstarâ€. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + â€¦). Â  -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model. Â  0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the â€œEstimateâ€ by its standard error. It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + â€¦). Â  5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\). Â  2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot â€œ.â€ implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like. Â  0.0004029 This is the estimate of the coefficient of the interaction term. Â  0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE. Â 2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\). Â on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO. Â 0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car. Â Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data. Â 0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that â€œat least one is not.â€ Â 34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom. Â on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero. Â p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that â€œat least oneâ€ of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the â€œOverviewâ€ sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the â€œfitted-valueâ€ or â€œpredicted-valueâ€ for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œpredictionâ€) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance canâ€™t go negative) feet to 60.96915 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œconfidenceâ€) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BICâ€¦ There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (â€œR-squaredâ€). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The â€œsimplestâ€ but â€œbestâ€ model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it â€œan information criterion (AIC)â€ when he published the method and other people later began calling it the â€œAkaike Information Criterion.â€) Model Selection (Expand) pairs plots, added variable plots, and pattern recognitionâ€¦ Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots {.tabset .tabset-pills} A useful visualization tool for model selection is the â€œpairs plot.â€ This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice thatâ€¦ the y-axis of each plot is found by locating the variable name (like â€œmpgâ€) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like â€œdispâ€) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model {.tabset .tabset-pills} Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldnâ€™t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Letâ€™s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander() Â  Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a modelâ€™s ability to generalize to new dataâ€¦ The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesnâ€™t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, letâ€™s remind ourselves why we use regression models in the first place. The main goal is to capture the â€œessenceâ€ of the data. In other words, the general pattern is what we are after. We want a model that tells us how â€œall suchâ€ data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the â€œcomplicated modelâ€ is overfitting the original data. It does not capture the â€œessenceâ€ of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-valueâ€¦ The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the modelâ€¦ The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions{#check} The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read moreâ€¦) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) Outlier Analysis (Expand) Cookâ€™s Distances and Leverage Valuesâ€¦ The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs.Â fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cookâ€™s Distances The idea behind Cookâ€™s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article â€œDetection of Influential Observation in Linear Regressionâ€ (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, letâ€™s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2)) Â  1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the â€œdifferencesâ€ in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cookâ€™s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cookâ€™s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cookâ€™s Distances using the code cooks.distance(lmObject). Also, a graph of Cookâ€™s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of â€œleverageâ€ and is â€œpullingâ€ the regression toward itself. A value near 0 implies the point is just â€œone of manyâ€ and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that â€œminimizeâ€ the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)â€™s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the â€œhat matrixâ€ \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the â€œleverage valuesâ€ and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a â€œlot of pull,â€ and values close to 0 represent â€œlittle pull.â€ In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with â€œlots of leverageâ€ and a large â€œCookâ€™s Distanceâ€ are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regressionâ€¦ Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class. Examples: Civic Vs Corolla cadillacs Binomial Y | Quantitative X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t. We might use an explanatory variable X of â€œheightâ€ to see if taller students are more likely to get an A in Math 325 than shorter students. (They arenâ€™t, if you were wondering.) Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1â€™s and 0â€™s (this does happen or this doesnâ€™t happen): Does the width of a childâ€™s foot predict whether or not they are right-handed? What is the probability the cat adopted is a male based on how many days it took them to get adopted? Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted? (Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\") Tests Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The â€œPâ€ stands for â€œProbability thatâ€¦â€ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1â€¦ This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)â€¦ in other words, the â€œ|â€ says â€œgivenâ€ and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the â€œnatural constantâ€ number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14â€¦) Part What it means \\(P(Y_i = 1| x_i)\\) - The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s) - The symbol used to represent the notation in logisitc regression is \\(\\pi_i\\) \\(Y_i\\) (Responsive Variable) Describes the probability of $Y_i = $ 1 or 0 for a given scenario. \\(x_i\\) (Explanatory variable) Describes the observed value and how it effects the \\(Y_i\\) \\(\\frac{e^{\\text{stuff}}}{1+e^{\\text{stuff}}}\\) The content for â€œstuffâ€ being simple linear regression of the form \\(\\beta_0 + \\beta_{1x_i}\\) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -1.5651 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.6648 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -0.2460 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  0.7276 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  2.2691 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The test statistic is a regular old z-score. It is most reliable when the sample size is â€œlarge.â€ It is a measurement of the number of standard errors the estimate is from 0. Â  Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds. Â  1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds. Â  0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a â€œstarâ€. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about. Â  Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\). Â 43.230 Â on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates. Â 29.732 This can be calculated by sum(log(myglm$res^2)). Â on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, â€œA version of Akaikeâ€™s An Information Criterionâ€¦â€ The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model. Â 33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 * ## disp -0.014604 0.005168 -2.826 0.00471 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0â€™s or 1â€™s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == â€œBâ€ or height < 68. In other words, Y needs to be a collection of 0â€™s and 1â€™s. Â ~Â  The tilde is read â€œY on X.â€ X, X is some quantitative variable. Â data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(â€¦) function. curve( A function in R that draws a â€œcurveâ€ on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the â€œInterceptâ€ estimate from your logistic regression summary output. Â \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case â€œxâ€ in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula. Â exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator. Â add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(â€¦) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set. Â aes( The aes(â€¦) function stands for â€œaestheticsâ€ and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The plus sign adds a new layer to the ggplot. Â  geom_point( Tells the plot to add the physical geometry of â€œpointsâ€ to the plot. ) Closing parenthesis for the geom_point() function. Â + Add another layer to the plot. Â  geom_smooth( Add a smoothing line to the graph. method=â€œglmâ€, This adds a general linear model to the graph. method.args = list(family=â€œbinomialâ€), This tells the method=â€œglmâ€ to choose specifically the â€œbinomialâ€ model, otherwise known as the â€œlogistic regressionâ€ model. Â se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function. Â + Add another layer to the ggplot. Â  theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),Â  The xVariableName is the same one you used in your glm(y ~ x, â€¦) statement for â€œxâ€. Input any desired value for the â€œsomeNumericValueâ€ spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016 Explanation Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal). The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\). Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page. Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful. Checking Model Assumptions {#diagnostics} The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit. Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease. Binomial Y | Multiple X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t. We might use an explanatory variable X1 of â€œheightâ€ and a second explanatory variable X2 of â€œgenderâ€ to try to predict whether or not a student will get an A in Math 325. Test(s) used for this category: Multiple Logistic Regression Model Tests Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two. Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2Â  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. â€¦, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -2.9446 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.2364 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.0000 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q Â  0.2776 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  1.9968 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The Z-score testing the hypothesis that \\(\\beta_j=0\\) Â  Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\). Â  1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\). Â  4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\). Â  3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\). Â  0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\). Â  0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a â€œstarâ€. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\). Â  471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option. Â  Null Deviance: The deviance of the null model. Â 800.44 In this case, the null deviance is 800.44 Â on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression. Â 256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit. Â on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model. Â 272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -4.97603 0.65316 -7.618 2.57e-14 *** ## Time 0.39111 0.04979 7.854 4.02e-15 *** ## Diet2 0.58283 1.04061 0.560 0.5754 ## Diet3 -8.44476 4.32324 -1.953 0.0508 . ## Diet4 -67.41995 3768.10023 -0.018 0.9857 ## Time:Diet2 0.02391 0.08679 0.275 0.7830 ## Time:Diet3 1.20955 0.51249 2.360 0.0183 * ## Time:Diet4 8.83168 471.01259 0.019 0.9850 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,Â  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, â€¦), X2 = c(Value 1, Value 2, â€¦), â€¦) command. You should see the GSS example file for an example of how to use this function. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the â€œbâ€ vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(â€œSomeColorâ€,â€œDifferentColorâ€,â€¦) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(â€¦) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==â€œBâ€. Â ~Â  The formula operator in R. X1, The first x-variable in your glm code. Â data = YourDataSet, Specify the name of your data set. Â pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(â€¦) function computes e^(stuff) in R and stands for the â€œexponential function.â€ (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(â€¦) function requires that you call the x-variable â€œxâ€ although you can change this behavior using xname=â€œSomeOtherNameâ€ if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)). Â col = palette()[1], Pull the first color from the color palette for the first curve. Â add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters. Â col = palette()[2], Set the color of this second curve to the second color in the palette. Â add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. â€œtoprightâ€, Typically the legend is placed in the top-right corner of the graph. But it could be placed â€œtopâ€, â€œtopleftâ€, â€œleftâ€, â€œbottomleftâ€, â€œbottomâ€, â€œcenterâ€, â€œrightâ€, or â€œbottomrightâ€. Â legend = Why you have to write legend again, no one knows, but you do. c(â€œLable 1â€, â€œLabel 2â€), These are the values that will appear in the legend, one entry on each line of the legend. Â col = palette(), This specifies the colors of the symbols in the legend. First color goes with â€œLabel 1â€, and second goes with â€œLabel 2â€ and so on. Â lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist. Â bty = Specifies the â€œbox typeâ€ (bty) that should be drawn around the legend. â€˜nâ€™) By placing a â€œnoâ€ option (â€˜nâ€™) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot. Â aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + Add a layer to the ggplot. Â  geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function. Â + Add a layer to the ggplot. Â  geom_smooth(method=â€œglmâ€, method.args=list(family=â€œbinomialâ€), se=FALSE, Add the logistic regression curve to the plot. Â  aes(color=â€œX2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function. Â + Add a layer to the plot. Â  theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x' Explanation Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative. The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations. Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant. Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression. Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\). Categorical Y | Categorical X Graphics Y is a single categorical variable of interest, like gender. X is another categorical variable of interest, like â€œhair colorâ€. This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender. Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year? Whats the probability that you have brown hair and blue eyes? Tests Add your own notes about appropriate inferential procedures for this type of data here.",
    "sentences": ["One Sample t Test A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable.", "Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average?", "Is human body temperature really 98.6&deg; F on average?", "Do I spend less than $3 a day,on average, purchasing snacks?", "Requirements This test is only appropriate when both of the following are satisfied.", "The sample is representative of the population.", "(Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal.", "This is a safe assumption when either: the population data can be assumed to be normally distributed using a Q-Q Plot OR the size of the sample (n) that was taken from the population is large (at least n > 30, but â€œlargeâ€ really depends on how badly the data is skewed).", "If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population.", "If the requirements are not met, then that doesnâ€™t mean the results of the test are necessarily bad, but there is no guarantee that they are good."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 28,
    "title": "ğŸ“ Quantitative Y | Categorical X (2\r\nGroups)",
    "url": "index.html#quantitativeycategoricalx2groups",
    "content": "Quantitative Y | Categorical X (2\r\nGroups) Graphics Y is a single quantitative variable of interest. This would be like â€œheightsâ€ of BYU-Idaho students. X is a qualitative (categorical) variable of interest like â€œgenderâ€ that has just two groups â€œAâ€ and â€œBâ€. So this logo represents situtations where we would want to compare heights of male (group A) and female (group B) students. Test(s) used for this category: Independent Samples T-test Analysis : High School Seniors t Test Wilcoxon Rank Sum (Mann- Whitney Test) Analysis : The Benefits of Word Recall Strategies Used for the comparison of two groups, Quantitative Y (ex. height, weight, money, distance) and Categorical X (ex. gender, birth month, phone number) Questions that this answers: How long are boys feet and how long are girls feet? Do boys (B) or girls (G) have longer feet, on average, in the KidsFeet dataset? These are the best graphics to use: Examples: How long are boys feet and how long are girls feet? (Independent Sample) Can use box or dot plot! plot_ly(KidsFeet, y=~length, x=~as.factor(sex),type=\"box\",color=~sex, colors=c(\"lightblue\",\"pink\")) %>% layout( title=\"4th Grader's Average Foot Size\", xaxis=list(title=\"Gender\"),yaxis=list(title=\"Length (cm)\")) ggplot(KidsFeet, aes(x=factor(sex),y=length, fill=sex)) + coord_flip() + geom_dotplot(binaxis=\"y\", stackdir=\"up\", position=\"dodge\", dotsize=0.45, binwidth=0.5) + scale_fill_manual(values=c(\"B\"=\"lightblue\",\"G\"=\"pink\")) + labs(title= \"Kennedy 4th Grader's Foot Size\", x=\"Length (cm)\", y=\"Gender\") Does the Meshed or Before approach have any positive benefits on memory recall when it comes to remembering content? (Wilcoxon Rank Sum Test) Dot plot over box plot shows the distribution and the individuals within each distribution",
    "sentences": ["Graphics Y is a single quantitative variable of interest.", "This would be like â€œheightsâ€ of BYU-Idaho students.", "X is a qualitative (categorical) variable of interest like â€œgenderâ€ that has just two groups â€œAâ€ and â€œBâ€.", "So this logo represents situtations where we would want to compare heights of male (group A) and female (group B) students.", "Test(s) used for this category: Independent Samples T-test Analysis : High School Seniors t Test Wilcoxon Rank Sum (Mann- Whitney Test) Analysis : The Benefits of Word Recall Strategies Used for the comparison of two groups, Quantitative Y (ex.", "height, weight, money, distance) and Categorical X (ex.", "gender, birth month, phone number) Questions that this answers: How long are boys feet and how long are girls feet?", "Do boys (B) or girls (G) have longer feet, on average, in the KidsFeet dataset?", "These are the best graphics to use: Examples: How long are boys feet and how long are girls feet?", "(Independent Sample) Can use box or dot plot!"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Quantitative Y | Categorical X (2\r\nGroups)"
  },
  {
    "id": 29,
    "title": "ğŸ“ Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single quantitative variable of interest. This would be like â€œheightsâ€ of BYU-Idaho students. X is a qualitative (categorical) variable of interest like â€œgenderâ€ that has just two groups â€œAâ€ and â€œBâ€. So this logo represents situtations where we would want to compare heights of male (group A) and female (group B) students. Test(s) used for this category: Independent Samples T-test Analysis : High School Seniors t Test Wilcoxon Rank Sum (Mann- Whitney Test) Analysis : The Benefits of Word Recall Strategies Used for the comparison of two groups, Quantitative Y (ex. height, weight, money, distance) and Categorical X (ex. gender, birth month, phone number) Questions that this answers: How long are boys feet and how long are girls feet? Do boys (B) or girls (G) have longer feet, on average, in the KidsFeet dataset? These are the best graphics to use: Examples: How long are boys feet and how long are girls feet? (Independent Sample) Can use box or dot plot! plot_ly(KidsFeet, y=~length, x=~as.factor(sex),type=\"box\",color=~sex, colors=c(\"lightblue\",\"pink\")) %>% layout( title=\"4th Grader's Average Foot Size\", xaxis=list(title=\"Gender\"),yaxis=list(title=\"Length (cm)\")) ggplot(KidsFeet, aes(x=factor(sex),y=length, fill=sex)) + coord_flip() + geom_dotplot(binaxis=\"y\", stackdir=\"up\", position=\"dodge\", dotsize=0.45, binwidth=0.5) + scale_fill_manual(values=c(\"B\"=\"lightblue\",\"G\"=\"pink\")) + labs(title= \"Kennedy 4th Grader's Foot Size\", x=\"Length (cm)\", y=\"Gender\") Does the Meshed or Before approach have any positive benefits on memory recall when it comes to remembering content? (Wilcoxon Rank Sum Test) Dot plot over box plot shows the distribution and the individuals within each distribution",
    "sentences": ["Y is a single quantitative variable of interest.", "This would be like â€œheightsâ€ of BYU-Idaho students.", "X is a qualitative (categorical) variable of interest like â€œgenderâ€ that has just two groups â€œAâ€ and â€œBâ€.", "So this logo represents situtations where we would want to compare heights of male (group A) and female (group B) students.", "Test(s) used for this category: Independent Samples T-test Analysis : High School Seniors t Test Wilcoxon Rank Sum (Mann- Whitney Test) Analysis : The Benefits of Word Recall Strategies Used for the comparison of two groups, Quantitative Y (ex.", "height, weight, money, distance) and Categorical X (ex.", "gender, birth month, phone number) Questions that this answers: How long are boys feet and how long are girls feet?", "Do boys (B) or girls (G) have longer feet, on average, in the KidsFeet dataset?", "These are the best graphics to use: Examples: How long are boys feet and how long are girls feet?", "(Independent Sample) Can use box or dot plot!"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 30,
    "title": "ğŸ“ Tests",
    "url": "index.html#tests",
    "content": "Tests Independent Samples t Test The independent samples t test is used when a value is hypothesized for the difference between two (possibly) different population means, \\(\\mu_1 - \\mu_2\\). finds the mean of each data set, then subtracts those means for their difference two groups donâ€™t depend on each otherâ€™s results Overview Questions The Independent Samples t Test can be used to answer questions like: Are boys taller than girls on average? Do students who show up to class everyday get higher scores on average than those who donâ€™t? Do you take more steps on average on weekdays or on weekends? Requirements The test is only appropriate when both of the following are satisfied. Both samples are representative of the population. (Simple random samples are the best way to do this.) The sampling distribution of the difference of the sample means \\((\\bar{x}_1 - \\bar{x}_2)\\) can be assumed to be normal. This is a safe assumption when the sample size of each group is \\(30\\) or greater or when the population data from each group can be assumed to be normal with a Q-Q Plot. Hypotheses Math Code $$ H_0: \\mu_\\text{label 1} - \\mu_\\text{label 2} = 13.2 $$ $$ H_a: \\mu_\\text{label 1} - \\mu_\\text{label 2} \\neq 13.2 $$ \\(H_0: \\mu_1 - \\mu_2 = \\text{some number, but typically 0}\\) \\(H_a: \\mu_1 - \\mu_2 \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepInd student1 student2 R Instructions Console Help Command: ?t.test() There are two ways to perform the test. Option 1: t.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a â€œnumericâ€ vector from YourData that represents the data for both samples. X must be a â€œfactorâ€ or â€œcharacterâ€ vector from YourData that represents the group assignment for each observation. There can only be two groups specified in this column of data. YourNull is the numeric value from your null hypothesis for \\(\\mu_1-\\mu_2\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y ~ X, data=YourData) Example Code Hover your mouse over the example codes to learn more. t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. lengthÂ  â€˜lengthâ€™ is a quantitative variable (numeric vector). ~Â  â€˜~â€™ is the tilde symbol. sex,Â  â€˜sexâ€™ is a â€˜factorâ€™ or â€˜characterâ€™ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet,Â  â€˜KidsFeetâ€™ is a dataset in library(mosaic). Type View(KidsFeet) to view it. mu = 0,Â  The numeric value from the null hypothesis for Î¼1-Î¼2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo-sidedâ€ meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\) Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. Welch Two Sample t-test EXPLANATION. data: length by sex EXPLANATION. t = 1.9174, EXPLANATION. Â df = 36.275, EXPLANATION. Â p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â -0.04502067 EXPLANATION. Â 1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean in group B mean in group G EXPLANATION. Â  25.10500 EXPLANATION. Â  24.32105 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. lengthÂ  â€˜lengthâ€™ is a quantitative variable (numeric vector). ~Â  â€˜~â€™ is the tilde symbol. sex,Â  â€˜sexâ€™ is a â€œfactorâ€ or â€œcharacterâ€ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet) â€˜KidsFeetâ€™ is a dataset in library(mosaic). Type View(KidsFeet) to view it. Â Click to Show OutputÂ  Click to View Output. Option 2: t.test(NameOfYourData$Y1, NameOfYourData$Y2, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a â€œnumericâ€ vector that represents the quantitative data from the first sample. Y2 must be a â€œnumericâ€ vector that represents the quantitative data from the second sample. YourNull is the numeric value from your null hypothesis for the difference of \\(\\mu_1-\\mu_2\\). This is typically zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) par(mfrow=c(1,2)) qqPlot(NameOfYourData$Y1) qqPlot(NameOfYourData$Y2) Example Code Hover your mouse over the example codes to learn more. t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. KidsFeet$length[KidsFeet$sexÂ ==Â â€œBâ€],Â  A numeric vector that represents the quantitative data or the foot length for the first sample of data which in this case is the boys. KidsFeet$length[KidsFeet$sexÂ ==Â â€œGâ€],Â  A numeric vector that represents the quantitative data or the foot length for the second sample of data which in this case is the girls. mu = 0,Â  The numeric value from the null hypothesis for Î¼1-Î¼2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo-sidedâ€ meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\) Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Welch Two Sample t-test EXPLANATION. data: KidsFeet$length[KidsFeet$sex == â€œBâ€] and KidsFeet$length[KidsFeet$sex == â€œGâ€] EXPLANATION. t = 1.9174, EXPLANATION. Â df = 36.275, EXPLANATION. Â p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â -0.04502067 EXPLANATION. Â 1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean of x mean of y EXPLANATION. Â  25.10500 EXPLANATION. Â  24.32105 EXPLANATION. par( â€˜parâ€™ is a R function that can be used to set or query graphical parameters. mfrow=c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. KidsFeet$length[KidsFeet$sexÂ ==Â â€œBâ€]) A numeric vector that represents the quantitative data or the foot length for the first sample of data which in this case is the boys. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. KidsFeet$length[KidsFeet$sexÂ ==Â â€œGâ€]) A numeric vector that represents the quantitative data or the foot length for the second sample of data which in this case is the girls. Â â€¦Â  Click to View Output. ## [1] 9 8 ## [1] 18 7 Explanation The first figure below depicts the scenario where the difference in means of two separate normal distributions is non-zero. - In other words, the two distributions have different means, \\(\\mu_1\\) and \\(\\mu_2\\), respectively. - It is worth emphasizing that the values of \\(\\mu_1\\) and \\(\\mu_2\\) are unknown to the researcher. The only thing observed are two separate samples of data (blue dots) of sizes \\(n_1\\) and \\(n_2\\), respectively. For the scenario depicted, the null hypothesis that \\(H_0: \\mu_1 - \\mu_2 = 0\\) (i.e., that \\(\\mu_1=\\mu_2\\)) is rejected in favor of the alternative that \\(H_a: \\mu_1 - \\mu_2 \\neq 0\\) based on the sample data observed. - This dicision would be correct as the true difference in means, \\(\\mu_1-\\mu_2\\) is non-zero in this case. When the null hypothesis is true, that \\(H_0: \\mu_1 - \\mu_2 = 0\\) : - the test statistic \\(t\\) that is obtained by measuring the distance between the two sample means, \\(\\bar{x}_1-\\bar{x}_2\\) - appropriately standardizing the result follows a \\(t\\) distribution with degrees of freedom less than or equal to \\(n_1+n_2-2\\). - Thus, the \\(p\\)-value of the independent samples \\(t\\) test is obtained by using this \\(t\\) distribution to calculate the probability of a test statistic \\(t\\) being as extreme or more extreme than the one observed assuming the null hypothesis is true. \\[ t = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{s_1/n_1+s_2/n_2 }} \\] Wilcoxon Rank Sum (Mann-Whitney) Test For testing the equality of the medians of two (possibly different) distributions of a quantitative variable. Box plots + Dot plots are great to show off this kind of data (because it shows the median!) Median line closer to the bottom of the box plot = Right Skewed Median line is closer to the top of the box plot = Left Skewed Overview The nonparametric equivalent of the Independent Samples t Test. Can also be used when data is ordered (ordinal) but does not have an exact measurement. For example, first place, second place, and so on. The Independent Samples t Test is more appropriate when the distributions are normal, or when the sample size for each sample is large. The test is negatively affected when there are ties (repeated values) present in the data, but the results are still useful if there are relatively few ties. Hypotheses Originally designed to test for the equality of medians from two identically shaped distributions. Math Code $$ H_0: \\text{difference in medians} = 0 $$ $$ H_a: \\text{difference in medians} \\neq 0 $$ \\(H_0: \\text{difference in medians} = 0\\) \\(H_a: \\text{difference in medians} \\neq 0\\) However, the test also allows for the more general hypotheses that one distribution is stochastically greater than the other. \\(H_0: \\text{the distributions are stochastically equal}\\) \\(H_a: \\text{one distribution is stochastically greater than the other}\\) If these hypotheses are used, then the distributions do not have to be identically distributed. (Note: Menâ€™s heights are stochastically greater than womenâ€™s heights because men are generally taller than women, but not all men are taller than all women.) Examples: BugSpray, MoralIntegration R Instructions Console Help Command: ?wilcox.test() There are two ways to perform the test. Option 1: wilcox.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a â€œnumericâ€ vector from YourData that represents the data for both samples. X must be a â€œfactorâ€ or â€œcharacterâ€ vector from YourData that represents the group assignment for each observation. There can only be two groups specified in this column of data. YourNull is the numeric value from your null hypothesis for the difference in medians from the two groups. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests. lengthÂ  â€˜lengthâ€™ is a quantitative variable (numeric vector). ~Â  â€˜~â€™ is the tilde symbol. sex,Â  â€˜sexâ€™ is a â€˜factorâ€™ or â€˜characterâ€™ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet,Â  â€˜KidsFeetâ€™ is a dataset in library(mosaic). Type View(KidsFeet) to view it. mu = 0,Â  The numeric value from the null hypothesis for the difference in medians from the two groups is 0 meaning the null hypothesis is \\(\\text{difference in medians} = 0\\) alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo-sidedâ€ meaning the alternative hypothesis is \\(\\text{difference in medians} \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\) Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Wilcoxon rank sum test with continuity correction This states the test that was performed and that a normal approximation to the test statistic was used instead of the exact distribution. data: length by sex This states that the length column was split into the two groups found in the â€œsexâ€ column. Unfortunately, it forgets to remind us that the test used the KidsFeet data set. V = 252, The test statistic of the test. In this case, the sum of the ranks from the alphabetically first group minus the minimum sum of ranks possible. Â p-value = 0.0836 The p-value of the test. alternative hypothesis: true location shift is not equal to 0 This reports that the test used an alternative hypothesis of â€œnot equalâ€ (a two-sided test). Further, the phrase â€œtrue location shiftâ€ emphasizes that the Wilcoxon Rank Sum Test is testing to see if one distribution is shifted higher or lower than the other. Option 2: wilcox.test(object1, object2, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object1 must be a â€œnumericâ€ vector that represents the first sample of data. obejct2 must be a â€œnumericâ€ vector that represents the second sample of data. YourNull is the numeric value from your null hypothesis for the difference in medians from the two groups. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests. KidsFeet$length[KidsFeet$sexÂ ==Â â€œBâ€],Â  A numeric vector of foot length for the first sample of data or for the group of boys. KidsFeet$length[KidsFeet$sexÂ ==Â â€œGâ€],Â  A numeric vector of foot length for the second sample of data or for the group of girls. mu = 0,Â  The numeric value from the null hypothesis for the difference in medians from the two groups is 0 meaning the null hypothesis is \\(\\text{difference in medians} = 0\\) alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo-sidedâ€ meaning the alternative hypothesis is \\(\\text{difference in medians} \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\) Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Wilcoxon rank sum test with continuity correction This states the type of test performed. data: KidsFeet$length[KidsFeet$sex == â€œBâ€] and KidsFeet$length[KidsFeet$sex == â€œGâ€] Reminds you what data was used for the test and points out that the first set of data listed is â€œGroup 1â€ and the second set of data listed is â€œGroup 2.â€ V = 252, The test statistic of the test. Â p-value = 0.0836 The p-value of the test. alternative hypothesis: true location shift is not equal to 0 The alternative hypothesis of the test. The phrase â€œnot equalâ€ implies a â€œtwo-sidedâ€ alternative was used. Explanation In many cases it is of interest to perform a hypothesis test concerning the equality of the centers of two (possibly different) distributions. In other words, an independent samples test. The Wilcoxon Rank Sum Test allows a nonparametric approach to doing this. It is often considered the nonparametric equivalent of the independent samples t test. The method is most easily explained through an example. The theory behind it is very similar to the theory behind the Wilcoxon Signed-Rank Test. Independent Samples Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon. Background The percent of flies (bugs) killed from two different concentrations of a certain spray were recorded from 16 different trials, 8 trials per treatment concentration. The experiment hypothesized that the center of the distributions of the percent killed by either concentration were the same. In other words, that both treatments were equally effective. The alternative hypothesis was that the treatments differed in their effectiveness. Spray Concentration Percent Killed A 68, 68, 59, 72, 64, 67, 70, 74 B 60, 67, 61, 62, 67, 63, 56, 58 Step 1 The first step of the Wilcoxon Rank Sum Test is to order all the data from smallest magnitude to largest magnitude, while keeping track of the group. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Step 2 The next step is to rank the ordered values. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=16\\). Any ranks that are tied need to have the average rank assigned to each of those that are tied. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Rank 1 2 3 4 5 6 7 8 10 10 10 12.5 12.5 14 15 16 Step 3 The ranks are then returned to their original groups. Ranks of Spray A Ranks of Spray B 3, 8, 10, 12.5, 12.5, 14, 15, 16 1, 2, 4, 5, 6, 7, 10, 10 Step 4 The ranks are summed for one of the groups. (It does not matter which group.) Sum of Ranks for Spray A: \\[ 3+8+10+12.5+12.5+14+15+16 = 91 \\] Step 5 The \\(p\\)-value of the test is then obtained by computing the probabilities of all possible sums that one group can achieve using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=16\\) ranks, with just \\(8\\) of the ranks assigned to one group, the possible sums of ranks range from \\(36\\) to \\(100\\) and include every integer in between, i.e., \\(36, 37, 38, \\ldots, 100\\). Note that the smallest sum would be obtained if the ranks 1-8 were in the group. The largest sum would be obtained if the ranks 9-16 were in the group. The probability of each possible sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 12,870 total different sets of ranks possible when there are \\(n=16\\) ranks and \\(8\\) are assigned to one group.) The distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(91\\) (or its opposite of \\(136-91=45\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of negative ranks as extreme or more extreme than \\(91\\) is \\(p=0.01476\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would reject the null hypothesis that the difference in the center of the distributions is zero. We conclude that the Spray Concentration A is more effective at killing flies (bugs). Comment The hypotheses for the Wilcoxon Rank Sum Test are difficult to write out in simple mathematical statements. The test is often referred to as a test of medians, but goes deeper than this. Technically, it allows us to determine if one distribution is stochastically larger than another. In other words, if one distribution typically gives larger values than does another distribution. If the distributions are identically shaped and have the same spread, then this implies the medians (and means) are different. Thus, the hypotheses for the test can be written mathematically as \\[ H_0: \\text{difference in medians} = 0 \\] \\[ H_a: \\text{difference in medians} \\neq 0 \\] or even as \\[ H_0: \\mu_1-\\mu_2 = 0 \\] \\[ H_a: \\mu_1-\\mu_2 \\neq 0 \\] However, it is important to remember that the estimated difference in location parameters that results from the test does not provide a measurement on either of these things. However, the \\(p\\)-value can lead us to determine whether to reject, or fail to reject the null, whichever of the above hypotheses is used. As stated in the R help file for this test ?wilcox.test() â€œthe estimator for the difference in location parameters does not estimate the difference in medians (a common misconception) but rather the median of the difference between a sample from [the first population] and a sample from [the second population].â€ These are technical details that most people ignore without encountering too much difficulty. However, it does remind us that the t test is more easily interpreted whenever it is appropriate to use that test. Final Comment Notice that when the sample size is large the distribution of the test statistic can be approximated by a normal distribution. Most software applications use this approximation when the sample size is over \\(50\\), i.e., for \\(n\\geq50\\) because computing all the possible sums of ranks becomes incredibly time consuming. Thus, for large sample sizes the results will be almost identical whether the Wilcoxon Tests or a t Test is used.",
    "sentences": ["Independent Samples t Test The independent samples t test is used when a value is hypothesized for the difference between two (possibly) different population means, \\(\\mu_1 - \\mu_2\\).", "finds the mean of each data set, then subtracts those means for their difference two groups donâ€™t depend on each otherâ€™s results Overview Questions The Independent Samples t Test can be used to answer questions like: Are boys taller than girls on average?", "Do students who show up to class everyday get higher scores on average than those who donâ€™t?", "Do you take more steps on average on weekdays or on weekends?", "Requirements The test is only appropriate when both of the following are satisfied.", "Both samples are representative of the population.", "(Simple random samples are the best way to do this.) The sampling distribution of the difference of the sample means \\((\\bar{x}_1 - \\bar{x}_2)\\) can be assumed to be normal.", "This is a safe assumption when the sample size of each group is \\(30\\) or greater or when the population data from each group can be assumed to be normal with a Q-Q Plot.", "Hypotheses Math Code $$ H_0: \\mu_\\text{label 1} - \\mu_\\text{label 2} = 13.2 $$ $$ H_a: \\mu_\\text{label 1} - \\mu_\\text{label 2} \\neq 13.2 $$ \\(H_0: \\mu_1 - \\mu_2 = \\text{some number, but typically 0}\\) \\(H_a: \\mu_1 - \\mu_2 \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepInd student1 student2 R Instructions Console Help Command: ?t.test() There are two ways to perform the test.", "Option 1: t.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a â€œnumericâ€ vector from YourData that represents the data for both samples."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 31,
    "title": "ğŸ“ Quantitative Y | Categorical X (3+\r\nGroups)",
    "url": "index.html#quantitativeycategoricalx3groups",
    "content": "Quantitative Y | Categorical X (3+\r\nGroups) Graphics Y is a single quantitative variable of interest, like â€œheightsâ€ of BYU-Idaho students. X is a categorical (qualitative) variable like which Math 221 you took, 221A, 221B, or 221C. In other words, X has three or more groups. So â€œClassrankâ€ could be X, with groups â€œFreshmanâ€, â€œSophomoreâ€, â€œJuniorâ€, and â€œSeniorâ€. Test(s) used for this category: One-Way ANOVA (means) Block Design Kruskal-Wallis Rank Sum Test (distributions) Analysis : College Studentâ€™s Food Perception Used comparing several groups based on means or distributions: How long are their feet based of width and gender? Are there certain months of the year that are associated with children having longer feet, on average, than others? Does a college studentâ€™s perception of food result in a change in their weight? (Kruskal-Wallis Rank Sum Test) # Read data food <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv\") # Data transformation and cleaning food_cleaned <- food %>% # Convert weight to numeric right away mutate(weight = as.numeric(weight)) %>% # Remove rows with NA weight or missing drink values filter(!is.na(weight) & !is.na(drink)) %>% # Create a new column `Food Perception Score` mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) # Create plots boxplot(weight ~ `Food Perception Score`, data=food_cleaned, col=\"lightgreen\", xlab=\"Food Perception Score\", main=\"Weight of College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=food_cleaned, pch=16, vertical=TRUE, add=TRUE, col=\"palegreen4\") Tests One-way ANOVA Each experimental unit is assigned to exactly one factor-level combination. Another way to say this is â€œone measurement per individualâ€ (no repeated measures) and â€œequal numbers of individuals per groupâ€. Overview An ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA can be assumed to be normally distributed when seen in a Q-Q Plot.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out in a Residuals versus fitted-values plot.) comparing means Hypotheses For a One-way ANOVA \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] Mathematical Model A typical model for a one-way ANOVA is of the form \\[ Y_{ij} = \\mu_i + \\epsilon_{ij} \\] where \\(\\mu_i\\) is the mean for level (group) \\(i\\), and \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) is the error term for each point \\(j\\) within level (group) \\(i\\). R Instructions Console Help Command: ?aov() myaov is some name you come up with to store the results of the aov() test. Y must be a â€œnumericâ€ vector of the quantitative response variable. X is a qualitative variable (should have class(X) equal to factor or character. If it does not, use as.factor(X) inside the aov(Y ~ as.factor(X),...) command. YourDataSet is the name of your data set. Perform the ANOVA myaov <- aov(Y ~ X, data=YourDataSet) summary(myaov) Diagnose ANOVA Assumptions par(mfrow=c(1,2)) plot(myaov, which=1:2) Example Code Hover your mouse over the example codes to learn more. Perform the ANOVA chick.aovÂ <-Â  Saves the results of the ANOVA test as an object named â€˜chick.aovâ€™. aov( â€˜aov()â€™ is a function in R used to perform the ANOVA. weight Y is â€˜weightâ€™, which is a numeric variable from the chickwts dataset. Â ~Â  â€˜~â€™ is the tilde symbol used to separate the Y and X in a model formula. feed, X is â€˜feedâ€™, which is a qualitative variable in the chickwts dataset, or more specifically, a factor with six levels: â€œcaseinâ€, â€œhorsebeanâ€, and so onâ€¦ Use str(chickwts) to see this. Â dataÂ =Â chickwts) â€˜chickwtsâ€™ is a dataset in R. summary( â€˜summary()â€™ shows the results of the ANOVA. chick.aov) â€˜chick.aovâ€™ is the name of the ANOVA. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to View OutputÂ  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## feed 5 231129 46226 15.37 5.94e-10 *** ## Residuals 65 195556 3009 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Diagnose the ANOVA par( â€˜parâ€™ is a R function that can be used to set or query graphical parameters. mfrowÂ =Â c(1,2)) The mfrow parameter controls â€œmultiple frames on a rowâ€. In this case, the c(1,2) specifies 1 row of 2 plots. This will cause the two diagnostic plots to be placed side-by-side. plot( â€˜plotâ€™ is a R function for the plotting of R objects. chick.aov, â€˜chick.aovâ€™ is the name of the ANOVA. Â whichÂ =Â 1:2) The which=1:2 selects â€œwhichâ€ of 6 available plots we want to have graphed. In this case, 1 shows the Residuals vs Fitted, and 2 shows the Normal QQ-plot. Both are needed to check the ANOVA assumptions. Â Click to View OutputÂ  Click to View Output. Explanation Analysis of variance (ANOVA) is often applied to the scenario of testing for the equality of three or more means from (possibly) separate normal distributions of data. The normality assumption is required. No matter the sample size. If the distributions are skewed then a nonparametric test should be applied instead of ANOVA. One-Way ANOVA One-way ANOVA is when a completely randomized design is used with a single factor of interest. A typical mathematical model for a one-way ANOVA is of the form \\[ Y_{ik} = \\mu_i + \\epsilon_{ik} \\quad (\\text{sometimes written}\\ Y_{ik} = \\mu + \\alpha_i + \\epsilon_{ik}) \\] where \\(\\mu_i\\) is the mean of each group (or level) \\(i\\) of a factor, and \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) is the error term. The plot below demonstrates what these symbols represent. Note that the notation \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) states that we are assuming the error term \\(\\epsilon_{ik}\\) is normally distributed with a mean of 0 and a standard deviation of \\(\\sigma\\). Hypotheses The aim of ANOVA is to determine which hypothesis is more plausible, that the means of the different distributions are all equal (the null), or that at least one group mean differs (the alternative). Mathematically, \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_m = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\quad \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\}. \\] In other words, the goal is to determine if it is more plausible that each of the \\(m\\) different samples (where each sample is of size \\(n\\)) came from the same normal distribution (this is what the null hypothesis claims) or that at least one of the samples (and possibly several or all) come from different normal distributions (this is what the alternative hypothesis claims). Visualizing the Hypotheses The first figure below demonstrates what a given scenario might look like when all \\(m=3\\) samples of data are from the same normal distribution. In this case, the null hypothesis \\(H_0\\) is true. Notice that the variability of the sample means is smaller than the variability of the points. The figure below shows what a given scenario might look like for \\(m=3\\) samples of data from three different normal distributions. In this case, the alternative hypothesis \\(H_a\\) is true. Notice that the variability of the sample means, i.e., \\((\\bar{x}_1,\\bar{x}_2,\\bar{x}_3)\\), is greater than the variability of the points. Explaining the Name The above plots are useful in understanding the mathematical details behind ANOVA and why it is called analysis of variance. Recall that variance is a measure of the spread of data. When data is very spread out, the variance is large. When the data is close together, the variance is small. ANOVA utilizes two important variances, the between groups variance and the within groups variance. Between groups varianceâ€“a measure of the variability in the sample means, the \\(\\bar{x}\\)â€™s. Within groups varianceâ€“a combined measure of the variability of the points within each sample. The plot below combines the information from the previous plots for ease of reference. It emphasizes the fact that when the null hypothesis is true, the points should have a large variance (be really spread out) while the sample means are relatively close together. On the other hand, when the points are relative close together within each sample and the sample means have a large variance (are really spread out) then the alternative hypothesis is true. This is the theory behind analysis of variance, or ANOVA. Calculating the Test Statistic, \\(F\\) The ratio of the â€œbetween groups variationâ€ to the â€œwithin groups variationâ€ provides the test statistic for ANOVA. Note that the test statistic of ANOVA is an \\(F\\) statistic. \\[ F = \\frac{\\text{Between groups variation}}{\\text{Within groups variation}} \\] It would be good to take a minute and review the \\(F\\) distribution. The \\(p\\)-value for ANOVA thus comes from an \\(F\\) distribution with parameters \\(p_1 = m-1\\) and \\(p_2 = n-m\\) where \\(m\\) is the number of samples and \\(n\\) is the total number of data points. A Deeper Look at Variance It is useful to take a few minutes and explain the word variance as well as mathematically define the terms â€œwithin group varianceâ€ and â€œbetween groups variance.â€ Variance is a statistical measure of the variability in data. The square root of the variance is called the standard deviation and is by far the more typical measure of spread. This is because standard deviation is easier to interpret. However, mathematically speaking, the variance is the more important measurement. As mentioned previously, the variance turns out to be the key to determining which hypothesis is the most plausible, \\(H_0\\) or \\(H_a\\), when several means are under consideration. There are two variances that are important for ANOVA, the â€œwithin groups varianceâ€ and the â€œbetween groups variance.â€ Recall that the formula for computing a sample variance is given by \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})^2}{n-1} \\quad\\leftarrow \\frac{\\text{sum of squares}}{\\text{degrees of freedom}} \\] This formula has a couple of important pieces that are so important they have been given special names. The \\(n-1\\) in the denominator of the formula is called the â€œdegrees of freedom.â€ The other important part of this formula is the \\(\\sum_{i=1}^n(x_i - \\bar{x})^2\\), which is called the â€œsum of squared errorsâ€ or sometimes just the â€œsum of squaresâ€ or â€œSSâ€ for short. Thus, the sample variance is calculated by computing a â€œsum of squaresâ€ and dividing this by the â€œdegrees of freedom.â€ It turns out that this general approach works for many different contexts. Specifically, it allows us to compute the â€œwithin groups varianceâ€ and the â€œbetween groups variance.â€ To introduce the mathematical definitions of these two variances, we need to introduce some new notation. Let \\(\\bar{y}_{i\\bullet}\\) represent the sample mean of group \\(i\\) for \\(i=1,\\ldots,m\\). Let \\(n_i\\) denote the sample size in group \\(i\\). Let \\(\\bar{y}_{\\bullet\\bullet}\\) represent the sample mean of all \\(n = n_1+n_2+\\cdots+n_m\\) data points. The mathematical calculations for each of these variances is given as follows. \\[ \\text{Between groups variance} = \\frac{\\sum_{i=1}^m (\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2}{m-1} \\leftarrow \\frac{\\text{Between groups sum of squares}}{\\text{Between groups degrees of freedom}} \\] \\[ \\text{Within groups variance} = \\frac{\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2}{n-m} \\leftarrow \\frac{\\text{Within groups sum of squares}}{\\text{Within groups degrees of freedom}} \\] A Fabricated Example The following table provides three samples of data: A, B, and C. These samples were randomly generated from normal distributions using a computer. The true means \\(\\mu_1, \\mu_2\\), and \\(\\mu_3\\) of the normal distributions are thus known, but withheld from you at this point of the example. A B C 13.15457 13.17463 16.66831 12.65225 12.16277 15.54719 13.73061 12.76905 16.63074 14.43471 13.38524 15.06726 13.79728 12.02690 15.57534 13.88599 13.24651 15.99915 12.77753 12.58386 15.58995 13.81536 12.64615 16.99429 13.03635 12.52055 15.47153 14.26062 14.03566 16.13330 An ANOVA will be performed with the sample data to determine which hypothesis is more plausible: \\[ H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\in \\{1,\\ldots,m\\} \\] To perform an ANOVA, we must compute the between groups variance and the within groups variance. This requires the Between groups sums of squares, within groups sums of squares, between groups degrees of freedom, and the within groups degrees of freedom. Note that to get the sums of squares, we first had to calculate \\(\\bar{y}_{1\\bullet}\\), \\(\\bar{y}_{2\\bullet}\\), \\(\\bar{y}_{3\\bullet}\\), and \\(\\bar{y}_{\\bullet\\bullet}\\) where the 1, 2, 3 corresponds to Samples A, B, and C, respectively. After some work, we find these values to be \\[ \\bar{y}_{1\\bullet} = 13.55, \\quad \\bar{y}_{2\\bullet} = 12.86 \\quad \\bar{y}_{3\\bullet} = 15.97 \\] and \\[ \\bar{y}_{\\bullet\\bullet} = \\frac{13.55+12.86+15.97}{3} = 14.13 \\] Using these values we can then compute the between groups sum of squares and the within groups sum of squares according to the formulas stated previously. This process is very tedious and will not be demonstrated. Only the results are shown in the following table which summarizes all the important information. Â  Degrees of Freedom Sum of Squares Variance F-value p-value Between groups 2 53.3 26.67 70.2 2e-11 Within groups 27 10.3 0.38 ANOVA Table In general, the ANOVA table is created by Â  Degrees of Freedom Sum of Squares Variance F-value p-value Between groups \\(m-1\\) \\(\\sum_{i=1}^m n_i(\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) \\(\\frac{\\text{Between groups variance}}{\\text{Within groups variance}}\\) \\(F\\)-distribution tail probability Within groups \\(n-m\\) \\(\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) ANOVA Assumptions{#residuals} The requirements for an analysis of variance (the assumptions of the test) are two-fold and concern only the error terms, the \\(\\epsilon_{ik}\\). The errors are normally distributed. The variance of the errors is constant. Both of these assumptions were stated in the mathematical model where we assumed that \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\). Checking ANOVA Assumptions To check that the ANOVA assumptions are satisfied, it is required to check the data in each group for normality using QQ-Plots. Also, the sample variance of each group must be relatively constant. The fastest way to check these two assumptions is by analyzing the residuals. An ANOVA residual is defined as the difference between the observed value of \\(y_{ik}\\) and the mean \\(\\bar{y}_{i\\bullet}\\). Mathematically, \\[ r_{ik} = y_{ik} - \\bar{y}_{i\\bullet} \\] One QQ-Plot of the residuals will provide the necessary evidence to decide if it is reasonable to assume that the error terms are normally distributed. Also, the constant variance can be checked visually by using what is known as a residuals versus fitted values plot. For the Fabricated Example above, the QQ-Plot and residuals versus fitted values plots show the two assumptions of ANOVA appear to be satisfied. Examples: chickwts (One-way) Block Design Repeated measures or other factors that group individuals into similar groups (blocks) are included in the study design. Overview A typical model for a block design is of the form \\[ Y_{lijk} = \\mu + B_l + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijlk} \\] where \\(\\mu\\) is the grand mean, \\(B_l\\) is the blocking factor, \\(\\alpha_i\\) is one factor with at least two levels, \\(\\beta_j\\) is another factor with at least two levels, \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors, and \\(\\epsilon_{ijlk} \\sim N(0,\\sigma^2)\\) is the error term. Only one block and one factor is required. Multiple blocks and multiple factors are allowed. It is not required to include interaction terms. The error term is always required. R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ Block+X1+X2+X1:X2, data=YourDataSet) Perform the test summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a â€œnumericâ€ vector of the quantitative response variable. Block is a qualitative variable that is not of direct interest, but is included in the model to account for variability in the data. It should have class(Block) equal to either factor or character. Use as.factor() if it does not. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. If it does not, use as.factor(X2). Note that factors C, D, and so on could also be added + to the model if desired. X1:X2 denotes the interaction of the factors X1 and X2. It is not required and should only be included if the interaction term is of interest. YourDataSet is the name of your data set. Examples: ChickWeight Kruskal-Wallis Rank Sum Test Allows for deciding if several samples come from the same population or if at least one sample comes from a different population. Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated. Can also be used when data is ordered (ordinal) but does not have an exact measurement. Best used when the distribution of the data is not normal. ANOVA is appropriate when the data is normal. comparing distributions Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, â€œis a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly. In many cases, this is practically equivalent to the mean of at least one population differing from the others.â€ Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable. g is a qualitative grouping variable defining which groups each value in x belongs to. It must either be a character vector or a factor vector. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. weight â€˜weightâ€™ is a numeric variable from the chickwts dataset that represents the quantatitive response variable. Â ~Â  â€˜~â€™ is the tilde symbol used to separate the left- and right-hand side in a model formula. feed, â€˜feedâ€™ is a qualitative grouping variable in the chickwts dataset. Â dataÂ =Â chickwts) â€˜chickwtsâ€™ is a dataset in R. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to View OutputÂ  Click to View Output. ## ## Kruskal-Wallis rank sum test ## ## data: weight by feed ## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07 Alternatively, you could use the following approach as well. kruskal.test(list(x, y, z)) x is a numeric vector of data values that represents the quantatitive response variable for the first group. y is a numeric vector of data values that represents the quantatitive response variable for the second group. z is a numeric vector of data values that represents the quantatitive response variable for the third group. Note that more than three vectors of data could be included inside of list(). Example Code Hover your mouse over the example codes to learn more. feed1 <- filter(chickwts, feed == â€œhorsebeanâ€) This splits out the first group of feed (horsebean) from the chickwts dataset. feed2 <- filter(chickwts, feed == â€œlinseedâ€) This splits out the second group of feed (linseed) from the chickwts dataset. feed3 <- filter(chickwts, feed == â€œsoybeanâ€) This splits out the third group of feed (soybean) from the chickwts dataset. feed4 <- filter(chickwts, feed == â€œsunflowerâ€) This splits out the fourth group of feed (sunflower) from the chickwts dataset. feed5 <- filter(chickwts, feed == â€œmeatmealâ€) This splits out the fifth group of feed (meatmeal) from the chickwts dataset. feed6 <- filter(chickwts, feed == â€œcaseinâ€) This splits out the sixth group of feed (casein) from the chickwts dataset. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. list(feed1$weight,feed2$weight,feed3$weight,feed4$weight,feed5$weight,feed6$weight)) A list of numeric vecotrs that represent the quantitative response variable for each group. Â Click to View OutputÂ  Click to View Output. Load library(tidyverse) to run this code in R. ## ## Kruskal-Wallis rank sum test ## ## data: list(feed1$weight, feed2$weight, feed3$weight, feed4$weight, feed5$weight, feed6$weight) ## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07 Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population. The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples. The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity. Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated. (Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\). Note that the notation \\(n_i\\) allows for each sample to be a different size. In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\). Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest. Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\). Award any tied values the average of the ranks of those values that are tied. In the bottle-cap data we have the following. Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample. Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\). In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\). In the bottle-cap data, \\(\\bar{R}_1 = \\frac{R_1}{n_1} = \\frac{24}{5} = 4.8\\) \\(\\bar{R}_2 = \\frac{R_2}{n_2} = \\frac{14}{3} = 4.667\\) \\(\\bar{R}_3 = \\frac{R_3}{n_3} = \\frac{40}{4} = 10.0\\) Step 5: If there are no ties, calculate the test statistic \\(H\\) by the formula \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] See the original article for the formula to use when there are ties present in the data. In the bottle-cap data, \\(H = 5.656\\). To understand the logic behind the test statistic \\(H\\), note that the null hypothesis assumes that each of the \\(C\\) samples are taken from the same population. Under this assumption, the ranks assigned to each sample should represent a uniform sample of the ranks \\(1,\\ldots,N\\). If the null hypothesis were true, we would expect the average of the ranks from each group, the \\(\\bar{R}_i\\), to be roughly about the same. More precisely, using the properties of the discrete uniform distribution (see wikipedia), we expect each \\(\\bar{R}_i\\) to be close to \\(\\frac{1}{2}(N+1)\\), which is the mean of the discrete uniform distribution. That \\(\\bar{R}_i\\) should be close to this value comes from the fact that the mean of a distribution of means is equal to the population mean. Further, the variance of \\(\\bar{R}_i\\) is given by the population variance, \\(\\frac{1}{12}(N^2-1)\\) in this case, divided by the sample size \\(n_i\\). Finally, the \\(\\frac{N-1}{N}\\) is a correction factor that will not be explained here. The full article explains the details about how a multivariate normal distribution is used to uncover the formula for \\(H\\). Thus, understanding \\(H\\) completely will require an advanced degree in statistical theory. However, the short story is that the distribution of the test statistic \\(H\\) is approximately a chi squared distribution with \\(C-1\\) degrees of freedom and has the form of a sum of values consisting of the (observed mean - expected mean)^2/(variance of the observed mean). Step 6: Calculate the p-value of \\(H\\) using a chi squared distribution with \\(C-1\\) degrees of freedom. \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] In the bottle-cap data, \\(p = 0.059\\), which represents the probability of \\(H\\) being as extreme or more extreme than the observed value of 5.656, assuming the null hypothesis is true. Examples: wages, pigweights Quantitative Y | Multiple Categorical X Graphics Y is a single quantitative variable of interest, like â€œheightsâ€ of BYU-Idaho students. X1 is a categorical (qualitative) variable like gender, with levels â€œboyâ€ and â€œgirl.â€ X2 is another categorical (qualitative) variable like â€œClassrankâ€ with levels â€œFreshmanâ€, â€œSophomoreâ€, and â€œJuniorâ€. Of course, both X1 and X2 can have as many levels as you want to work with, but must have at least two levels each. Test(s) used for this category: Two-Way ANOVA (means) Used to answer THREE questions: the effects of factor 1 on the quantitative variable the effects of factor 2 on the quantitative variable the interaction between the two factors best paired with numerical summary looking at mean Analysis : Two-way ANOVA: Final Beginning Algebra Grades Question Examples: Based on your p-values and these graphs, which of the following is a correct conclusion to reach? xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1))) ANSWER : Among fourth graders, right handed boys have longer feet on average than left handed boys while the opposite is true for girls. Does the amount a student participates in the course affect the average final Beginning Algebra grades? Does the semester in which a student takes Beginning Algebra affect their average final grade? Does a studentâ€™s participation level vary depending on the semester? (Alternatively, does the semester influence a studentâ€™s participation?) BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") BAFinalGrades$Semester <- factor(BAFinalGrades$Semester, levels = c(\"Fall\",\"Winter\",\"Spring\")) ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=1)) + geom_point(color=\"firebrick1\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Spring Semester Seems to Have Higher Final Grades\", x=\"Semester\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades$Participation <- factor(BAFinalGrades$Participation, levels = c(\"Low\",\"Moderate\",\"High\")) ggplot(BAFinalGrades, aes(x=Participation, y=FinalGrade, group=1)) + geom_point(color=\"darkred\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Participation Seems to Lead to Higher Grades\", x=\"Participation\", y=\"Beginning Algebra Final Grades\") + theme_minimal() ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=Participation, color=Participation)) + geom_point() + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Beginning Algebra Seems Better during the Spring Semester\", x=\"BYU-Idaho Semesters\", y=\"Beginning Algebra Final Grades\") + theme_minimal() Tests Two-way ANOVA Overview A two-way ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses. Writing out the hypotheses can be very involved depending on whether you use the official â€œeffects modelâ€ notation (very mathematically correct) or a simplified â€œmeans modelâ€ notation (which isnâ€™t very mathematically correct, but gets the idea across in an acceptable way). Means Model Effects Model The first set of hypotheses are a â€œone-wayâ€ set of hypotheses for the first factor of the ANOVA. Factor: X1 with say, levels \\(A\\) and \\(B\\). \\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a â€œone-wayâ€ set of hypotheses, but for the second factor of the ANOVA. Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\mu_C = \\mu_D = \\mu_E = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one}\\ i\\in\\{1=C,2=D,3=E\\} \\] The third set of hypotheses are the most interesting hypotheses in a two-way ANOVA. These are called the interaction hypotheses. They test to see if the levels of one of the factors, say \\(X1\\), impact \\(Y\\) differently for the differing levels of the other factor, \\(X2\\). The hypotheses read formally as \\[ H_0: \\text{The effect of the first factor on Y} \\\\ \\text{is the same for all levels of the second factor.} \\] \\[ H_a: \\text{The effect of the first factor on Y is not the same} \\\\ \\text{for all levels of the second factor.} \\] A mathematically correct way to state the two-way ANOVA model is with the equation \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] In this model, \\(\\mu\\) is the grand mean (which is the average Y-value ignoring all information contained in the factors); \\(\\alpha_i\\) is the first factor \\(X1\\) with levels \\(A\\) and \\(B\\) (though there could be more levels in \\(X1\\) depending on your data); \\(\\beta_j\\) is the second factor with levels \\(C\\), \\(D\\), and \\(E\\) (though there could be fewer or more levels to this factor depending on your data); \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors which has \\(2\\times3=6\\) (may differ for your data) levels; and \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) is the normally distributed error term for each point \\(k\\) found within level \\(i\\) of \\(X1\\) and level \\(j\\) of \\(X2\\). This model allows us to more formally state the hypotheses as First factor \\(X1\\) having say, levels \\(A\\) and \\(B\\). \\[ H_0: \\alpha_A = \\alpha_B = 0 \\] \\[ H_a: \\alpha_i \\neq 0 \\ \\text{for at least one}\\ i\\in\\{1=A,2=B\\} \\] Second factor \\(X2\\) with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\beta_C = \\beta_D = \\beta_E = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j\\in\\{1=C,2=D,3=E\\} \\] Does the effect of the first factor (\\(X1\\)) change for the different levels of the second factor (\\(X2\\))? In other words, is there an interaction between the two factors \\(X1\\) and \\(X2\\)? \\[ H_0: \\alpha\\beta_{ij} = 0 \\ \\text{for all } i,j \\] \\[ H_a: \\alpha\\beta_{ij} \\neq 0 \\ \\text{for at least one } i,j \\] R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ X1+X2+X1:X2, data=YourDataSet) Perform the ANOVA summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a â€œnumericâ€ vector of the quantitative response variable. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. Note that factors X3, X4, and so on could also be added + to the model if desired, but this would create a three-way, or four-way ANOVA model, and so on. X1:X2 denotes the interaction of the factors X1 and X2. It is not required, but is usually included. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. warp.aovÂ <-Â  Saves the results of the ANOVA test as an object named â€˜warp.aovâ€™. aov( â€˜aov()â€™ is a function in R used to perform the ANOVA. breaks \\(Y\\) is â€˜breaksâ€™, which is a numeric variable from the warpbreaks dataset. Â ~Â  â€˜~â€™ is the tilde symbol used to separate the left- and right-hand side in a model formula. woolÂ  The first factor \\(X1\\) is â€˜woolâ€™, which is a qualitative variable in the warpbreaks dataset. In this case, wool is a factor with two levels. Use str(warpbreaks) to see this. +Â tensionÂ  The second factor \\(X2\\) is â€˜tensionâ€™, which is another qualitative variable in the warpbreaks dataset. In this case, tension is a factor with three levels. Use str(warpbreaks) to see this. +Â wool:tension, The interaction of the two factors: wool and tension. Â dataÂ =Â warpbreaks) â€˜warpbreaksâ€™ is a dataset in R. summary( â€˜summary()â€™ shows the results of the ANOVA. warp.aov) â€˜warp.aovâ€™ is the name of the ANOVA. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to View OutputÂ  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## wool 1 451 450.7 3.765 0.058213 . ## tension 2 2034 1017.1 8.498 0.000693 *** ## wool:tension 2 1003 501.4 4.189 0.021044 * ## Residuals 48 5745 119.7 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 par( â€˜parâ€™ is a R function that can be used to set or query graphical parameters. mfrowÂ =Â c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. plot( â€˜plotâ€™ is a R function for the plotting of R objects. warp.aov, â€˜warp.aovâ€™ is the name of the ANOVA. Â whichÂ =Â 1:2) Will show the Residuals vs Fitted and the Normal QQ-plot to check the ANOVA assumptions. Â Click to View OutputÂ  Click to View Output. Explanation Hypotheses in Two-way ANOVA The hypotheses that can be tested in a two-way ANOVA that includes an interaction term are three-fold. Hypotheses about \\(\\alpha\\) where \\(\\alpha\\) has \\(m\\) levels. \\[ H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_m = 0 \\] \\[ H_a: \\alpha_i \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\} \\] Hypotheses about \\(\\beta\\) where \\(\\beta\\) has \\(q\\) levels. \\[ H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_q = 0 \\] \\[ H_a: \\beta_j \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,q\\} \\] Hypotheses about the interaction term \\(\\alpha\\beta\\). \\[ H_0: \\text{the effect of one factor is the same across all levels of the other factor} \\] \\[ H_a: \\text{the effect of one factor differs for at least one level of the other factor} \\] Expanding the ANOVA Model{#expanding} It turns out that more can be done with ANOVA than simply checking to see if the means of several groups differ. Reconsider the mathematical model of two-way ANOVA. \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] This model could be expanded to include any number of new terms in the model. The power of this approach is in the several questions (hypotheses) that can be posed to data simultaneously. What happens if your ANOVA fails both requirement tests? Example: BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") ## Rows: 204 Columns: 3 ## â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ## Delimiter: \",\" ## chr (2): Semester, Participation ## dbl (1): FinalGrade ## ## â„¹ Use `spec()` to retrieve the full column specification for this data. ## â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message. BAanova <- aov(FinalGrade ~ Semester + Participation + Semester:Participation, data=BAFinalGrades) par(mfrow=c(1,2)) plot(BAanova, which=1:2, pch=16) The residual plot shows that the constant (equal) variance assumption raises some concern as the points within each group have varying spread. The spread of each group seems to gets closer and closer as we go from left to right as well as some straying point from the more clustered groups. Thus, showing the variance of the residuals are not constant and do not accurately show a relationship between each variable. For the Q-Q plot, it shows that the residuals of the data is not quite normally distributed. There are visible points at the lower and upper ends of the graph deviating away form the diagonal line, which represents a normal distribution. Thus, showing us that the residuals of the data are not normal distributed. Although both graphs indicate that the data fails to meet both requirements, this primarily means our findings may be less reliable due to the abnormalities in the data. As a result, we should interpret the results cautiously as we proceed with our Two-Way ANOVA test! Examples: warpbreaks (Two-way), CO2 (three-way) Quantitative Y | Quantitative X Graphics Y is a single quantitative variable of interest, like â€œheightâ€. X is another single quantitative variable of interest, like â€œshoe-sizeâ€. This would imply we are using â€œshoe-sizeâ€ (X) to explain â€œheightâ€ (Y). Test(s) used for this category: Simple Linear Regression Analysis : Simple Linear Regression: BYU-Idaho Math Tutoring Used to find the relationship between one independent variable (predictor) and one dependent variable(outcome): Is there a relationship between the length of childrenâ€™s feet and the width of their foot? Example: Does the amount of tutoring a student receives correlate with their chapter exam scores? (Simple Linear Regression) ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams) Tests Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\). Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$\\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$$ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is â€œequal toâ€ the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to â€œGodâ€™s Lawâ€ or â€œNatural Lawâ€. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are â€œrandomâ€ and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)â€¦ \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced â€œy-hatâ€, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what followsâ€¦ \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, â€œthe change in the average y-value for a one unit change in the x-value.â€ It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, â€œthe average y-value when x is zero.â€ It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful. R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your lm() code into mylm name. lm( lm(â€¦) is an R function that stands for â€œLinear Modelâ€. It performs a linear regression analysis for Y ~ X. YÂ  Y is your quantitative response variable. It is the name of one of the columns in your data set. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X. X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value. Â data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X. ) Closing parenthesis for the lm(â€¦) function. summary(mylm) The summary command allows you to print the results of your linear regression that were previously saved in mylm name. Â Click to Show OutputÂ  Click to View Output. Example output from a regression. Hover each piece to learn more. Call: lm(formula = dist ~ speed, data = cars) This is simply a statement of your original lm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(â€¦). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min Â  -29.069 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -9.525 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -2.272 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -2.272 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-2.272) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  9.215 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  43.201 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  t value To learn more about the â€œt valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  Pr(>|t|) The â€œPrâ€ stands for â€œProbabilityâ€ and the â€œ(> |t|)â€ stands for â€œmore extreme than the observed t-valueâ€. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the â€œp-valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. (Intercept) This always says â€œInterceptâ€ for any lm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -17.5791 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  6.7584 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -2.601 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the intercept (-17.5791) by its standard error (6.7584). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of -17.5791 is -2.601 standard errors (6.7584) from zero, which is a fairly surprising distance as shown by the p-value. 0.0123 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). speed This is always the name of your X-variable in your lm(Y ~ X, â€¦). Â  3.9324 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.4155 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 9.464 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the slope (3.9324) by its standard error (0.4155). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from zero, which is a really surprising distance as shown by the smallness of the p-value. 1.49e-12 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE. Â 15.38 For this particular regression, the estimate of \\(\\sigma\\) is 15.38. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\). Â on 48 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 50 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 50-2 = 48. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO. Â 0.6511, In this particular regression, 65.11% of the variation in stopping distance dist is explained by the regression model using speed of the car. Â Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data. Â 0.6438 In this case, the value of 0.6438 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is found as the ratio of the MSR/MSE where MSR = SSR/(p-1) and MSE = SSE/(n-p) where n is the sample size and p is the number of parameters in the regression model. Â 89.57 This is the value of the F-statistic for the lm(dist ~ speed, data=cars) regression. Note that SSE = sum( cars.lm$res^2 ) = 11353.52 with n - p = 50 - 2 = 48 degrees of freedom for this data. Further, SSR = sum( (cars.lm$fit - mean(cars$dist))^2 ) = 21185.46 with p - 1 = 1 degree of freedom. So MSR = 21185.46 and MSE = 11353.52 / 48 = 236.5317. So MSR / MSE = 21185.46 / 236.5317 = 89.56711. Â on 1 and 48 DF, The 1 degree of freedom is the SSR degrees of freedom (p-1). The 48 is the SSE degrees of freedom (n-p). Â p-value: 1.49e-12 The p-value for an F-statistic is found by the code pf(89.56711, 1, 48, lower.tail=FALSE), which gives the probability of being more extreme than the observed F-statistic in an F distribution with 1 and 48 degrees of freedom. Check Assumptions 1, 2, 3, and 5 When making it in an r chunk make your height 3! { r, fig.height = 3 } par( The par(â€¦) command stands for â€œGraphical PARametersâ€. It allows you to control various aspects of graphics in Base R. mfrow= This stands for â€œmultiple frames filled by rowâ€, which means, put lots of plots on the same row, starting with the plot on the left, then working towards the right as more plots are created. c( The combine function c(â€¦) is used to specify how many rows and columns of graphics should be placed together. 1, This specifies that 1 row of graphics should be produced. 3 This states that 3 columns of graphics should be produced. ) Closing parenthesis for c(â€¦) function. ) Closing parenthesis for par(â€¦) function. plot( This version of plot(â€¦) will actually create several regression diagnostic plots by default. mylm, This is the name of an lm object that you created previously. which= This allows you to select â€œwhichâ€ regression diagnostic plots should be drawn. 1 Selecting 1, would give the residuals vs.Â fitted values plot only. : The colon allows you to select more than just one plot. 2 Selecting 2 also gives the Q-Q Plot of residuals. If you wanted to instead you could just use which=1 to get the residuals vs fitted values plot, then you could use qqPlot(mylm$residuals) to create a fancier Q-Q Plot of the residuals. ) Closing parenthesis for plot(â€¦) function. plot( This version of plot(â€¦) will be used to create a time-ordered plot of the residuals. The order of the residuals is the original order of the x-values in the original data set. If the original data set doesnâ€™t have an order, then this plot is not interesting. mylm The lm object that you created previously. $ This allows you to access various elements from the regression that was performed. residuals This grabs the residuals for each observation in the regression. ) Closing parenthesis for plot(â€¦) function. Â Click to Show OutputÂ  Click to View Output. Which regression assumption(s) does the residuals versus fitted values plot diagnose? Linear Relation Constance Variance Which regression assumption(s) does the Q-Q Plot of the residuals diagnose? Normal Errors Which regression assumption(s) does the Residuals versus Order plot diagnose? (Remember, this plot can only be created when the data was collected in a specific order.) Independent Errors Plotting the Regression Line Base R ggplot2 To add the regression line to a scatterplot use the abline(...) command: plot( The plot(â€¦) function is used to create a scatterplot with a y-axis (the vertical axis) and an x-axis (the horizontal axis). YÂ  This is the â€œresponse variableâ€ of your regression. The thing you are interested in predicting. This is the name of a â€œnumericâ€ column of data from the data set called YourDataSet. ~Â  The tilde â€œ~â€ is used to relate Y to X and can be found on the top-left key of your keyboard. X,Â  This is the explanatory variable of your regression. It is the name of a â€œnumericâ€ column of data from YourDataSet. data= The data= statement is used to specify the name of the data set where the columns of â€œXâ€ and â€œYâ€ are located. YourDataSet This is the name of your data set, like KidsFeet or cars or airquality. ) Closing parenthesis for plot(â€¦) function. abline( This stands for â€œaâ€ (intercept) â€œbâ€ (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm This is the name of an lm(â€¦) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(â€¦) function will locate these two values from within mylm and use them to add a line to your current plot(â€¦). ) Closing parenthesis for abline(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm) You can customize the look of the regression line with abline( This stands for â€œaâ€ (intercept) â€œbâ€ (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm, This is the name of an lm(â€¦) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(â€¦) function will locate these two values from within mylm and use them to add a line to your current plot(â€¦). lty= The lty= stands for â€œline typeâ€ and allows you to select between 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. 1, This creates a solid line. Remember, other options include: 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. lwd= The lwd= allows you to specify the width of the line. The default width is 1. Using lwd=2 would double the thickness, and so on. Any positive value is allowed. 1, Default line width. To make a thicker line, us 2 or 3â€¦ To make a thinner line, try 0.5, but 1 is already pretty thin. col= This allows you to specify the color of the line using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). â€œsomeColorâ€ Type colors() in R for options. ) Closing parenthesis for abline(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm, lty=1, lwd=1, col=\"firebrick\") You can add points to the regression withâ€¦ points( This is like plot(â€¦) but adds points to the current plot(â€¦) instead of creating a new plot. newYÂ  newY should be a column of values from some data set. Or, use points(newX, newY) to add a single point to a graph. ~Â  This links Y to X in the plot. newX,Â  newX should be a column of values from some data set. It should be the same length as newY. If just a single value, use points(newX, newY) instead. data=YourDataSet,Â  If newY and newX come from a dataset, then use data= to tell the points(â€¦) function what data set they come from. If newY and newX are just single values, then data= is not needed. col=â€œskyblueâ€, This allows you to specify the color of the points using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). pch=16 This allows you to specify the type of plotting symbol to be used for the points. Type ?pch and scroll half way down in the help file that appears to learn about other possible symbols. ) Closing parenthesis for points(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) points(7,40, pch=16, col=\"skyblue\", cex=2) text(7,40, \"New Dot\", pos=3, cex=0.5) points(dist ~ speed, data=filter(cars, mylm$res > 2), cex=.8, col=\"red\") abline(mylm, lty=1, lwd=1, col=\"firebrick\") To add the regression line to a scatterplot using the ggplot2 approach, first ensure: library(ggplot2) or library(tidyverse) is loaded. Then, use the geom_smooth(method = lm) command: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. YourDataSet,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. X, This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. Â y =Â  â€œy=Â â€ declares which variable will become the y-axis of the graphic. Y This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. + Here the + is used to add yet another layer to ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. There are several other methods that could be used here. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(â€¦) to place more complicated models on the plot. Â se = FALSE se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. ) Closing parenthesis for the geom_smooth() function. Â Click to Show OutputÂ  Click to View Output. ggplot(cars, aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = \"lm\", formula=y~x, se=FALSE) There are a number of ways to customize the appearance of the regression line: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. speed,Â  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. y =Â  â€œy=Â â€ declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. Â + Here the + is used to add yet another layer to ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(â€¦) to place more complicated models on the plot. Â se = FALSE, se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. Â size = 2, Use size = 2 to adjust the thickness of the line to size 2. Â color = â€œorangeâ€, Use color = â€œorangeâ€ to change the color of the line to orange. Â Â linetype = â€œdashedâ€ Use linetype = â€œdashedâ€ to change the solid line to a dashed line. Some linetype options include â€œdashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. ) Closing parenthesis for the geom_smooth() function. Â Click to Show OutputÂ  Click to View Output. In addition to customizing the regression line, you can customize the points, add points, add lines, and much more. ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. speed,Â  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. y =Â  â€œy=Â â€ declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point( geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. size = 1.5, Use size = 1.5 to change the size of the points. Â color = â€œskyblueâ€ Use color = â€œskyblueâ€ to change the color of the points to Brother Saundersâ€™ favorite color. Â alpha = 0.5 Use alpha = 0.5 to change the transparency of the points to 0.5. ) Closing parenthesis of geom_point() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in ways similar to lm(â€¦) to place more complicated models on the plot. Â se = FALSE, se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. Â color = â€œnavyâ€, Use color = â€œnavyâ€ to change the color of the line to navy blue. Â size = 1.5 Use size = 1.5 to adjust the thickness of the line to 1.5. ) Closing parenthesis of geom_smooth() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_hline( Use geom_hline() to add a horizontal line at a specified y-intercept. You can also use geom_vline(xintercept = some_number) to add a vertical line to the graph. yintercept = Use â€œyintercept =â€ to tell geom_hline() that you are going to declare a y intercept for the horizontal line. Â 75 75 is the value of the y-intercept. , color = â€œfirebrickâ€ Use color = â€œfirebrickâ€ to change the color of the horizontal line to firebrick red. , size = 1, Use size = 1 to adjust the thickness of the horizontal line to size 1. Â Â Â Â Â Â Â Â Â Â Â Â Â linetype = â€œlongdashâ€ Use linetype = â€œlongdashâ€ to change the solid line to a dashed line with longer dashes. Some linetype options include â€œdashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. , alpha = 0.5 Use alpha = 0.5 to change the transparency of the horizontal line to 0.5. ) Closing parenthesis of geom_hline function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_segment( geom_segment() allows you to add a line segment to ggplot() by using specified start and end points. x = â€œx =â€ tells geom_segment() that you are going to declare the x-coordinate for the starting point of the line segment. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the starting point of the line segment. Â y =â€œy =â€ tells geom_segment() that you are going to declare the y-coordinate for the starting point of the line segment. Â 75, 75 is a number on the y-axis of your graph. It is the y-coordinate of the starting point of the line segment. Â xend = â€œxend =â€ tells geom_segment() that you are going to declare the x-coordinate for the end point of the line segment. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the end point of the line segment. Â yend = â€œyend =â€ tells geom_segment() that you are going to declare the y-coordinate for the end point of the line segment. Â 38, 38 is a number on the y-axis of your graph. It is the y-coordinate of the end point of the line segment. Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â size = 1 Use size = 1 to adjust the thickness of the line segment. , color = â€œlightgrayâ€ Use color = â€œlightgrayâ€ to change the color of the line segment to light gray. , linetype = â€œlongdashâ€ Use *linetype = â€œlongdash* to change the solid line segment to a dashed one. Some linetype options includeâ€dashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. ) Closing parenthesis for geom_segment() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_point( geom_point() can also be used to add individual points to the graph. Simply declare the x and y coordinates of the point you want to plot. x = â€œx =â€ tells geom_point() that you are going to declare the x-coordinate for the point. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the point. Â y = â€œy =â€ tells geom_point() that you are going to declare the y-coordinate for the point. Â 75 75 is a number on the y-axis of your graph. It is the y-coordinate of the point. , size = 3 Use size = 3 to make the point stand out more. , color = â€œfirebrickâ€ Use color = â€œfirebrickâ€ to change the color of the point to firebrick red. ) Closing parenthesis of the geom_point() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_text( geom_text() allows you to add customized text anywhere on the graph. It is very similar to the base R equivalent, text(â€¦). x = â€œx =â€ tells geom_text() that you are going to declare the x-coordinate for the text. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the text. Â y = â€œy =â€ tells geom_text() that you are going to declare the y-coordinate for the text. Â 84, 84 is a number on the y-axis of your graph. It is the y-coordinate of the text. Â label = â€œlabel =â€ tells geom_text() that you are going to give it the label. Â â€œMy Point (14, 75)â€, â€œMy Point (14, 75)â€ is the text that will appear on the graph. Â Â Â Â Â Â Â Â Â Â Â Â color = â€œnavyâ€ Use color = â€œnavyâ€ to change the color of the text to navy blue. , size = 3 Use size = 3 to change the size of the text. ) Closing parenthesis of the geom_text() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â theme_minimal() Add a minimalistic theme to the graph. There are many other themes that you can try out. Â Click to Show OutputÂ  Click to View Output. ## `geom_smooth()` using formula = 'y ~ x' Accessing Parts of the Regression Finally, note that the mylm object contains the names(mylm) of mylm$coefficients Contains two values. The first is the estimated \\(y\\)-intercept. The second is the estimated slope. ## (Intercept) speed ## -17.579095 3.932409 mylm$residuals Contains the residuals from the regression in the same order as the actual dataset. ## 1 2 3 4 5 6 7 8 9 ## 3.849460 11.849460 -5.947766 12.052234 2.119825 -7.812584 -3.744993 4.255007 12.255007 ## 10 11 12 13 14 15 16 17 18 ## -8.677401 2.322599 -15.609810 -9.609810 -5.609810 -1.609810 -7.542219 0.457781 0.457781 ## 19 20 21 22 23 24 25 26 27 ## 12.457781 -11.474628 -1.474628 22.525372 42.525372 -21.407036 -15.407036 12.592964 -13.339445 ## 28 29 30 31 32 33 34 35 36 ## -5.339445 -17.271854 -9.271854 0.728146 -11.204263 2.795737 22.795737 30.795737 -21.136672 ## 37 38 39 40 41 42 43 44 45 ## -11.136672 10.863328 -29.069080 -13.069080 -9.069080 -5.069080 2.930920 -2.933898 -18.866307 ## 46 47 48 49 50 ## -6.798715 15.201285 16.201285 43.201285 4.268876 mylm$fitted.values The values of \\(\\hat{Y}\\) in the same order as the original dataset. ## 1 2 3 4 5 6 7 8 9 10 ## -1.849460 -1.849460 9.947766 9.947766 13.880175 17.812584 21.744993 21.744993 21.744993 25.677401 ## 11 12 13 14 15 16 17 18 19 20 ## 25.677401 29.609810 29.609810 29.609810 29.609810 33.542219 33.542219 33.542219 33.542219 37.474628 ## 21 22 23 24 25 26 27 28 29 30 ## 37.474628 37.474628 37.474628 41.407036 41.407036 41.407036 45.339445 45.339445 49.271854 49.271854 ## 31 32 33 34 35 36 37 38 39 40 ## 49.271854 53.204263 53.204263 53.204263 53.204263 57.136672 57.136672 57.136672 61.069080 61.069080 ## 41 42 43 44 45 46 47 48 49 50 ## 61.069080 61.069080 61.069080 68.933898 72.866307 76.798715 76.798715 76.798715 76.798715 80.731124 mylm$â€¦ several other things that will not be explained here. Making Predictions predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ) Closing parenthesis for the data.frame(â€¦) function. ) Closing parenthesis for the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12)) 1 Â  29.60981 The value given is the â€œfitted-valueâ€ or â€œpredicted-valueâ€ for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(â€¦) function. Â interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. â€œpredictionâ€ This specifies that a prediction interval will be included with the predicted value. A prediction interval gives you a 95% confidence interval that captures 95% of the data, or \\(Y_i\\) values for the specific \\(X\\)-value specified in the prediction. ) Closing parenthesis of the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"prediction\") Â  fit The â€œfitâ€ is the predicted value. Â  lwr The â€œlwrâ€ is the lower bound. Â  upr The â€œuprâ€ is the upper bound. 1 29.60981 In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance canâ€™t go negative) feet to 60.96915 feet. -1.749529 This is the lower bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us the stopping distance could be as quick as -1.749529 feet (or 0 because distance canâ€™t go negative). 60.96915 This is the upper bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us that the actual stopping distance could be as high as 60.96915 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(â€¦) function. Â interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. â€œconfidenceâ€ This specifies that a confidence interval for the prediction should be provided. This is of use whenever your interest is in just estimating the average y-value, not the actual y-values. ) Closing parenthesis of the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"confidence\") Â  fit The â€œfitâ€ is the predicted value. Â  lwr The â€œlwrâ€ is the lower bound. Â  upr The â€œuprâ€ is the upper bound. 1 29.60981 In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. 24.39514 This is the lower bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is greater than this value. 34.82448 This is the upper bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is less than this value. Finding Confidence Intervals for Model Parameters confint( The R function confint(â€¦) allows you to use an lm(â€¦) object to compute confidence intervals for one or more parameters (like \\(\\beta_0\\) or \\(\\beta_1\\)) in your model. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â level = â€œlevel =â€ tells the confint(â€¦) function that you are going to declare at what level of confidence you want the interval. The default is â€œlevel = 0.95.â€ If you want to find 95% confidence intervals for your parameters, then just run confint(mylm). Â someConfidenceLevel someConfidenceLevel is simply a confidence level you choose when you want something other than a 95% confidence interval. Some examples of appropriate levels include 0.90 and 0.99. ) Closing parenthesis for confint(..) function. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.90) Â  Â  5 % The lower bound of a 90% confidence interval occurs at the 5th percentile. This is because at 90% confidence, 10% is left in the tails, with 5% on each end. The upper bound will thus end at the 95th percentile, hence the 5% and 95% as the column names. Â  95 % The upper bound of a 90% confidence interval ends at the 95th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -28.914514 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -6.243676 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.235501 and 4.629317. 3.235501 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. 4.629317 This is the upper bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.95) Â  Â  2.5 % The lower bound of a 95% confidence interval occurs at the 2.5th percentile. This is because at 95% confidence, 5% is left in the tails, with 2.5% on each end. The upper bound will thus end at the 97.5th percentile, hence the 2.5% and 97.5% as the column names for the lower and upper bounds, respectively. Â  97.5 % The upper bound of a 95% confidence interval ends at the 97.5th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -31.167850 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -3.990340 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.096964 and 4.767853. 3.096964 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.096964 and 4.767853 4.767853 This is the upper bound of the confidence interval. In this case, you can be 95% confident that the slope lies between 3.096964 and 4.767853 Explanation Linear regression has a rich mathematical theory behind it. This is because it uses a mathematical function and a random error term to describe the regression relation between a response variable \\(Y\\) and an explanatory variable called \\(X\\). Expand each element below to learn more. Regression Cheat Sheet (Expand) Term Pronunciation Meaning Math R Code \\(Y_i\\) $Y_i$ â€œwhy-eyeâ€ The data \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)\\) $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$ YourDataSet$YourYvariable \\(\\hat{Y}_i\\) $\\hat{Y}_i$ â€œwhy-hat-eyeâ€ The fitted line \\(\\hat{Y}_i = b_0 + b_1 X_i\\) $\\hat{Y}_i = b_0 + b_1 X_i$ lmObject$fitted.values \\(E\\{Y_i\\}\\) $E\\{Y_i\\}$ â€œexpected value of why-eyeâ€ True mean y-value \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) $E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i$ <none> \\(\\beta_0\\) $\\beta_0$ â€œbeta-zeroâ€ True y-intercept <none> <none> \\(\\beta_1\\) $\\beta_1$ â€œbeta-oneâ€ True slope <none> <none> \\(b_0\\) $b_0$ â€œb-zeroâ€ Estimated y-intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) $b_0 = \\bar{Y} - b_1\\bar{X} b_0 <- mean(Y) - b_1*mean(X)$ \\(b_1\\) $b_1$ â€œb-oneâ€ Estimated slope \\(b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\) $b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})} {\\sum(X_i - \\bar{X})^2}$ b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) \\(\\epsilon_i\\) $\\epsilon_i$ â€œepsilon-eyeâ€ Distance of dot to true line \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) $\\epsilon_i = Y_i - E\\{Y_i\\}$ <none> \\(r_i\\) $r_i$ â€œr-eyeâ€ or â€œresidual-eyeâ€ Distance of dot to estimated line \\(r_i = Y_i - \\hat{Y}_i\\) $r_i = Y_i - \\hat{Y}_i$ lmObject$residuals \\(\\sigma^2\\) $\\sigma^2$ â€œsigma-squaredâ€ Variance of the \\(\\epsilon_i\\) \\(Var\\{\\epsilon_i\\} = \\sigma^2\\)$Var\\{\\epsilon_i\\} = \\sigma^2$ <none> \\(MSE\\) $MSE$ â€œmean squared errorâ€ Estimate of \\(\\sigma^2\\) \\(MSE = \\frac{SSE}{n-p}\\)$MSE = \\frac{SSE}{n-p}$ sum( lmObject$res^2 ) / (n - p) \\(SSE\\) $SSE$ â€œsum of squared errorâ€ (residuals) Measure of dotâ€™s total deviation from the line \\(SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\)$SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$ sum( lmObject$res^2 ) \\(SSR\\) $SSR$ â€œsum of squared regression errorâ€ Measure of lineâ€™s deviation from y-bar \\(SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\)$SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2$ sum( (lmObject$fit - mean(YourData$Y))^2 ) \\(SSTO\\) $SSTO$ â€œtotal sum of squaresâ€ Measure of total variation in Y \\(SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\)$SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2$ sum( (YourData$Y - mean(YourData$Y))^2 ) \\(R^2\\) $R^2$ â€œR-squaredâ€ Proportion of variation in Y explained by the regression \\(R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\)$R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}$ SSR/SSTO \\(r\\) $r$ â€œrâ€ Correlation between X and Y. \\(r = \\sqrt{R^2}\\)$r = \\sqrt{R^2}$ sqrt(R^2) \\(\\hat{Y}_h\\) $\\hat{Y}_h$ â€œwhy-hat-aitchâ€ Estimated mean y-value for some x-value called \\(X_h\\) \\(\\hat{Y}_h = b_0 + b_1 X_h\\)$\\hat{Y}_h = b_0 + b_1 X_h$ predict(lmObject, data.frame(XvarName=#)) \\(X_h\\) $X_h$ â€œex-aitchâ€ Some x-value, not necessarily one of the \\(X_i\\) values used in the regression \\(X_h =\\) some number$X_h = $ Xh = # Confidence Interval â€œconfidence intervalâ€ Estimated bounds at a certain level of confidence for a parameter \\(b_0 \\pm t^* \\cdot s_{b_0}\\)b_0 \\pm t^* \\cdot s_{b_0} or \\(b_1 \\pm t^* \\cdot s_{b_1}\\)b_1 \\pm t^* \\cdot s_{b_1} confint(mylm, level = someConfidenceLevel) Parameter Estimate \\(\\beta_0\\) \\(b_0\\) \\(\\beta_1\\) \\(b_1\\) \\(\\epsilon_i\\) \\(r_i\\) \\(\\sigma^2\\) \\(MSE\\) \\(\\sigma\\) \\(\\sqrt{MSE}\\), the Residual standard error The Mathematical Model (Expand) \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)â€¦ There are three main elements to the mathematical model of regression. Each of these three elements is pictured below in the â€œRegression Relation Diagram.â€ Study both the three bullet points and their visual representations in the plot below for a clearer understanding. The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read moreâ€¦) The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as â€œnatural lawâ€ or â€œGodâ€™s lawâ€. It is some true line that is unknown to us. The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line. The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\). Note: \\(E\\{Y\\}\\) is pronounced â€œthe expected value of yâ€ because, wellâ€¦ the mean is the typical, average, or â€œexpectedâ€ value. The dots, i.e., the regression relation plus an error term: \\(Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}} + \\underbrace{\\epsilon_i}_\\text{error term} \\quad \\text{where} \\ \\epsilon_i\\sim N(0,\\sigma^2)\\) (Read moreâ€¦) This is shown by the dots in the graph below. This is the data. In regression, the assumption is that the y-value for individual \\(i\\), denoted by \\(Y_i\\), was â€œcreatedâ€ by adding an error term \\(\\epsilon_i\\) to each individualâ€™s â€œexpectedâ€ value \\(\\beta_0 + \\beta_1 X_i\\). Note the â€œorder of creationâ€ would require first knowing an indivualâ€™s x-value, \\(X_i\\), then their expected value from the regression relation \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) and then adding their \\(\\epsilon_i\\) value to the result. The \\(\\epsilon_i\\) allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance \\(\\epsilon_i\\) from the line. Note: \\(Y_i\\) is pronounced â€œwhy-eyeâ€ because it is the y-value for individual \\(i\\). Sometimes also called â€œwhy-sub-eyeâ€ because \\(i\\) is in the subscript of \\(Y\\). The estimated line, i.e., the line we get from a sample of data. \\(\\underbrace{\\hat{Y}_i}_{\\substack{\\text{estimated mean} \\\\ \\text{y-value}}} = \\underbrace{b_0 + b_1 X_i}_\\text{estimated regression equation}\\) (Read moreâ€¦) The estimated line is shown by the solid line in the graph below. \\(\\hat{Y}\\) is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation \\(E\\{Y\\}\\). So \\(\\hat{Y}\\) is interpreted as the estimated average (or mean) \\(Y\\)-value for any given \\(X\\)-value. Thus, \\(b_0\\) is the estimated y-intercept and \\(b_1\\) is the estimated slope. The bâ€™s are sample statistics, like \\(\\bar{x}\\) and the \\(\\beta\\)â€™s are population parameters like \\(\\mu\\). The \\(b\\)â€™s estimate the \\(\\beta\\)â€™s. Note: \\(\\hat{Y}_i\\) is pronounced â€œwhy-hat-eyeâ€ and is known as the â€œestimated y-valueâ€ or â€œfitted y-valueâ€ because it is the y-value you get from \\(b_0 + b_1 X_i\\). It is always different from \\(Y_i\\) because dots are rarely if ever exactly on the estimated regression line. This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line). Something to ponder: The true line, when coupled with the error terms, â€œcreatesâ€ the data. The estimated (or fitted) line uses the sampled data to try to â€œre-createâ€ the true line. We could loosely call this the â€œorder of creationâ€ as shown by the following diagram. par(mfrow=c(1,3), mai=c(.2,.2,.4,.1)) plot(y ~ x, col=\"white\", main=\"A Law is Given\", yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, main=\"Data is Created\", xaxt='n', yaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, xaxt='n', yaxt='n', main=\"The Law is Estimated\") curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) A Law is Given Data is Created The Law is Estimated \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) \\(Y_i = E\\{Y_i\\} + \\epsilon_i\\) \\(\\hat{Y}_i = b_0 + b_1 X_i\\) The true line is the â€œlawâ€. The \\(Y_i\\) are created by adding \\(\\epsilon_i\\) to \\(E\\{Y_i\\}\\) where \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\). The law is estimated with \\(\\hat{Y}_i\\) which is given with lm(...). Click open the â€œCodeâ€ buttom below to the right to find code that runs a simulation demonstrating this â€œorder of creationâ€. ## Simulating Data from a Regression Model ## This R-chunk is meant to be played in your R Console. ## It allows you to explore how the various elements ## of the regression model combine together to \"create\" ## data and then use the data to \"re-create\" the line. set.seed(101) #Allows us to always get the same \"random\" sample #Change to a new number to get a new sample n <- 3 #set the sample size X_i <- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45. beta0 <- 3 #Our choice for the y-intercept. beta1 <- .1 #Our choice for the slope. sigma <- 12.5 #Our choice for the std. deviation of the error terms. epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data View(fabData) #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it. fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData. summary(fab.lm) #Summarize your model. plot(y ~ x, data=fabData) #Plot the data. abline(fab.lm) #Add the estimated regression line to your plot. # Now for something you can't do in real life... but since we created the data... abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). legend(\"topleft\", legend=c(\"True Line\", \"Estimated Line\"), lty=c(2,1), bty=\"n\") #Add a legend to your plot specifying which line is which. Interpreting the Model Parameters (Expand) \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted asâ€¦ The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model. If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\). The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\). It is often misunderstood to be the â€œaverage change in yâ€ or just â€œthe change in yâ€ but it is more correctly referred to as the â€œchange in the average yâ€. To better see this, consider the three graphics shown below. par(mfrow=c(1,3)) hist(mtcars$mpg, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Number of Vehicles\", xlab=\"Gas Mileage (mpg)\", col=\"skyblue\") boxplot(mpg ~ cyl, data=mtcars, border=\"skyblue\", boxwex=0.5, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Number of Cylinders of Engine (cyl)\") plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=\"skyblue\", main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Quarter Mile Time (qsec)\") abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=\"darkgray\") mtext(side=3, text=\"Automatic Transmissions Only (am==0)\", cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=\"gray\") The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09. The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg. The scatterplot on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value. Residuals and Errors (Expand) \\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true errorâ€¦ Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\). The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\). We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summaryâ€¦ Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\). \\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms. Keep in mind the idea that the errors \\(\\epsilon_i\\) â€œcreatedâ€ the data and that the residuals \\(r_i\\) are computed after using the data to â€œre-createâ€ the line. Residuals have many uses in regression analysis. They allow us to diagnose the regression assumptions, See the â€œAssumptionsâ€ section below for more details. estimate the regression relation, See the â€œEstimating the Model Parametersâ€ section below for more details. estimate the variance of the error terms, See the â€œEstimating the Model Varianceâ€ section below for more details. and assess the fit of the regression relation. See the â€œAssessing the Fit of a Regressionâ€ section below for more details. Assessing the Fit of a Regression (Expand) \\(R^2\\), SSTO, SSR, and SSEâ€¦ Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line. par(mfrow=c(1,3), mai=c(.1,.1,.5,.1)) set.seed(2) x <- runif(30,0,20) y1 <- 2 + 3.5*x + rnorm(30,0,2) y2 <- 2 + 3.5*x + rnorm(30,0,8) y3 <- 2 + 3.5*x + rnorm(30,0,27) plot(y1 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Excellent Fit\") abline(lm(y1 ~ x), col=\"gray\") plot(y2 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Good Fit\") abline(lm(y2 ~ x), col=\"gray\") plot(y3 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Poor Fit\") abline(lm(y3 ~ x), col=\"gray\") A common way to measure the fit of a regression is with correlation. While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\). (If you are a Math 325 student, just stick with correlation for now and skip on to the next section of this Explanation tab. If you are a Math 425 student, it is critical that you come to understand \\(R^2\\) deeply, so read on.) Before you can understand \\(R^2\\), you must understand three important â€œsums of squaresâ€. (Read more about sumsâ€¦) A sum is just a fancy word for adding things together. \\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand. So we use the symbol \\(\\Sigma\\) to denote the word â€œsumâ€. Further, we use a subscript \\(\\Sigma_{i=1}\\) to state what value the sum is beginning with, and a superscript \\(\\Sigma_{i=1}^6\\) to state the value we are ending at. This gives \\[ \\sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Test your knowledge, do you see why the answer is 6 to the sum below? \\[ \\sum_{i=1}^3 i = 6 \\] Computing sums in R is fairly easy. Type the following codes in your R Console. sum(1:6) #gives the answer of 21 sum(1:3) #gives the answer of 6 However, sums really become useful when used with a data set. Each row of a data set represents an â€œindividualâ€™sâ€ data. We can reference each individual with a row number. In the data below, individual 3, denoted by \\(i=3\\), has a speed of 7 and a dist of 4. pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3) Individual speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 To compute the sum of the speed column, use sum(speed). If we divided this sum by 6, we would get the mean of speed mean(speed). In fact, the two most used statistics mean(...) and sd(...) both use sums. Take a moment to review the formulas for mean and standard deviation. It is strongly recommended that you study the Explanation tab for both as well. Weâ€™ll wait. See you back here shortly. â€¦ Welcome back. Suppose we let X = speed and Y = dist. Then \\(X_3 = 7\\) and \\(Y_3 = 4\\) because we are accessing row 3 of both the \\(X\\) (or speed) column and \\(Y\\) (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, sum(speed) would be written mathematically as \\(\\sum_{i=1}^6 X_i\\) and sum(dist) would be written as \\(\\sum_{i=1}^6 Y_i\\). Sum of Squared Errors Sum of Squares Regression Total Sum of Squares \\(\\text{SSE} = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\) \\(\\text{SSR} = \\sum_{i=1}^n \\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\) \\(\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\\) Measures how much the residuals deviate from the line. Measures how much the regression line deviates from the average y-value. Measures how much the y-values deviate from the average y-value. Equals SSTO - SSR Equals SSTO - SSE Equals SSE + SSR sum( (Y - mylm$fit)^2 ) sum( (mylm$fit - mean(Y))^2 ) sum( (Y - mean(Y))^2 ) It is important to remember that SSE and SSR split up SSTO, so that \\[ \\text{SSTO} = \\text{SSE} + \\text{SSR} \\] This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works. The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called \\(R^2\\) (â€œr-squaredâ€). R-Squared (\\(R^2\\)) \\[ \\underbrace{R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}}_\\text{Interpretation: Proportion of variation in Y explained by the regression.} \\] The smallest \\(R^2\\) can be is zero, and the largest it can be is 1. This is because \\(SSR\\) must be between 0 and SSTO, inclusive. Residual Plots & Regression Assumptions (Expand) Residuals vs.Â fitted-values, Q-Q Plot of the residuals, and residuals vs.Â order plotsâ€¦ There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate. Each assumption is labeled in the regression equation below. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions. (Read moreâ€¦) As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not. Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3 The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(\\hat{Y}_i\\). The residuals are the \\(r_i\\). This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable. | Show Examples | (Read moreâ€¦) The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption. The linear relation is assumed to be satisfied if there are no apparent trends in the plot. The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values. The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance. set.seed(2) X <- rnorm(30,15,3) notLin <- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8)) notLin.lm <- lm(Y~X, data=notLin) set.seed(15) Lin <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) Lin.lm <- lm(Y~X, data=Lin) par(mfrow=c(3,3), mai=c(.25,.25,.25,.25), mgp=c(1,.75,0)) plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Not Linear\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(notLin.lm$fitted.values,notLin.lm$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Good: Linear, Constant Variance\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) set.seed(6) notCon <- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5)) notCon.lm <- lm(Y~X, data=notCon) LinO <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) LinO[1] <- LinO[1]^2 LinO.lm <- lm(Y~X, data=LinO) plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Unconstant Variance\", cex.main=0.95, yaxt='n', xaxt='n', col=\"firebrick\") polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) # plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Outliers\", cex.main=0.95) # abline(h=0) tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(Girth ~ Volume, data=trees[-31,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(Height ~ Volume, data=trees) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) abline(h=0) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals, f=.4) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(weight ~ repwt, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(weight ~ repht, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) abline(h=0) Q-Q Plot of the Residuals: Checks Assumption #2 The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption. | Show Examples | (Read moreâ€¦) There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot. Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) set.seed(123) tmp <- rnorm(100) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"skyblue\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Normal\", col=\"skyblue\") tmp <- Davis$weight qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Right-skewed\", breaks=15, col=\"firebrick\") par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- rbeta(100, 5,1) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Left-skewed\", breaks=seq(min(tmp),max(tmp), length.out=13), col=\"firebrick\") tmp <- rbeta(100,2,2) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Heavy-tailed\", col=\"firebrick\") Residuals versus Order Plot: Checks Assumption #5 When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated. | Show Examples | (Read moreâ€¦) Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Good: No Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: General Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ qsec, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: Interesting Patterns\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),]) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Bad: Obvious Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") Problems from Failed Assumptions There are various problems that can arise when certain of the regression assupmtions are not satisfied. Lack of Linearity When the linearity assumption is violated, pretty much everything we obtain from the regression summary is no longer meaningful. The y-intercept estimate can be drastically off from its actual true value. Important model information is lost by trying to use a simple slope term \\(\\beta_1\\) to describe the model with respect to \\(X\\). The residual standard error will be much higher than it otherwise would be because of curvature patterns in the data that the line cannot capture. Thus, R-squared will be lower than it otherwise should be. P-values can become non-significant, when in fact there is a strong pattern in the data, but that pattern just cannot be captured by a simple line. *Normality of the errors is often put into question as well when a simplified line is used to try to capture a more complicated curved model. The plot below demonstrate these difficulties. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 7.5 #True slope beta_2 <- -0.25 #True bend X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors Y_i <- beta_0 + beta_1*X_i + beta_2*X_i^2 + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Non-Linear Relation\") abline(mylm, col=\"gray\") #Add fitted line to plot curve(beta_0 + beta_1*x + beta_2*x^2, col=\"gray\", lty=2, add=TRUE) #Add True line to plot #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (True value:\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (True value:\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (True value:\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Non-normal Error Terms When the normality assumption for the error terms is violated, not all is lost. In fact, the estimate of the slope and intercept are still often fairly meaningful. However, it is unwise to put too much trust in the residual standard error as an estimate of the standard deviation \\(\\sigma\\). This is because the standard deviation in skewed distributions does not carry the same meaning it has in normal distributions. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rchisq(n, 1)*3 - 1 #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Unconstant Variance When variance of the error term changes across the regression, the regression approximates the â€œaverage varianceâ€ of the errors because the regression is still assuming the variance is constant across the regression. The estimates of the slope and intercept are still typically quite good, and can be used for interpretation. The residual standard error however should not be considered to be meaningful as it will be too large on one end of the regression and too small on the other end. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma + X_i) #normally distributed errors #with increasing variance Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Variance Varies (Non-Constant)\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 13.82 Slope 3.5 3.768 Sigma 2.5 13.02 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", round(mean(sigma + X_i), 2), \", mean)\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Normality Violated As silly as it sounds, if the only problem with the regression is the lack of normality of the error terms, it isnâ€™t all that big of a problem. Depending on how non-normal the residuals appear, there could be some skewing to the residual standard error, but otherwise, the slope and intercept are still interpretable and meaningful. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- runif(n, -sqrt(12*sigma^2)/2, sqrt(12*sigma^2)/2) #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 14.7 Slope 3.5 3.449 Sigma 2.5 2.317 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Independence Assumption Violated While the slope and intercept are often still meaningful when the independence assumption is violated, the residual standard error is unnecessarily large in this case. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, 2.5) + (1:n -n/2)*.5 #normal, but correlated errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Independence Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 16.47 Slope 3.5 3.296 Sigma 2.5 4.819 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Outliers Present While outliers do not violate any of the regression assumptions, they do pose substantial difficulties for the least squares regression estimates of the slope and intercept. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors epsilon_i[3] <- ifelse(X_i[3] < 10, runif(1,25,35), -runif(1,25,35)) #create outlier Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"An Outlier Present\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 13.18 Slope 3.5 3.447 Sigma 2.5 6.389 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) The material below this section is meant for Math 425 students only. Estimating the Model Parameters (Expand) How to get \\(b_0\\) and \\(b_1\\): least squares & maximum likelihoodâ€¦ There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical. The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas. Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 <- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\). When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\). Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i. \\label{exp} \\end{equation}\\] Least Squares{#leastSquares} To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\). \\[ Q = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2 \\] Then we use the function Q as if it were a function of \\(\\beta_0\\) and \\(\\beta_1\\). Ironically, the values of \\(Y\\) and \\(X\\) are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we donâ€™t know are the values of \\(\\beta_0\\) and \\(\\beta_1\\). This least squares applet is a good way to explore how various choices of the slope and intercept yield different values of the â€œsum of squared residualsâ€. But it turns out that there is one â€œbestâ€ choice of the slope and intercept that yields a â€œsmallestâ€ value of the â€œsum of squared residuals.â€ This best choice can actually be found using calculus by taking the partial derivatives of \\(Q\\) with respect to both \\(\\beta_0\\) and \\(\\beta_1\\). \\[ \\frac{\\partial Q}{\\partial \\beta_0} = -2\\sum (Y_i - \\beta_0 - \\beta_1X_i) \\] \\[ \\frac{\\partial Q}{\\partial \\beta_1} = -2\\sum X_i(Y_i-\\beta_0-\\beta_1X_i) \\] Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize \\(Q\\) for a given set of data. After all the calculations are completed we find the values of the parameter estimators \\(b_0\\) and \\(b_1\\) (of \\(\\beta_0\\) and \\(\\beta_1\\), respectively) are as stated previously. Maximum Likelihood{#mle} The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of \\(\\beta_0\\) and \\(\\beta_1\\) which minime the least squares \\(Q\\) function, we choose the values of \\(\\beta_0\\) and \\(\\beta_1\\) which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the \\(Y_i\\) for all observations \\(i=1,\\ldots,n\\). We can do this rather simply by using the assumption that the errors, \\(\\epsilon_i\\) are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if \\(f(Y_i)\\) denotes the probability density function for \\(Y_i\\), then the joint probability density for all \\(Y_i\\), \\(f(Y_1,\\ldots,Y_n)\\) is given by \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) \\] Since each \\(Y_i\\) is assumed to be normally distributed with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\) (see model (\\(\\ref{model}\\))) we have that \\[ f(Y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{Y_i-\\beta_0-\\beta_1X_i}{\\sigma}\\right)^2\\right]} \\] which provides the joint probability as \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] The likelihood function \\(L\\) is then given by consider the \\(Y_i\\) and \\(X_i\\) fixed and the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) as the variables in the function. \\[ L(\\beta_0,\\beta_1,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] Instead of taking partial derivatives of \\(L\\) directly (with respect to all parameters) we take the partial derivatives of the \\(\\log\\) of \\(L\\), which is easier to work with. In a similar, but more difficult calculation, to that of minimizing \\(Q\\), we obtain the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) which maximize the log of \\(L\\), and which therefore maximize \\(L\\). (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate \\(\\hat{\\sigma}^2\\) of \\(\\sigma^2\\). \\[ \\hat{\\sigma}^2 = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n} \\] Estimating the Model Variance (Expand) Estimating \\(\\sigma^2\\) with MSEâ€¦ As shown previously in the â€œEstimating Model Parametersâ€ section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation. Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\). This estimate turns out to be a biased estimator. This means that it is consistently wrong in its estimates of \\(\\sigma^2\\). If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong. This is bad. Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\). Without going into all the details, to â€œfixâ€ the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently. To find the correct degrees of freedom, we have to notice that the \\(\\hat{Y}_i\\) in the numerator of \\(\\widehat{\\sigma}^2\\) is defined by \\[\\begin{equation} \\widehat{Y}_i = b_0 + b_1X_i \\label{hatY} \\end{equation}\\] From this equation, we notice that two means, \\(\\bar{X}\\) and \\(\\bar{Y}\\), were estimated from the data in order to obtain \\(\\hat{Y}_i\\). (See the formulas for \\(b_0\\) and \\(b_1\\) above, and note how they use both \\(\\bar{X}\\) and \\(\\bar{Y}\\) in their calculation.) Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for \\(\\hat{\\sigma}^2\\) should be \\(n-2\\) instead of \\(n\\). Some incredibly long calculations will show that the â€œfixedâ€ estimator \\[\\begin{equation} s^2 = MSE = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n-2} \\quad \\text{(Unbiased Estimator of $\\sigma^2$)} \\end{equation}\\] is an unbiased estimator of \\(\\sigma^2\\). Here \\(MSE\\) stands for mean squared error, which is the most obvious name for a formula that squares the errors \\(Y_i-\\hat{Y}_i\\) then adds them up and divides by their degrees of freedom. Similarly, we call the numerator \\(\\sum(Y_i-\\hat{Y}_i)^2\\) the sum of the squared errors, denoted by \\(SSE\\). It is also important to note that the errors are often denoted by \\(r_i = Y_i-\\hat{Y}_i\\), the residuals. Putting this all together we get the following equivalent statements for \\(MSE\\). \\[\\begin{equation} s^2 = MSE = \\frac{SSE}{n-2} = \\frac{\\sum(Y_i-\\widehat{Y}_i)^2}{n-2} = \\frac{\\sum r_i^2}{n-2} \\end{equation}\\] As a final note, even though the expected value \\(E\\{MSE\\} = \\sigma^2\\), which shows \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\), it unfortunately isnâ€™t true that \\(\\sqrt{MSE}\\) is an unbiased estimator of \\(\\sigma\\). This presents a few problems later on, but these are minimal enough that we can overlook the issue and move forward. Transformations (Expand) \\(Y'\\), \\(X'\\), and returning to the original spaceâ€¦ Y transformations are denoted by y-prime, written \\(Y'\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\). \\[ Y' = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y' = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y' = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y' = \\log(Y)\\) lm(log(Y) ~ X) 0.25 \\(Y' = \\sqrt(\\sqrt(Y))\\) lm(sqrt(sqrt(Y)) ~ X) 0.5 \\(Y' = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y' = Y\\) lm(Y ~ X) 2 \\(Y' = Y^2\\) lm(Y^2 ~ X) Using â€œmaximum-likelihoodâ€ estimation, the Box-Cox procedure can actually automatically detect the â€œoptimalâ€ value of \\(\\lambda\\) to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise. Scatterplot Recognition Box-Cox Suggestion An Example Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\). set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Ya <- 1/sqrt(Y) #1/Y^2 Lam = -2 Yb <- 1/Y #1/Y Lam = -1 Yc <- exp(.02*Y) #log(Y) Lam = 0 Yd <- Y^2 #sqrt(Y) Lam = 0.5 Ye <- Y #Y Lam = 1 Yf <- sqrt(Y) #Y^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Ya ~ X, main=expression(paste(\"Use \", lambda == -2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ya^-2 ~ X)) curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yb ~ X, main=expression(paste(\"Use \", lambda == -1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yb^-1 ~ X)) curve(1/(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yc ~ X, main=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(log(Yc) ~ X)) curve(exp(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yd ~ X, main=expression(paste(\"Use \", lambda == 0.5)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(sqrt(Yd) ~ X)) curve((b[1] + b[2]*x)^2, add=TRUE, col=\"green\", lwd=2) plot(Ye ~ X, main=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ye ~ X)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yf ~ X, main=expression(paste(\"Use \", lambda == 2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yf^2 ~ X)) curve(sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try. par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0)) boxCox(lm(Ya ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -2)), line=.5) boxCox(lm(Yb ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -1)), line=.5) boxCox(lm(Yc ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), line=.5) boxCox(lm(Yd ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0.5)), line=.5) boxCox(lm(Ye ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), line=.5) boxCox(lm(Yf ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 2)), line=.5) An Example Suppose we were running a simple linear regression on the cars dataset. This would be done with the code cars.lm <- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesnâ€™t quite fit the data as well as we would hope. Instead, the data looks a little curved. cars.lm <-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") abline(cars.lm, col=\"gray\") Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful. library(car) boxCox(cars.lm) The output from the boxCox(...) function looks as follows. This plot tells use to use the \\(\\lambda = 0.5\\) transformation, so that \\(Y' = Y^0.5 = \\sqrt{Y}\\). (To see this yourself, click on the â€œBox-Cox Suggestionâ€ tab above, as well as on the â€œScatterplot Recognitionâ€ tab.) Now, a transformation regression is performed using sqrt(Y) in place of Y as follows: cars.lm.t <- lm(sqrt(dist) ~ speed, data=cars) summary(cars.lm.t) Â  Estimate Std. Error t value Pr(> (Intercept) 1.277 0.4844 2.636 0.01126 speed 0.3224 0.02978 10.83 1.773e-14 Then, \\[ \\widehat{Y}_i' = 1.277 + 0.3224 X_i \\] And replacing \\(\\hat{Y}_i' = \\sqrt{\\hat{Y}_i}\\) we have \\[ \\sqrt{\\widehat{Y}_i} = 1.277 + 0.3224 X_i \\] Solving for \\(\\hat{Y}_i\\) gives \\[ \\widehat{Y}_i = (1.277 + 0.3224 X_i)^2 \\] Which, using curve((1.277 + 0.3224*x)^2, add=TRUE) (see code for details) looks like this: plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") curve( (1.277 + 0.3224*x)^2, add=TRUE, col=\"firebrick\") X-Transformations X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them. The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated. The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation. set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Xa <- 1/sqrt(X) #1/X^2 Lam = -2 Xb <- 1/X #1/X Lam = -1 Xc <- exp(.02*X) #log(X) Lam = 0 Xd <- X^2 #sqrt(X) Lam = 0.5 Xe <- X #X Lam = 1 Xf <- sqrt(X) #X^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Y ~ Xa, main=expression(paste(\"Use \", X*minute == X^-2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xa^-2))) curve(b[1] + b[2]*x^-2, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xb, main=expression(paste(\"Use \", X*minute == X^-1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xb^-1))) curve(b[1] + b[2]*x^-1, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xc, main=expression(paste(\"Use \", X*minute == log(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ log(Xc))) curve(b[1] + b[2]*log(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xd, main=expression(paste(\"Use \", X*minute == sqrt(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ sqrt(Xd))) curve(b[1] + b[2]*sqrt(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xe, main=expression(paste(\"Use \", X*minute == X, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ Xe)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xf, main=expression(paste(\"Use \", X*minute == X^2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xf^2))) curve(b[1] + b[2]*x^2, add=TRUE, col=\"green\", lwd=2) Inference for the Model Parameters (Expand) t test formulas, sampling distributions, confidence intervals, and F testsâ€¦ When fitting the regression model given by the equation \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] to a sample of data, we typically test hypotheses about the parameters \\(\\beta_0\\), \\(\\beta_1\\), or both. Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_0 - \\overbrace{0}^\\text{a number}}{s_{b_0}}\\] This is the formula for the test statistic. It measures how far the estimated y-intercept \\(b_0\\) is from the null hypothesis for \\(\\beta_0\\) in units of â€œstandard errors of \\(b_0\\)â€. Thus the division by \\(s_{b_0}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0, it could be any number. \\(H_0: \\beta_1 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_1\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_1 - \\overbrace{0}^\\text{a number}}{s_{b_1}}\\] This is the formula for the test statistic. It measures how far the estimated slope \\(b_1\\) is from the null hypothesis for \\(\\beta_1\\) in units of â€œstandard errors of \\(b_1\\)â€. Thus the division by \\(s_{b_1}\\). Though the hypothesized value of \\(\\beta_1\\) is typically 0, it could be any number. Left-tailed p-value = pt(-abs(tvalue), degrees of freedom). Double it to get the two-sided p-value. In R, these values correspond to the output summary of an lm as follows. (Show Example) Consider the cars data in R. Suppose we used the regression model given by \\[ \\underbrace{Y_i}_\\text{Feet to Stop} = \\beta_0 + \\beta_1 \\underbrace{X_{i}}_\\text{mph} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\quad \\sim N(0,\\sigma^2) \\] to model the feet a vehicle (from the 1920â€™s) takes to stop when traveling at a certain speed (in miles per hour, mph) prior to stopping. When the regression is performed and summarized in R, it is always testing the following two hypotheses: \\[ H_0: \\beta_0 = 0 \\quad\\quad H_0: \\beta_1 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\quad\\quad H_a: \\beta_1 \\neq 0 \\] To perform the test of these hypotheses for the regression stated above, we would run the following codes in R. cars.lm <- lm(dist ~ speed, data=cars) pander(summary(cars.lm)$coefficients) These would produce summary output like the following, but the following output has been labeled with the math notation corresponding to each value. Letâ€™s emphasize what is happening in this summary output table. First, here is how the â€œt valueâ€ is calculated for the â€œ(Intercept)â€ in the summary table above. \\[ t = \\frac{b_0-0}{s_{b_0}} = \\frac{-17.58 - 0}{6.758} = -2.601 \\] Second, here is a visual representation of how the P-value, the â€œPr(>|t|)â€ as it is called in the summary table above, is calculated for this test statistic. (Click the graph to view an interactive applet showing this calculation.) Notice both ends of the t-distribution are being shaded to compute the P-value because the alternative hypothesis was \\(H_a: \\beta_0 \\neq 0\\). To compute the P-value in R, we use the â€œpercentile function for the t-distributionâ€ called pt( ). This function requires two things, the t-value and the degrees of freedom, in our case pt(-2.601, 48). Note the degrees of freedom (df) are 48 because the sample size is \\(n=50\\) and there are two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) in our regression model. Running this code in R gives: pt(-2.601, 48) = 0.00616 However, note that this value is only half of the actual P-value of 0.0123. To get the â€œtwo-sidedâ€ P-value (note that our alternative hypothesis used a \\(\\neq\\) symbol) we need to double this left-tailed P-value. 2*pt(-2.601, 48)) = 0.0123 Finally, note that the same procedure can be used to test hypotheses that use a value other than 0 in the null and alternative. For example, to test the hypotheses: \\[ H_0: \\beta_1 = 3 \\\\ H_a: \\beta_1 \\neq 3 \\] Use the t-formula \\[ t = \\frac{b_1 - 3}{s_{b_1}} = \\frac{\\overbrace{3.932}^{b_1} - \\overbrace{3}^{H_0}}{\\underbrace{0.4155}_{s_{b_1}}} = 2.243 \\] then the P-value is calculated in R by 2*pt(-abs(2.243), 48) = 0.0295495 To obtain confidence intervals in R use confint(mylm). Confidence Interval Formula Standard Error \\(\\beta_0\\) \\(b_0 \\pm\\) \\(t^*\\) This is called the â€œcritical valueâ€ and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\). Use qt(0.975, df) to get \\(t*\\) in R. \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_0\\). \\(s_{b_0}\\) The standard error of \\(b_0\\), denoted by \\(s_{b_0}\\) is provided in the regression summary output under the column header called â€œStd. Errorâ€ for the â€œ(Intercept)â€ row of the output. It is calculated using the formula shown below. \\[s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\] This is called the â€œestimated variance of \\(b_0\\)â€. Taking the square root of this number gives the â€œstandard error of \\(b_0\\)â€. \\(\\beta_1\\) \\(b_1 \\pm\\) \\(t^*\\) This is called the â€œcritical valueâ€ and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\) (sample size - number of parameters in the regression model). Use qt(0.975, df) to get \\(t*\\) in R \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_1\\). \\(s_{b_1}\\) The standard error of \\(b_1\\), denoted by \\(s_{b_1}\\) is provided in the regression summary output under the column header called â€œStd. It is calculated using the formula shown below. \\[s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\] This is called the â€œestimated variance of \\(b_1\\)â€. Taking the square root of this number gives the â€œstandard error of \\(b_1\\)â€. To be more exact, the types of inference we are interested in are the following. Determine if there is evidence of a meaningful linear relationship in the data. If \\(\\beta_1 = 0\\), then there is no relation between \\(X\\) and \\(E\\{Y\\}\\). Hence we might be interested in testing the hypotheses \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) is some hypothesized number. To provide a confidence interval for the true value of \\(\\beta_1\\). Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the sampling distribution of the estimate \\(b_1\\) of the parameter \\(\\beta_1\\). And, while we are at it, we may as well come to understand the sampling distribution of the estimate \\(b_0\\) of the parameter \\(\\beta_0\\). Review sampling distributions from Math 221. Since \\(b_1\\) is an estimate, it will vary from sample to sample, even though the truth, \\(\\beta_1\\), remains fixed. (The same holds for \\(b_0\\) and \\(\\beta_0\\).) It turns out that the sampling distribution of \\(b_1\\) (where the \\(X\\) values remain fixed from study to study) is normal with mean and variance: \\[ \\mu_{b_1} = \\beta_1 \\] \\[ \\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} \\] ## Simulation to Show relationship between Standard Errors ##----------------------------------------------- ## Edit anything in this area... n <- 100 #sample size Xstart <- 30 #lower-bound for x-axis Xstop <- 100 #upper-bound for x-axis beta_0 <- 2 #choice of true y-intercept beta_1 <- 3.5 #choice of true slope sigma <- 13.8 #choice of st. deviation of error terms ## End of Editable area. ##----------------------------------------------- # Create X, which will be used in the next R-chunk. X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) ## After playing this chunk, play the next chunk as well. To see that this is true, consider the regression model with values specified for each parameter as follows. \\[ Y_i = \\overbrace{\\beta_0}^{2} + \\overbrace{\\beta_1}^{3.5} X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\overbrace{\\sigma^2}^{\\sigma=13.8}) \\] Using the equations above for \\(\\mu_{b_1}\\) and \\(\\sigma^2_{b_1}\\) we obtain that the mean of the sampling distribution of \\(b_1\\) will be \\(\\mu_{b_1} = \\beta_1 = 3.5\\) Further, we see that the variance of the sampling distribution of \\(b_1\\) will be \\(\\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} = \\frac{13.8^2}{4.25\\times 10^{4}}\\) Taking the square root of the variance, the standard deviation of the sampling distribution of \\(b_1\\) will be \\(\\sigma_{b_1} = 0.067\\). Thatâ€™s very nice. But to really believe it, letâ€™s run a simulation ourselves. The â€œCodeâ€ below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of \\(b_1\\) and \\(b_0\\) for that regression. After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of \\(b_0\\) is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of \\(b_1\\) is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of \\(\\beta_0\\) or \\(\\beta_1\\), respectively. Also, the â€œStd. Errorâ€ of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of \\(\\sigma_{b_0}\\) and \\(\\sigma_{b_1}\\), respectively. N <- 5000 #number of times to pull a random sample storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N) for (i in 1:N){ Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model mylm <- lm(Y ~ X) storage_b0[i] <- coef(mylm)[1] storage_b1[i] <- coef(mylm)[2] storage_rmse[i] <- summary(mylm)$sigma } layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3)) Ystart <- 0 #min(0,min(Y)) Ystop <- 500 #max(max(Y), 0) Yrange <- Ystop - Ystart plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), ylim=c(Ystart, Ystop), pch=16, col=\"gray\", main=\"Regression Lines from many Samples (gray lines) \\n Plus Residual Standard Deviation Lines (green lines)\") text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1) text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1) text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1) for (i in 1:N){ abline(storage_b0[i], storage_b1[i], col=\"darkgray\") } abline(beta_0, beta_1, col=\"green\", lwd=3) abline(beta_0+sigma, beta_1, col=\"green\", lwd=2) abline(beta_0-sigma, beta_1, col=\"green\", lwd=2) abline(beta_0+2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0-2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0+3*sigma, beta_1, col=\"green\", lwd=.5) abline(beta_0-3*sigma, beta_1, col=\"green\", lwd=.5) par(mai=c(1,.6,.5,.01)) addnorm <- function(m,s, col=\"firebrick\"){ curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2) lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col) lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col) lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col) lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col) lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col) lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col) lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col) legend(\"topleft\", legend=paste(\"Std. Error = \", round(s,3)), cex=0.7, bty=\"n\") } h0 <- hist(storage_b0, col=\"skyblue3\", main=\"Sampling Distribution\\n Y-intercept\", xlab=expression(paste(\"Estimates of \", beta[0], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m0 <- mean(storage_b0) s0 <- sd(storage_b0) addnorm(m0,s0, col=\"green\") h1 <- hist(storage_b1, col=\"skyblue3\", main=\"Sampling Distribution\\n Slope\", xlab=expression(paste(\"Estimates of \", beta[1], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m1 <- mean(storage_b1) s1 <- sd(storage_b1) addnorm(m1,s1, col=\"green\") t Tests{#tTests} Using the information above about the sampling distributions of \\(b_1\\) and \\(b_0\\), an immediate choice of statistical test to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) can be zero, or any other value, is a t test given by \\[ t = \\frac{b_1 - \\beta_{10}}{s_{b_1}} \\] where \\(s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\). (You may want to review the section â€œEstimating the Model Varianceâ€ of this file to know where MSE came from.) With quite a bit of work it has been shown that \\(t\\) is distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. The nearly identical test statistic for testing \\[ H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_0 \\neq \\beta_{00} \\] is given by \\[ t = \\frac{b_0 - \\beta_{00}}{s_{b_0}} \\] where \\(s^2_{b_0} = MSE\\left[\\frac{1}{n}+\\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\). This version of \\(t\\) has also been shown to be distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. Confidence Intervals Creating a confidence interval for either \\(\\beta_1\\) or \\(\\beta_0\\) follows immediately from these results using the formulas \\[ b_1 \\pm t^*_{n-2}\\cdot s_{b_1} \\] \\[ b_0 \\pm t^*_{n-2}\\cdot s_{b_0} \\] where \\(t^*_{n-2}\\) is the critical value from a t distribution with \\(n-2\\) degrees of freedom corresponding to the chosen confidence level. F tests{#Ftests} Another way to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\quad\\quad \\text{or} \\quad\\quad H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\quad\\quad \\ \\ \\quad \\quad H_a: \\beta_0 \\neq \\beta_{00} \\] is with an \\(F\\) Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an \\(F\\) test is very general and can be used in many places that a t test cannot. In its most general form, the \\(F\\) test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that \\(H_0:\\beta_1=0\\) against the alternative that \\(H_a: \\beta_1\\neq 0\\), we are essentially comparing two models against each other. If \\(\\beta_1=0\\), then the corresponding model would be \\(E\\{Y_i\\} = \\beta_0\\). If \\(\\beta_1\\neq0\\), then the model remains \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the \\(F\\) Test, that the null model (reduced model) have fewer â€œfreeâ€ parameters than the alternative model (full model). To demonstrate what we mean by â€œfreeâ€ parameters, consider the following example. Say we wanted to test the hypothesis that \\(H_0:\\beta_1 = 2.5\\) against the alternative that \\(\\beta_1\\neq2.5\\). Then the null, or reduced model, would be \\(E\\{Y_i\\}=\\beta_0+2.5X_i\\). The alternative, or full model, would be \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). Thus, the null (reduced) model contains only one â€œfreeâ€ parameter because \\(\\beta_1\\) has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two â€œfreeâ€ parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model. Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing \\(H_0: \\beta_1=0\\) against \\(H_a:\\beta_1\\neq0\\) we have the partition \\[ \\underbrace{Y_i-\\bar{Y}}_{Total} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{Regression} + \\underbrace{Y_i-\\hat{Y}_i}_{Error} \\] The reason we use \\(\\bar{Y}\\) for the null model is that \\(\\bar{Y}\\) is the unbiased estimator of \\(\\beta_0\\) for the null model, \\(E\\{Y_i\\} = \\beta_0\\). Thus we would compute the following sums of squares: \\[ SSTO = \\sum(Y_i-\\bar{Y})^2 \\] \\[ SSR = \\sum(\\hat{Y}_i-\\bar{Y})^2 \\] \\[ SSE = \\sum(Y_i-\\hat{Y}_i)^2 \\] and note that \\(SSTO = SSR + SSE\\). Important to note is that \\(SSTO\\) uses the difference between the observations \\(Y_i\\) and the null (reduced) model. The \\(SSR\\) uses the diffences between the alternative (full) and null (reduced) model. The \\(SSE\\) uses the differences between the observations \\(Y_i\\) and the alternative (full) model. From these we could set up a General \\(F\\) table of the form Â  Sum Sq Df Mean Sq F Value Model Error \\(SSR\\) \\(df_R-df_F\\) \\(\\frac{SSR}{df_R-df_F}\\) \\(\\frac{SSR}{df_R-df_F}\\cdot\\frac{df_F}{SSE}\\) Residual Error \\(SSE\\) \\(df_F\\) \\(\\frac{SSE}{df_F}\\) Total Error \\(SSTO\\) \\(df_R\\) Prediction and Confidence Intervals for \\(\\hat{Y}_h\\) (Expand) predict(â€¦, interval=â€œpredictionâ€)â€¦ It is a common mistake to assume that averages (means) describe individuals. They do not. So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line. Interval R Code Math Equation When to Use Prediction predict(..., interval=\"prediction\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\text{Pred}\\ Y}\\) Predict an individualâ€™s value. Confidence predict(..., interval=\"confidence\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\hat{Y}}\\) Estimate location of the mean y-value. predict(mylm, data.frame(XvarName = number), interval=...) For example, consider this graph. Then click here to read about the graph. Notice the three dots above 15 mph in the graph. Each of these dots show a car that was going 15 mph when it applied the brakes. However, stopping distances of the three individual cars differ with one at 20 feet, one at 26 feet and one at 54 feet. The regression line represents the average stopping distance of cars. In this case, cars going 15 mph are estimated to have an average stopping distance of about 40 feet, as shown by the line. But individual vehicles, all going the same speed of 15 mph, varied from stopping distances of 20 feet up to 54 feet! So, to predict that a car going 15 mph will take 41.4 feet to stop, doesnâ€™t tell the whole story. Far more revealing is the complete statement, â€œCars going 15 mph are predicted to take anywhere from 10.2 to 72.6 feet to stop, with an average stopping distance of 41.4 feet.â€ This is called the â€œprediction intervalâ€ and is shown in the graph in blue. It is obtained in R with the codes: cars.lm <- lm(dist ~ speed, data=cars) predict(cars.lm, data.frame(speed=15), interval=\"prediction\") cars.lm <- lm(dist ~ speed, data=cars) pander(predict(cars.lm, data.frame(speed=15), interval=\"prediction\")) fit lwr upr 41.41 10.17 72.64 plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=\"firebrick2\", cex=1.5) cars.lm <- lm(dist ~ speed, data=cars) abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3)) abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2)) abline(v=15, lty=2, col=\"firebrick\") preds <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12) lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8)) lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8)) Now, for the details behind prediction intervals and confidence intervals. Letâ€™s begin by recalling some details (from the section â€œInference for the Model Parametersâ€) about the standard error of the y-intercept, \\(b_0\\). Recall that the y-intercept is the average y-value for the given x-value of \\(x=0\\). Recall further that the formula for the standard error of \\(b_0\\) is given by the formula \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] If we wanted to be more exact with this formula, we would write it as \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{(0-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice the addition of \\((0 - \\bar{X})^2\\) instead of just \\(\\bar{X}^2\\) in the numerator of the right-most part of the equation? This more complete statement obviously would reduce to just \\(\\bar{X}^2\\), but that is only because \\(X=0\\) when we are working with the y-intercept, \\(b_0\\). We could be working with other values of \\(X\\) than just zero. Letâ€™s take a quick detour and talk notation for a second. Typically, \\(X_i\\) and \\(Y_i\\) are used to denote the x-value and y-value of points that are contained in our data set. When we want to reference a point that wasnâ€™t within our original data set, we use the notation \\(X_h\\) and \\(Y_h\\). (The letter h is close to i, but different from i, so why not. There is really no other reason to use h.) Thus, \\(Y_h\\) is the y-value for the \\(X_h\\) x-value, neither of which were included in our original regression of \\(X_i\\)â€™s and \\(Y_i\\)â€™s. Now, back to the previous discussion. If \\(X_h = 0\\), then \\(\\hat{Y}_h\\) is the y-intercept, so \\(\\hat{Y}_h = b_0\\) when \\(X_h=0\\). So, we could write, \\[ s^2_{\\hat{Y}_h} = MSE\\left[\\frac{1}{n} + \\frac{(X_h-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice how the \\(b_0\\) in \\(s_{b_0}\\) was replaced with \\(\\hat{Y}_h\\) to get \\(s_{\\hat{Y}_h}\\) and the 0 in \\((0 - \\bar{X})^2\\) was replaced with \\(X_h\\) to get \\((X_h - \\bar{X})^2\\)? Interesting. We now have a formula that would give us the standard error of \\(\\hat{Y}_h\\) for any \\(X_h\\) value, not just \\(X_h = 0\\), or the y-intercept, \\(b_0\\). That is fantastic. It would look like this if plotted. Notice how the gray region is showing the standard error for each \\(\\hat{Y}_h\\) value? (It is technically showing the confidence interval for \\(E\\{Y_h\\}\\) at every possible \\(X_h\\) value, but that is just \\(\\hat{Y}_h \\pm t^* \\cdot s_{\\hat{Y}_h}\\).) ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + theme_bw() ## `geom_smooth()` using formula = 'y ~ x' Confidence Interval for \\(\\hat{Y}_h\\) \\[ \\hat{Y}_h \\pm t^* s_{\\hat{Y}_h} \\quad \\text{where} \\ s_{\\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] The confidence interval is a wonderful tool for estimating \\(E\\{Y_h\\}\\), the â€œtrueâ€ average y-value for a given x-value of \\(X_h\\). However, it is not valuable for predicting an individual dot, or \\(Y_h\\) value. Notice how few of the dots of the regression are actually contained within the confidence interval band in the plot? The confidence interval does not really predict where the dots will land, just where the average y-value is located for each x-value. Remember the 68-95-99.7 Rule of the normal distribution? If not, here is a link back to that concept in the Math 221 textbook. This rule states that roughly 95% of data, when normally distributed, will be between \\(z=-2\\) and \\(z=2\\) standard deviations from the mean. So, is going two â€œresidual standard errorsâ€ to both sides of the regression line enough to capture 95% of the data? The answer is, not quite. The reason for this is because our knowledge of where the true mean lies is uncertain. (Notice the confidence interval band shown in the plot.) However, adding two standard errors to the edges of the confidence band would get us in the right place. In other words, there are two sources of variability at play here, (1) our uncertaintity in where the regression line is sitting, and (2) the natural variability of the data points around the line. Thus, the â€œprediction intervalâ€ requires accounting for both of these sources of variability to produce the following equation. Prediction Interval for \\(Y_h\\) \\[ \\hat{Y}_h \\pm t^* s_{Pred \\hat{Y}_h} \\quad \\text{where} \\ s_{Pred \\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + 1 + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] This formula provides a useful band for identifying a region where we are 95% confident that a new observation for \\(Y_h\\) will land, given the value of \\(X_h\\). It looks as follows. Notice the prediction interval is much wider than the confidence interval. This is because data varies far more than do means. Prediction is for where the individual data points will land, confidence is for where the mean will land. cars.lm <- lm(dist ~ speed, data=cars) predy <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + geom_segment(aes(x=15, xend=15, y=predy[2], yend=predy[3]), lwd=4, color=rgb(.5,.7,.5,.01)) + geom_point(aes(x=15, y=predy[1]), cex=2, color=\"skyblue\", pch=15) + theme_bw() Lowess (and Loess) Curves (Expand) A non-parametric approach to estimating \\(E\\{Y_i\\}\\)â€¦ Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value. Using Base R # Select the relevant columns and remove NA values air2 <- airquality %>% dplyr::select(Temp, Ozone) %>% # Use dplyr's select function explicitly drop_na() # Drop rows with NA values in Temp or Ozone plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") lines(lowess(air2$Ozone, air2$Temp), col=\"firebrick\") Using ggplot2 # Your dataset (air2) should have been created earlier, ensuring no NAs air2 <- na.omit(airquality[c(\"Temp\", \"Ozone\")]) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=\"darkgray\") + # Add points with dark gray color geom_smooth(se=FALSE, method=\"loess\", method.args = list(degree=1)) + # Lowess curve, no standard error shading theme_bw() # Black and white theme ## `geom_smooth()` using formula = 'y ~ x' # Optionally, allow for predictions as well as the graph: # Uncomment the following lines if you want predictions as well: # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fitted, x=Ozone)) # Use fitted values from loess Advantages Disadvantages Quick. Good at ignoring outliers. Good at capturing the general pattern in the data. Good for making predictions within the scope of the data. No mathematical model. Not interpretable. No p-values. No adjusted R-squared. How it Works The Lowess curve localizes the regression model to a â€œneighborhoodâ€ of points, and then joins these localized regressions together into a smooth line. It minimizes the effect of outliers, and letâ€™s the data â€œspeak for itselfâ€. As a downside, it is not interpretable, and has no final way to write the model mathematically. All the same, it is a very powerful tool for identifying an appropriate model, or verifying the fit of a model, or making predictions when no reasonable model does an adequate job. Study this graphic and the explanations below to learn how it works. Recommendation: run the code in this â€œCodeâ€ chunk to the right in your Console, and flip through the resulting graphics. X <- cars$speed Y <- cars$dist X <- X[!is.na(X) & !is.na(Y)] Y <- Y[!is.na(X) & !is.na(Y)] f <- 1/2 n <- length(X) lfit <- rep(NA,n) for (xh in 1:n){ xdists <- X - X[xh] nn <- floor(n*f) r <- sort(abs(xdists))[nn] xdists.nbrhd <- which(abs(xdists) < r) w <- rep(0, length(xdists)) w[xdists.nbrhd] <- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w), col=rgb(.2,.2,.2,.3), cex=1.5, yaxt='n', xaxt='n', xlab=\"\", ylab=\"\") points(Y[xh] ~ X[xh], pch=16, col=\"orange\") lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2)) cat(\"\\n\\n\") readline(prompt=paste0(\"Center point is point #\", xh, \"... Press [enter] to continue...\")) MADnotThereYet <- TRUE count <- 0 while(MADnotThereYet){ readline(prompt=paste0(\"\\n Adjusting line to account for outliers in the y-direction... Press [enter] to continue...\")) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"wheat\", add=TRUE) MAD <- median(abs(lmc$res)) resm <- lmc$res/(6*MAD) resm[resm>1] <- 1 bisq <- (1-resm^2)^2 w <- w*bisq obs <- coef(lmc) lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) count <- count + 1 if ( (sum(abs(obs-lmc$coef))<.1) | (count > 3)) MADnotThereYet <- FALSE } curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"green\", add=TRUE) points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=\"green\") readline(prompt=paste0(\"\\n Use final line to get fitted value for this point... Press [enter] to continue to next point...\")) lfit[xh] <- predict(lmc, data.frame(X=X[xh])) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") if (xh == n){ readline(prompt=paste0(\"\\n Press [enter] to see actual Lowess curve...\")) lines(lowess(X,Y, f=f), col=\"firebrick\") legend(\"topleft\", bty=\"n\", legend=\"Actual lowess Curve using lowess(...)\", col=\"firebrick\", lty=1) } } Select a fraction of the data to use for the â€œneighborhoodâ€ of points (shown in blue in the graph above). The lowess function in R uses â€œf=2/3â€ and the loess function uses â€œspan=0.75â€ for this value, which selects the nearest two-thirds or 75% of the data, respectively, depending on which function you use. For this example, we set the fraction of points at 50%. Both functions can be set to whatever you want. Pick any point in the regression, eventually selecting all points one at a time. The selected point becomes the â€œcenterâ€ of a â€œneighborhoodâ€ of points surrounding it. In this example, the center point is in orange, and the neighboring points are in blue. Use the points within the neighborhood to fit a regression line. However, make the regression depend most on points closest to â€œcenterâ€ and least on points furthest from â€œcenter.â€ This is called a weighted regression. Weights are decided according to what is called the tricubic weight function, so that the weight \\(w\\) given to point \\(j\\) of the neighborhood of points is defined by \\[ w_j = \\left(1- \\left( \\frac{|X_c - X_j|}{\\max_k |X_c - X_k|}\\right)^3\\right)^3 \\] where \\(X_c\\) is the x-value of the â€œcenterâ€ dot and \\(X_j\\) is the x-value of any other dot in the neighborhood. The fitted-value of \\(\\hat{Y}_c\\) is obtained for the center point \\(X_c\\) of the current regression. This point is used as the Lowess (or Loess) curveâ€™s value at that particular x-value. Well, almost. Itâ€™s a first guess at where this value will end up, but thereâ€™s a little more to the algorithm before we are done. Initial guesses for each of these fitted values are obtained for each point in the regression. Now each local regression for each neighborhood is re-run a few times in such a way the the effect of outliers is minimized. The final line for each neighborhood is obtained by the following steps. Compute all residuals for points in the neighborhood of the current regression, denoted by \\(r_i\\). Then compute the MAD, median absolute deviation, of the residuals \\(MAD = \\text{median} (|r_1|, |r_2|, \\ldots)\\). Divide all residuals by 6 times the MAD: \\(u_i = r_i/(6\\cdot MAD)\\) (If \\(r_i > 6\\cdot MAD\\) then set \\(u_i = 0\\).) Compute what are called bisquare weights using the formula: \\(b_i = (1 - u_i^2)^2\\) Perform a regression using the weights \\(w_i = w_i b_i\\) Repeat the above process with the new weights \\(w_i\\) until the weights stop changing very much. The final fitted values for each \\(X\\)-value in the regression are obtained from the final regression line for each neighborhood. These fitted values make up the Lowess (or loess) curve. Note that the default of the loess function in R is to use quadratic regressions in each neighborhood instead of linear regressions. This can be controlled with the loess option of â€œdegree=2â€ (quadratic fits) or â€œdegree = 1â€. In the lowess function only a linear regression in each neighborhood is allowed. Examples: bodyweight, cars Quantitative Y | Multiple X Graphics Y is a single quantitative variable of interest, like height. While we could use an X1 of â€œshoe-sizeâ€ to explain height, we might also want to use a second x-variable, X2, like â€œgenderâ€ to help explain height. Further x-variables could also be used. Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a studentâ€™s final exam grade? How do factors like age, weight, exercise level, and diet impact a personâ€™s blood pressure? Examples: Is one Navy base more prone to heatwaves than the other? The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH) Tests Multiple Linear Regression Multiple regression is a way to understand how different factors affect something weâ€™re interested in. Itâ€™s like having more than one clue to solve a mystery. Hereâ€™s what you need to know: We can use several different pieces of information (called variables) to predict an outcome. There are many ways to combine these pieces of information. We start with simple combinations and build up to more complex ones, kind of like building with Lego blocks. This helps us make better predictions and understand complex situations more clearly. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model is a bit like using the same ingredient twice in a recipe, but in different ways. It uses the same factor (\\(X_i^2\\)) in two different forms: Once in its regular form (\\(\\beta_1 X_i\\)) Once squared (\\(\\beta_2 X_i^2\\)) This squared part is what we call the â€œquadraticâ€ term. It helps us describe relationships that arenâ€™t just straight lines, but have curves to them. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and â€œsteepnessâ€ of the Model: negative values face down, positive values face up; large values imply â€œsteeperâ€ parabolas and low values imply â€œflatterâ€ parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)â€™s explanation. An Example Using the airquality data set, we run the following â€œquadraticâ€ regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our â€œquadraticâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. Temp Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These â€œEstimatesâ€ can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(Month^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of Month from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, ) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will â€œopen downâ€ (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be â€œmonth zero.â€ Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the â€œcubicâ€ term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the â€œbase slope coefficientâ€ and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following â€œcubicâ€ regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our â€œcubicâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. uptake Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^3) \\(X_{i}^3\\), where the function I(â€¦) protects the cubing of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will â€œopen upâ€. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The â€œtwo-linesâ€ model is a way to compare two different groups in statistics using numbers. Hereâ€™s how it works: It uses two types of numbers: A regular number that can be any value (\\(X_{1i}\\)) A special number thatâ€™s either 0 or 1 (called an â€œindicator variableâ€, \\(X_{2i}\\)) The special number (\\(X_{2i}\\)) helps us turn group information (like â€œGroup Aâ€ or â€œGroup Bâ€) into numbers we can use in calculations. This model creates two separate lines on a graph: One line for Group A One line for Group B By using this method, we can easily see and compare how the two groups are different from each other. Think of it like having two recipes for the same dish, but with slightly different ingredients. This model helps us see how those small changes affect the final result. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the â€œbase-lineâ€ of the model, the â€œGroup 0â€ line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the â€œbase-lineâ€ line. \\(\\beta_3\\) Called the â€œinteractionâ€ term. Controls the change in the slope for the second line in the model as compared to the slope of the â€œbase-lineâ€ line. An Example Using the mtcars data set, we run the following â€œtwo-linesâ€ regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our â€œtwo-linesâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. mpg Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) (beta zero): This is like the starting point. Itâ€™s estimated by something called the â€œInterceptâ€. \\(\\beta_1\\) (beta one): This shows how one thing changes when another thing (called â€œqsecâ€) changes. Itâ€™s estimated to be 1.439. \\(\\beta_2\\) (beta two): This shows how something else (called â€œamâ€) affects the overall result. Itâ€™s estimated to be -14.51. \\(\\beta_3\\) (beta three): This shows how â€œqsecâ€ and â€œamâ€ work together to affect the result. Itâ€™s estimated to be 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called â€œ3Dâ€ regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to â€œbendâ€. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the â€œTempâ€ direction is estimated to be 2.659 and the slope in the â€œMonthâ€ direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction termâ€™s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the â€œslopesâ€ in each direction to change, creating a â€œcurvedâ€ surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called â€œHDâ€, or â€œHigh Dimensionalâ€, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isnâ€™t really even possible to â€œmentally connectâ€ with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)â€™s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the â€œWindâ€ direction is estimated to be -3.334. The slope in the â€œTempâ€ direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the â€œSolar.Râ€ direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here. R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the â€œOverviewâ€ section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the â€œcolumn (c) bindâ€ function and it joins together things as columns. Res =Â  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,Â  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals. Â panel=panel.smooth,Â  This places a lowess smoothing line on each scatterplot. col =Â  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(â€œcolor1â€,â€œcolor2â€, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for â€œlinear model.â€ Y Y must be a â€œnumericâ€ vector of the quantitative response variable. Â ~Â  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats â€œnumericâ€ variables as quantitative and â€œcharacterâ€ or â€œfactorâ€ variables as qualitative. R will automatcially recode qualitative variables to become â€œnumericâ€ variables using a 0,1 encoding. See the Explanation tab for details. Â + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details. Â + â€¦, * ... emphasizes that as many explanatory variables as are desired can be included in the model. Â data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(â€¦) function displays the results of an lm(â€¦) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(â€¦) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(â€¦). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min Â  -4.3818 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -2.2696 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.1344 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  1.7058 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  5.8752 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  t value To learn more about the â€œt valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  Pr(>|t|) The â€œPrâ€ stands for â€œProbabilityâ€ and the â€œ(> |t|)â€ stands for â€œmore extreme than the observed t-valueâ€. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the â€œp-valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. (Intercept) This always says â€œInterceptâ€ for any lm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero. Â  2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the intercept (26.6248479) by its standard error (2.1829432). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a â€œstarâ€. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + â€¦). Â  -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model. Â  0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the â€œEstimateâ€ by its standard error. It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + â€¦). Â  5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\). Â  2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot â€œ.â€ implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like. Â  0.0004029 This is the estimate of the coefficient of the interaction term. Â  0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE. Â 2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\). Â on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO. Â 0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car. Â Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data. Â 0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that â€œat least one is not.â€ Â 34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom. Â on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero. Â p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that â€œat least oneâ€ of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the â€œOverviewâ€ sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the â€œfitted-valueâ€ or â€œpredicted-valueâ€ for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œpredictionâ€) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance canâ€™t go negative) feet to 60.96915 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œconfidenceâ€) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BICâ€¦ There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (â€œR-squaredâ€). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The â€œsimplestâ€ but â€œbestâ€ model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it â€œan information criterion (AIC)â€ when he published the method and other people later began calling it the â€œAkaike Information Criterion.â€) Model Selection (Expand) pairs plots, added variable plots, and pattern recognitionâ€¦ Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots {.tabset .tabset-pills} A useful visualization tool for model selection is the â€œpairs plot.â€ This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice thatâ€¦ the y-axis of each plot is found by locating the variable name (like â€œmpgâ€) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like â€œdispâ€) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model {.tabset .tabset-pills} Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldnâ€™t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Letâ€™s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander() Â  Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a modelâ€™s ability to generalize to new dataâ€¦ The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesnâ€™t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, letâ€™s remind ourselves why we use regression models in the first place. The main goal is to capture the â€œessenceâ€ of the data. In other words, the general pattern is what we are after. We want a model that tells us how â€œall suchâ€ data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the â€œcomplicated modelâ€ is overfitting the original data. It does not capture the â€œessenceâ€ of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-valueâ€¦ The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the modelâ€¦ The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions{#check} The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read moreâ€¦) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) Outlier Analysis (Expand) Cookâ€™s Distances and Leverage Valuesâ€¦ The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs.Â fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cookâ€™s Distances The idea behind Cookâ€™s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article â€œDetection of Influential Observation in Linear Regressionâ€ (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, letâ€™s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2)) Â  1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the â€œdifferencesâ€ in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cookâ€™s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cookâ€™s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cookâ€™s Distances using the code cooks.distance(lmObject). Also, a graph of Cookâ€™s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of â€œleverageâ€ and is â€œpullingâ€ the regression toward itself. A value near 0 implies the point is just â€œone of manyâ€ and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that â€œminimizeâ€ the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)â€™s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the â€œhat matrixâ€ \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the â€œleverage valuesâ€ and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a â€œlot of pull,â€ and values close to 0 represent â€œlittle pull.â€ In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with â€œlots of leverageâ€ and a large â€œCookâ€™s Distanceâ€ are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regressionâ€¦ Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class. Examples: Civic Vs Corolla cadillacs Binomial Y | Quantitative X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t. We might use an explanatory variable X of â€œheightâ€ to see if taller students are more likely to get an A in Math 325 than shorter students. (They arenâ€™t, if you were wondering.) Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1â€™s and 0â€™s (this does happen or this doesnâ€™t happen): Does the width of a childâ€™s foot predict whether or not they are right-handed? What is the probability the cat adopted is a male based on how many days it took them to get adopted? Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted? (Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\") Tests Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The â€œPâ€ stands for â€œProbability thatâ€¦â€ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1â€¦ This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)â€¦ in other words, the â€œ|â€ says â€œgivenâ€ and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the â€œnatural constantâ€ number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14â€¦) Part What it means \\(P(Y_i = 1| x_i)\\) - The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s) - The symbol used to represent the notation in logisitc regression is \\(\\pi_i\\) \\(Y_i\\) (Responsive Variable) Describes the probability of $Y_i = $ 1 or 0 for a given scenario. \\(x_i\\) (Explanatory variable) Describes the observed value and how it effects the \\(Y_i\\) \\(\\frac{e^{\\text{stuff}}}{1+e^{\\text{stuff}}}\\) The content for â€œstuffâ€ being simple linear regression of the form \\(\\beta_0 + \\beta_{1x_i}\\) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -1.5651 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.6648 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -0.2460 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  0.7276 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  2.2691 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The test statistic is a regular old z-score. It is most reliable when the sample size is â€œlarge.â€ It is a measurement of the number of standard errors the estimate is from 0. Â  Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds. Â  1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds. Â  0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a â€œstarâ€. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about. Â  Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\). Â 43.230 Â on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates. Â 29.732 This can be calculated by sum(log(myglm$res^2)). Â on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, â€œA version of Akaikeâ€™s An Information Criterionâ€¦â€ The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model. Â 33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 * ## disp -0.014604 0.005168 -2.826 0.00471 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0â€™s or 1â€™s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == â€œBâ€ or height < 68. In other words, Y needs to be a collection of 0â€™s and 1â€™s. Â ~Â  The tilde is read â€œY on X.â€ X, X is some quantitative variable. Â data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(â€¦) function. curve( A function in R that draws a â€œcurveâ€ on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the â€œInterceptâ€ estimate from your logistic regression summary output. Â \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case â€œxâ€ in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula. Â exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator. Â add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(â€¦) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set. Â aes( The aes(â€¦) function stands for â€œaestheticsâ€ and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The plus sign adds a new layer to the ggplot. Â  geom_point( Tells the plot to add the physical geometry of â€œpointsâ€ to the plot. ) Closing parenthesis for the geom_point() function. Â + Add another layer to the plot. Â  geom_smooth( Add a smoothing line to the graph. method=â€œglmâ€, This adds a general linear model to the graph. method.args = list(family=â€œbinomialâ€), This tells the method=â€œglmâ€ to choose specifically the â€œbinomialâ€ model, otherwise known as the â€œlogistic regressionâ€ model. Â se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function. Â + Add another layer to the ggplot. Â  theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),Â  The xVariableName is the same one you used in your glm(y ~ x, â€¦) statement for â€œxâ€. Input any desired value for the â€œsomeNumericValueâ€ spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016 Explanation Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal). The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\). Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page. Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful. Checking Model Assumptions {#diagnostics} The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit. Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease. Binomial Y | Multiple X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t. We might use an explanatory variable X1 of â€œheightâ€ and a second explanatory variable X2 of â€œgenderâ€ to try to predict whether or not a student will get an A in Math 325. Test(s) used for this category: Multiple Logistic Regression Model Tests Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two. Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2Â  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. â€¦, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -2.9446 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.2364 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.0000 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q Â  0.2776 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  1.9968 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The Z-score testing the hypothesis that \\(\\beta_j=0\\) Â  Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\). Â  1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\). Â  4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\). Â  3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\). Â  0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\). Â  0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a â€œstarâ€. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\). Â  471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option. Â  Null Deviance: The deviance of the null model. Â 800.44 In this case, the null deviance is 800.44 Â on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression. Â 256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit. Â on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model. Â 272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -4.97603 0.65316 -7.618 2.57e-14 *** ## Time 0.39111 0.04979 7.854 4.02e-15 *** ## Diet2 0.58283 1.04061 0.560 0.5754 ## Diet3 -8.44476 4.32324 -1.953 0.0508 . ## Diet4 -67.41995 3768.10023 -0.018 0.9857 ## Time:Diet2 0.02391 0.08679 0.275 0.7830 ## Time:Diet3 1.20955 0.51249 2.360 0.0183 * ## Time:Diet4 8.83168 471.01259 0.019 0.9850 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,Â  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, â€¦), X2 = c(Value 1, Value 2, â€¦), â€¦) command. You should see the GSS example file for an example of how to use this function. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the â€œbâ€ vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(â€œSomeColorâ€,â€œDifferentColorâ€,â€¦) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(â€¦) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==â€œBâ€. Â ~Â  The formula operator in R. X1, The first x-variable in your glm code. Â data = YourDataSet, Specify the name of your data set. Â pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(â€¦) function computes e^(stuff) in R and stands for the â€œexponential function.â€ (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(â€¦) function requires that you call the x-variable â€œxâ€ although you can change this behavior using xname=â€œSomeOtherNameâ€ if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)). Â col = palette()[1], Pull the first color from the color palette for the first curve. Â add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters. Â col = palette()[2], Set the color of this second curve to the second color in the palette. Â add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. â€œtoprightâ€, Typically the legend is placed in the top-right corner of the graph. But it could be placed â€œtopâ€, â€œtopleftâ€, â€œleftâ€, â€œbottomleftâ€, â€œbottomâ€, â€œcenterâ€, â€œrightâ€, or â€œbottomrightâ€. Â legend = Why you have to write legend again, no one knows, but you do. c(â€œLable 1â€, â€œLabel 2â€), These are the values that will appear in the legend, one entry on each line of the legend. Â col = palette(), This specifies the colors of the symbols in the legend. First color goes with â€œLabel 1â€, and second goes with â€œLabel 2â€ and so on. Â lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist. Â bty = Specifies the â€œbox typeâ€ (bty) that should be drawn around the legend. â€˜nâ€™) By placing a â€œnoâ€ option (â€˜nâ€™) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot. Â aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + Add a layer to the ggplot. Â  geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function. Â + Add a layer to the ggplot. Â  geom_smooth(method=â€œglmâ€, method.args=list(family=â€œbinomialâ€), se=FALSE, Add the logistic regression curve to the plot. Â  aes(color=â€œX2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function. Â + Add a layer to the plot. Â  theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x' Explanation Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative. The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations. Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant. Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression. Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\). Categorical Y | Categorical X Graphics Y is a single categorical variable of interest, like gender. X is another categorical variable of interest, like â€œhair colorâ€. This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender. Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year? Whats the probability that you have brown hair and blue eyes? Tests Add your own notes about appropriate inferential procedures for this type of data here.",
    "sentences": ["Graphics Y is a single quantitative variable of interest, like â€œheightsâ€ of BYU-Idaho students.", "X is a categorical (qualitative) variable like which Math 221 you took, 221A, 221B, or 221C.", "In other words, X has three or more groups.", "So â€œClassrankâ€ could be X, with groups â€œFreshmanâ€, â€œSophomoreâ€, â€œJuniorâ€, and â€œSeniorâ€.", "Test(s) used for this category: One-Way ANOVA (means) Block Design Kruskal-Wallis Rank Sum Test (distributions) Analysis : College Studentâ€™s Food Perception Used comparing several groups based on means or distributions: How long are their feet based of width and gender?", "Are there certain months of the year that are associated with children having longer feet, on average, than others?", "Does a college studentâ€™s perception of food result in a change in their weight?", "(Kruskal-Wallis Rank Sum Test) # Read data food <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv\") # Data transformation and cleaning food_cleaned <- food %>% # Convert weight to numeric right away mutate(weight = as.numeric(weight)) %>% # Remove rows with NA weight or missing drink values filter(!is.na(weight) & !is.na(drink)) %>% # Create a new column `Food Perception Score` mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) # Create plots boxplot(weight ~ `Food Perception Score`, data=food_cleaned, col=\"lightgreen\", xlab=\"Food Perception Score\", main=\"Weight of College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=food_cleaned, pch=16, vertical=TRUE, add=TRUE, col=\"palegreen4\")", "Tests One-way ANOVA Each experimental unit is assigned to exactly one factor-level combination.", "Another way to say this is â€œone measurement per individualâ€ (no repeated measures) and â€œequal numbers of individuals per groupâ€."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Quantitative Y | Categorical X (3+\r\nGroups)"
  },
  {
    "id": 32,
    "title": "ğŸ“ Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single quantitative variable of interest, like â€œheightsâ€ of BYU-Idaho students. X is a categorical (qualitative) variable like which Math 221 you took, 221A, 221B, or 221C. In other words, X has three or more groups. So â€œClassrankâ€ could be X, with groups â€œFreshmanâ€, â€œSophomoreâ€, â€œJuniorâ€, and â€œSeniorâ€. Test(s) used for this category: One-Way ANOVA (means) Block Design Kruskal-Wallis Rank Sum Test (distributions) Analysis : College Studentâ€™s Food Perception Used comparing several groups based on means or distributions: How long are their feet based of width and gender? Are there certain months of the year that are associated with children having longer feet, on average, than others? Does a college studentâ€™s perception of food result in a change in their weight? (Kruskal-Wallis Rank Sum Test) # Read data food <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv\") # Data transformation and cleaning food_cleaned <- food %>% # Convert weight to numeric right away mutate(weight = as.numeric(weight)) %>% # Remove rows with NA weight or missing drink values filter(!is.na(weight) & !is.na(drink)) %>% # Create a new column `Food Perception Score` mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) # Create plots boxplot(weight ~ `Food Perception Score`, data=food_cleaned, col=\"lightgreen\", xlab=\"Food Perception Score\", main=\"Weight of College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=food_cleaned, pch=16, vertical=TRUE, add=TRUE, col=\"palegreen4\")",
    "sentences": ["Y is a single quantitative variable of interest, like â€œheightsâ€ of BYU-Idaho students.", "X is a categorical (qualitative) variable like which Math 221 you took, 221A, 221B, or 221C.", "In other words, X has three or more groups.", "So â€œClassrankâ€ could be X, with groups â€œFreshmanâ€, â€œSophomoreâ€, â€œJuniorâ€, and â€œSeniorâ€.", "Test(s) used for this category: One-Way ANOVA (means) Block Design Kruskal-Wallis Rank Sum Test (distributions) Analysis : College Studentâ€™s Food Perception Used comparing several groups based on means or distributions: How long are their feet based of width and gender?", "Are there certain months of the year that are associated with children having longer feet, on average, than others?", "Does a college studentâ€™s perception of food result in a change in their weight?", "(Kruskal-Wallis Rank Sum Test) # Read data food <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv\") # Data transformation and cleaning food_cleaned <- food %>% # Convert weight to numeric right away mutate(weight = as.numeric(weight)) %>% # Remove rows with NA weight or missing drink values filter(!is.na(weight) & !is.na(drink)) %>% # Create a new column `Food Perception Score` mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) # Create plots boxplot(weight ~ `Food Perception Score`, data=food_cleaned, col=\"lightgreen\", xlab=\"Food Perception Score\", main=\"Weight of College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=food_cleaned, pch=16, vertical=TRUE, add=TRUE, col=\"palegreen4\")"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 33,
    "title": "ğŸ“ Tests",
    "url": "index.html#tests",
    "content": "Tests One-way ANOVA Each experimental unit is assigned to exactly one factor-level combination. Another way to say this is â€œone measurement per individualâ€ (no repeated measures) and â€œequal numbers of individuals per groupâ€. Overview An ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA can be assumed to be normally distributed when seen in a Q-Q Plot.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out in a Residuals versus fitted-values plot.) comparing means Hypotheses For a One-way ANOVA \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] Mathematical Model A typical model for a one-way ANOVA is of the form \\[ Y_{ij} = \\mu_i + \\epsilon_{ij} \\] where \\(\\mu_i\\) is the mean for level (group) \\(i\\), and \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) is the error term for each point \\(j\\) within level (group) \\(i\\). R Instructions Console Help Command: ?aov() myaov is some name you come up with to store the results of the aov() test. Y must be a â€œnumericâ€ vector of the quantitative response variable. X is a qualitative variable (should have class(X) equal to factor or character. If it does not, use as.factor(X) inside the aov(Y ~ as.factor(X),...) command. YourDataSet is the name of your data set. Perform the ANOVA myaov <- aov(Y ~ X, data=YourDataSet) summary(myaov) Diagnose ANOVA Assumptions par(mfrow=c(1,2)) plot(myaov, which=1:2) Example Code Hover your mouse over the example codes to learn more. Perform the ANOVA chick.aovÂ <-Â  Saves the results of the ANOVA test as an object named â€˜chick.aovâ€™. aov( â€˜aov()â€™ is a function in R used to perform the ANOVA. weight Y is â€˜weightâ€™, which is a numeric variable from the chickwts dataset. Â ~Â  â€˜~â€™ is the tilde symbol used to separate the Y and X in a model formula. feed, X is â€˜feedâ€™, which is a qualitative variable in the chickwts dataset, or more specifically, a factor with six levels: â€œcaseinâ€, â€œhorsebeanâ€, and so onâ€¦ Use str(chickwts) to see this. Â dataÂ =Â chickwts) â€˜chickwtsâ€™ is a dataset in R. summary( â€˜summary()â€™ shows the results of the ANOVA. chick.aov) â€˜chick.aovâ€™ is the name of the ANOVA. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to View OutputÂ  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## feed 5 231129 46226 15.37 5.94e-10 *** ## Residuals 65 195556 3009 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Diagnose the ANOVA par( â€˜parâ€™ is a R function that can be used to set or query graphical parameters. mfrowÂ =Â c(1,2)) The mfrow parameter controls â€œmultiple frames on a rowâ€. In this case, the c(1,2) specifies 1 row of 2 plots. This will cause the two diagnostic plots to be placed side-by-side. plot( â€˜plotâ€™ is a R function for the plotting of R objects. chick.aov, â€˜chick.aovâ€™ is the name of the ANOVA. Â whichÂ =Â 1:2) The which=1:2 selects â€œwhichâ€ of 6 available plots we want to have graphed. In this case, 1 shows the Residuals vs Fitted, and 2 shows the Normal QQ-plot. Both are needed to check the ANOVA assumptions. Â Click to View OutputÂ  Click to View Output. Explanation Analysis of variance (ANOVA) is often applied to the scenario of testing for the equality of three or more means from (possibly) separate normal distributions of data. The normality assumption is required. No matter the sample size. If the distributions are skewed then a nonparametric test should be applied instead of ANOVA. One-Way ANOVA One-way ANOVA is when a completely randomized design is used with a single factor of interest. A typical mathematical model for a one-way ANOVA is of the form \\[ Y_{ik} = \\mu_i + \\epsilon_{ik} \\quad (\\text{sometimes written}\\ Y_{ik} = \\mu + \\alpha_i + \\epsilon_{ik}) \\] where \\(\\mu_i\\) is the mean of each group (or level) \\(i\\) of a factor, and \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) is the error term. The plot below demonstrates what these symbols represent. Note that the notation \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) states that we are assuming the error term \\(\\epsilon_{ik}\\) is normally distributed with a mean of 0 and a standard deviation of \\(\\sigma\\). Hypotheses The aim of ANOVA is to determine which hypothesis is more plausible, that the means of the different distributions are all equal (the null), or that at least one group mean differs (the alternative). Mathematically, \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_m = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\quad \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\}. \\] In other words, the goal is to determine if it is more plausible that each of the \\(m\\) different samples (where each sample is of size \\(n\\)) came from the same normal distribution (this is what the null hypothesis claims) or that at least one of the samples (and possibly several or all) come from different normal distributions (this is what the alternative hypothesis claims). Visualizing the Hypotheses The first figure below demonstrates what a given scenario might look like when all \\(m=3\\) samples of data are from the same normal distribution. In this case, the null hypothesis \\(H_0\\) is true. Notice that the variability of the sample means is smaller than the variability of the points. The figure below shows what a given scenario might look like for \\(m=3\\) samples of data from three different normal distributions. In this case, the alternative hypothesis \\(H_a\\) is true. Notice that the variability of the sample means, i.e., \\((\\bar{x}_1,\\bar{x}_2,\\bar{x}_3)\\), is greater than the variability of the points. Explaining the Name The above plots are useful in understanding the mathematical details behind ANOVA and why it is called analysis of variance. Recall that variance is a measure of the spread of data. When data is very spread out, the variance is large. When the data is close together, the variance is small. ANOVA utilizes two important variances, the between groups variance and the within groups variance. Between groups varianceâ€“a measure of the variability in the sample means, the \\(\\bar{x}\\)â€™s. Within groups varianceâ€“a combined measure of the variability of the points within each sample. The plot below combines the information from the previous plots for ease of reference. It emphasizes the fact that when the null hypothesis is true, the points should have a large variance (be really spread out) while the sample means are relatively close together. On the other hand, when the points are relative close together within each sample and the sample means have a large variance (are really spread out) then the alternative hypothesis is true. This is the theory behind analysis of variance, or ANOVA. Calculating the Test Statistic, \\(F\\) The ratio of the â€œbetween groups variationâ€ to the â€œwithin groups variationâ€ provides the test statistic for ANOVA. Note that the test statistic of ANOVA is an \\(F\\) statistic. \\[ F = \\frac{\\text{Between groups variation}}{\\text{Within groups variation}} \\] It would be good to take a minute and review the \\(F\\) distribution. The \\(p\\)-value for ANOVA thus comes from an \\(F\\) distribution with parameters \\(p_1 = m-1\\) and \\(p_2 = n-m\\) where \\(m\\) is the number of samples and \\(n\\) is the total number of data points. A Deeper Look at Variance It is useful to take a few minutes and explain the word variance as well as mathematically define the terms â€œwithin group varianceâ€ and â€œbetween groups variance.â€ Variance is a statistical measure of the variability in data. The square root of the variance is called the standard deviation and is by far the more typical measure of spread. This is because standard deviation is easier to interpret. However, mathematically speaking, the variance is the more important measurement. As mentioned previously, the variance turns out to be the key to determining which hypothesis is the most plausible, \\(H_0\\) or \\(H_a\\), when several means are under consideration. There are two variances that are important for ANOVA, the â€œwithin groups varianceâ€ and the â€œbetween groups variance.â€ Recall that the formula for computing a sample variance is given by \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})^2}{n-1} \\quad\\leftarrow \\frac{\\text{sum of squares}}{\\text{degrees of freedom}} \\] This formula has a couple of important pieces that are so important they have been given special names. The \\(n-1\\) in the denominator of the formula is called the â€œdegrees of freedom.â€ The other important part of this formula is the \\(\\sum_{i=1}^n(x_i - \\bar{x})^2\\), which is called the â€œsum of squared errorsâ€ or sometimes just the â€œsum of squaresâ€ or â€œSSâ€ for short. Thus, the sample variance is calculated by computing a â€œsum of squaresâ€ and dividing this by the â€œdegrees of freedom.â€ It turns out that this general approach works for many different contexts. Specifically, it allows us to compute the â€œwithin groups varianceâ€ and the â€œbetween groups variance.â€ To introduce the mathematical definitions of these two variances, we need to introduce some new notation. Let \\(\\bar{y}_{i\\bullet}\\) represent the sample mean of group \\(i\\) for \\(i=1,\\ldots,m\\). Let \\(n_i\\) denote the sample size in group \\(i\\). Let \\(\\bar{y}_{\\bullet\\bullet}\\) represent the sample mean of all \\(n = n_1+n_2+\\cdots+n_m\\) data points. The mathematical calculations for each of these variances is given as follows. \\[ \\text{Between groups variance} = \\frac{\\sum_{i=1}^m (\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2}{m-1} \\leftarrow \\frac{\\text{Between groups sum of squares}}{\\text{Between groups degrees of freedom}} \\] \\[ \\text{Within groups variance} = \\frac{\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2}{n-m} \\leftarrow \\frac{\\text{Within groups sum of squares}}{\\text{Within groups degrees of freedom}} \\] A Fabricated Example The following table provides three samples of data: A, B, and C. These samples were randomly generated from normal distributions using a computer. The true means \\(\\mu_1, \\mu_2\\), and \\(\\mu_3\\) of the normal distributions are thus known, but withheld from you at this point of the example. A B C 13.15457 13.17463 16.66831 12.65225 12.16277 15.54719 13.73061 12.76905 16.63074 14.43471 13.38524 15.06726 13.79728 12.02690 15.57534 13.88599 13.24651 15.99915 12.77753 12.58386 15.58995 13.81536 12.64615 16.99429 13.03635 12.52055 15.47153 14.26062 14.03566 16.13330 An ANOVA will be performed with the sample data to determine which hypothesis is more plausible: \\[ H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\in \\{1,\\ldots,m\\} \\] To perform an ANOVA, we must compute the between groups variance and the within groups variance. This requires the Between groups sums of squares, within groups sums of squares, between groups degrees of freedom, and the within groups degrees of freedom. Note that to get the sums of squares, we first had to calculate \\(\\bar{y}_{1\\bullet}\\), \\(\\bar{y}_{2\\bullet}\\), \\(\\bar{y}_{3\\bullet}\\), and \\(\\bar{y}_{\\bullet\\bullet}\\) where the 1, 2, 3 corresponds to Samples A, B, and C, respectively. After some work, we find these values to be \\[ \\bar{y}_{1\\bullet} = 13.55, \\quad \\bar{y}_{2\\bullet} = 12.86 \\quad \\bar{y}_{3\\bullet} = 15.97 \\] and \\[ \\bar{y}_{\\bullet\\bullet} = \\frac{13.55+12.86+15.97}{3} = 14.13 \\] Using these values we can then compute the between groups sum of squares and the within groups sum of squares according to the formulas stated previously. This process is very tedious and will not be demonstrated. Only the results are shown in the following table which summarizes all the important information. Â  Degrees of Freedom Sum of Squares Variance F-value p-value Between groups 2 53.3 26.67 70.2 2e-11 Within groups 27 10.3 0.38 ANOVA Table In general, the ANOVA table is created by Â  Degrees of Freedom Sum of Squares Variance F-value p-value Between groups \\(m-1\\) \\(\\sum_{i=1}^m n_i(\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) \\(\\frac{\\text{Between groups variance}}{\\text{Within groups variance}}\\) \\(F\\)-distribution tail probability Within groups \\(n-m\\) \\(\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) ANOVA Assumptions{#residuals} The requirements for an analysis of variance (the assumptions of the test) are two-fold and concern only the error terms, the \\(\\epsilon_{ik}\\). The errors are normally distributed. The variance of the errors is constant. Both of these assumptions were stated in the mathematical model where we assumed that \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\). Checking ANOVA Assumptions To check that the ANOVA assumptions are satisfied, it is required to check the data in each group for normality using QQ-Plots. Also, the sample variance of each group must be relatively constant. The fastest way to check these two assumptions is by analyzing the residuals. An ANOVA residual is defined as the difference between the observed value of \\(y_{ik}\\) and the mean \\(\\bar{y}_{i\\bullet}\\). Mathematically, \\[ r_{ik} = y_{ik} - \\bar{y}_{i\\bullet} \\] One QQ-Plot of the residuals will provide the necessary evidence to decide if it is reasonable to assume that the error terms are normally distributed. Also, the constant variance can be checked visually by using what is known as a residuals versus fitted values plot. For the Fabricated Example above, the QQ-Plot and residuals versus fitted values plots show the two assumptions of ANOVA appear to be satisfied. Examples: chickwts (One-way) Block Design Repeated measures or other factors that group individuals into similar groups (blocks) are included in the study design. Overview A typical model for a block design is of the form \\[ Y_{lijk} = \\mu + B_l + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijlk} \\] where \\(\\mu\\) is the grand mean, \\(B_l\\) is the blocking factor, \\(\\alpha_i\\) is one factor with at least two levels, \\(\\beta_j\\) is another factor with at least two levels, \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors, and \\(\\epsilon_{ijlk} \\sim N(0,\\sigma^2)\\) is the error term. Only one block and one factor is required. Multiple blocks and multiple factors are allowed. It is not required to include interaction terms. The error term is always required. R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ Block+X1+X2+X1:X2, data=YourDataSet) Perform the test summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a â€œnumericâ€ vector of the quantitative response variable. Block is a qualitative variable that is not of direct interest, but is included in the model to account for variability in the data. It should have class(Block) equal to either factor or character. Use as.factor() if it does not. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. If it does not, use as.factor(X2). Note that factors C, D, and so on could also be added + to the model if desired. X1:X2 denotes the interaction of the factors X1 and X2. It is not required and should only be included if the interaction term is of interest. YourDataSet is the name of your data set. Examples: ChickWeight Kruskal-Wallis Rank Sum Test Allows for deciding if several samples come from the same population or if at least one sample comes from a different population. Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated. Can also be used when data is ordered (ordinal) but does not have an exact measurement. Best used when the distribution of the data is not normal. ANOVA is appropriate when the data is normal. comparing distributions Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, â€œis a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly. In many cases, this is practically equivalent to the mean of at least one population differing from the others.â€ Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable. g is a qualitative grouping variable defining which groups each value in x belongs to. It must either be a character vector or a factor vector. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. weight â€˜weightâ€™ is a numeric variable from the chickwts dataset that represents the quantatitive response variable. Â ~Â  â€˜~â€™ is the tilde symbol used to separate the left- and right-hand side in a model formula. feed, â€˜feedâ€™ is a qualitative grouping variable in the chickwts dataset. Â dataÂ =Â chickwts) â€˜chickwtsâ€™ is a dataset in R. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to View OutputÂ  Click to View Output. ## ## Kruskal-Wallis rank sum test ## ## data: weight by feed ## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07 Alternatively, you could use the following approach as well. kruskal.test(list(x, y, z)) x is a numeric vector of data values that represents the quantatitive response variable for the first group. y is a numeric vector of data values that represents the quantatitive response variable for the second group. z is a numeric vector of data values that represents the quantatitive response variable for the third group. Note that more than three vectors of data could be included inside of list(). Example Code Hover your mouse over the example codes to learn more. feed1 <- filter(chickwts, feed == â€œhorsebeanâ€) This splits out the first group of feed (horsebean) from the chickwts dataset. feed2 <- filter(chickwts, feed == â€œlinseedâ€) This splits out the second group of feed (linseed) from the chickwts dataset. feed3 <- filter(chickwts, feed == â€œsoybeanâ€) This splits out the third group of feed (soybean) from the chickwts dataset. feed4 <- filter(chickwts, feed == â€œsunflowerâ€) This splits out the fourth group of feed (sunflower) from the chickwts dataset. feed5 <- filter(chickwts, feed == â€œmeatmealâ€) This splits out the fifth group of feed (meatmeal) from the chickwts dataset. feed6 <- filter(chickwts, feed == â€œcaseinâ€) This splits out the sixth group of feed (casein) from the chickwts dataset. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. list(feed1$weight,feed2$weight,feed3$weight,feed4$weight,feed5$weight,feed6$weight)) A list of numeric vecotrs that represent the quantitative response variable for each group. Â Click to View OutputÂ  Click to View Output. Load library(tidyverse) to run this code in R. ## ## Kruskal-Wallis rank sum test ## ## data: list(feed1$weight, feed2$weight, feed3$weight, feed4$weight, feed5$weight, feed6$weight) ## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07 Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population. The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples. The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity. Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated. (Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\). Note that the notation \\(n_i\\) allows for each sample to be a different size. In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\). Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest. Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\). Award any tied values the average of the ranks of those values that are tied. In the bottle-cap data we have the following. Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample. Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\). In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\). In the bottle-cap data, \\(\\bar{R}_1 = \\frac{R_1}{n_1} = \\frac{24}{5} = 4.8\\) \\(\\bar{R}_2 = \\frac{R_2}{n_2} = \\frac{14}{3} = 4.667\\) \\(\\bar{R}_3 = \\frac{R_3}{n_3} = \\frac{40}{4} = 10.0\\) Step 5: If there are no ties, calculate the test statistic \\(H\\) by the formula \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] See the original article for the formula to use when there are ties present in the data. In the bottle-cap data, \\(H = 5.656\\). To understand the logic behind the test statistic \\(H\\), note that the null hypothesis assumes that each of the \\(C\\) samples are taken from the same population. Under this assumption, the ranks assigned to each sample should represent a uniform sample of the ranks \\(1,\\ldots,N\\). If the null hypothesis were true, we would expect the average of the ranks from each group, the \\(\\bar{R}_i\\), to be roughly about the same. More precisely, using the properties of the discrete uniform distribution (see wikipedia), we expect each \\(\\bar{R}_i\\) to be close to \\(\\frac{1}{2}(N+1)\\), which is the mean of the discrete uniform distribution. That \\(\\bar{R}_i\\) should be close to this value comes from the fact that the mean of a distribution of means is equal to the population mean. Further, the variance of \\(\\bar{R}_i\\) is given by the population variance, \\(\\frac{1}{12}(N^2-1)\\) in this case, divided by the sample size \\(n_i\\). Finally, the \\(\\frac{N-1}{N}\\) is a correction factor that will not be explained here. The full article explains the details about how a multivariate normal distribution is used to uncover the formula for \\(H\\). Thus, understanding \\(H\\) completely will require an advanced degree in statistical theory. However, the short story is that the distribution of the test statistic \\(H\\) is approximately a chi squared distribution with \\(C-1\\) degrees of freedom and has the form of a sum of values consisting of the (observed mean - expected mean)^2/(variance of the observed mean). Step 6: Calculate the p-value of \\(H\\) using a chi squared distribution with \\(C-1\\) degrees of freedom. \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] In the bottle-cap data, \\(p = 0.059\\), which represents the probability of \\(H\\) being as extreme or more extreme than the observed value of 5.656, assuming the null hypothesis is true. Examples: wages, pigweights Quantitative Y | Multiple Categorical X Graphics Y is a single quantitative variable of interest, like â€œheightsâ€ of BYU-Idaho students. X1 is a categorical (qualitative) variable like gender, with levels â€œboyâ€ and â€œgirl.â€ X2 is another categorical (qualitative) variable like â€œClassrankâ€ with levels â€œFreshmanâ€, â€œSophomoreâ€, and â€œJuniorâ€. Of course, both X1 and X2 can have as many levels as you want to work with, but must have at least two levels each. Test(s) used for this category: Two-Way ANOVA (means) Used to answer THREE questions: the effects of factor 1 on the quantitative variable the effects of factor 2 on the quantitative variable the interaction between the two factors best paired with numerical summary looking at mean Analysis : Two-way ANOVA: Final Beginning Algebra Grades Question Examples: Based on your p-values and these graphs, which of the following is a correct conclusion to reach? xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1))) ANSWER : Among fourth graders, right handed boys have longer feet on average than left handed boys while the opposite is true for girls. Does the amount a student participates in the course affect the average final Beginning Algebra grades? Does the semester in which a student takes Beginning Algebra affect their average final grade? Does a studentâ€™s participation level vary depending on the semester? (Alternatively, does the semester influence a studentâ€™s participation?) BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") BAFinalGrades$Semester <- factor(BAFinalGrades$Semester, levels = c(\"Fall\",\"Winter\",\"Spring\")) ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=1)) + geom_point(color=\"firebrick1\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Spring Semester Seems to Have Higher Final Grades\", x=\"Semester\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades$Participation <- factor(BAFinalGrades$Participation, levels = c(\"Low\",\"Moderate\",\"High\")) ggplot(BAFinalGrades, aes(x=Participation, y=FinalGrade, group=1)) + geom_point(color=\"darkred\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Participation Seems to Lead to Higher Grades\", x=\"Participation\", y=\"Beginning Algebra Final Grades\") + theme_minimal() ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=Participation, color=Participation)) + geom_point() + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Beginning Algebra Seems Better during the Spring Semester\", x=\"BYU-Idaho Semesters\", y=\"Beginning Algebra Final Grades\") + theme_minimal() Tests Two-way ANOVA Overview A two-way ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses. Writing out the hypotheses can be very involved depending on whether you use the official â€œeffects modelâ€ notation (very mathematically correct) or a simplified â€œmeans modelâ€ notation (which isnâ€™t very mathematically correct, but gets the idea across in an acceptable way). Means Model Effects Model The first set of hypotheses are a â€œone-wayâ€ set of hypotheses for the first factor of the ANOVA. Factor: X1 with say, levels \\(A\\) and \\(B\\). \\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a â€œone-wayâ€ set of hypotheses, but for the second factor of the ANOVA. Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\mu_C = \\mu_D = \\mu_E = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one}\\ i\\in\\{1=C,2=D,3=E\\} \\] The third set of hypotheses are the most interesting hypotheses in a two-way ANOVA. These are called the interaction hypotheses. They test to see if the levels of one of the factors, say \\(X1\\), impact \\(Y\\) differently for the differing levels of the other factor, \\(X2\\). The hypotheses read formally as \\[ H_0: \\text{The effect of the first factor on Y} \\\\ \\text{is the same for all levels of the second factor.} \\] \\[ H_a: \\text{The effect of the first factor on Y is not the same} \\\\ \\text{for all levels of the second factor.} \\] A mathematically correct way to state the two-way ANOVA model is with the equation \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] In this model, \\(\\mu\\) is the grand mean (which is the average Y-value ignoring all information contained in the factors); \\(\\alpha_i\\) is the first factor \\(X1\\) with levels \\(A\\) and \\(B\\) (though there could be more levels in \\(X1\\) depending on your data); \\(\\beta_j\\) is the second factor with levels \\(C\\), \\(D\\), and \\(E\\) (though there could be fewer or more levels to this factor depending on your data); \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors which has \\(2\\times3=6\\) (may differ for your data) levels; and \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) is the normally distributed error term for each point \\(k\\) found within level \\(i\\) of \\(X1\\) and level \\(j\\) of \\(X2\\). This model allows us to more formally state the hypotheses as First factor \\(X1\\) having say, levels \\(A\\) and \\(B\\). \\[ H_0: \\alpha_A = \\alpha_B = 0 \\] \\[ H_a: \\alpha_i \\neq 0 \\ \\text{for at least one}\\ i\\in\\{1=A,2=B\\} \\] Second factor \\(X2\\) with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\beta_C = \\beta_D = \\beta_E = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j\\in\\{1=C,2=D,3=E\\} \\] Does the effect of the first factor (\\(X1\\)) change for the different levels of the second factor (\\(X2\\))? In other words, is there an interaction between the two factors \\(X1\\) and \\(X2\\)? \\[ H_0: \\alpha\\beta_{ij} = 0 \\ \\text{for all } i,j \\] \\[ H_a: \\alpha\\beta_{ij} \\neq 0 \\ \\text{for at least one } i,j \\] R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ X1+X2+X1:X2, data=YourDataSet) Perform the ANOVA summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a â€œnumericâ€ vector of the quantitative response variable. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. Note that factors X3, X4, and so on could also be added + to the model if desired, but this would create a three-way, or four-way ANOVA model, and so on. X1:X2 denotes the interaction of the factors X1 and X2. It is not required, but is usually included. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. warp.aovÂ <-Â  Saves the results of the ANOVA test as an object named â€˜warp.aovâ€™. aov( â€˜aov()â€™ is a function in R used to perform the ANOVA. breaks \\(Y\\) is â€˜breaksâ€™, which is a numeric variable from the warpbreaks dataset. Â ~Â  â€˜~â€™ is the tilde symbol used to separate the left- and right-hand side in a model formula. woolÂ  The first factor \\(X1\\) is â€˜woolâ€™, which is a qualitative variable in the warpbreaks dataset. In this case, wool is a factor with two levels. Use str(warpbreaks) to see this. +Â tensionÂ  The second factor \\(X2\\) is â€˜tensionâ€™, which is another qualitative variable in the warpbreaks dataset. In this case, tension is a factor with three levels. Use str(warpbreaks) to see this. +Â wool:tension, The interaction of the two factors: wool and tension. Â dataÂ =Â warpbreaks) â€˜warpbreaksâ€™ is a dataset in R. summary( â€˜summary()â€™ shows the results of the ANOVA. warp.aov) â€˜warp.aovâ€™ is the name of the ANOVA. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to View OutputÂ  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## wool 1 451 450.7 3.765 0.058213 . ## tension 2 2034 1017.1 8.498 0.000693 *** ## wool:tension 2 1003 501.4 4.189 0.021044 * ## Residuals 48 5745 119.7 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 par( â€˜parâ€™ is a R function that can be used to set or query graphical parameters. mfrowÂ =Â c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. plot( â€˜plotâ€™ is a R function for the plotting of R objects. warp.aov, â€˜warp.aovâ€™ is the name of the ANOVA. Â whichÂ =Â 1:2) Will show the Residuals vs Fitted and the Normal QQ-plot to check the ANOVA assumptions. Â Click to View OutputÂ  Click to View Output. Explanation Hypotheses in Two-way ANOVA The hypotheses that can be tested in a two-way ANOVA that includes an interaction term are three-fold. Hypotheses about \\(\\alpha\\) where \\(\\alpha\\) has \\(m\\) levels. \\[ H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_m = 0 \\] \\[ H_a: \\alpha_i \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\} \\] Hypotheses about \\(\\beta\\) where \\(\\beta\\) has \\(q\\) levels. \\[ H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_q = 0 \\] \\[ H_a: \\beta_j \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,q\\} \\] Hypotheses about the interaction term \\(\\alpha\\beta\\). \\[ H_0: \\text{the effect of one factor is the same across all levels of the other factor} \\] \\[ H_a: \\text{the effect of one factor differs for at least one level of the other factor} \\] Expanding the ANOVA Model{#expanding} It turns out that more can be done with ANOVA than simply checking to see if the means of several groups differ. Reconsider the mathematical model of two-way ANOVA. \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] This model could be expanded to include any number of new terms in the model. The power of this approach is in the several questions (hypotheses) that can be posed to data simultaneously. What happens if your ANOVA fails both requirement tests? Example: BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") ## Rows: 204 Columns: 3 ## â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ## Delimiter: \",\" ## chr (2): Semester, Participation ## dbl (1): FinalGrade ## ## â„¹ Use `spec()` to retrieve the full column specification for this data. ## â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message. BAanova <- aov(FinalGrade ~ Semester + Participation + Semester:Participation, data=BAFinalGrades) par(mfrow=c(1,2)) plot(BAanova, which=1:2, pch=16) The residual plot shows that the constant (equal) variance assumption raises some concern as the points within each group have varying spread. The spread of each group seems to gets closer and closer as we go from left to right as well as some straying point from the more clustered groups. Thus, showing the variance of the residuals are not constant and do not accurately show a relationship between each variable. For the Q-Q plot, it shows that the residuals of the data is not quite normally distributed. There are visible points at the lower and upper ends of the graph deviating away form the diagonal line, which represents a normal distribution. Thus, showing us that the residuals of the data are not normal distributed. Although both graphs indicate that the data fails to meet both requirements, this primarily means our findings may be less reliable due to the abnormalities in the data. As a result, we should interpret the results cautiously as we proceed with our Two-Way ANOVA test! Examples: warpbreaks (Two-way), CO2 (three-way) Quantitative Y | Quantitative X Graphics Y is a single quantitative variable of interest, like â€œheightâ€. X is another single quantitative variable of interest, like â€œshoe-sizeâ€. This would imply we are using â€œshoe-sizeâ€ (X) to explain â€œheightâ€ (Y). Test(s) used for this category: Simple Linear Regression Analysis : Simple Linear Regression: BYU-Idaho Math Tutoring Used to find the relationship between one independent variable (predictor) and one dependent variable(outcome): Is there a relationship between the length of childrenâ€™s feet and the width of their foot? Example: Does the amount of tutoring a student receives correlate with their chapter exam scores? (Simple Linear Regression) ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams) Tests Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\). Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$\\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$$ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is â€œequal toâ€ the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to â€œGodâ€™s Lawâ€ or â€œNatural Lawâ€. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are â€œrandomâ€ and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)â€¦ \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced â€œy-hatâ€, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what followsâ€¦ \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, â€œthe change in the average y-value for a one unit change in the x-value.â€ It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, â€œthe average y-value when x is zero.â€ It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful. R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your lm() code into mylm name. lm( lm(â€¦) is an R function that stands for â€œLinear Modelâ€. It performs a linear regression analysis for Y ~ X. YÂ  Y is your quantitative response variable. It is the name of one of the columns in your data set. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X. X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value. Â data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X. ) Closing parenthesis for the lm(â€¦) function. summary(mylm) The summary command allows you to print the results of your linear regression that were previously saved in mylm name. Â Click to Show OutputÂ  Click to View Output. Example output from a regression. Hover each piece to learn more. Call: lm(formula = dist ~ speed, data = cars) This is simply a statement of your original lm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(â€¦). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min Â  -29.069 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -9.525 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -2.272 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -2.272 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-2.272) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  9.215 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  43.201 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  t value To learn more about the â€œt valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  Pr(>|t|) The â€œPrâ€ stands for â€œProbabilityâ€ and the â€œ(> |t|)â€ stands for â€œmore extreme than the observed t-valueâ€. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the â€œp-valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. (Intercept) This always says â€œInterceptâ€ for any lm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -17.5791 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  6.7584 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -2.601 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the intercept (-17.5791) by its standard error (6.7584). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of -17.5791 is -2.601 standard errors (6.7584) from zero, which is a fairly surprising distance as shown by the p-value. 0.0123 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). speed This is always the name of your X-variable in your lm(Y ~ X, â€¦). Â  3.9324 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.4155 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 9.464 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the slope (3.9324) by its standard error (0.4155). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from zero, which is a really surprising distance as shown by the smallness of the p-value. 1.49e-12 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE. Â 15.38 For this particular regression, the estimate of \\(\\sigma\\) is 15.38. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\). Â on 48 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 50 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 50-2 = 48. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO. Â 0.6511, In this particular regression, 65.11% of the variation in stopping distance dist is explained by the regression model using speed of the car. Â Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data. Â 0.6438 In this case, the value of 0.6438 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is found as the ratio of the MSR/MSE where MSR = SSR/(p-1) and MSE = SSE/(n-p) where n is the sample size and p is the number of parameters in the regression model. Â 89.57 This is the value of the F-statistic for the lm(dist ~ speed, data=cars) regression. Note that SSE = sum( cars.lm$res^2 ) = 11353.52 with n - p = 50 - 2 = 48 degrees of freedom for this data. Further, SSR = sum( (cars.lm$fit - mean(cars$dist))^2 ) = 21185.46 with p - 1 = 1 degree of freedom. So MSR = 21185.46 and MSE = 11353.52 / 48 = 236.5317. So MSR / MSE = 21185.46 / 236.5317 = 89.56711. Â on 1 and 48 DF, The 1 degree of freedom is the SSR degrees of freedom (p-1). The 48 is the SSE degrees of freedom (n-p). Â p-value: 1.49e-12 The p-value for an F-statistic is found by the code pf(89.56711, 1, 48, lower.tail=FALSE), which gives the probability of being more extreme than the observed F-statistic in an F distribution with 1 and 48 degrees of freedom. Check Assumptions 1, 2, 3, and 5 When making it in an r chunk make your height 3! { r, fig.height = 3 } par( The par(â€¦) command stands for â€œGraphical PARametersâ€. It allows you to control various aspects of graphics in Base R. mfrow= This stands for â€œmultiple frames filled by rowâ€, which means, put lots of plots on the same row, starting with the plot on the left, then working towards the right as more plots are created. c( The combine function c(â€¦) is used to specify how many rows and columns of graphics should be placed together. 1, This specifies that 1 row of graphics should be produced. 3 This states that 3 columns of graphics should be produced. ) Closing parenthesis for c(â€¦) function. ) Closing parenthesis for par(â€¦) function. plot( This version of plot(â€¦) will actually create several regression diagnostic plots by default. mylm, This is the name of an lm object that you created previously. which= This allows you to select â€œwhichâ€ regression diagnostic plots should be drawn. 1 Selecting 1, would give the residuals vs.Â fitted values plot only. : The colon allows you to select more than just one plot. 2 Selecting 2 also gives the Q-Q Plot of residuals. If you wanted to instead you could just use which=1 to get the residuals vs fitted values plot, then you could use qqPlot(mylm$residuals) to create a fancier Q-Q Plot of the residuals. ) Closing parenthesis for plot(â€¦) function. plot( This version of plot(â€¦) will be used to create a time-ordered plot of the residuals. The order of the residuals is the original order of the x-values in the original data set. If the original data set doesnâ€™t have an order, then this plot is not interesting. mylm The lm object that you created previously. $ This allows you to access various elements from the regression that was performed. residuals This grabs the residuals for each observation in the regression. ) Closing parenthesis for plot(â€¦) function. Â Click to Show OutputÂ  Click to View Output. Which regression assumption(s) does the residuals versus fitted values plot diagnose? Linear Relation Constance Variance Which regression assumption(s) does the Q-Q Plot of the residuals diagnose? Normal Errors Which regression assumption(s) does the Residuals versus Order plot diagnose? (Remember, this plot can only be created when the data was collected in a specific order.) Independent Errors Plotting the Regression Line Base R ggplot2 To add the regression line to a scatterplot use the abline(...) command: plot( The plot(â€¦) function is used to create a scatterplot with a y-axis (the vertical axis) and an x-axis (the horizontal axis). YÂ  This is the â€œresponse variableâ€ of your regression. The thing you are interested in predicting. This is the name of a â€œnumericâ€ column of data from the data set called YourDataSet. ~Â  The tilde â€œ~â€ is used to relate Y to X and can be found on the top-left key of your keyboard. X,Â  This is the explanatory variable of your regression. It is the name of a â€œnumericâ€ column of data from YourDataSet. data= The data= statement is used to specify the name of the data set where the columns of â€œXâ€ and â€œYâ€ are located. YourDataSet This is the name of your data set, like KidsFeet or cars or airquality. ) Closing parenthesis for plot(â€¦) function. abline( This stands for â€œaâ€ (intercept) â€œbâ€ (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm This is the name of an lm(â€¦) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(â€¦) function will locate these two values from within mylm and use them to add a line to your current plot(â€¦). ) Closing parenthesis for abline(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm) You can customize the look of the regression line with abline( This stands for â€œaâ€ (intercept) â€œbâ€ (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm, This is the name of an lm(â€¦) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(â€¦) function will locate these two values from within mylm and use them to add a line to your current plot(â€¦). lty= The lty= stands for â€œline typeâ€ and allows you to select between 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. 1, This creates a solid line. Remember, other options include: 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. lwd= The lwd= allows you to specify the width of the line. The default width is 1. Using lwd=2 would double the thickness, and so on. Any positive value is allowed. 1, Default line width. To make a thicker line, us 2 or 3â€¦ To make a thinner line, try 0.5, but 1 is already pretty thin. col= This allows you to specify the color of the line using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). â€œsomeColorâ€ Type colors() in R for options. ) Closing parenthesis for abline(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm, lty=1, lwd=1, col=\"firebrick\") You can add points to the regression withâ€¦ points( This is like plot(â€¦) but adds points to the current plot(â€¦) instead of creating a new plot. newYÂ  newY should be a column of values from some data set. Or, use points(newX, newY) to add a single point to a graph. ~Â  This links Y to X in the plot. newX,Â  newX should be a column of values from some data set. It should be the same length as newY. If just a single value, use points(newX, newY) instead. data=YourDataSet,Â  If newY and newX come from a dataset, then use data= to tell the points(â€¦) function what data set they come from. If newY and newX are just single values, then data= is not needed. col=â€œskyblueâ€, This allows you to specify the color of the points using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). pch=16 This allows you to specify the type of plotting symbol to be used for the points. Type ?pch and scroll half way down in the help file that appears to learn about other possible symbols. ) Closing parenthesis for points(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) points(7,40, pch=16, col=\"skyblue\", cex=2) text(7,40, \"New Dot\", pos=3, cex=0.5) points(dist ~ speed, data=filter(cars, mylm$res > 2), cex=.8, col=\"red\") abline(mylm, lty=1, lwd=1, col=\"firebrick\") To add the regression line to a scatterplot using the ggplot2 approach, first ensure: library(ggplot2) or library(tidyverse) is loaded. Then, use the geom_smooth(method = lm) command: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. YourDataSet,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. X, This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. Â y =Â  â€œy=Â â€ declares which variable will become the y-axis of the graphic. Y This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. + Here the + is used to add yet another layer to ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. There are several other methods that could be used here. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(â€¦) to place more complicated models on the plot. Â se = FALSE se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. ) Closing parenthesis for the geom_smooth() function. Â Click to Show OutputÂ  Click to View Output. ggplot(cars, aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = \"lm\", formula=y~x, se=FALSE) There are a number of ways to customize the appearance of the regression line: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. speed,Â  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. y =Â  â€œy=Â â€ declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. Â + Here the + is used to add yet another layer to ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(â€¦) to place more complicated models on the plot. Â se = FALSE, se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. Â size = 2, Use size = 2 to adjust the thickness of the line to size 2. Â color = â€œorangeâ€, Use color = â€œorangeâ€ to change the color of the line to orange. Â Â linetype = â€œdashedâ€ Use linetype = â€œdashedâ€ to change the solid line to a dashed line. Some linetype options include â€œdashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. ) Closing parenthesis for the geom_smooth() function. Â Click to Show OutputÂ  Click to View Output. In addition to customizing the regression line, you can customize the points, add points, add lines, and much more. ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. speed,Â  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. y =Â  â€œy=Â â€ declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point( geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. size = 1.5, Use size = 1.5 to change the size of the points. Â color = â€œskyblueâ€ Use color = â€œskyblueâ€ to change the color of the points to Brother Saundersâ€™ favorite color. Â alpha = 0.5 Use alpha = 0.5 to change the transparency of the points to 0.5. ) Closing parenthesis of geom_point() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in ways similar to lm(â€¦) to place more complicated models on the plot. Â se = FALSE, se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. Â color = â€œnavyâ€, Use color = â€œnavyâ€ to change the color of the line to navy blue. Â size = 1.5 Use size = 1.5 to adjust the thickness of the line to 1.5. ) Closing parenthesis of geom_smooth() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_hline( Use geom_hline() to add a horizontal line at a specified y-intercept. You can also use geom_vline(xintercept = some_number) to add a vertical line to the graph. yintercept = Use â€œyintercept =â€ to tell geom_hline() that you are going to declare a y intercept for the horizontal line. Â 75 75 is the value of the y-intercept. , color = â€œfirebrickâ€ Use color = â€œfirebrickâ€ to change the color of the horizontal line to firebrick red. , size = 1, Use size = 1 to adjust the thickness of the horizontal line to size 1. Â Â Â Â Â Â Â Â Â Â Â Â Â linetype = â€œlongdashâ€ Use linetype = â€œlongdashâ€ to change the solid line to a dashed line with longer dashes. Some linetype options include â€œdashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. , alpha = 0.5 Use alpha = 0.5 to change the transparency of the horizontal line to 0.5. ) Closing parenthesis of geom_hline function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_segment( geom_segment() allows you to add a line segment to ggplot() by using specified start and end points. x = â€œx =â€ tells geom_segment() that you are going to declare the x-coordinate for the starting point of the line segment. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the starting point of the line segment. Â y =â€œy =â€ tells geom_segment() that you are going to declare the y-coordinate for the starting point of the line segment. Â 75, 75 is a number on the y-axis of your graph. It is the y-coordinate of the starting point of the line segment. Â xend = â€œxend =â€ tells geom_segment() that you are going to declare the x-coordinate for the end point of the line segment. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the end point of the line segment. Â yend = â€œyend =â€ tells geom_segment() that you are going to declare the y-coordinate for the end point of the line segment. Â 38, 38 is a number on the y-axis of your graph. It is the y-coordinate of the end point of the line segment. Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â size = 1 Use size = 1 to adjust the thickness of the line segment. , color = â€œlightgrayâ€ Use color = â€œlightgrayâ€ to change the color of the line segment to light gray. , linetype = â€œlongdashâ€ Use *linetype = â€œlongdash* to change the solid line segment to a dashed one. Some linetype options includeâ€dashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. ) Closing parenthesis for geom_segment() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_point( geom_point() can also be used to add individual points to the graph. Simply declare the x and y coordinates of the point you want to plot. x = â€œx =â€ tells geom_point() that you are going to declare the x-coordinate for the point. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the point. Â y = â€œy =â€ tells geom_point() that you are going to declare the y-coordinate for the point. Â 75 75 is a number on the y-axis of your graph. It is the y-coordinate of the point. , size = 3 Use size = 3 to make the point stand out more. , color = â€œfirebrickâ€ Use color = â€œfirebrickâ€ to change the color of the point to firebrick red. ) Closing parenthesis of the geom_point() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_text( geom_text() allows you to add customized text anywhere on the graph. It is very similar to the base R equivalent, text(â€¦). x = â€œx =â€ tells geom_text() that you are going to declare the x-coordinate for the text. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the text. Â y = â€œy =â€ tells geom_text() that you are going to declare the y-coordinate for the text. Â 84, 84 is a number on the y-axis of your graph. It is the y-coordinate of the text. Â label = â€œlabel =â€ tells geom_text() that you are going to give it the label. Â â€œMy Point (14, 75)â€, â€œMy Point (14, 75)â€ is the text that will appear on the graph. Â Â Â Â Â Â Â Â Â Â Â Â color = â€œnavyâ€ Use color = â€œnavyâ€ to change the color of the text to navy blue. , size = 3 Use size = 3 to change the size of the text. ) Closing parenthesis of the geom_text() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â theme_minimal() Add a minimalistic theme to the graph. There are many other themes that you can try out. Â Click to Show OutputÂ  Click to View Output. ## `geom_smooth()` using formula = 'y ~ x' Accessing Parts of the Regression Finally, note that the mylm object contains the names(mylm) of mylm$coefficients Contains two values. The first is the estimated \\(y\\)-intercept. The second is the estimated slope. ## (Intercept) speed ## -17.579095 3.932409 mylm$residuals Contains the residuals from the regression in the same order as the actual dataset. ## 1 2 3 4 5 6 7 8 9 ## 3.849460 11.849460 -5.947766 12.052234 2.119825 -7.812584 -3.744993 4.255007 12.255007 ## 10 11 12 13 14 15 16 17 18 ## -8.677401 2.322599 -15.609810 -9.609810 -5.609810 -1.609810 -7.542219 0.457781 0.457781 ## 19 20 21 22 23 24 25 26 27 ## 12.457781 -11.474628 -1.474628 22.525372 42.525372 -21.407036 -15.407036 12.592964 -13.339445 ## 28 29 30 31 32 33 34 35 36 ## -5.339445 -17.271854 -9.271854 0.728146 -11.204263 2.795737 22.795737 30.795737 -21.136672 ## 37 38 39 40 41 42 43 44 45 ## -11.136672 10.863328 -29.069080 -13.069080 -9.069080 -5.069080 2.930920 -2.933898 -18.866307 ## 46 47 48 49 50 ## -6.798715 15.201285 16.201285 43.201285 4.268876 mylm$fitted.values The values of \\(\\hat{Y}\\) in the same order as the original dataset. ## 1 2 3 4 5 6 7 8 9 10 ## -1.849460 -1.849460 9.947766 9.947766 13.880175 17.812584 21.744993 21.744993 21.744993 25.677401 ## 11 12 13 14 15 16 17 18 19 20 ## 25.677401 29.609810 29.609810 29.609810 29.609810 33.542219 33.542219 33.542219 33.542219 37.474628 ## 21 22 23 24 25 26 27 28 29 30 ## 37.474628 37.474628 37.474628 41.407036 41.407036 41.407036 45.339445 45.339445 49.271854 49.271854 ## 31 32 33 34 35 36 37 38 39 40 ## 49.271854 53.204263 53.204263 53.204263 53.204263 57.136672 57.136672 57.136672 61.069080 61.069080 ## 41 42 43 44 45 46 47 48 49 50 ## 61.069080 61.069080 61.069080 68.933898 72.866307 76.798715 76.798715 76.798715 76.798715 80.731124 mylm$â€¦ several other things that will not be explained here. Making Predictions predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ) Closing parenthesis for the data.frame(â€¦) function. ) Closing parenthesis for the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12)) 1 Â  29.60981 The value given is the â€œfitted-valueâ€ or â€œpredicted-valueâ€ for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(â€¦) function. Â interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. â€œpredictionâ€ This specifies that a prediction interval will be included with the predicted value. A prediction interval gives you a 95% confidence interval that captures 95% of the data, or \\(Y_i\\) values for the specific \\(X\\)-value specified in the prediction. ) Closing parenthesis of the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"prediction\") Â  fit The â€œfitâ€ is the predicted value. Â  lwr The â€œlwrâ€ is the lower bound. Â  upr The â€œuprâ€ is the upper bound. 1 29.60981 In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance canâ€™t go negative) feet to 60.96915 feet. -1.749529 This is the lower bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us the stopping distance could be as quick as -1.749529 feet (or 0 because distance canâ€™t go negative). 60.96915 This is the upper bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us that the actual stopping distance could be as high as 60.96915 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(â€¦) function. Â interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. â€œconfidenceâ€ This specifies that a confidence interval for the prediction should be provided. This is of use whenever your interest is in just estimating the average y-value, not the actual y-values. ) Closing parenthesis of the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"confidence\") Â  fit The â€œfitâ€ is the predicted value. Â  lwr The â€œlwrâ€ is the lower bound. Â  upr The â€œuprâ€ is the upper bound. 1 29.60981 In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. 24.39514 This is the lower bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is greater than this value. 34.82448 This is the upper bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is less than this value. Finding Confidence Intervals for Model Parameters confint( The R function confint(â€¦) allows you to use an lm(â€¦) object to compute confidence intervals for one or more parameters (like \\(\\beta_0\\) or \\(\\beta_1\\)) in your model. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â level = â€œlevel =â€ tells the confint(â€¦) function that you are going to declare at what level of confidence you want the interval. The default is â€œlevel = 0.95.â€ If you want to find 95% confidence intervals for your parameters, then just run confint(mylm). Â someConfidenceLevel someConfidenceLevel is simply a confidence level you choose when you want something other than a 95% confidence interval. Some examples of appropriate levels include 0.90 and 0.99. ) Closing parenthesis for confint(..) function. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.90) Â  Â  5 % The lower bound of a 90% confidence interval occurs at the 5th percentile. This is because at 90% confidence, 10% is left in the tails, with 5% on each end. The upper bound will thus end at the 95th percentile, hence the 5% and 95% as the column names. Â  95 % The upper bound of a 90% confidence interval ends at the 95th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -28.914514 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -6.243676 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.235501 and 4.629317. 3.235501 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. 4.629317 This is the upper bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.95) Â  Â  2.5 % The lower bound of a 95% confidence interval occurs at the 2.5th percentile. This is because at 95% confidence, 5% is left in the tails, with 2.5% on each end. The upper bound will thus end at the 97.5th percentile, hence the 2.5% and 97.5% as the column names for the lower and upper bounds, respectively. Â  97.5 % The upper bound of a 95% confidence interval ends at the 97.5th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -31.167850 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -3.990340 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.096964 and 4.767853. 3.096964 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.096964 and 4.767853 4.767853 This is the upper bound of the confidence interval. In this case, you can be 95% confident that the slope lies between 3.096964 and 4.767853 Explanation Linear regression has a rich mathematical theory behind it. This is because it uses a mathematical function and a random error term to describe the regression relation between a response variable \\(Y\\) and an explanatory variable called \\(X\\). Expand each element below to learn more. Regression Cheat Sheet (Expand) Term Pronunciation Meaning Math R Code \\(Y_i\\) $Y_i$ â€œwhy-eyeâ€ The data \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)\\) $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$ YourDataSet$YourYvariable \\(\\hat{Y}_i\\) $\\hat{Y}_i$ â€œwhy-hat-eyeâ€ The fitted line \\(\\hat{Y}_i = b_0 + b_1 X_i\\) $\\hat{Y}_i = b_0 + b_1 X_i$ lmObject$fitted.values \\(E\\{Y_i\\}\\) $E\\{Y_i\\}$ â€œexpected value of why-eyeâ€ True mean y-value \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) $E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i$ <none> \\(\\beta_0\\) $\\beta_0$ â€œbeta-zeroâ€ True y-intercept <none> <none> \\(\\beta_1\\) $\\beta_1$ â€œbeta-oneâ€ True slope <none> <none> \\(b_0\\) $b_0$ â€œb-zeroâ€ Estimated y-intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) $b_0 = \\bar{Y} - b_1\\bar{X} b_0 <- mean(Y) - b_1*mean(X)$ \\(b_1\\) $b_1$ â€œb-oneâ€ Estimated slope \\(b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\) $b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})} {\\sum(X_i - \\bar{X})^2}$ b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) \\(\\epsilon_i\\) $\\epsilon_i$ â€œepsilon-eyeâ€ Distance of dot to true line \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) $\\epsilon_i = Y_i - E\\{Y_i\\}$ <none> \\(r_i\\) $r_i$ â€œr-eyeâ€ or â€œresidual-eyeâ€ Distance of dot to estimated line \\(r_i = Y_i - \\hat{Y}_i\\) $r_i = Y_i - \\hat{Y}_i$ lmObject$residuals \\(\\sigma^2\\) $\\sigma^2$ â€œsigma-squaredâ€ Variance of the \\(\\epsilon_i\\) \\(Var\\{\\epsilon_i\\} = \\sigma^2\\)$Var\\{\\epsilon_i\\} = \\sigma^2$ <none> \\(MSE\\) $MSE$ â€œmean squared errorâ€ Estimate of \\(\\sigma^2\\) \\(MSE = \\frac{SSE}{n-p}\\)$MSE = \\frac{SSE}{n-p}$ sum( lmObject$res^2 ) / (n - p) \\(SSE\\) $SSE$ â€œsum of squared errorâ€ (residuals) Measure of dotâ€™s total deviation from the line \\(SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\)$SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$ sum( lmObject$res^2 ) \\(SSR\\) $SSR$ â€œsum of squared regression errorâ€ Measure of lineâ€™s deviation from y-bar \\(SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\)$SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2$ sum( (lmObject$fit - mean(YourData$Y))^2 ) \\(SSTO\\) $SSTO$ â€œtotal sum of squaresâ€ Measure of total variation in Y \\(SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\)$SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2$ sum( (YourData$Y - mean(YourData$Y))^2 ) \\(R^2\\) $R^2$ â€œR-squaredâ€ Proportion of variation in Y explained by the regression \\(R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\)$R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}$ SSR/SSTO \\(r\\) $r$ â€œrâ€ Correlation between X and Y. \\(r = \\sqrt{R^2}\\)$r = \\sqrt{R^2}$ sqrt(R^2) \\(\\hat{Y}_h\\) $\\hat{Y}_h$ â€œwhy-hat-aitchâ€ Estimated mean y-value for some x-value called \\(X_h\\) \\(\\hat{Y}_h = b_0 + b_1 X_h\\)$\\hat{Y}_h = b_0 + b_1 X_h$ predict(lmObject, data.frame(XvarName=#)) \\(X_h\\) $X_h$ â€œex-aitchâ€ Some x-value, not necessarily one of the \\(X_i\\) values used in the regression \\(X_h =\\) some number$X_h = $ Xh = # Confidence Interval â€œconfidence intervalâ€ Estimated bounds at a certain level of confidence for a parameter \\(b_0 \\pm t^* \\cdot s_{b_0}\\)b_0 \\pm t^* \\cdot s_{b_0} or \\(b_1 \\pm t^* \\cdot s_{b_1}\\)b_1 \\pm t^* \\cdot s_{b_1} confint(mylm, level = someConfidenceLevel) Parameter Estimate \\(\\beta_0\\) \\(b_0\\) \\(\\beta_1\\) \\(b_1\\) \\(\\epsilon_i\\) \\(r_i\\) \\(\\sigma^2\\) \\(MSE\\) \\(\\sigma\\) \\(\\sqrt{MSE}\\), the Residual standard error The Mathematical Model (Expand) \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)â€¦ There are three main elements to the mathematical model of regression. Each of these three elements is pictured below in the â€œRegression Relation Diagram.â€ Study both the three bullet points and their visual representations in the plot below for a clearer understanding. The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read moreâ€¦) The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as â€œnatural lawâ€ or â€œGodâ€™s lawâ€. It is some true line that is unknown to us. The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line. The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\). Note: \\(E\\{Y\\}\\) is pronounced â€œthe expected value of yâ€ because, wellâ€¦ the mean is the typical, average, or â€œexpectedâ€ value. The dots, i.e., the regression relation plus an error term: \\(Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}} + \\underbrace{\\epsilon_i}_\\text{error term} \\quad \\text{where} \\ \\epsilon_i\\sim N(0,\\sigma^2)\\) (Read moreâ€¦) This is shown by the dots in the graph below. This is the data. In regression, the assumption is that the y-value for individual \\(i\\), denoted by \\(Y_i\\), was â€œcreatedâ€ by adding an error term \\(\\epsilon_i\\) to each individualâ€™s â€œexpectedâ€ value \\(\\beta_0 + \\beta_1 X_i\\). Note the â€œorder of creationâ€ would require first knowing an indivualâ€™s x-value, \\(X_i\\), then their expected value from the regression relation \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) and then adding their \\(\\epsilon_i\\) value to the result. The \\(\\epsilon_i\\) allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance \\(\\epsilon_i\\) from the line. Note: \\(Y_i\\) is pronounced â€œwhy-eyeâ€ because it is the y-value for individual \\(i\\). Sometimes also called â€œwhy-sub-eyeâ€ because \\(i\\) is in the subscript of \\(Y\\). The estimated line, i.e., the line we get from a sample of data. \\(\\underbrace{\\hat{Y}_i}_{\\substack{\\text{estimated mean} \\\\ \\text{y-value}}} = \\underbrace{b_0 + b_1 X_i}_\\text{estimated regression equation}\\) (Read moreâ€¦) The estimated line is shown by the solid line in the graph below. \\(\\hat{Y}\\) is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation \\(E\\{Y\\}\\). So \\(\\hat{Y}\\) is interpreted as the estimated average (or mean) \\(Y\\)-value for any given \\(X\\)-value. Thus, \\(b_0\\) is the estimated y-intercept and \\(b_1\\) is the estimated slope. The bâ€™s are sample statistics, like \\(\\bar{x}\\) and the \\(\\beta\\)â€™s are population parameters like \\(\\mu\\). The \\(b\\)â€™s estimate the \\(\\beta\\)â€™s. Note: \\(\\hat{Y}_i\\) is pronounced â€œwhy-hat-eyeâ€ and is known as the â€œestimated y-valueâ€ or â€œfitted y-valueâ€ because it is the y-value you get from \\(b_0 + b_1 X_i\\). It is always different from \\(Y_i\\) because dots are rarely if ever exactly on the estimated regression line. This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line). Something to ponder: The true line, when coupled with the error terms, â€œcreatesâ€ the data. The estimated (or fitted) line uses the sampled data to try to â€œre-createâ€ the true line. We could loosely call this the â€œorder of creationâ€ as shown by the following diagram. par(mfrow=c(1,3), mai=c(.2,.2,.4,.1)) plot(y ~ x, col=\"white\", main=\"A Law is Given\", yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, main=\"Data is Created\", xaxt='n', yaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, xaxt='n', yaxt='n', main=\"The Law is Estimated\") curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) A Law is Given Data is Created The Law is Estimated \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) \\(Y_i = E\\{Y_i\\} + \\epsilon_i\\) \\(\\hat{Y}_i = b_0 + b_1 X_i\\) The true line is the â€œlawâ€. The \\(Y_i\\) are created by adding \\(\\epsilon_i\\) to \\(E\\{Y_i\\}\\) where \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\). The law is estimated with \\(\\hat{Y}_i\\) which is given with lm(...). Click open the â€œCodeâ€ buttom below to the right to find code that runs a simulation demonstrating this â€œorder of creationâ€. ## Simulating Data from a Regression Model ## This R-chunk is meant to be played in your R Console. ## It allows you to explore how the various elements ## of the regression model combine together to \"create\" ## data and then use the data to \"re-create\" the line. set.seed(101) #Allows us to always get the same \"random\" sample #Change to a new number to get a new sample n <- 3 #set the sample size X_i <- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45. beta0 <- 3 #Our choice for the y-intercept. beta1 <- .1 #Our choice for the slope. sigma <- 12.5 #Our choice for the std. deviation of the error terms. epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data View(fabData) #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it. fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData. summary(fab.lm) #Summarize your model. plot(y ~ x, data=fabData) #Plot the data. abline(fab.lm) #Add the estimated regression line to your plot. # Now for something you can't do in real life... but since we created the data... abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). legend(\"topleft\", legend=c(\"True Line\", \"Estimated Line\"), lty=c(2,1), bty=\"n\") #Add a legend to your plot specifying which line is which. Interpreting the Model Parameters (Expand) \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted asâ€¦ The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model. If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\). The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\). It is often misunderstood to be the â€œaverage change in yâ€ or just â€œthe change in yâ€ but it is more correctly referred to as the â€œchange in the average yâ€. To better see this, consider the three graphics shown below. par(mfrow=c(1,3)) hist(mtcars$mpg, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Number of Vehicles\", xlab=\"Gas Mileage (mpg)\", col=\"skyblue\") boxplot(mpg ~ cyl, data=mtcars, border=\"skyblue\", boxwex=0.5, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Number of Cylinders of Engine (cyl)\") plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=\"skyblue\", main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Quarter Mile Time (qsec)\") abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=\"darkgray\") mtext(side=3, text=\"Automatic Transmissions Only (am==0)\", cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=\"gray\") The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09. The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg. The scatterplot on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value. Residuals and Errors (Expand) \\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true errorâ€¦ Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\). The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\). We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summaryâ€¦ Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\). \\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms. Keep in mind the idea that the errors \\(\\epsilon_i\\) â€œcreatedâ€ the data and that the residuals \\(r_i\\) are computed after using the data to â€œre-createâ€ the line. Residuals have many uses in regression analysis. They allow us to diagnose the regression assumptions, See the â€œAssumptionsâ€ section below for more details. estimate the regression relation, See the â€œEstimating the Model Parametersâ€ section below for more details. estimate the variance of the error terms, See the â€œEstimating the Model Varianceâ€ section below for more details. and assess the fit of the regression relation. See the â€œAssessing the Fit of a Regressionâ€ section below for more details. Assessing the Fit of a Regression (Expand) \\(R^2\\), SSTO, SSR, and SSEâ€¦ Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line. par(mfrow=c(1,3), mai=c(.1,.1,.5,.1)) set.seed(2) x <- runif(30,0,20) y1 <- 2 + 3.5*x + rnorm(30,0,2) y2 <- 2 + 3.5*x + rnorm(30,0,8) y3 <- 2 + 3.5*x + rnorm(30,0,27) plot(y1 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Excellent Fit\") abline(lm(y1 ~ x), col=\"gray\") plot(y2 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Good Fit\") abline(lm(y2 ~ x), col=\"gray\") plot(y3 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Poor Fit\") abline(lm(y3 ~ x), col=\"gray\") A common way to measure the fit of a regression is with correlation. While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\). (If you are a Math 325 student, just stick with correlation for now and skip on to the next section of this Explanation tab. If you are a Math 425 student, it is critical that you come to understand \\(R^2\\) deeply, so read on.) Before you can understand \\(R^2\\), you must understand three important â€œsums of squaresâ€. (Read more about sumsâ€¦) A sum is just a fancy word for adding things together. \\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand. So we use the symbol \\(\\Sigma\\) to denote the word â€œsumâ€. Further, we use a subscript \\(\\Sigma_{i=1}\\) to state what value the sum is beginning with, and a superscript \\(\\Sigma_{i=1}^6\\) to state the value we are ending at. This gives \\[ \\sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Test your knowledge, do you see why the answer is 6 to the sum below? \\[ \\sum_{i=1}^3 i = 6 \\] Computing sums in R is fairly easy. Type the following codes in your R Console. sum(1:6) #gives the answer of 21 sum(1:3) #gives the answer of 6 However, sums really become useful when used with a data set. Each row of a data set represents an â€œindividualâ€™sâ€ data. We can reference each individual with a row number. In the data below, individual 3, denoted by \\(i=3\\), has a speed of 7 and a dist of 4. pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3) Individual speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 To compute the sum of the speed column, use sum(speed). If we divided this sum by 6, we would get the mean of speed mean(speed). In fact, the two most used statistics mean(...) and sd(...) both use sums. Take a moment to review the formulas for mean and standard deviation. It is strongly recommended that you study the Explanation tab for both as well. Weâ€™ll wait. See you back here shortly. â€¦ Welcome back. Suppose we let X = speed and Y = dist. Then \\(X_3 = 7\\) and \\(Y_3 = 4\\) because we are accessing row 3 of both the \\(X\\) (or speed) column and \\(Y\\) (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, sum(speed) would be written mathematically as \\(\\sum_{i=1}^6 X_i\\) and sum(dist) would be written as \\(\\sum_{i=1}^6 Y_i\\). Sum of Squared Errors Sum of Squares Regression Total Sum of Squares \\(\\text{SSE} = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\) \\(\\text{SSR} = \\sum_{i=1}^n \\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\) \\(\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\\) Measures how much the residuals deviate from the line. Measures how much the regression line deviates from the average y-value. Measures how much the y-values deviate from the average y-value. Equals SSTO - SSR Equals SSTO - SSE Equals SSE + SSR sum( (Y - mylm$fit)^2 ) sum( (mylm$fit - mean(Y))^2 ) sum( (Y - mean(Y))^2 ) It is important to remember that SSE and SSR split up SSTO, so that \\[ \\text{SSTO} = \\text{SSE} + \\text{SSR} \\] This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works. The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called \\(R^2\\) (â€œr-squaredâ€). R-Squared (\\(R^2\\)) \\[ \\underbrace{R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}}_\\text{Interpretation: Proportion of variation in Y explained by the regression.} \\] The smallest \\(R^2\\) can be is zero, and the largest it can be is 1. This is because \\(SSR\\) must be between 0 and SSTO, inclusive. Residual Plots & Regression Assumptions (Expand) Residuals vs.Â fitted-values, Q-Q Plot of the residuals, and residuals vs.Â order plotsâ€¦ There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate. Each assumption is labeled in the regression equation below. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions. (Read moreâ€¦) As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not. Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3 The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(\\hat{Y}_i\\). The residuals are the \\(r_i\\). This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable. | Show Examples | (Read moreâ€¦) The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption. The linear relation is assumed to be satisfied if there are no apparent trends in the plot. The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values. The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance. set.seed(2) X <- rnorm(30,15,3) notLin <- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8)) notLin.lm <- lm(Y~X, data=notLin) set.seed(15) Lin <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) Lin.lm <- lm(Y~X, data=Lin) par(mfrow=c(3,3), mai=c(.25,.25,.25,.25), mgp=c(1,.75,0)) plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Not Linear\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(notLin.lm$fitted.values,notLin.lm$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Good: Linear, Constant Variance\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) set.seed(6) notCon <- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5)) notCon.lm <- lm(Y~X, data=notCon) LinO <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) LinO[1] <- LinO[1]^2 LinO.lm <- lm(Y~X, data=LinO) plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Unconstant Variance\", cex.main=0.95, yaxt='n', xaxt='n', col=\"firebrick\") polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) # plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Outliers\", cex.main=0.95) # abline(h=0) tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(Girth ~ Volume, data=trees[-31,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(Height ~ Volume, data=trees) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) abline(h=0) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals, f=.4) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(weight ~ repwt, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(weight ~ repht, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) abline(h=0) Q-Q Plot of the Residuals: Checks Assumption #2 The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption. | Show Examples | (Read moreâ€¦) There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot. Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) set.seed(123) tmp <- rnorm(100) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"skyblue\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Normal\", col=\"skyblue\") tmp <- Davis$weight qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Right-skewed\", breaks=15, col=\"firebrick\") par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- rbeta(100, 5,1) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Left-skewed\", breaks=seq(min(tmp),max(tmp), length.out=13), col=\"firebrick\") tmp <- rbeta(100,2,2) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Heavy-tailed\", col=\"firebrick\") Residuals versus Order Plot: Checks Assumption #5 When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated. | Show Examples | (Read moreâ€¦) Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Good: No Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: General Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ qsec, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: Interesting Patterns\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),]) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Bad: Obvious Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") Problems from Failed Assumptions There are various problems that can arise when certain of the regression assupmtions are not satisfied. Lack of Linearity When the linearity assumption is violated, pretty much everything we obtain from the regression summary is no longer meaningful. The y-intercept estimate can be drastically off from its actual true value. Important model information is lost by trying to use a simple slope term \\(\\beta_1\\) to describe the model with respect to \\(X\\). The residual standard error will be much higher than it otherwise would be because of curvature patterns in the data that the line cannot capture. Thus, R-squared will be lower than it otherwise should be. P-values can become non-significant, when in fact there is a strong pattern in the data, but that pattern just cannot be captured by a simple line. *Normality of the errors is often put into question as well when a simplified line is used to try to capture a more complicated curved model. The plot below demonstrate these difficulties. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 7.5 #True slope beta_2 <- -0.25 #True bend X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors Y_i <- beta_0 + beta_1*X_i + beta_2*X_i^2 + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Non-Linear Relation\") abline(mylm, col=\"gray\") #Add fitted line to plot curve(beta_0 + beta_1*x + beta_2*x^2, col=\"gray\", lty=2, add=TRUE) #Add True line to plot #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (True value:\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (True value:\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (True value:\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Non-normal Error Terms When the normality assumption for the error terms is violated, not all is lost. In fact, the estimate of the slope and intercept are still often fairly meaningful. However, it is unwise to put too much trust in the residual standard error as an estimate of the standard deviation \\(\\sigma\\). This is because the standard deviation in skewed distributions does not carry the same meaning it has in normal distributions. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rchisq(n, 1)*3 - 1 #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Unconstant Variance When variance of the error term changes across the regression, the regression approximates the â€œaverage varianceâ€ of the errors because the regression is still assuming the variance is constant across the regression. The estimates of the slope and intercept are still typically quite good, and can be used for interpretation. The residual standard error however should not be considered to be meaningful as it will be too large on one end of the regression and too small on the other end. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma + X_i) #normally distributed errors #with increasing variance Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Variance Varies (Non-Constant)\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 13.82 Slope 3.5 3.768 Sigma 2.5 13.02 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", round(mean(sigma + X_i), 2), \", mean)\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Normality Violated As silly as it sounds, if the only problem with the regression is the lack of normality of the error terms, it isnâ€™t all that big of a problem. Depending on how non-normal the residuals appear, there could be some skewing to the residual standard error, but otherwise, the slope and intercept are still interpretable and meaningful. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- runif(n, -sqrt(12*sigma^2)/2, sqrt(12*sigma^2)/2) #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 14.7 Slope 3.5 3.449 Sigma 2.5 2.317 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Independence Assumption Violated While the slope and intercept are often still meaningful when the independence assumption is violated, the residual standard error is unnecessarily large in this case. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, 2.5) + (1:n -n/2)*.5 #normal, but correlated errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Independence Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 16.47 Slope 3.5 3.296 Sigma 2.5 4.819 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Outliers Present While outliers do not violate any of the regression assumptions, they do pose substantial difficulties for the least squares regression estimates of the slope and intercept. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors epsilon_i[3] <- ifelse(X_i[3] < 10, runif(1,25,35), -runif(1,25,35)) #create outlier Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"An Outlier Present\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 13.18 Slope 3.5 3.447 Sigma 2.5 6.389 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) The material below this section is meant for Math 425 students only. Estimating the Model Parameters (Expand) How to get \\(b_0\\) and \\(b_1\\): least squares & maximum likelihoodâ€¦ There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical. The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas. Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 <- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\). When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\). Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i. \\label{exp} \\end{equation}\\] Least Squares{#leastSquares} To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\). \\[ Q = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2 \\] Then we use the function Q as if it were a function of \\(\\beta_0\\) and \\(\\beta_1\\). Ironically, the values of \\(Y\\) and \\(X\\) are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we donâ€™t know are the values of \\(\\beta_0\\) and \\(\\beta_1\\). This least squares applet is a good way to explore how various choices of the slope and intercept yield different values of the â€œsum of squared residualsâ€. But it turns out that there is one â€œbestâ€ choice of the slope and intercept that yields a â€œsmallestâ€ value of the â€œsum of squared residuals.â€ This best choice can actually be found using calculus by taking the partial derivatives of \\(Q\\) with respect to both \\(\\beta_0\\) and \\(\\beta_1\\). \\[ \\frac{\\partial Q}{\\partial \\beta_0} = -2\\sum (Y_i - \\beta_0 - \\beta_1X_i) \\] \\[ \\frac{\\partial Q}{\\partial \\beta_1} = -2\\sum X_i(Y_i-\\beta_0-\\beta_1X_i) \\] Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize \\(Q\\) for a given set of data. After all the calculations are completed we find the values of the parameter estimators \\(b_0\\) and \\(b_1\\) (of \\(\\beta_0\\) and \\(\\beta_1\\), respectively) are as stated previously. Maximum Likelihood{#mle} The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of \\(\\beta_0\\) and \\(\\beta_1\\) which minime the least squares \\(Q\\) function, we choose the values of \\(\\beta_0\\) and \\(\\beta_1\\) which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the \\(Y_i\\) for all observations \\(i=1,\\ldots,n\\). We can do this rather simply by using the assumption that the errors, \\(\\epsilon_i\\) are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if \\(f(Y_i)\\) denotes the probability density function for \\(Y_i\\), then the joint probability density for all \\(Y_i\\), \\(f(Y_1,\\ldots,Y_n)\\) is given by \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) \\] Since each \\(Y_i\\) is assumed to be normally distributed with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\) (see model (\\(\\ref{model}\\))) we have that \\[ f(Y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{Y_i-\\beta_0-\\beta_1X_i}{\\sigma}\\right)^2\\right]} \\] which provides the joint probability as \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] The likelihood function \\(L\\) is then given by consider the \\(Y_i\\) and \\(X_i\\) fixed and the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) as the variables in the function. \\[ L(\\beta_0,\\beta_1,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] Instead of taking partial derivatives of \\(L\\) directly (with respect to all parameters) we take the partial derivatives of the \\(\\log\\) of \\(L\\), which is easier to work with. In a similar, but more difficult calculation, to that of minimizing \\(Q\\), we obtain the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) which maximize the log of \\(L\\), and which therefore maximize \\(L\\). (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate \\(\\hat{\\sigma}^2\\) of \\(\\sigma^2\\). \\[ \\hat{\\sigma}^2 = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n} \\] Estimating the Model Variance (Expand) Estimating \\(\\sigma^2\\) with MSEâ€¦ As shown previously in the â€œEstimating Model Parametersâ€ section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation. Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\). This estimate turns out to be a biased estimator. This means that it is consistently wrong in its estimates of \\(\\sigma^2\\). If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong. This is bad. Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\). Without going into all the details, to â€œfixâ€ the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently. To find the correct degrees of freedom, we have to notice that the \\(\\hat{Y}_i\\) in the numerator of \\(\\widehat{\\sigma}^2\\) is defined by \\[\\begin{equation} \\widehat{Y}_i = b_0 + b_1X_i \\label{hatY} \\end{equation}\\] From this equation, we notice that two means, \\(\\bar{X}\\) and \\(\\bar{Y}\\), were estimated from the data in order to obtain \\(\\hat{Y}_i\\). (See the formulas for \\(b_0\\) and \\(b_1\\) above, and note how they use both \\(\\bar{X}\\) and \\(\\bar{Y}\\) in their calculation.) Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for \\(\\hat{\\sigma}^2\\) should be \\(n-2\\) instead of \\(n\\). Some incredibly long calculations will show that the â€œfixedâ€ estimator \\[\\begin{equation} s^2 = MSE = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n-2} \\quad \\text{(Unbiased Estimator of $\\sigma^2$)} \\end{equation}\\] is an unbiased estimator of \\(\\sigma^2\\). Here \\(MSE\\) stands for mean squared error, which is the most obvious name for a formula that squares the errors \\(Y_i-\\hat{Y}_i\\) then adds them up and divides by their degrees of freedom. Similarly, we call the numerator \\(\\sum(Y_i-\\hat{Y}_i)^2\\) the sum of the squared errors, denoted by \\(SSE\\). It is also important to note that the errors are often denoted by \\(r_i = Y_i-\\hat{Y}_i\\), the residuals. Putting this all together we get the following equivalent statements for \\(MSE\\). \\[\\begin{equation} s^2 = MSE = \\frac{SSE}{n-2} = \\frac{\\sum(Y_i-\\widehat{Y}_i)^2}{n-2} = \\frac{\\sum r_i^2}{n-2} \\end{equation}\\] As a final note, even though the expected value \\(E\\{MSE\\} = \\sigma^2\\), which shows \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\), it unfortunately isnâ€™t true that \\(\\sqrt{MSE}\\) is an unbiased estimator of \\(\\sigma\\). This presents a few problems later on, but these are minimal enough that we can overlook the issue and move forward. Transformations (Expand) \\(Y'\\), \\(X'\\), and returning to the original spaceâ€¦ Y transformations are denoted by y-prime, written \\(Y'\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\). \\[ Y' = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y' = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y' = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y' = \\log(Y)\\) lm(log(Y) ~ X) 0.25 \\(Y' = \\sqrt(\\sqrt(Y))\\) lm(sqrt(sqrt(Y)) ~ X) 0.5 \\(Y' = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y' = Y\\) lm(Y ~ X) 2 \\(Y' = Y^2\\) lm(Y^2 ~ X) Using â€œmaximum-likelihoodâ€ estimation, the Box-Cox procedure can actually automatically detect the â€œoptimalâ€ value of \\(\\lambda\\) to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise. Scatterplot Recognition Box-Cox Suggestion An Example Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\). set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Ya <- 1/sqrt(Y) #1/Y^2 Lam = -2 Yb <- 1/Y #1/Y Lam = -1 Yc <- exp(.02*Y) #log(Y) Lam = 0 Yd <- Y^2 #sqrt(Y) Lam = 0.5 Ye <- Y #Y Lam = 1 Yf <- sqrt(Y) #Y^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Ya ~ X, main=expression(paste(\"Use \", lambda == -2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ya^-2 ~ X)) curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yb ~ X, main=expression(paste(\"Use \", lambda == -1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yb^-1 ~ X)) curve(1/(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yc ~ X, main=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(log(Yc) ~ X)) curve(exp(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yd ~ X, main=expression(paste(\"Use \", lambda == 0.5)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(sqrt(Yd) ~ X)) curve((b[1] + b[2]*x)^2, add=TRUE, col=\"green\", lwd=2) plot(Ye ~ X, main=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ye ~ X)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yf ~ X, main=expression(paste(\"Use \", lambda == 2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yf^2 ~ X)) curve(sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try. par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0)) boxCox(lm(Ya ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -2)), line=.5) boxCox(lm(Yb ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -1)), line=.5) boxCox(lm(Yc ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), line=.5) boxCox(lm(Yd ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0.5)), line=.5) boxCox(lm(Ye ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), line=.5) boxCox(lm(Yf ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 2)), line=.5) An Example Suppose we were running a simple linear regression on the cars dataset. This would be done with the code cars.lm <- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesnâ€™t quite fit the data as well as we would hope. Instead, the data looks a little curved. cars.lm <-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") abline(cars.lm, col=\"gray\") Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful. library(car) boxCox(cars.lm) The output from the boxCox(...) function looks as follows. This plot tells use to use the \\(\\lambda = 0.5\\) transformation, so that \\(Y' = Y^0.5 = \\sqrt{Y}\\). (To see this yourself, click on the â€œBox-Cox Suggestionâ€ tab above, as well as on the â€œScatterplot Recognitionâ€ tab.) Now, a transformation regression is performed using sqrt(Y) in place of Y as follows: cars.lm.t <- lm(sqrt(dist) ~ speed, data=cars) summary(cars.lm.t) Â  Estimate Std. Error t value Pr(> (Intercept) 1.277 0.4844 2.636 0.01126 speed 0.3224 0.02978 10.83 1.773e-14 Then, \\[ \\widehat{Y}_i' = 1.277 + 0.3224 X_i \\] And replacing \\(\\hat{Y}_i' = \\sqrt{\\hat{Y}_i}\\) we have \\[ \\sqrt{\\widehat{Y}_i} = 1.277 + 0.3224 X_i \\] Solving for \\(\\hat{Y}_i\\) gives \\[ \\widehat{Y}_i = (1.277 + 0.3224 X_i)^2 \\] Which, using curve((1.277 + 0.3224*x)^2, add=TRUE) (see code for details) looks like this: plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") curve( (1.277 + 0.3224*x)^2, add=TRUE, col=\"firebrick\") X-Transformations X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them. The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated. The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation. set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Xa <- 1/sqrt(X) #1/X^2 Lam = -2 Xb <- 1/X #1/X Lam = -1 Xc <- exp(.02*X) #log(X) Lam = 0 Xd <- X^2 #sqrt(X) Lam = 0.5 Xe <- X #X Lam = 1 Xf <- sqrt(X) #X^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Y ~ Xa, main=expression(paste(\"Use \", X*minute == X^-2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xa^-2))) curve(b[1] + b[2]*x^-2, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xb, main=expression(paste(\"Use \", X*minute == X^-1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xb^-1))) curve(b[1] + b[2]*x^-1, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xc, main=expression(paste(\"Use \", X*minute == log(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ log(Xc))) curve(b[1] + b[2]*log(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xd, main=expression(paste(\"Use \", X*minute == sqrt(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ sqrt(Xd))) curve(b[1] + b[2]*sqrt(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xe, main=expression(paste(\"Use \", X*minute == X, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ Xe)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xf, main=expression(paste(\"Use \", X*minute == X^2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xf^2))) curve(b[1] + b[2]*x^2, add=TRUE, col=\"green\", lwd=2) Inference for the Model Parameters (Expand) t test formulas, sampling distributions, confidence intervals, and F testsâ€¦ When fitting the regression model given by the equation \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] to a sample of data, we typically test hypotheses about the parameters \\(\\beta_0\\), \\(\\beta_1\\), or both. Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_0 - \\overbrace{0}^\\text{a number}}{s_{b_0}}\\] This is the formula for the test statistic. It measures how far the estimated y-intercept \\(b_0\\) is from the null hypothesis for \\(\\beta_0\\) in units of â€œstandard errors of \\(b_0\\)â€. Thus the division by \\(s_{b_0}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0, it could be any number. \\(H_0: \\beta_1 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_1\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_1 - \\overbrace{0}^\\text{a number}}{s_{b_1}}\\] This is the formula for the test statistic. It measures how far the estimated slope \\(b_1\\) is from the null hypothesis for \\(\\beta_1\\) in units of â€œstandard errors of \\(b_1\\)â€. Thus the division by \\(s_{b_1}\\). Though the hypothesized value of \\(\\beta_1\\) is typically 0, it could be any number. Left-tailed p-value = pt(-abs(tvalue), degrees of freedom). Double it to get the two-sided p-value. In R, these values correspond to the output summary of an lm as follows. (Show Example) Consider the cars data in R. Suppose we used the regression model given by \\[ \\underbrace{Y_i}_\\text{Feet to Stop} = \\beta_0 + \\beta_1 \\underbrace{X_{i}}_\\text{mph} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\quad \\sim N(0,\\sigma^2) \\] to model the feet a vehicle (from the 1920â€™s) takes to stop when traveling at a certain speed (in miles per hour, mph) prior to stopping. When the regression is performed and summarized in R, it is always testing the following two hypotheses: \\[ H_0: \\beta_0 = 0 \\quad\\quad H_0: \\beta_1 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\quad\\quad H_a: \\beta_1 \\neq 0 \\] To perform the test of these hypotheses for the regression stated above, we would run the following codes in R. cars.lm <- lm(dist ~ speed, data=cars) pander(summary(cars.lm)$coefficients) These would produce summary output like the following, but the following output has been labeled with the math notation corresponding to each value. Letâ€™s emphasize what is happening in this summary output table. First, here is how the â€œt valueâ€ is calculated for the â€œ(Intercept)â€ in the summary table above. \\[ t = \\frac{b_0-0}{s_{b_0}} = \\frac{-17.58 - 0}{6.758} = -2.601 \\] Second, here is a visual representation of how the P-value, the â€œPr(>|t|)â€ as it is called in the summary table above, is calculated for this test statistic. (Click the graph to view an interactive applet showing this calculation.) Notice both ends of the t-distribution are being shaded to compute the P-value because the alternative hypothesis was \\(H_a: \\beta_0 \\neq 0\\). To compute the P-value in R, we use the â€œpercentile function for the t-distributionâ€ called pt( ). This function requires two things, the t-value and the degrees of freedom, in our case pt(-2.601, 48). Note the degrees of freedom (df) are 48 because the sample size is \\(n=50\\) and there are two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) in our regression model. Running this code in R gives: pt(-2.601, 48) = 0.00616 However, note that this value is only half of the actual P-value of 0.0123. To get the â€œtwo-sidedâ€ P-value (note that our alternative hypothesis used a \\(\\neq\\) symbol) we need to double this left-tailed P-value. 2*pt(-2.601, 48)) = 0.0123 Finally, note that the same procedure can be used to test hypotheses that use a value other than 0 in the null and alternative. For example, to test the hypotheses: \\[ H_0: \\beta_1 = 3 \\\\ H_a: \\beta_1 \\neq 3 \\] Use the t-formula \\[ t = \\frac{b_1 - 3}{s_{b_1}} = \\frac{\\overbrace{3.932}^{b_1} - \\overbrace{3}^{H_0}}{\\underbrace{0.4155}_{s_{b_1}}} = 2.243 \\] then the P-value is calculated in R by 2*pt(-abs(2.243), 48) = 0.0295495 To obtain confidence intervals in R use confint(mylm). Confidence Interval Formula Standard Error \\(\\beta_0\\) \\(b_0 \\pm\\) \\(t^*\\) This is called the â€œcritical valueâ€ and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\). Use qt(0.975, df) to get \\(t*\\) in R. \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_0\\). \\(s_{b_0}\\) The standard error of \\(b_0\\), denoted by \\(s_{b_0}\\) is provided in the regression summary output under the column header called â€œStd. Errorâ€ for the â€œ(Intercept)â€ row of the output. It is calculated using the formula shown below. \\[s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\] This is called the â€œestimated variance of \\(b_0\\)â€. Taking the square root of this number gives the â€œstandard error of \\(b_0\\)â€. \\(\\beta_1\\) \\(b_1 \\pm\\) \\(t^*\\) This is called the â€œcritical valueâ€ and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\) (sample size - number of parameters in the regression model). Use qt(0.975, df) to get \\(t*\\) in R \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_1\\). \\(s_{b_1}\\) The standard error of \\(b_1\\), denoted by \\(s_{b_1}\\) is provided in the regression summary output under the column header called â€œStd. It is calculated using the formula shown below. \\[s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\] This is called the â€œestimated variance of \\(b_1\\)â€. Taking the square root of this number gives the â€œstandard error of \\(b_1\\)â€. To be more exact, the types of inference we are interested in are the following. Determine if there is evidence of a meaningful linear relationship in the data. If \\(\\beta_1 = 0\\), then there is no relation between \\(X\\) and \\(E\\{Y\\}\\). Hence we might be interested in testing the hypotheses \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) is some hypothesized number. To provide a confidence interval for the true value of \\(\\beta_1\\). Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the sampling distribution of the estimate \\(b_1\\) of the parameter \\(\\beta_1\\). And, while we are at it, we may as well come to understand the sampling distribution of the estimate \\(b_0\\) of the parameter \\(\\beta_0\\). Review sampling distributions from Math 221. Since \\(b_1\\) is an estimate, it will vary from sample to sample, even though the truth, \\(\\beta_1\\), remains fixed. (The same holds for \\(b_0\\) and \\(\\beta_0\\).) It turns out that the sampling distribution of \\(b_1\\) (where the \\(X\\) values remain fixed from study to study) is normal with mean and variance: \\[ \\mu_{b_1} = \\beta_1 \\] \\[ \\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} \\] ## Simulation to Show relationship between Standard Errors ##----------------------------------------------- ## Edit anything in this area... n <- 100 #sample size Xstart <- 30 #lower-bound for x-axis Xstop <- 100 #upper-bound for x-axis beta_0 <- 2 #choice of true y-intercept beta_1 <- 3.5 #choice of true slope sigma <- 13.8 #choice of st. deviation of error terms ## End of Editable area. ##----------------------------------------------- # Create X, which will be used in the next R-chunk. X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) ## After playing this chunk, play the next chunk as well. To see that this is true, consider the regression model with values specified for each parameter as follows. \\[ Y_i = \\overbrace{\\beta_0}^{2} + \\overbrace{\\beta_1}^{3.5} X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\overbrace{\\sigma^2}^{\\sigma=13.8}) \\] Using the equations above for \\(\\mu_{b_1}\\) and \\(\\sigma^2_{b_1}\\) we obtain that the mean of the sampling distribution of \\(b_1\\) will be \\(\\mu_{b_1} = \\beta_1 = 3.5\\) Further, we see that the variance of the sampling distribution of \\(b_1\\) will be \\(\\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} = \\frac{13.8^2}{4.25\\times 10^{4}}\\) Taking the square root of the variance, the standard deviation of the sampling distribution of \\(b_1\\) will be \\(\\sigma_{b_1} = 0.067\\). Thatâ€™s very nice. But to really believe it, letâ€™s run a simulation ourselves. The â€œCodeâ€ below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of \\(b_1\\) and \\(b_0\\) for that regression. After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of \\(b_0\\) is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of \\(b_1\\) is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of \\(\\beta_0\\) or \\(\\beta_1\\), respectively. Also, the â€œStd. Errorâ€ of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of \\(\\sigma_{b_0}\\) and \\(\\sigma_{b_1}\\), respectively. N <- 5000 #number of times to pull a random sample storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N) for (i in 1:N){ Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model mylm <- lm(Y ~ X) storage_b0[i] <- coef(mylm)[1] storage_b1[i] <- coef(mylm)[2] storage_rmse[i] <- summary(mylm)$sigma } layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3)) Ystart <- 0 #min(0,min(Y)) Ystop <- 500 #max(max(Y), 0) Yrange <- Ystop - Ystart plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), ylim=c(Ystart, Ystop), pch=16, col=\"gray\", main=\"Regression Lines from many Samples (gray lines) \\n Plus Residual Standard Deviation Lines (green lines)\") text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1) text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1) text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1) for (i in 1:N){ abline(storage_b0[i], storage_b1[i], col=\"darkgray\") } abline(beta_0, beta_1, col=\"green\", lwd=3) abline(beta_0+sigma, beta_1, col=\"green\", lwd=2) abline(beta_0-sigma, beta_1, col=\"green\", lwd=2) abline(beta_0+2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0-2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0+3*sigma, beta_1, col=\"green\", lwd=.5) abline(beta_0-3*sigma, beta_1, col=\"green\", lwd=.5) par(mai=c(1,.6,.5,.01)) addnorm <- function(m,s, col=\"firebrick\"){ curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2) lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col) lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col) lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col) lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col) lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col) lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col) lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col) legend(\"topleft\", legend=paste(\"Std. Error = \", round(s,3)), cex=0.7, bty=\"n\") } h0 <- hist(storage_b0, col=\"skyblue3\", main=\"Sampling Distribution\\n Y-intercept\", xlab=expression(paste(\"Estimates of \", beta[0], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m0 <- mean(storage_b0) s0 <- sd(storage_b0) addnorm(m0,s0, col=\"green\") h1 <- hist(storage_b1, col=\"skyblue3\", main=\"Sampling Distribution\\n Slope\", xlab=expression(paste(\"Estimates of \", beta[1], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m1 <- mean(storage_b1) s1 <- sd(storage_b1) addnorm(m1,s1, col=\"green\") t Tests{#tTests} Using the information above about the sampling distributions of \\(b_1\\) and \\(b_0\\), an immediate choice of statistical test to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) can be zero, or any other value, is a t test given by \\[ t = \\frac{b_1 - \\beta_{10}}{s_{b_1}} \\] where \\(s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\). (You may want to review the section â€œEstimating the Model Varianceâ€ of this file to know where MSE came from.) With quite a bit of work it has been shown that \\(t\\) is distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. The nearly identical test statistic for testing \\[ H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_0 \\neq \\beta_{00} \\] is given by \\[ t = \\frac{b_0 - \\beta_{00}}{s_{b_0}} \\] where \\(s^2_{b_0} = MSE\\left[\\frac{1}{n}+\\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\). This version of \\(t\\) has also been shown to be distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. Confidence Intervals Creating a confidence interval for either \\(\\beta_1\\) or \\(\\beta_0\\) follows immediately from these results using the formulas \\[ b_1 \\pm t^*_{n-2}\\cdot s_{b_1} \\] \\[ b_0 \\pm t^*_{n-2}\\cdot s_{b_0} \\] where \\(t^*_{n-2}\\) is the critical value from a t distribution with \\(n-2\\) degrees of freedom corresponding to the chosen confidence level. F tests{#Ftests} Another way to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\quad\\quad \\text{or} \\quad\\quad H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\quad\\quad \\ \\ \\quad \\quad H_a: \\beta_0 \\neq \\beta_{00} \\] is with an \\(F\\) Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an \\(F\\) test is very general and can be used in many places that a t test cannot. In its most general form, the \\(F\\) test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that \\(H_0:\\beta_1=0\\) against the alternative that \\(H_a: \\beta_1\\neq 0\\), we are essentially comparing two models against each other. If \\(\\beta_1=0\\), then the corresponding model would be \\(E\\{Y_i\\} = \\beta_0\\). If \\(\\beta_1\\neq0\\), then the model remains \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the \\(F\\) Test, that the null model (reduced model) have fewer â€œfreeâ€ parameters than the alternative model (full model). To demonstrate what we mean by â€œfreeâ€ parameters, consider the following example. Say we wanted to test the hypothesis that \\(H_0:\\beta_1 = 2.5\\) against the alternative that \\(\\beta_1\\neq2.5\\). Then the null, or reduced model, would be \\(E\\{Y_i\\}=\\beta_0+2.5X_i\\). The alternative, or full model, would be \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). Thus, the null (reduced) model contains only one â€œfreeâ€ parameter because \\(\\beta_1\\) has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two â€œfreeâ€ parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model. Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing \\(H_0: \\beta_1=0\\) against \\(H_a:\\beta_1\\neq0\\) we have the partition \\[ \\underbrace{Y_i-\\bar{Y}}_{Total} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{Regression} + \\underbrace{Y_i-\\hat{Y}_i}_{Error} \\] The reason we use \\(\\bar{Y}\\) for the null model is that \\(\\bar{Y}\\) is the unbiased estimator of \\(\\beta_0\\) for the null model, \\(E\\{Y_i\\} = \\beta_0\\). Thus we would compute the following sums of squares: \\[ SSTO = \\sum(Y_i-\\bar{Y})^2 \\] \\[ SSR = \\sum(\\hat{Y}_i-\\bar{Y})^2 \\] \\[ SSE = \\sum(Y_i-\\hat{Y}_i)^2 \\] and note that \\(SSTO = SSR + SSE\\). Important to note is that \\(SSTO\\) uses the difference between the observations \\(Y_i\\) and the null (reduced) model. The \\(SSR\\) uses the diffences between the alternative (full) and null (reduced) model. The \\(SSE\\) uses the differences between the observations \\(Y_i\\) and the alternative (full) model. From these we could set up a General \\(F\\) table of the form Â  Sum Sq Df Mean Sq F Value Model Error \\(SSR\\) \\(df_R-df_F\\) \\(\\frac{SSR}{df_R-df_F}\\) \\(\\frac{SSR}{df_R-df_F}\\cdot\\frac{df_F}{SSE}\\) Residual Error \\(SSE\\) \\(df_F\\) \\(\\frac{SSE}{df_F}\\) Total Error \\(SSTO\\) \\(df_R\\) Prediction and Confidence Intervals for \\(\\hat{Y}_h\\) (Expand) predict(â€¦, interval=â€œpredictionâ€)â€¦ It is a common mistake to assume that averages (means) describe individuals. They do not. So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line. Interval R Code Math Equation When to Use Prediction predict(..., interval=\"prediction\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\text{Pred}\\ Y}\\) Predict an individualâ€™s value. Confidence predict(..., interval=\"confidence\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\hat{Y}}\\) Estimate location of the mean y-value. predict(mylm, data.frame(XvarName = number), interval=...) For example, consider this graph. Then click here to read about the graph. Notice the three dots above 15 mph in the graph. Each of these dots show a car that was going 15 mph when it applied the brakes. However, stopping distances of the three individual cars differ with one at 20 feet, one at 26 feet and one at 54 feet. The regression line represents the average stopping distance of cars. In this case, cars going 15 mph are estimated to have an average stopping distance of about 40 feet, as shown by the line. But individual vehicles, all going the same speed of 15 mph, varied from stopping distances of 20 feet up to 54 feet! So, to predict that a car going 15 mph will take 41.4 feet to stop, doesnâ€™t tell the whole story. Far more revealing is the complete statement, â€œCars going 15 mph are predicted to take anywhere from 10.2 to 72.6 feet to stop, with an average stopping distance of 41.4 feet.â€ This is called the â€œprediction intervalâ€ and is shown in the graph in blue. It is obtained in R with the codes: cars.lm <- lm(dist ~ speed, data=cars) predict(cars.lm, data.frame(speed=15), interval=\"prediction\") cars.lm <- lm(dist ~ speed, data=cars) pander(predict(cars.lm, data.frame(speed=15), interval=\"prediction\")) fit lwr upr 41.41 10.17 72.64 plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=\"firebrick2\", cex=1.5) cars.lm <- lm(dist ~ speed, data=cars) abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3)) abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2)) abline(v=15, lty=2, col=\"firebrick\") preds <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12) lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8)) lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8)) Now, for the details behind prediction intervals and confidence intervals. Letâ€™s begin by recalling some details (from the section â€œInference for the Model Parametersâ€) about the standard error of the y-intercept, \\(b_0\\). Recall that the y-intercept is the average y-value for the given x-value of \\(x=0\\). Recall further that the formula for the standard error of \\(b_0\\) is given by the formula \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] If we wanted to be more exact with this formula, we would write it as \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{(0-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice the addition of \\((0 - \\bar{X})^2\\) instead of just \\(\\bar{X}^2\\) in the numerator of the right-most part of the equation? This more complete statement obviously would reduce to just \\(\\bar{X}^2\\), but that is only because \\(X=0\\) when we are working with the y-intercept, \\(b_0\\). We could be working with other values of \\(X\\) than just zero. Letâ€™s take a quick detour and talk notation for a second. Typically, \\(X_i\\) and \\(Y_i\\) are used to denote the x-value and y-value of points that are contained in our data set. When we want to reference a point that wasnâ€™t within our original data set, we use the notation \\(X_h\\) and \\(Y_h\\). (The letter h is close to i, but different from i, so why not. There is really no other reason to use h.) Thus, \\(Y_h\\) is the y-value for the \\(X_h\\) x-value, neither of which were included in our original regression of \\(X_i\\)â€™s and \\(Y_i\\)â€™s. Now, back to the previous discussion. If \\(X_h = 0\\), then \\(\\hat{Y}_h\\) is the y-intercept, so \\(\\hat{Y}_h = b_0\\) when \\(X_h=0\\). So, we could write, \\[ s^2_{\\hat{Y}_h} = MSE\\left[\\frac{1}{n} + \\frac{(X_h-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice how the \\(b_0\\) in \\(s_{b_0}\\) was replaced with \\(\\hat{Y}_h\\) to get \\(s_{\\hat{Y}_h}\\) and the 0 in \\((0 - \\bar{X})^2\\) was replaced with \\(X_h\\) to get \\((X_h - \\bar{X})^2\\)? Interesting. We now have a formula that would give us the standard error of \\(\\hat{Y}_h\\) for any \\(X_h\\) value, not just \\(X_h = 0\\), or the y-intercept, \\(b_0\\). That is fantastic. It would look like this if plotted. Notice how the gray region is showing the standard error for each \\(\\hat{Y}_h\\) value? (It is technically showing the confidence interval for \\(E\\{Y_h\\}\\) at every possible \\(X_h\\) value, but that is just \\(\\hat{Y}_h \\pm t^* \\cdot s_{\\hat{Y}_h}\\).) ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + theme_bw() ## `geom_smooth()` using formula = 'y ~ x' Confidence Interval for \\(\\hat{Y}_h\\) \\[ \\hat{Y}_h \\pm t^* s_{\\hat{Y}_h} \\quad \\text{where} \\ s_{\\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] The confidence interval is a wonderful tool for estimating \\(E\\{Y_h\\}\\), the â€œtrueâ€ average y-value for a given x-value of \\(X_h\\). However, it is not valuable for predicting an individual dot, or \\(Y_h\\) value. Notice how few of the dots of the regression are actually contained within the confidence interval band in the plot? The confidence interval does not really predict where the dots will land, just where the average y-value is located for each x-value. Remember the 68-95-99.7 Rule of the normal distribution? If not, here is a link back to that concept in the Math 221 textbook. This rule states that roughly 95% of data, when normally distributed, will be between \\(z=-2\\) and \\(z=2\\) standard deviations from the mean. So, is going two â€œresidual standard errorsâ€ to both sides of the regression line enough to capture 95% of the data? The answer is, not quite. The reason for this is because our knowledge of where the true mean lies is uncertain. (Notice the confidence interval band shown in the plot.) However, adding two standard errors to the edges of the confidence band would get us in the right place. In other words, there are two sources of variability at play here, (1) our uncertaintity in where the regression line is sitting, and (2) the natural variability of the data points around the line. Thus, the â€œprediction intervalâ€ requires accounting for both of these sources of variability to produce the following equation. Prediction Interval for \\(Y_h\\) \\[ \\hat{Y}_h \\pm t^* s_{Pred \\hat{Y}_h} \\quad \\text{where} \\ s_{Pred \\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + 1 + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] This formula provides a useful band for identifying a region where we are 95% confident that a new observation for \\(Y_h\\) will land, given the value of \\(X_h\\). It looks as follows. Notice the prediction interval is much wider than the confidence interval. This is because data varies far more than do means. Prediction is for where the individual data points will land, confidence is for where the mean will land. cars.lm <- lm(dist ~ speed, data=cars) predy <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + geom_segment(aes(x=15, xend=15, y=predy[2], yend=predy[3]), lwd=4, color=rgb(.5,.7,.5,.01)) + geom_point(aes(x=15, y=predy[1]), cex=2, color=\"skyblue\", pch=15) + theme_bw() Lowess (and Loess) Curves (Expand) A non-parametric approach to estimating \\(E\\{Y_i\\}\\)â€¦ Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value. Using Base R # Select the relevant columns and remove NA values air2 <- airquality %>% dplyr::select(Temp, Ozone) %>% # Use dplyr's select function explicitly drop_na() # Drop rows with NA values in Temp or Ozone plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") lines(lowess(air2$Ozone, air2$Temp), col=\"firebrick\") Using ggplot2 # Your dataset (air2) should have been created earlier, ensuring no NAs air2 <- na.omit(airquality[c(\"Temp\", \"Ozone\")]) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=\"darkgray\") + # Add points with dark gray color geom_smooth(se=FALSE, method=\"loess\", method.args = list(degree=1)) + # Lowess curve, no standard error shading theme_bw() # Black and white theme ## `geom_smooth()` using formula = 'y ~ x' # Optionally, allow for predictions as well as the graph: # Uncomment the following lines if you want predictions as well: # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fitted, x=Ozone)) # Use fitted values from loess Advantages Disadvantages Quick. Good at ignoring outliers. Good at capturing the general pattern in the data. Good for making predictions within the scope of the data. No mathematical model. Not interpretable. No p-values. No adjusted R-squared. How it Works The Lowess curve localizes the regression model to a â€œneighborhoodâ€ of points, and then joins these localized regressions together into a smooth line. It minimizes the effect of outliers, and letâ€™s the data â€œspeak for itselfâ€. As a downside, it is not interpretable, and has no final way to write the model mathematically. All the same, it is a very powerful tool for identifying an appropriate model, or verifying the fit of a model, or making predictions when no reasonable model does an adequate job. Study this graphic and the explanations below to learn how it works. Recommendation: run the code in this â€œCodeâ€ chunk to the right in your Console, and flip through the resulting graphics. X <- cars$speed Y <- cars$dist X <- X[!is.na(X) & !is.na(Y)] Y <- Y[!is.na(X) & !is.na(Y)] f <- 1/2 n <- length(X) lfit <- rep(NA,n) for (xh in 1:n){ xdists <- X - X[xh] nn <- floor(n*f) r <- sort(abs(xdists))[nn] xdists.nbrhd <- which(abs(xdists) < r) w <- rep(0, length(xdists)) w[xdists.nbrhd] <- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w), col=rgb(.2,.2,.2,.3), cex=1.5, yaxt='n', xaxt='n', xlab=\"\", ylab=\"\") points(Y[xh] ~ X[xh], pch=16, col=\"orange\") lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2)) cat(\"\\n\\n\") readline(prompt=paste0(\"Center point is point #\", xh, \"... Press [enter] to continue...\")) MADnotThereYet <- TRUE count <- 0 while(MADnotThereYet){ readline(prompt=paste0(\"\\n Adjusting line to account for outliers in the y-direction... Press [enter] to continue...\")) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"wheat\", add=TRUE) MAD <- median(abs(lmc$res)) resm <- lmc$res/(6*MAD) resm[resm>1] <- 1 bisq <- (1-resm^2)^2 w <- w*bisq obs <- coef(lmc) lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) count <- count + 1 if ( (sum(abs(obs-lmc$coef))<.1) | (count > 3)) MADnotThereYet <- FALSE } curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"green\", add=TRUE) points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=\"green\") readline(prompt=paste0(\"\\n Use final line to get fitted value for this point... Press [enter] to continue to next point...\")) lfit[xh] <- predict(lmc, data.frame(X=X[xh])) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") if (xh == n){ readline(prompt=paste0(\"\\n Press [enter] to see actual Lowess curve...\")) lines(lowess(X,Y, f=f), col=\"firebrick\") legend(\"topleft\", bty=\"n\", legend=\"Actual lowess Curve using lowess(...)\", col=\"firebrick\", lty=1) } } Select a fraction of the data to use for the â€œneighborhoodâ€ of points (shown in blue in the graph above). The lowess function in R uses â€œf=2/3â€ and the loess function uses â€œspan=0.75â€ for this value, which selects the nearest two-thirds or 75% of the data, respectively, depending on which function you use. For this example, we set the fraction of points at 50%. Both functions can be set to whatever you want. Pick any point in the regression, eventually selecting all points one at a time. The selected point becomes the â€œcenterâ€ of a â€œneighborhoodâ€ of points surrounding it. In this example, the center point is in orange, and the neighboring points are in blue. Use the points within the neighborhood to fit a regression line. However, make the regression depend most on points closest to â€œcenterâ€ and least on points furthest from â€œcenter.â€ This is called a weighted regression. Weights are decided according to what is called the tricubic weight function, so that the weight \\(w\\) given to point \\(j\\) of the neighborhood of points is defined by \\[ w_j = \\left(1- \\left( \\frac{|X_c - X_j|}{\\max_k |X_c - X_k|}\\right)^3\\right)^3 \\] where \\(X_c\\) is the x-value of the â€œcenterâ€ dot and \\(X_j\\) is the x-value of any other dot in the neighborhood. The fitted-value of \\(\\hat{Y}_c\\) is obtained for the center point \\(X_c\\) of the current regression. This point is used as the Lowess (or Loess) curveâ€™s value at that particular x-value. Well, almost. Itâ€™s a first guess at where this value will end up, but thereâ€™s a little more to the algorithm before we are done. Initial guesses for each of these fitted values are obtained for each point in the regression. Now each local regression for each neighborhood is re-run a few times in such a way the the effect of outliers is minimized. The final line for each neighborhood is obtained by the following steps. Compute all residuals for points in the neighborhood of the current regression, denoted by \\(r_i\\). Then compute the MAD, median absolute deviation, of the residuals \\(MAD = \\text{median} (|r_1|, |r_2|, \\ldots)\\). Divide all residuals by 6 times the MAD: \\(u_i = r_i/(6\\cdot MAD)\\) (If \\(r_i > 6\\cdot MAD\\) then set \\(u_i = 0\\).) Compute what are called bisquare weights using the formula: \\(b_i = (1 - u_i^2)^2\\) Perform a regression using the weights \\(w_i = w_i b_i\\) Repeat the above process with the new weights \\(w_i\\) until the weights stop changing very much. The final fitted values for each \\(X\\)-value in the regression are obtained from the final regression line for each neighborhood. These fitted values make up the Lowess (or loess) curve. Note that the default of the loess function in R is to use quadratic regressions in each neighborhood instead of linear regressions. This can be controlled with the loess option of â€œdegree=2â€ (quadratic fits) or â€œdegree = 1â€. In the lowess function only a linear regression in each neighborhood is allowed. Examples: bodyweight, cars Quantitative Y | Multiple X Graphics Y is a single quantitative variable of interest, like height. While we could use an X1 of â€œshoe-sizeâ€ to explain height, we might also want to use a second x-variable, X2, like â€œgenderâ€ to help explain height. Further x-variables could also be used. Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a studentâ€™s final exam grade? How do factors like age, weight, exercise level, and diet impact a personâ€™s blood pressure? Examples: Is one Navy base more prone to heatwaves than the other? The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH) Tests Multiple Linear Regression Multiple regression is a way to understand how different factors affect something weâ€™re interested in. Itâ€™s like having more than one clue to solve a mystery. Hereâ€™s what you need to know: We can use several different pieces of information (called variables) to predict an outcome. There are many ways to combine these pieces of information. We start with simple combinations and build up to more complex ones, kind of like building with Lego blocks. This helps us make better predictions and understand complex situations more clearly. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model is a bit like using the same ingredient twice in a recipe, but in different ways. It uses the same factor (\\(X_i^2\\)) in two different forms: Once in its regular form (\\(\\beta_1 X_i\\)) Once squared (\\(\\beta_2 X_i^2\\)) This squared part is what we call the â€œquadraticâ€ term. It helps us describe relationships that arenâ€™t just straight lines, but have curves to them. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and â€œsteepnessâ€ of the Model: negative values face down, positive values face up; large values imply â€œsteeperâ€ parabolas and low values imply â€œflatterâ€ parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)â€™s explanation. An Example Using the airquality data set, we run the following â€œquadraticâ€ regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our â€œquadraticâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. Temp Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These â€œEstimatesâ€ can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(Month^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of Month from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, ) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will â€œopen downâ€ (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be â€œmonth zero.â€ Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the â€œcubicâ€ term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the â€œbase slope coefficientâ€ and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following â€œcubicâ€ regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our â€œcubicâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. uptake Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^3) \\(X_{i}^3\\), where the function I(â€¦) protects the cubing of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will â€œopen upâ€. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The â€œtwo-linesâ€ model is a way to compare two different groups in statistics using numbers. Hereâ€™s how it works: It uses two types of numbers: A regular number that can be any value (\\(X_{1i}\\)) A special number thatâ€™s either 0 or 1 (called an â€œindicator variableâ€, \\(X_{2i}\\)) The special number (\\(X_{2i}\\)) helps us turn group information (like â€œGroup Aâ€ or â€œGroup Bâ€) into numbers we can use in calculations. This model creates two separate lines on a graph: One line for Group A One line for Group B By using this method, we can easily see and compare how the two groups are different from each other. Think of it like having two recipes for the same dish, but with slightly different ingredients. This model helps us see how those small changes affect the final result. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the â€œbase-lineâ€ of the model, the â€œGroup 0â€ line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the â€œbase-lineâ€ line. \\(\\beta_3\\) Called the â€œinteractionâ€ term. Controls the change in the slope for the second line in the model as compared to the slope of the â€œbase-lineâ€ line. An Example Using the mtcars data set, we run the following â€œtwo-linesâ€ regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our â€œtwo-linesâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. mpg Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) (beta zero): This is like the starting point. Itâ€™s estimated by something called the â€œInterceptâ€. \\(\\beta_1\\) (beta one): This shows how one thing changes when another thing (called â€œqsecâ€) changes. Itâ€™s estimated to be 1.439. \\(\\beta_2\\) (beta two): This shows how something else (called â€œamâ€) affects the overall result. Itâ€™s estimated to be -14.51. \\(\\beta_3\\) (beta three): This shows how â€œqsecâ€ and â€œamâ€ work together to affect the result. Itâ€™s estimated to be 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called â€œ3Dâ€ regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to â€œbendâ€. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the â€œTempâ€ direction is estimated to be 2.659 and the slope in the â€œMonthâ€ direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction termâ€™s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the â€œslopesâ€ in each direction to change, creating a â€œcurvedâ€ surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called â€œHDâ€, or â€œHigh Dimensionalâ€, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isnâ€™t really even possible to â€œmentally connectâ€ with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)â€™s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the â€œWindâ€ direction is estimated to be -3.334. The slope in the â€œTempâ€ direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the â€œSolar.Râ€ direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here. R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the â€œOverviewâ€ section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the â€œcolumn (c) bindâ€ function and it joins together things as columns. Res =Â  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,Â  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals. Â panel=panel.smooth,Â  This places a lowess smoothing line on each scatterplot. col =Â  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(â€œcolor1â€,â€œcolor2â€, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for â€œlinear model.â€ Y Y must be a â€œnumericâ€ vector of the quantitative response variable. Â ~Â  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats â€œnumericâ€ variables as quantitative and â€œcharacterâ€ or â€œfactorâ€ variables as qualitative. R will automatcially recode qualitative variables to become â€œnumericâ€ variables using a 0,1 encoding. See the Explanation tab for details. Â + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details. Â + â€¦, * ... emphasizes that as many explanatory variables as are desired can be included in the model. Â data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(â€¦) function displays the results of an lm(â€¦) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(â€¦) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(â€¦). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min Â  -4.3818 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -2.2696 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.1344 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  1.7058 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  5.8752 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  t value To learn more about the â€œt valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  Pr(>|t|) The â€œPrâ€ stands for â€œProbabilityâ€ and the â€œ(> |t|)â€ stands for â€œmore extreme than the observed t-valueâ€. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the â€œp-valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. (Intercept) This always says â€œInterceptâ€ for any lm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero. Â  2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the intercept (26.6248479) by its standard error (2.1829432). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a â€œstarâ€. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + â€¦). Â  -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model. Â  0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the â€œEstimateâ€ by its standard error. It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + â€¦). Â  5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\). Â  2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot â€œ.â€ implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like. Â  0.0004029 This is the estimate of the coefficient of the interaction term. Â  0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE. Â 2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\). Â on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO. Â 0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car. Â Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data. Â 0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that â€œat least one is not.â€ Â 34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom. Â on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero. Â p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that â€œat least oneâ€ of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the â€œOverviewâ€ sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the â€œfitted-valueâ€ or â€œpredicted-valueâ€ for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œpredictionâ€) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance canâ€™t go negative) feet to 60.96915 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œconfidenceâ€) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BICâ€¦ There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (â€œR-squaredâ€). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The â€œsimplestâ€ but â€œbestâ€ model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it â€œan information criterion (AIC)â€ when he published the method and other people later began calling it the â€œAkaike Information Criterion.â€) Model Selection (Expand) pairs plots, added variable plots, and pattern recognitionâ€¦ Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots {.tabset .tabset-pills} A useful visualization tool for model selection is the â€œpairs plot.â€ This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice thatâ€¦ the y-axis of each plot is found by locating the variable name (like â€œmpgâ€) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like â€œdispâ€) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model {.tabset .tabset-pills} Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldnâ€™t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Letâ€™s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander() Â  Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a modelâ€™s ability to generalize to new dataâ€¦ The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesnâ€™t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, letâ€™s remind ourselves why we use regression models in the first place. The main goal is to capture the â€œessenceâ€ of the data. In other words, the general pattern is what we are after. We want a model that tells us how â€œall suchâ€ data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the â€œcomplicated modelâ€ is overfitting the original data. It does not capture the â€œessenceâ€ of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-valueâ€¦ The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the modelâ€¦ The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions{#check} The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read moreâ€¦) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) Outlier Analysis (Expand) Cookâ€™s Distances and Leverage Valuesâ€¦ The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs.Â fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cookâ€™s Distances The idea behind Cookâ€™s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article â€œDetection of Influential Observation in Linear Regressionâ€ (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, letâ€™s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2)) Â  1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the â€œdifferencesâ€ in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cookâ€™s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cookâ€™s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cookâ€™s Distances using the code cooks.distance(lmObject). Also, a graph of Cookâ€™s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of â€œleverageâ€ and is â€œpullingâ€ the regression toward itself. A value near 0 implies the point is just â€œone of manyâ€ and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that â€œminimizeâ€ the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)â€™s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the â€œhat matrixâ€ \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the â€œleverage valuesâ€ and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a â€œlot of pull,â€ and values close to 0 represent â€œlittle pull.â€ In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with â€œlots of leverageâ€ and a large â€œCookâ€™s Distanceâ€ are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regressionâ€¦ Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class. Examples: Civic Vs Corolla cadillacs Binomial Y | Quantitative X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t. We might use an explanatory variable X of â€œheightâ€ to see if taller students are more likely to get an A in Math 325 than shorter students. (They arenâ€™t, if you were wondering.) Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1â€™s and 0â€™s (this does happen or this doesnâ€™t happen): Does the width of a childâ€™s foot predict whether or not they are right-handed? What is the probability the cat adopted is a male based on how many days it took them to get adopted? Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted? (Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\") Tests Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The â€œPâ€ stands for â€œProbability thatâ€¦â€ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1â€¦ This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)â€¦ in other words, the â€œ|â€ says â€œgivenâ€ and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the â€œnatural constantâ€ number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14â€¦) Part What it means \\(P(Y_i = 1| x_i)\\) - The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s) - The symbol used to represent the notation in logisitc regression is \\(\\pi_i\\) \\(Y_i\\) (Responsive Variable) Describes the probability of $Y_i = $ 1 or 0 for a given scenario. \\(x_i\\) (Explanatory variable) Describes the observed value and how it effects the \\(Y_i\\) \\(\\frac{e^{\\text{stuff}}}{1+e^{\\text{stuff}}}\\) The content for â€œstuffâ€ being simple linear regression of the form \\(\\beta_0 + \\beta_{1x_i}\\) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -1.5651 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.6648 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -0.2460 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  0.7276 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  2.2691 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The test statistic is a regular old z-score. It is most reliable when the sample size is â€œlarge.â€ It is a measurement of the number of standard errors the estimate is from 0. Â  Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds. Â  1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds. Â  0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a â€œstarâ€. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about. Â  Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\). Â 43.230 Â on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates. Â 29.732 This can be calculated by sum(log(myglm$res^2)). Â on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, â€œA version of Akaikeâ€™s An Information Criterionâ€¦â€ The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model. Â 33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 * ## disp -0.014604 0.005168 -2.826 0.00471 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0â€™s or 1â€™s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == â€œBâ€ or height < 68. In other words, Y needs to be a collection of 0â€™s and 1â€™s. Â ~Â  The tilde is read â€œY on X.â€ X, X is some quantitative variable. Â data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(â€¦) function. curve( A function in R that draws a â€œcurveâ€ on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the â€œInterceptâ€ estimate from your logistic regression summary output. Â \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case â€œxâ€ in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula. Â exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator. Â add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(â€¦) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set. Â aes( The aes(â€¦) function stands for â€œaestheticsâ€ and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The plus sign adds a new layer to the ggplot. Â  geom_point( Tells the plot to add the physical geometry of â€œpointsâ€ to the plot. ) Closing parenthesis for the geom_point() function. Â + Add another layer to the plot. Â  geom_smooth( Add a smoothing line to the graph. method=â€œglmâ€, This adds a general linear model to the graph. method.args = list(family=â€œbinomialâ€), This tells the method=â€œglmâ€ to choose specifically the â€œbinomialâ€ model, otherwise known as the â€œlogistic regressionâ€ model. Â se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function. Â + Add another layer to the ggplot. Â  theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),Â  The xVariableName is the same one you used in your glm(y ~ x, â€¦) statement for â€œxâ€. Input any desired value for the â€œsomeNumericValueâ€ spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016 Explanation Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal). The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\). Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page. Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful. Checking Model Assumptions {#diagnostics} The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit. Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease. Binomial Y | Multiple X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t. We might use an explanatory variable X1 of â€œheightâ€ and a second explanatory variable X2 of â€œgenderâ€ to try to predict whether or not a student will get an A in Math 325. Test(s) used for this category: Multiple Logistic Regression Model Tests Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two. Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2Â  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. â€¦, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -2.9446 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.2364 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.0000 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q Â  0.2776 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  1.9968 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The Z-score testing the hypothesis that \\(\\beta_j=0\\) Â  Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\). Â  1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\). Â  4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\). Â  3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\). Â  0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\). Â  0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a â€œstarâ€. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\). Â  471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option. Â  Null Deviance: The deviance of the null model. Â 800.44 In this case, the null deviance is 800.44 Â on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression. Â 256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit. Â on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model. Â 272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -4.97603 0.65316 -7.618 2.57e-14 *** ## Time 0.39111 0.04979 7.854 4.02e-15 *** ## Diet2 0.58283 1.04061 0.560 0.5754 ## Diet3 -8.44476 4.32324 -1.953 0.0508 . ## Diet4 -67.41995 3768.10023 -0.018 0.9857 ## Time:Diet2 0.02391 0.08679 0.275 0.7830 ## Time:Diet3 1.20955 0.51249 2.360 0.0183 * ## Time:Diet4 8.83168 471.01259 0.019 0.9850 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,Â  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, â€¦), X2 = c(Value 1, Value 2, â€¦), â€¦) command. You should see the GSS example file for an example of how to use this function. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the â€œbâ€ vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(â€œSomeColorâ€,â€œDifferentColorâ€,â€¦) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(â€¦) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==â€œBâ€. Â ~Â  The formula operator in R. X1, The first x-variable in your glm code. Â data = YourDataSet, Specify the name of your data set. Â pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(â€¦) function computes e^(stuff) in R and stands for the â€œexponential function.â€ (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(â€¦) function requires that you call the x-variable â€œxâ€ although you can change this behavior using xname=â€œSomeOtherNameâ€ if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)). Â col = palette()[1], Pull the first color from the color palette for the first curve. Â add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters. Â col = palette()[2], Set the color of this second curve to the second color in the palette. Â add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. â€œtoprightâ€, Typically the legend is placed in the top-right corner of the graph. But it could be placed â€œtopâ€, â€œtopleftâ€, â€œleftâ€, â€œbottomleftâ€, â€œbottomâ€, â€œcenterâ€, â€œrightâ€, or â€œbottomrightâ€. Â legend = Why you have to write legend again, no one knows, but you do. c(â€œLable 1â€, â€œLabel 2â€), These are the values that will appear in the legend, one entry on each line of the legend. Â col = palette(), This specifies the colors of the symbols in the legend. First color goes with â€œLabel 1â€, and second goes with â€œLabel 2â€ and so on. Â lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist. Â bty = Specifies the â€œbox typeâ€ (bty) that should be drawn around the legend. â€˜nâ€™) By placing a â€œnoâ€ option (â€˜nâ€™) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot. Â aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + Add a layer to the ggplot. Â  geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function. Â + Add a layer to the ggplot. Â  geom_smooth(method=â€œglmâ€, method.args=list(family=â€œbinomialâ€), se=FALSE, Add the logistic regression curve to the plot. Â  aes(color=â€œX2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function. Â + Add a layer to the plot. Â  theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x' Explanation Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative. The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations. Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant. Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression. Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\). Categorical Y | Categorical X Graphics Y is a single categorical variable of interest, like gender. X is another categorical variable of interest, like â€œhair colorâ€. This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender. Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year? Whats the probability that you have brown hair and blue eyes? Tests Add your own notes about appropriate inferential procedures for this type of data here.",
    "sentences": ["One-way ANOVA Each experimental unit is assigned to exactly one factor-level combination.", "Another way to say this is â€œone measurement per individualâ€ (no repeated measures) and â€œequal numbers of individuals per groupâ€.", "Overview An ANOVA is only appropriate when all of the following are satisfied.", "The sample(s) of data can be considered to be representative of their population(s).", "The data is normally distributed in each group.", "(This can safely be assumed to be satisfied when the residuals from the ANOVA can be assumed to be normally distributed when seen in a Q-Q Plot.) The population variance of each group can be assumed to be the same.", "(This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out in a Residuals versus fitted-values plot.) comparing means Hypotheses For a One-way ANOVA \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] Mathematical Model A typical model for a one-way ANOVA is of the form \\[ Y_{ij} = \\mu_i + \\epsilon_{ij} \\] where \\(\\mu_i\\) is the mean for level (group) \\(i\\), and \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) is the error term for each point \\(j\\) within level (group) \\(i\\).", "R Instructions Console Help Command: ?aov() myaov is some name you come up with to store the results of the aov() test.", "Y must be a â€œnumericâ€ vector of the quantitative response variable.", "X is a qualitative variable (should have class(X) equal to factor or character."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 34,
    "title": "ğŸ“ Quantitative Y | Multiple Categorical\r\nX",
    "url": "index.html#quantitativeymultiplecategoricalx",
    "content": "Quantitative Y | Multiple Categorical\r\nX Graphics Y is a single quantitative variable of interest, like â€œheightsâ€ of BYU-Idaho students. X1 is a categorical (qualitative) variable like gender, with levels â€œboyâ€ and â€œgirl.â€ X2 is another categorical (qualitative) variable like â€œClassrankâ€ with levels â€œFreshmanâ€, â€œSophomoreâ€, and â€œJuniorâ€. Of course, both X1 and X2 can have as many levels as you want to work with, but must have at least two levels each. Test(s) used for this category: Two-Way ANOVA (means) Used to answer THREE questions: the effects of factor 1 on the quantitative variable the effects of factor 2 on the quantitative variable the interaction between the two factors best paired with numerical summary looking at mean Analysis : Two-way ANOVA: Final Beginning Algebra Grades Question Examples: Based on your p-values and these graphs, which of the following is a correct conclusion to reach? xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1))) ANSWER : Among fourth graders, right handed boys have longer feet on average than left handed boys while the opposite is true for girls. Does the amount a student participates in the course affect the average final Beginning Algebra grades? Does the semester in which a student takes Beginning Algebra affect their average final grade? Does a studentâ€™s participation level vary depending on the semester? (Alternatively, does the semester influence a studentâ€™s participation?) BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") BAFinalGrades$Semester <- factor(BAFinalGrades$Semester, levels = c(\"Fall\",\"Winter\",\"Spring\")) ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=1)) + geom_point(color=\"firebrick1\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Spring Semester Seems to Have Higher Final Grades\", x=\"Semester\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades$Participation <- factor(BAFinalGrades$Participation, levels = c(\"Low\",\"Moderate\",\"High\")) ggplot(BAFinalGrades, aes(x=Participation, y=FinalGrade, group=1)) + geom_point(color=\"darkred\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Participation Seems to Lead to Higher Grades\", x=\"Participation\", y=\"Beginning Algebra Final Grades\") + theme_minimal() ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=Participation, color=Participation)) + geom_point() + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Beginning Algebra Seems Better during the Spring Semester\", x=\"BYU-Idaho Semesters\", y=\"Beginning Algebra Final Grades\") + theme_minimal() Tests Two-way ANOVA Overview A two-way ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses. Writing out the hypotheses can be very involved depending on whether you use the official â€œeffects modelâ€ notation (very mathematically correct) or a simplified â€œmeans modelâ€ notation (which isnâ€™t very mathematically correct, but gets the idea across in an acceptable way). Means Model Effects Model The first set of hypotheses are a â€œone-wayâ€ set of hypotheses for the first factor of the ANOVA. Factor: X1 with say, levels \\(A\\) and \\(B\\). \\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a â€œone-wayâ€ set of hypotheses, but for the second factor of the ANOVA. Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\mu_C = \\mu_D = \\mu_E = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one}\\ i\\in\\{1=C,2=D,3=E\\} \\] The third set of hypotheses are the most interesting hypotheses in a two-way ANOVA. These are called the interaction hypotheses. They test to see if the levels of one of the factors, say \\(X1\\), impact \\(Y\\) differently for the differing levels of the other factor, \\(X2\\). The hypotheses read formally as \\[ H_0: \\text{The effect of the first factor on Y} \\\\ \\text{is the same for all levels of the second factor.} \\] \\[ H_a: \\text{The effect of the first factor on Y is not the same} \\\\ \\text{for all levels of the second factor.} \\] A mathematically correct way to state the two-way ANOVA model is with the equation \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] In this model, \\(\\mu\\) is the grand mean (which is the average Y-value ignoring all information contained in the factors); \\(\\alpha_i\\) is the first factor \\(X1\\) with levels \\(A\\) and \\(B\\) (though there could be more levels in \\(X1\\) depending on your data); \\(\\beta_j\\) is the second factor with levels \\(C\\), \\(D\\), and \\(E\\) (though there could be fewer or more levels to this factor depending on your data); \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors which has \\(2\\times3=6\\) (may differ for your data) levels; and \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) is the normally distributed error term for each point \\(k\\) found within level \\(i\\) of \\(X1\\) and level \\(j\\) of \\(X2\\). This model allows us to more formally state the hypotheses as First factor \\(X1\\) having say, levels \\(A\\) and \\(B\\). \\[ H_0: \\alpha_A = \\alpha_B = 0 \\] \\[ H_a: \\alpha_i \\neq 0 \\ \\text{for at least one}\\ i\\in\\{1=A,2=B\\} \\] Second factor \\(X2\\) with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\beta_C = \\beta_D = \\beta_E = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j\\in\\{1=C,2=D,3=E\\} \\] Does the effect of the first factor (\\(X1\\)) change for the different levels of the second factor (\\(X2\\))? In other words, is there an interaction between the two factors \\(X1\\) and \\(X2\\)? \\[ H_0: \\alpha\\beta_{ij} = 0 \\ \\text{for all } i,j \\] \\[ H_a: \\alpha\\beta_{ij} \\neq 0 \\ \\text{for at least one } i,j \\] R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ X1+X2+X1:X2, data=YourDataSet) Perform the ANOVA summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a â€œnumericâ€ vector of the quantitative response variable. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. Note that factors X3, X4, and so on could also be added + to the model if desired, but this would create a three-way, or four-way ANOVA model, and so on. X1:X2 denotes the interaction of the factors X1 and X2. It is not required, but is usually included. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. warp.aovÂ <-Â  Saves the results of the ANOVA test as an object named â€˜warp.aovâ€™. aov( â€˜aov()â€™ is a function in R used to perform the ANOVA. breaks \\(Y\\) is â€˜breaksâ€™, which is a numeric variable from the warpbreaks dataset. Â ~Â  â€˜~â€™ is the tilde symbol used to separate the left- and right-hand side in a model formula. woolÂ  The first factor \\(X1\\) is â€˜woolâ€™, which is a qualitative variable in the warpbreaks dataset. In this case, wool is a factor with two levels. Use str(warpbreaks) to see this. +Â tensionÂ  The second factor \\(X2\\) is â€˜tensionâ€™, which is another qualitative variable in the warpbreaks dataset. In this case, tension is a factor with three levels. Use str(warpbreaks) to see this. +Â wool:tension, The interaction of the two factors: wool and tension. Â dataÂ =Â warpbreaks) â€˜warpbreaksâ€™ is a dataset in R. summary( â€˜summary()â€™ shows the results of the ANOVA. warp.aov) â€˜warp.aovâ€™ is the name of the ANOVA. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to View OutputÂ  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## wool 1 451 450.7 3.765 0.058213 . ## tension 2 2034 1017.1 8.498 0.000693 *** ## wool:tension 2 1003 501.4 4.189 0.021044 * ## Residuals 48 5745 119.7 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 par( â€˜parâ€™ is a R function that can be used to set or query graphical parameters. mfrowÂ =Â c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. plot( â€˜plotâ€™ is a R function for the plotting of R objects. warp.aov, â€˜warp.aovâ€™ is the name of the ANOVA. Â whichÂ =Â 1:2) Will show the Residuals vs Fitted and the Normal QQ-plot to check the ANOVA assumptions. Â Click to View OutputÂ  Click to View Output. Explanation Hypotheses in Two-way ANOVA The hypotheses that can be tested in a two-way ANOVA that includes an interaction term are three-fold. Hypotheses about \\(\\alpha\\) where \\(\\alpha\\) has \\(m\\) levels. \\[ H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_m = 0 \\] \\[ H_a: \\alpha_i \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\} \\] Hypotheses about \\(\\beta\\) where \\(\\beta\\) has \\(q\\) levels. \\[ H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_q = 0 \\] \\[ H_a: \\beta_j \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,q\\} \\] Hypotheses about the interaction term \\(\\alpha\\beta\\). \\[ H_0: \\text{the effect of one factor is the same across all levels of the other factor} \\] \\[ H_a: \\text{the effect of one factor differs for at least one level of the other factor} \\] Expanding the ANOVA Model{#expanding} It turns out that more can be done with ANOVA than simply checking to see if the means of several groups differ. Reconsider the mathematical model of two-way ANOVA. \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] This model could be expanded to include any number of new terms in the model. The power of this approach is in the several questions (hypotheses) that can be posed to data simultaneously. What happens if your ANOVA fails both requirement tests? Example: BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") ## Rows: 204 Columns: 3 ## â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ## Delimiter: \",\" ## chr (2): Semester, Participation ## dbl (1): FinalGrade ## ## â„¹ Use `spec()` to retrieve the full column specification for this data. ## â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message. BAanova <- aov(FinalGrade ~ Semester + Participation + Semester:Participation, data=BAFinalGrades) par(mfrow=c(1,2)) plot(BAanova, which=1:2, pch=16) The residual plot shows that the constant (equal) variance assumption raises some concern as the points within each group have varying spread. The spread of each group seems to gets closer and closer as we go from left to right as well as some straying point from the more clustered groups. Thus, showing the variance of the residuals are not constant and do not accurately show a relationship between each variable. For the Q-Q plot, it shows that the residuals of the data is not quite normally distributed. There are visible points at the lower and upper ends of the graph deviating away form the diagonal line, which represents a normal distribution. Thus, showing us that the residuals of the data are not normal distributed. Although both graphs indicate that the data fails to meet both requirements, this primarily means our findings may be less reliable due to the abnormalities in the data. As a result, we should interpret the results cautiously as we proceed with our Two-Way ANOVA test! Examples: warpbreaks (Two-way), CO2 (three-way)",
    "sentences": ["Graphics Y is a single quantitative variable of interest, like â€œheightsâ€ of BYU-Idaho students.", "X1 is a categorical (qualitative) variable like gender, with levels â€œboyâ€ and â€œgirl.â€ X2 is another categorical (qualitative) variable like â€œClassrankâ€ with levels â€œFreshmanâ€, â€œSophomoreâ€, and â€œJuniorâ€.", "Of course, both X1 and X2 can have as many levels as you want to work with, but must have at least two levels each.", "Test(s) used for this category: Two-Way ANOVA (means) Used to answer THREE questions: the effects of factor 1 on the quantitative variable the effects of factor 2 on the quantitative variable the interaction between the two factors best paired with numerical summary looking at mean Analysis : Two-way ANOVA: Final Beginning Algebra Grades Question Examples: Based on your p-values and these graphs, which of the following is a correct conclusion to reach?", "xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1))) ANSWER : Among fourth graders, right handed boys have longer feet on average than left handed boys while the opposite is true for girls.", "Does the amount a student participates in the course affect the average final Beginning Algebra grades?", "Does the semester in which a student takes Beginning Algebra affect their average final grade?", "Does a studentâ€™s participation level vary depending on the semester?", "(Alternatively, does the semester influence a studentâ€™s participation?) BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") BAFinalGrades$Semester <- factor(BAFinalGrades$Semester, levels = c(\"Fall\",\"Winter\",\"Spring\")) ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=1)) + geom_point(color=\"firebrick1\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Spring Semester Seems to Have Higher Final Grades\", x=\"Semester\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades$Participation <- factor(BAFinalGrades$Participation, levels = c(\"Low\",\"Moderate\",\"High\")) ggplot(BAFinalGrades, aes(x=Participation, y=FinalGrade, group=1)) + geom_point(color=\"darkred\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Participation Seems to Lead to Higher Grades\", x=\"Participation\", y=\"Beginning Algebra Final Grades\") + theme_minimal() ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=Participation, color=Participation)) + geom_point() + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Beginning Algebra Seems Better during the Spring Semester\", x=\"BYU-Idaho Semesters\", y=\"Beginning Algebra Final Grades\") + theme_minimal()", "Tests Two-way ANOVA Overview A two-way ANOVA is only appropriate when all of the following are satisfied."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Quantitative Y | Multiple Categorical\r\nX"
  },
  {
    "id": 35,
    "title": "ğŸ“ Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single quantitative variable of interest, like â€œheightsâ€ of BYU-Idaho students. X1 is a categorical (qualitative) variable like gender, with levels â€œboyâ€ and â€œgirl.â€ X2 is another categorical (qualitative) variable like â€œClassrankâ€ with levels â€œFreshmanâ€, â€œSophomoreâ€, and â€œJuniorâ€. Of course, both X1 and X2 can have as many levels as you want to work with, but must have at least two levels each. Test(s) used for this category: Two-Way ANOVA (means) Used to answer THREE questions: the effects of factor 1 on the quantitative variable the effects of factor 2 on the quantitative variable the interaction between the two factors best paired with numerical summary looking at mean Analysis : Two-way ANOVA: Final Beginning Algebra Grades Question Examples: Based on your p-values and these graphs, which of the following is a correct conclusion to reach? xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1))) ANSWER : Among fourth graders, right handed boys have longer feet on average than left handed boys while the opposite is true for girls. Does the amount a student participates in the course affect the average final Beginning Algebra grades? Does the semester in which a student takes Beginning Algebra affect their average final grade? Does a studentâ€™s participation level vary depending on the semester? (Alternatively, does the semester influence a studentâ€™s participation?) BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") BAFinalGrades$Semester <- factor(BAFinalGrades$Semester, levels = c(\"Fall\",\"Winter\",\"Spring\")) ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=1)) + geom_point(color=\"firebrick1\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Spring Semester Seems to Have Higher Final Grades\", x=\"Semester\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades$Participation <- factor(BAFinalGrades$Participation, levels = c(\"Low\",\"Moderate\",\"High\")) ggplot(BAFinalGrades, aes(x=Participation, y=FinalGrade, group=1)) + geom_point(color=\"darkred\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Participation Seems to Lead to Higher Grades\", x=\"Participation\", y=\"Beginning Algebra Final Grades\") + theme_minimal() ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=Participation, color=Participation)) + geom_point() + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Beginning Algebra Seems Better during the Spring Semester\", x=\"BYU-Idaho Semesters\", y=\"Beginning Algebra Final Grades\") + theme_minimal()",
    "sentences": ["Y is a single quantitative variable of interest, like â€œheightsâ€ of BYU-Idaho students.", "X1 is a categorical (qualitative) variable like gender, with levels â€œboyâ€ and â€œgirl.â€ X2 is another categorical (qualitative) variable like â€œClassrankâ€ with levels â€œFreshmanâ€, â€œSophomoreâ€, and â€œJuniorâ€.", "Of course, both X1 and X2 can have as many levels as you want to work with, but must have at least two levels each.", "Test(s) used for this category: Two-Way ANOVA (means) Used to answer THREE questions: the effects of factor 1 on the quantitative variable the effects of factor 2 on the quantitative variable the interaction between the two factors best paired with numerical summary looking at mean Analysis : Two-way ANOVA: Final Beginning Algebra Grades Question Examples: Based on your p-values and these graphs, which of the following is a correct conclusion to reach?", "xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1))) ANSWER : Among fourth graders, right handed boys have longer feet on average than left handed boys while the opposite is true for girls.", "Does the amount a student participates in the course affect the average final Beginning Algebra grades?", "Does the semester in which a student takes Beginning Algebra affect their average final grade?", "Does a studentâ€™s participation level vary depending on the semester?", "(Alternatively, does the semester influence a studentâ€™s participation?) BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") BAFinalGrades$Semester <- factor(BAFinalGrades$Semester, levels = c(\"Fall\",\"Winter\",\"Spring\")) ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=1)) + geom_point(color=\"firebrick1\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Spring Semester Seems to Have Higher Final Grades\", x=\"Semester\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades$Participation <- factor(BAFinalGrades$Participation, levels = c(\"Low\",\"Moderate\",\"High\")) ggplot(BAFinalGrades, aes(x=Participation, y=FinalGrade, group=1)) + geom_point(color=\"darkred\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Participation Seems to Lead to Higher Grades\", x=\"Participation\", y=\"Beginning Algebra Final Grades\") + theme_minimal() ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=Participation, color=Participation)) + geom_point() + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Beginning Algebra Seems Better during the Spring Semester\", x=\"BYU-Idaho Semesters\", y=\"Beginning Algebra Final Grades\") + theme_minimal()"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 36,
    "title": "ğŸ“ Tests",
    "url": "index.html#tests",
    "content": "Tests Two-way ANOVA Overview A two-way ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses. Writing out the hypotheses can be very involved depending on whether you use the official â€œeffects modelâ€ notation (very mathematically correct) or a simplified â€œmeans modelâ€ notation (which isnâ€™t very mathematically correct, but gets the idea across in an acceptable way). Means Model Effects Model The first set of hypotheses are a â€œone-wayâ€ set of hypotheses for the first factor of the ANOVA. Factor: X1 with say, levels \\(A\\) and \\(B\\). \\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a â€œone-wayâ€ set of hypotheses, but for the second factor of the ANOVA. Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\mu_C = \\mu_D = \\mu_E = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one}\\ i\\in\\{1=C,2=D,3=E\\} \\] The third set of hypotheses are the most interesting hypotheses in a two-way ANOVA. These are called the interaction hypotheses. They test to see if the levels of one of the factors, say \\(X1\\), impact \\(Y\\) differently for the differing levels of the other factor, \\(X2\\). The hypotheses read formally as \\[ H_0: \\text{The effect of the first factor on Y} \\\\ \\text{is the same for all levels of the second factor.} \\] \\[ H_a: \\text{The effect of the first factor on Y is not the same} \\\\ \\text{for all levels of the second factor.} \\] A mathematically correct way to state the two-way ANOVA model is with the equation \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] In this model, \\(\\mu\\) is the grand mean (which is the average Y-value ignoring all information contained in the factors); \\(\\alpha_i\\) is the first factor \\(X1\\) with levels \\(A\\) and \\(B\\) (though there could be more levels in \\(X1\\) depending on your data); \\(\\beta_j\\) is the second factor with levels \\(C\\), \\(D\\), and \\(E\\) (though there could be fewer or more levels to this factor depending on your data); \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors which has \\(2\\times3=6\\) (may differ for your data) levels; and \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) is the normally distributed error term for each point \\(k\\) found within level \\(i\\) of \\(X1\\) and level \\(j\\) of \\(X2\\). This model allows us to more formally state the hypotheses as First factor \\(X1\\) having say, levels \\(A\\) and \\(B\\). \\[ H_0: \\alpha_A = \\alpha_B = 0 \\] \\[ H_a: \\alpha_i \\neq 0 \\ \\text{for at least one}\\ i\\in\\{1=A,2=B\\} \\] Second factor \\(X2\\) with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\beta_C = \\beta_D = \\beta_E = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j\\in\\{1=C,2=D,3=E\\} \\] Does the effect of the first factor (\\(X1\\)) change for the different levels of the second factor (\\(X2\\))? In other words, is there an interaction between the two factors \\(X1\\) and \\(X2\\)? \\[ H_0: \\alpha\\beta_{ij} = 0 \\ \\text{for all } i,j \\] \\[ H_a: \\alpha\\beta_{ij} \\neq 0 \\ \\text{for at least one } i,j \\] R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ X1+X2+X1:X2, data=YourDataSet) Perform the ANOVA summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a â€œnumericâ€ vector of the quantitative response variable. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. Note that factors X3, X4, and so on could also be added + to the model if desired, but this would create a three-way, or four-way ANOVA model, and so on. X1:X2 denotes the interaction of the factors X1 and X2. It is not required, but is usually included. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. warp.aovÂ <-Â  Saves the results of the ANOVA test as an object named â€˜warp.aovâ€™. aov( â€˜aov()â€™ is a function in R used to perform the ANOVA. breaks \\(Y\\) is â€˜breaksâ€™, which is a numeric variable from the warpbreaks dataset. Â ~Â  â€˜~â€™ is the tilde symbol used to separate the left- and right-hand side in a model formula. woolÂ  The first factor \\(X1\\) is â€˜woolâ€™, which is a qualitative variable in the warpbreaks dataset. In this case, wool is a factor with two levels. Use str(warpbreaks) to see this. +Â tensionÂ  The second factor \\(X2\\) is â€˜tensionâ€™, which is another qualitative variable in the warpbreaks dataset. In this case, tension is a factor with three levels. Use str(warpbreaks) to see this. +Â wool:tension, The interaction of the two factors: wool and tension. Â dataÂ =Â warpbreaks) â€˜warpbreaksâ€™ is a dataset in R. summary( â€˜summary()â€™ shows the results of the ANOVA. warp.aov) â€˜warp.aovâ€™ is the name of the ANOVA. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to View OutputÂ  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## wool 1 451 450.7 3.765 0.058213 . ## tension 2 2034 1017.1 8.498 0.000693 *** ## wool:tension 2 1003 501.4 4.189 0.021044 * ## Residuals 48 5745 119.7 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 par( â€˜parâ€™ is a R function that can be used to set or query graphical parameters. mfrowÂ =Â c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. plot( â€˜plotâ€™ is a R function for the plotting of R objects. warp.aov, â€˜warp.aovâ€™ is the name of the ANOVA. Â whichÂ =Â 1:2) Will show the Residuals vs Fitted and the Normal QQ-plot to check the ANOVA assumptions. Â Click to View OutputÂ  Click to View Output. Explanation Hypotheses in Two-way ANOVA The hypotheses that can be tested in a two-way ANOVA that includes an interaction term are three-fold. Hypotheses about \\(\\alpha\\) where \\(\\alpha\\) has \\(m\\) levels. \\[ H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_m = 0 \\] \\[ H_a: \\alpha_i \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\} \\] Hypotheses about \\(\\beta\\) where \\(\\beta\\) has \\(q\\) levels. \\[ H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_q = 0 \\] \\[ H_a: \\beta_j \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,q\\} \\] Hypotheses about the interaction term \\(\\alpha\\beta\\). \\[ H_0: \\text{the effect of one factor is the same across all levels of the other factor} \\] \\[ H_a: \\text{the effect of one factor differs for at least one level of the other factor} \\] Expanding the ANOVA Model{#expanding} It turns out that more can be done with ANOVA than simply checking to see if the means of several groups differ. Reconsider the mathematical model of two-way ANOVA. \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] This model could be expanded to include any number of new terms in the model. The power of this approach is in the several questions (hypotheses) that can be posed to data simultaneously. What happens if your ANOVA fails both requirement tests? Example: BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") ## Rows: 204 Columns: 3 ## â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ## Delimiter: \",\" ## chr (2): Semester, Participation ## dbl (1): FinalGrade ## ## â„¹ Use `spec()` to retrieve the full column specification for this data. ## â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message. BAanova <- aov(FinalGrade ~ Semester + Participation + Semester:Participation, data=BAFinalGrades) par(mfrow=c(1,2)) plot(BAanova, which=1:2, pch=16) The residual plot shows that the constant (equal) variance assumption raises some concern as the points within each group have varying spread. The spread of each group seems to gets closer and closer as we go from left to right as well as some straying point from the more clustered groups. Thus, showing the variance of the residuals are not constant and do not accurately show a relationship between each variable. For the Q-Q plot, it shows that the residuals of the data is not quite normally distributed. There are visible points at the lower and upper ends of the graph deviating away form the diagonal line, which represents a normal distribution. Thus, showing us that the residuals of the data are not normal distributed. Although both graphs indicate that the data fails to meet both requirements, this primarily means our findings may be less reliable due to the abnormalities in the data. As a result, we should interpret the results cautiously as we proceed with our Two-Way ANOVA test! Examples: warpbreaks (Two-way), CO2 (three-way)",
    "sentences": ["Two-way ANOVA Overview A two-way ANOVA is only appropriate when all of the following are satisfied.", "The sample(s) of data can be considered to be representative of their population(s).", "The data is normally distributed in each group.", "(This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same.", "(This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses.", "Writing out the hypotheses can be very involved depending on whether you use the official â€œeffects modelâ€ notation (very mathematically correct) or a simplified â€œmeans modelâ€ notation (which isnâ€™t very mathematically correct, but gets the idea across in an acceptable way).", "Means Model Effects Model The first set of hypotheses are a â€œone-wayâ€ set of hypotheses for the first factor of the ANOVA.", "Factor: X1 with say, levels \\(A\\) and \\(B\\).", "\\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a â€œone-wayâ€ set of hypotheses, but for the second factor of the ANOVA.", "Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\)."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 37,
    "title": "ğŸ“ Quantitative Y | Quantitative X",
    "url": "index.html#quantitativeyquantitativex",
    "content": "Quantitative Y | Quantitative X Graphics Y is a single quantitative variable of interest, like â€œheightâ€. X is another single quantitative variable of interest, like â€œshoe-sizeâ€. This would imply we are using â€œshoe-sizeâ€ (X) to explain â€œheightâ€ (Y). Test(s) used for this category: Simple Linear Regression Analysis : Simple Linear Regression: BYU-Idaho Math Tutoring Used to find the relationship between one independent variable (predictor) and one dependent variable(outcome): Is there a relationship between the length of childrenâ€™s feet and the width of their foot? Example: Does the amount of tutoring a student receives correlate with their chapter exam scores? (Simple Linear Regression) ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams) Tests Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\). Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$\\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$$ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is â€œequal toâ€ the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to â€œGodâ€™s Lawâ€ or â€œNatural Lawâ€. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are â€œrandomâ€ and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)â€¦ \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced â€œy-hatâ€, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what followsâ€¦ \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, â€œthe change in the average y-value for a one unit change in the x-value.â€ It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, â€œthe average y-value when x is zero.â€ It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful. R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your lm() code into mylm name. lm( lm(â€¦) is an R function that stands for â€œLinear Modelâ€. It performs a linear regression analysis for Y ~ X. YÂ  Y is your quantitative response variable. It is the name of one of the columns in your data set. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X. X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value. Â data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X. ) Closing parenthesis for the lm(â€¦) function. summary(mylm) The summary command allows you to print the results of your linear regression that were previously saved in mylm name. Â Click to Show OutputÂ  Click to View Output. Example output from a regression. Hover each piece to learn more. Call: lm(formula = dist ~ speed, data = cars) This is simply a statement of your original lm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(â€¦). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min Â  -29.069 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -9.525 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -2.272 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -2.272 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-2.272) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  9.215 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  43.201 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  t value To learn more about the â€œt valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  Pr(>|t|) The â€œPrâ€ stands for â€œProbabilityâ€ and the â€œ(> |t|)â€ stands for â€œmore extreme than the observed t-valueâ€. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the â€œp-valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. (Intercept) This always says â€œInterceptâ€ for any lm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -17.5791 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  6.7584 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -2.601 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the intercept (-17.5791) by its standard error (6.7584). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of -17.5791 is -2.601 standard errors (6.7584) from zero, which is a fairly surprising distance as shown by the p-value. 0.0123 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). speed This is always the name of your X-variable in your lm(Y ~ X, â€¦). Â  3.9324 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.4155 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 9.464 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the slope (3.9324) by its standard error (0.4155). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from zero, which is a really surprising distance as shown by the smallness of the p-value. 1.49e-12 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE. Â 15.38 For this particular regression, the estimate of \\(\\sigma\\) is 15.38. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\). Â on 48 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 50 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 50-2 = 48. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO. Â 0.6511, In this particular regression, 65.11% of the variation in stopping distance dist is explained by the regression model using speed of the car. Â Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data. Â 0.6438 In this case, the value of 0.6438 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is found as the ratio of the MSR/MSE where MSR = SSR/(p-1) and MSE = SSE/(n-p) where n is the sample size and p is the number of parameters in the regression model. Â 89.57 This is the value of the F-statistic for the lm(dist ~ speed, data=cars) regression. Note that SSE = sum( cars.lm$res^2 ) = 11353.52 with n - p = 50 - 2 = 48 degrees of freedom for this data. Further, SSR = sum( (cars.lm$fit - mean(cars$dist))^2 ) = 21185.46 with p - 1 = 1 degree of freedom. So MSR = 21185.46 and MSE = 11353.52 / 48 = 236.5317. So MSR / MSE = 21185.46 / 236.5317 = 89.56711. Â on 1 and 48 DF, The 1 degree of freedom is the SSR degrees of freedom (p-1). The 48 is the SSE degrees of freedom (n-p). Â p-value: 1.49e-12 The p-value for an F-statistic is found by the code pf(89.56711, 1, 48, lower.tail=FALSE), which gives the probability of being more extreme than the observed F-statistic in an F distribution with 1 and 48 degrees of freedom. Check Assumptions 1, 2, 3, and 5 When making it in an r chunk make your height 3! { r, fig.height = 3 } par( The par(â€¦) command stands for â€œGraphical PARametersâ€. It allows you to control various aspects of graphics in Base R. mfrow= This stands for â€œmultiple frames filled by rowâ€, which means, put lots of plots on the same row, starting with the plot on the left, then working towards the right as more plots are created. c( The combine function c(â€¦) is used to specify how many rows and columns of graphics should be placed together. 1, This specifies that 1 row of graphics should be produced. 3 This states that 3 columns of graphics should be produced. ) Closing parenthesis for c(â€¦) function. ) Closing parenthesis for par(â€¦) function. plot( This version of plot(â€¦) will actually create several regression diagnostic plots by default. mylm, This is the name of an lm object that you created previously. which= This allows you to select â€œwhichâ€ regression diagnostic plots should be drawn. 1 Selecting 1, would give the residuals vs.Â fitted values plot only. : The colon allows you to select more than just one plot. 2 Selecting 2 also gives the Q-Q Plot of residuals. If you wanted to instead you could just use which=1 to get the residuals vs fitted values plot, then you could use qqPlot(mylm$residuals) to create a fancier Q-Q Plot of the residuals. ) Closing parenthesis for plot(â€¦) function. plot( This version of plot(â€¦) will be used to create a time-ordered plot of the residuals. The order of the residuals is the original order of the x-values in the original data set. If the original data set doesnâ€™t have an order, then this plot is not interesting. mylm The lm object that you created previously. $ This allows you to access various elements from the regression that was performed. residuals This grabs the residuals for each observation in the regression. ) Closing parenthesis for plot(â€¦) function. Â Click to Show OutputÂ  Click to View Output. Which regression assumption(s) does the residuals versus fitted values plot diagnose? Linear Relation Constance Variance Which regression assumption(s) does the Q-Q Plot of the residuals diagnose? Normal Errors Which regression assumption(s) does the Residuals versus Order plot diagnose? (Remember, this plot can only be created when the data was collected in a specific order.) Independent Errors Plotting the Regression Line Base R ggplot2 To add the regression line to a scatterplot use the abline(...) command: plot( The plot(â€¦) function is used to create a scatterplot with a y-axis (the vertical axis) and an x-axis (the horizontal axis). YÂ  This is the â€œresponse variableâ€ of your regression. The thing you are interested in predicting. This is the name of a â€œnumericâ€ column of data from the data set called YourDataSet. ~Â  The tilde â€œ~â€ is used to relate Y to X and can be found on the top-left key of your keyboard. X,Â  This is the explanatory variable of your regression. It is the name of a â€œnumericâ€ column of data from YourDataSet. data= The data= statement is used to specify the name of the data set where the columns of â€œXâ€ and â€œYâ€ are located. YourDataSet This is the name of your data set, like KidsFeet or cars or airquality. ) Closing parenthesis for plot(â€¦) function. abline( This stands for â€œaâ€ (intercept) â€œbâ€ (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm This is the name of an lm(â€¦) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(â€¦) function will locate these two values from within mylm and use them to add a line to your current plot(â€¦). ) Closing parenthesis for abline(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm) You can customize the look of the regression line with abline( This stands for â€œaâ€ (intercept) â€œbâ€ (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm, This is the name of an lm(â€¦) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(â€¦) function will locate these two values from within mylm and use them to add a line to your current plot(â€¦). lty= The lty= stands for â€œline typeâ€ and allows you to select between 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. 1, This creates a solid line. Remember, other options include: 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. lwd= The lwd= allows you to specify the width of the line. The default width is 1. Using lwd=2 would double the thickness, and so on. Any positive value is allowed. 1, Default line width. To make a thicker line, us 2 or 3â€¦ To make a thinner line, try 0.5, but 1 is already pretty thin. col= This allows you to specify the color of the line using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). â€œsomeColorâ€ Type colors() in R for options. ) Closing parenthesis for abline(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm, lty=1, lwd=1, col=\"firebrick\") You can add points to the regression withâ€¦ points( This is like plot(â€¦) but adds points to the current plot(â€¦) instead of creating a new plot. newYÂ  newY should be a column of values from some data set. Or, use points(newX, newY) to add a single point to a graph. ~Â  This links Y to X in the plot. newX,Â  newX should be a column of values from some data set. It should be the same length as newY. If just a single value, use points(newX, newY) instead. data=YourDataSet,Â  If newY and newX come from a dataset, then use data= to tell the points(â€¦) function what data set they come from. If newY and newX are just single values, then data= is not needed. col=â€œskyblueâ€, This allows you to specify the color of the points using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). pch=16 This allows you to specify the type of plotting symbol to be used for the points. Type ?pch and scroll half way down in the help file that appears to learn about other possible symbols. ) Closing parenthesis for points(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) points(7,40, pch=16, col=\"skyblue\", cex=2) text(7,40, \"New Dot\", pos=3, cex=0.5) points(dist ~ speed, data=filter(cars, mylm$res > 2), cex=.8, col=\"red\") abline(mylm, lty=1, lwd=1, col=\"firebrick\") To add the regression line to a scatterplot using the ggplot2 approach, first ensure: library(ggplot2) or library(tidyverse) is loaded. Then, use the geom_smooth(method = lm) command: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. YourDataSet,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. X, This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. Â y =Â  â€œy=Â â€ declares which variable will become the y-axis of the graphic. Y This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. + Here the + is used to add yet another layer to ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. There are several other methods that could be used here. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(â€¦) to place more complicated models on the plot. Â se = FALSE se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. ) Closing parenthesis for the geom_smooth() function. Â Click to Show OutputÂ  Click to View Output. ggplot(cars, aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = \"lm\", formula=y~x, se=FALSE) There are a number of ways to customize the appearance of the regression line: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. speed,Â  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. y =Â  â€œy=Â â€ declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. Â + Here the + is used to add yet another layer to ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(â€¦) to place more complicated models on the plot. Â se = FALSE, se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. Â size = 2, Use size = 2 to adjust the thickness of the line to size 2. Â color = â€œorangeâ€, Use color = â€œorangeâ€ to change the color of the line to orange. Â Â linetype = â€œdashedâ€ Use linetype = â€œdashedâ€ to change the solid line to a dashed line. Some linetype options include â€œdashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. ) Closing parenthesis for the geom_smooth() function. Â Click to Show OutputÂ  Click to View Output. In addition to customizing the regression line, you can customize the points, add points, add lines, and much more. ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. speed,Â  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. y =Â  â€œy=Â â€ declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point( geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. size = 1.5, Use size = 1.5 to change the size of the points. Â color = â€œskyblueâ€ Use color = â€œskyblueâ€ to change the color of the points to Brother Saundersâ€™ favorite color. Â alpha = 0.5 Use alpha = 0.5 to change the transparency of the points to 0.5. ) Closing parenthesis of geom_point() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in ways similar to lm(â€¦) to place more complicated models on the plot. Â se = FALSE, se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. Â color = â€œnavyâ€, Use color = â€œnavyâ€ to change the color of the line to navy blue. Â size = 1.5 Use size = 1.5 to adjust the thickness of the line to 1.5. ) Closing parenthesis of geom_smooth() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_hline( Use geom_hline() to add a horizontal line at a specified y-intercept. You can also use geom_vline(xintercept = some_number) to add a vertical line to the graph. yintercept = Use â€œyintercept =â€ to tell geom_hline() that you are going to declare a y intercept for the horizontal line. Â 75 75 is the value of the y-intercept. , color = â€œfirebrickâ€ Use color = â€œfirebrickâ€ to change the color of the horizontal line to firebrick red. , size = 1, Use size = 1 to adjust the thickness of the horizontal line to size 1. Â Â Â Â Â Â Â Â Â Â Â Â Â linetype = â€œlongdashâ€ Use linetype = â€œlongdashâ€ to change the solid line to a dashed line with longer dashes. Some linetype options include â€œdashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. , alpha = 0.5 Use alpha = 0.5 to change the transparency of the horizontal line to 0.5. ) Closing parenthesis of geom_hline function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_segment( geom_segment() allows you to add a line segment to ggplot() by using specified start and end points. x = â€œx =â€ tells geom_segment() that you are going to declare the x-coordinate for the starting point of the line segment. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the starting point of the line segment. Â y =â€œy =â€ tells geom_segment() that you are going to declare the y-coordinate for the starting point of the line segment. Â 75, 75 is a number on the y-axis of your graph. It is the y-coordinate of the starting point of the line segment. Â xend = â€œxend =â€ tells geom_segment() that you are going to declare the x-coordinate for the end point of the line segment. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the end point of the line segment. Â yend = â€œyend =â€ tells geom_segment() that you are going to declare the y-coordinate for the end point of the line segment. Â 38, 38 is a number on the y-axis of your graph. It is the y-coordinate of the end point of the line segment. Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â size = 1 Use size = 1 to adjust the thickness of the line segment. , color = â€œlightgrayâ€ Use color = â€œlightgrayâ€ to change the color of the line segment to light gray. , linetype = â€œlongdashâ€ Use *linetype = â€œlongdash* to change the solid line segment to a dashed one. Some linetype options includeâ€dashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. ) Closing parenthesis for geom_segment() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_point( geom_point() can also be used to add individual points to the graph. Simply declare the x and y coordinates of the point you want to plot. x = â€œx =â€ tells geom_point() that you are going to declare the x-coordinate for the point. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the point. Â y = â€œy =â€ tells geom_point() that you are going to declare the y-coordinate for the point. Â 75 75 is a number on the y-axis of your graph. It is the y-coordinate of the point. , size = 3 Use size = 3 to make the point stand out more. , color = â€œfirebrickâ€ Use color = â€œfirebrickâ€ to change the color of the point to firebrick red. ) Closing parenthesis of the geom_point() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_text( geom_text() allows you to add customized text anywhere on the graph. It is very similar to the base R equivalent, text(â€¦). x = â€œx =â€ tells geom_text() that you are going to declare the x-coordinate for the text. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the text. Â y = â€œy =â€ tells geom_text() that you are going to declare the y-coordinate for the text. Â 84, 84 is a number on the y-axis of your graph. It is the y-coordinate of the text. Â label = â€œlabel =â€ tells geom_text() that you are going to give it the label. Â â€œMy Point (14, 75)â€, â€œMy Point (14, 75)â€ is the text that will appear on the graph. Â Â Â Â Â Â Â Â Â Â Â Â color = â€œnavyâ€ Use color = â€œnavyâ€ to change the color of the text to navy blue. , size = 3 Use size = 3 to change the size of the text. ) Closing parenthesis of the geom_text() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â theme_minimal() Add a minimalistic theme to the graph. There are many other themes that you can try out. Â Click to Show OutputÂ  Click to View Output. ## `geom_smooth()` using formula = 'y ~ x' Accessing Parts of the Regression Finally, note that the mylm object contains the names(mylm) of mylm$coefficients Contains two values. The first is the estimated \\(y\\)-intercept. The second is the estimated slope. ## (Intercept) speed ## -17.579095 3.932409 mylm$residuals Contains the residuals from the regression in the same order as the actual dataset. ## 1 2 3 4 5 6 7 8 9 ## 3.849460 11.849460 -5.947766 12.052234 2.119825 -7.812584 -3.744993 4.255007 12.255007 ## 10 11 12 13 14 15 16 17 18 ## -8.677401 2.322599 -15.609810 -9.609810 -5.609810 -1.609810 -7.542219 0.457781 0.457781 ## 19 20 21 22 23 24 25 26 27 ## 12.457781 -11.474628 -1.474628 22.525372 42.525372 -21.407036 -15.407036 12.592964 -13.339445 ## 28 29 30 31 32 33 34 35 36 ## -5.339445 -17.271854 -9.271854 0.728146 -11.204263 2.795737 22.795737 30.795737 -21.136672 ## 37 38 39 40 41 42 43 44 45 ## -11.136672 10.863328 -29.069080 -13.069080 -9.069080 -5.069080 2.930920 -2.933898 -18.866307 ## 46 47 48 49 50 ## -6.798715 15.201285 16.201285 43.201285 4.268876 mylm$fitted.values The values of \\(\\hat{Y}\\) in the same order as the original dataset. ## 1 2 3 4 5 6 7 8 9 10 ## -1.849460 -1.849460 9.947766 9.947766 13.880175 17.812584 21.744993 21.744993 21.744993 25.677401 ## 11 12 13 14 15 16 17 18 19 20 ## 25.677401 29.609810 29.609810 29.609810 29.609810 33.542219 33.542219 33.542219 33.542219 37.474628 ## 21 22 23 24 25 26 27 28 29 30 ## 37.474628 37.474628 37.474628 41.407036 41.407036 41.407036 45.339445 45.339445 49.271854 49.271854 ## 31 32 33 34 35 36 37 38 39 40 ## 49.271854 53.204263 53.204263 53.204263 53.204263 57.136672 57.136672 57.136672 61.069080 61.069080 ## 41 42 43 44 45 46 47 48 49 50 ## 61.069080 61.069080 61.069080 68.933898 72.866307 76.798715 76.798715 76.798715 76.798715 80.731124 mylm$â€¦ several other things that will not be explained here. Making Predictions predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ) Closing parenthesis for the data.frame(â€¦) function. ) Closing parenthesis for the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12)) 1 Â  29.60981 The value given is the â€œfitted-valueâ€ or â€œpredicted-valueâ€ for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(â€¦) function. Â interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. â€œpredictionâ€ This specifies that a prediction interval will be included with the predicted value. A prediction interval gives you a 95% confidence interval that captures 95% of the data, or \\(Y_i\\) values for the specific \\(X\\)-value specified in the prediction. ) Closing parenthesis of the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"prediction\") Â  fit The â€œfitâ€ is the predicted value. Â  lwr The â€œlwrâ€ is the lower bound. Â  upr The â€œuprâ€ is the upper bound. 1 29.60981 In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance canâ€™t go negative) feet to 60.96915 feet. -1.749529 This is the lower bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us the stopping distance could be as quick as -1.749529 feet (or 0 because distance canâ€™t go negative). 60.96915 This is the upper bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us that the actual stopping distance could be as high as 60.96915 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(â€¦) function. Â interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. â€œconfidenceâ€ This specifies that a confidence interval for the prediction should be provided. This is of use whenever your interest is in just estimating the average y-value, not the actual y-values. ) Closing parenthesis of the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"confidence\") Â  fit The â€œfitâ€ is the predicted value. Â  lwr The â€œlwrâ€ is the lower bound. Â  upr The â€œuprâ€ is the upper bound. 1 29.60981 In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. 24.39514 This is the lower bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is greater than this value. 34.82448 This is the upper bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is less than this value. Finding Confidence Intervals for Model Parameters confint( The R function confint(â€¦) allows you to use an lm(â€¦) object to compute confidence intervals for one or more parameters (like \\(\\beta_0\\) or \\(\\beta_1\\)) in your model. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â level = â€œlevel =â€ tells the confint(â€¦) function that you are going to declare at what level of confidence you want the interval. The default is â€œlevel = 0.95.â€ If you want to find 95% confidence intervals for your parameters, then just run confint(mylm). Â someConfidenceLevel someConfidenceLevel is simply a confidence level you choose when you want something other than a 95% confidence interval. Some examples of appropriate levels include 0.90 and 0.99. ) Closing parenthesis for confint(..) function. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.90) Â  Â  5 % The lower bound of a 90% confidence interval occurs at the 5th percentile. This is because at 90% confidence, 10% is left in the tails, with 5% on each end. The upper bound will thus end at the 95th percentile, hence the 5% and 95% as the column names. Â  95 % The upper bound of a 90% confidence interval ends at the 95th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -28.914514 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -6.243676 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.235501 and 4.629317. 3.235501 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. 4.629317 This is the upper bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.95) Â  Â  2.5 % The lower bound of a 95% confidence interval occurs at the 2.5th percentile. This is because at 95% confidence, 5% is left in the tails, with 2.5% on each end. The upper bound will thus end at the 97.5th percentile, hence the 2.5% and 97.5% as the column names for the lower and upper bounds, respectively. Â  97.5 % The upper bound of a 95% confidence interval ends at the 97.5th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -31.167850 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -3.990340 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.096964 and 4.767853. 3.096964 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.096964 and 4.767853 4.767853 This is the upper bound of the confidence interval. In this case, you can be 95% confident that the slope lies between 3.096964 and 4.767853 Explanation Linear regression has a rich mathematical theory behind it. This is because it uses a mathematical function and a random error term to describe the regression relation between a response variable \\(Y\\) and an explanatory variable called \\(X\\). Expand each element below to learn more. Regression Cheat Sheet (Expand) Term Pronunciation Meaning Math R Code \\(Y_i\\) $Y_i$ â€œwhy-eyeâ€ The data \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)\\) $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$ YourDataSet$YourYvariable \\(\\hat{Y}_i\\) $\\hat{Y}_i$ â€œwhy-hat-eyeâ€ The fitted line \\(\\hat{Y}_i = b_0 + b_1 X_i\\) $\\hat{Y}_i = b_0 + b_1 X_i$ lmObject$fitted.values \\(E\\{Y_i\\}\\) $E\\{Y_i\\}$ â€œexpected value of why-eyeâ€ True mean y-value \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) $E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i$ <none> \\(\\beta_0\\) $\\beta_0$ â€œbeta-zeroâ€ True y-intercept <none> <none> \\(\\beta_1\\) $\\beta_1$ â€œbeta-oneâ€ True slope <none> <none> \\(b_0\\) $b_0$ â€œb-zeroâ€ Estimated y-intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) $b_0 = \\bar{Y} - b_1\\bar{X} b_0 <- mean(Y) - b_1*mean(X)$ \\(b_1\\) $b_1$ â€œb-oneâ€ Estimated slope \\(b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\) $b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})} {\\sum(X_i - \\bar{X})^2}$ b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) \\(\\epsilon_i\\) $\\epsilon_i$ â€œepsilon-eyeâ€ Distance of dot to true line \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) $\\epsilon_i = Y_i - E\\{Y_i\\}$ <none> \\(r_i\\) $r_i$ â€œr-eyeâ€ or â€œresidual-eyeâ€ Distance of dot to estimated line \\(r_i = Y_i - \\hat{Y}_i\\) $r_i = Y_i - \\hat{Y}_i$ lmObject$residuals \\(\\sigma^2\\) $\\sigma^2$ â€œsigma-squaredâ€ Variance of the \\(\\epsilon_i\\) \\(Var\\{\\epsilon_i\\} = \\sigma^2\\)$Var\\{\\epsilon_i\\} = \\sigma^2$ <none> \\(MSE\\) $MSE$ â€œmean squared errorâ€ Estimate of \\(\\sigma^2\\) \\(MSE = \\frac{SSE}{n-p}\\)$MSE = \\frac{SSE}{n-p}$ sum( lmObject$res^2 ) / (n - p) \\(SSE\\) $SSE$ â€œsum of squared errorâ€ (residuals) Measure of dotâ€™s total deviation from the line \\(SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\)$SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$ sum( lmObject$res^2 ) \\(SSR\\) $SSR$ â€œsum of squared regression errorâ€ Measure of lineâ€™s deviation from y-bar \\(SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\)$SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2$ sum( (lmObject$fit - mean(YourData$Y))^2 ) \\(SSTO\\) $SSTO$ â€œtotal sum of squaresâ€ Measure of total variation in Y \\(SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\)$SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2$ sum( (YourData$Y - mean(YourData$Y))^2 ) \\(R^2\\) $R^2$ â€œR-squaredâ€ Proportion of variation in Y explained by the regression \\(R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\)$R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}$ SSR/SSTO \\(r\\) $r$ â€œrâ€ Correlation between X and Y. \\(r = \\sqrt{R^2}\\)$r = \\sqrt{R^2}$ sqrt(R^2) \\(\\hat{Y}_h\\) $\\hat{Y}_h$ â€œwhy-hat-aitchâ€ Estimated mean y-value for some x-value called \\(X_h\\) \\(\\hat{Y}_h = b_0 + b_1 X_h\\)$\\hat{Y}_h = b_0 + b_1 X_h$ predict(lmObject, data.frame(XvarName=#)) \\(X_h\\) $X_h$ â€œex-aitchâ€ Some x-value, not necessarily one of the \\(X_i\\) values used in the regression \\(X_h =\\) some number$X_h = $ Xh = # Confidence Interval â€œconfidence intervalâ€ Estimated bounds at a certain level of confidence for a parameter \\(b_0 \\pm t^* \\cdot s_{b_0}\\)b_0 \\pm t^* \\cdot s_{b_0} or \\(b_1 \\pm t^* \\cdot s_{b_1}\\)b_1 \\pm t^* \\cdot s_{b_1} confint(mylm, level = someConfidenceLevel) Parameter Estimate \\(\\beta_0\\) \\(b_0\\) \\(\\beta_1\\) \\(b_1\\) \\(\\epsilon_i\\) \\(r_i\\) \\(\\sigma^2\\) \\(MSE\\) \\(\\sigma\\) \\(\\sqrt{MSE}\\), the Residual standard error The Mathematical Model (Expand) \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)â€¦ There are three main elements to the mathematical model of regression. Each of these three elements is pictured below in the â€œRegression Relation Diagram.â€ Study both the three bullet points and their visual representations in the plot below for a clearer understanding. The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read moreâ€¦) The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as â€œnatural lawâ€ or â€œGodâ€™s lawâ€. It is some true line that is unknown to us. The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line. The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\). Note: \\(E\\{Y\\}\\) is pronounced â€œthe expected value of yâ€ because, wellâ€¦ the mean is the typical, average, or â€œexpectedâ€ value. The dots, i.e., the regression relation plus an error term: \\(Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}} + \\underbrace{\\epsilon_i}_\\text{error term} \\quad \\text{where} \\ \\epsilon_i\\sim N(0,\\sigma^2)\\) (Read moreâ€¦) This is shown by the dots in the graph below. This is the data. In regression, the assumption is that the y-value for individual \\(i\\), denoted by \\(Y_i\\), was â€œcreatedâ€ by adding an error term \\(\\epsilon_i\\) to each individualâ€™s â€œexpectedâ€ value \\(\\beta_0 + \\beta_1 X_i\\). Note the â€œorder of creationâ€ would require first knowing an indivualâ€™s x-value, \\(X_i\\), then their expected value from the regression relation \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) and then adding their \\(\\epsilon_i\\) value to the result. The \\(\\epsilon_i\\) allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance \\(\\epsilon_i\\) from the line. Note: \\(Y_i\\) is pronounced â€œwhy-eyeâ€ because it is the y-value for individual \\(i\\). Sometimes also called â€œwhy-sub-eyeâ€ because \\(i\\) is in the subscript of \\(Y\\). The estimated line, i.e., the line we get from a sample of data. \\(\\underbrace{\\hat{Y}_i}_{\\substack{\\text{estimated mean} \\\\ \\text{y-value}}} = \\underbrace{b_0 + b_1 X_i}_\\text{estimated regression equation}\\) (Read moreâ€¦) The estimated line is shown by the solid line in the graph below. \\(\\hat{Y}\\) is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation \\(E\\{Y\\}\\). So \\(\\hat{Y}\\) is interpreted as the estimated average (or mean) \\(Y\\)-value for any given \\(X\\)-value. Thus, \\(b_0\\) is the estimated y-intercept and \\(b_1\\) is the estimated slope. The bâ€™s are sample statistics, like \\(\\bar{x}\\) and the \\(\\beta\\)â€™s are population parameters like \\(\\mu\\). The \\(b\\)â€™s estimate the \\(\\beta\\)â€™s. Note: \\(\\hat{Y}_i\\) is pronounced â€œwhy-hat-eyeâ€ and is known as the â€œestimated y-valueâ€ or â€œfitted y-valueâ€ because it is the y-value you get from \\(b_0 + b_1 X_i\\). It is always different from \\(Y_i\\) because dots are rarely if ever exactly on the estimated regression line. This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line). Something to ponder: The true line, when coupled with the error terms, â€œcreatesâ€ the data. The estimated (or fitted) line uses the sampled data to try to â€œre-createâ€ the true line. We could loosely call this the â€œorder of creationâ€ as shown by the following diagram. par(mfrow=c(1,3), mai=c(.2,.2,.4,.1)) plot(y ~ x, col=\"white\", main=\"A Law is Given\", yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, main=\"Data is Created\", xaxt='n', yaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, xaxt='n', yaxt='n', main=\"The Law is Estimated\") curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) A Law is Given Data is Created The Law is Estimated \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) \\(Y_i = E\\{Y_i\\} + \\epsilon_i\\) \\(\\hat{Y}_i = b_0 + b_1 X_i\\) The true line is the â€œlawâ€. The \\(Y_i\\) are created by adding \\(\\epsilon_i\\) to \\(E\\{Y_i\\}\\) where \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\). The law is estimated with \\(\\hat{Y}_i\\) which is given with lm(...). Click open the â€œCodeâ€ buttom below to the right to find code that runs a simulation demonstrating this â€œorder of creationâ€. ## Simulating Data from a Regression Model ## This R-chunk is meant to be played in your R Console. ## It allows you to explore how the various elements ## of the regression model combine together to \"create\" ## data and then use the data to \"re-create\" the line. set.seed(101) #Allows us to always get the same \"random\" sample #Change to a new number to get a new sample n <- 3 #set the sample size X_i <- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45. beta0 <- 3 #Our choice for the y-intercept. beta1 <- .1 #Our choice for the slope. sigma <- 12.5 #Our choice for the std. deviation of the error terms. epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data View(fabData) #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it. fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData. summary(fab.lm) #Summarize your model. plot(y ~ x, data=fabData) #Plot the data. abline(fab.lm) #Add the estimated regression line to your plot. # Now for something you can't do in real life... but since we created the data... abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). legend(\"topleft\", legend=c(\"True Line\", \"Estimated Line\"), lty=c(2,1), bty=\"n\") #Add a legend to your plot specifying which line is which. Interpreting the Model Parameters (Expand) \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted asâ€¦ The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model. If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\). The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\). It is often misunderstood to be the â€œaverage change in yâ€ or just â€œthe change in yâ€ but it is more correctly referred to as the â€œchange in the average yâ€. To better see this, consider the three graphics shown below. par(mfrow=c(1,3)) hist(mtcars$mpg, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Number of Vehicles\", xlab=\"Gas Mileage (mpg)\", col=\"skyblue\") boxplot(mpg ~ cyl, data=mtcars, border=\"skyblue\", boxwex=0.5, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Number of Cylinders of Engine (cyl)\") plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=\"skyblue\", main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Quarter Mile Time (qsec)\") abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=\"darkgray\") mtext(side=3, text=\"Automatic Transmissions Only (am==0)\", cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=\"gray\") The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09. The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg. The scatterplot on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value. Residuals and Errors (Expand) \\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true errorâ€¦ Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\). The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\). We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summaryâ€¦ Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\). \\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms. Keep in mind the idea that the errors \\(\\epsilon_i\\) â€œcreatedâ€ the data and that the residuals \\(r_i\\) are computed after using the data to â€œre-createâ€ the line. Residuals have many uses in regression analysis. They allow us to diagnose the regression assumptions, See the â€œAssumptionsâ€ section below for more details. estimate the regression relation, See the â€œEstimating the Model Parametersâ€ section below for more details. estimate the variance of the error terms, See the â€œEstimating the Model Varianceâ€ section below for more details. and assess the fit of the regression relation. See the â€œAssessing the Fit of a Regressionâ€ section below for more details. Assessing the Fit of a Regression (Expand) \\(R^2\\), SSTO, SSR, and SSEâ€¦ Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line. par(mfrow=c(1,3), mai=c(.1,.1,.5,.1)) set.seed(2) x <- runif(30,0,20) y1 <- 2 + 3.5*x + rnorm(30,0,2) y2 <- 2 + 3.5*x + rnorm(30,0,8) y3 <- 2 + 3.5*x + rnorm(30,0,27) plot(y1 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Excellent Fit\") abline(lm(y1 ~ x), col=\"gray\") plot(y2 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Good Fit\") abline(lm(y2 ~ x), col=\"gray\") plot(y3 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Poor Fit\") abline(lm(y3 ~ x), col=\"gray\") A common way to measure the fit of a regression is with correlation. While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\). (If you are a Math 325 student, just stick with correlation for now and skip on to the next section of this Explanation tab. If you are a Math 425 student, it is critical that you come to understand \\(R^2\\) deeply, so read on.) Before you can understand \\(R^2\\), you must understand three important â€œsums of squaresâ€. (Read more about sumsâ€¦) A sum is just a fancy word for adding things together. \\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand. So we use the symbol \\(\\Sigma\\) to denote the word â€œsumâ€. Further, we use a subscript \\(\\Sigma_{i=1}\\) to state what value the sum is beginning with, and a superscript \\(\\Sigma_{i=1}^6\\) to state the value we are ending at. This gives \\[ \\sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Test your knowledge, do you see why the answer is 6 to the sum below? \\[ \\sum_{i=1}^3 i = 6 \\] Computing sums in R is fairly easy. Type the following codes in your R Console. sum(1:6) #gives the answer of 21 sum(1:3) #gives the answer of 6 However, sums really become useful when used with a data set. Each row of a data set represents an â€œindividualâ€™sâ€ data. We can reference each individual with a row number. In the data below, individual 3, denoted by \\(i=3\\), has a speed of 7 and a dist of 4. pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3) Individual speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 To compute the sum of the speed column, use sum(speed). If we divided this sum by 6, we would get the mean of speed mean(speed). In fact, the two most used statistics mean(...) and sd(...) both use sums. Take a moment to review the formulas for mean and standard deviation. It is strongly recommended that you study the Explanation tab for both as well. Weâ€™ll wait. See you back here shortly. â€¦ Welcome back. Suppose we let X = speed and Y = dist. Then \\(X_3 = 7\\) and \\(Y_3 = 4\\) because we are accessing row 3 of both the \\(X\\) (or speed) column and \\(Y\\) (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, sum(speed) would be written mathematically as \\(\\sum_{i=1}^6 X_i\\) and sum(dist) would be written as \\(\\sum_{i=1}^6 Y_i\\). Sum of Squared Errors Sum of Squares Regression Total Sum of Squares \\(\\text{SSE} = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\) \\(\\text{SSR} = \\sum_{i=1}^n \\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\) \\(\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\\) Measures how much the residuals deviate from the line. Measures how much the regression line deviates from the average y-value. Measures how much the y-values deviate from the average y-value. Equals SSTO - SSR Equals SSTO - SSE Equals SSE + SSR sum( (Y - mylm$fit)^2 ) sum( (mylm$fit - mean(Y))^2 ) sum( (Y - mean(Y))^2 ) It is important to remember that SSE and SSR split up SSTO, so that \\[ \\text{SSTO} = \\text{SSE} + \\text{SSR} \\] This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works. The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called \\(R^2\\) (â€œr-squaredâ€). R-Squared (\\(R^2\\)) \\[ \\underbrace{R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}}_\\text{Interpretation: Proportion of variation in Y explained by the regression.} \\] The smallest \\(R^2\\) can be is zero, and the largest it can be is 1. This is because \\(SSR\\) must be between 0 and SSTO, inclusive. Residual Plots & Regression Assumptions (Expand) Residuals vs.Â fitted-values, Q-Q Plot of the residuals, and residuals vs.Â order plotsâ€¦ There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate. Each assumption is labeled in the regression equation below. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions. (Read moreâ€¦) As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not. Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3 The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(\\hat{Y}_i\\). The residuals are the \\(r_i\\). This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable. | Show Examples | (Read moreâ€¦) The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption. The linear relation is assumed to be satisfied if there are no apparent trends in the plot. The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values. The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance. set.seed(2) X <- rnorm(30,15,3) notLin <- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8)) notLin.lm <- lm(Y~X, data=notLin) set.seed(15) Lin <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) Lin.lm <- lm(Y~X, data=Lin) par(mfrow=c(3,3), mai=c(.25,.25,.25,.25), mgp=c(1,.75,0)) plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Not Linear\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(notLin.lm$fitted.values,notLin.lm$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Good: Linear, Constant Variance\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) set.seed(6) notCon <- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5)) notCon.lm <- lm(Y~X, data=notCon) LinO <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) LinO[1] <- LinO[1]^2 LinO.lm <- lm(Y~X, data=LinO) plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Unconstant Variance\", cex.main=0.95, yaxt='n', xaxt='n', col=\"firebrick\") polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) # plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Outliers\", cex.main=0.95) # abline(h=0) tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(Girth ~ Volume, data=trees[-31,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(Height ~ Volume, data=trees) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) abline(h=0) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals, f=.4) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(weight ~ repwt, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(weight ~ repht, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) abline(h=0) Q-Q Plot of the Residuals: Checks Assumption #2 The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption. | Show Examples | (Read moreâ€¦) There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot. Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) set.seed(123) tmp <- rnorm(100) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"skyblue\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Normal\", col=\"skyblue\") tmp <- Davis$weight qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Right-skewed\", breaks=15, col=\"firebrick\") par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- rbeta(100, 5,1) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Left-skewed\", breaks=seq(min(tmp),max(tmp), length.out=13), col=\"firebrick\") tmp <- rbeta(100,2,2) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Heavy-tailed\", col=\"firebrick\") Residuals versus Order Plot: Checks Assumption #5 When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated. | Show Examples | (Read moreâ€¦) Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Good: No Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: General Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ qsec, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: Interesting Patterns\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),]) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Bad: Obvious Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") Problems from Failed Assumptions There are various problems that can arise when certain of the regression assupmtions are not satisfied. Lack of Linearity When the linearity assumption is violated, pretty much everything we obtain from the regression summary is no longer meaningful. The y-intercept estimate can be drastically off from its actual true value. Important model information is lost by trying to use a simple slope term \\(\\beta_1\\) to describe the model with respect to \\(X\\). The residual standard error will be much higher than it otherwise would be because of curvature patterns in the data that the line cannot capture. Thus, R-squared will be lower than it otherwise should be. P-values can become non-significant, when in fact there is a strong pattern in the data, but that pattern just cannot be captured by a simple line. *Normality of the errors is often put into question as well when a simplified line is used to try to capture a more complicated curved model. The plot below demonstrate these difficulties. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 7.5 #True slope beta_2 <- -0.25 #True bend X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors Y_i <- beta_0 + beta_1*X_i + beta_2*X_i^2 + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Non-Linear Relation\") abline(mylm, col=\"gray\") #Add fitted line to plot curve(beta_0 + beta_1*x + beta_2*x^2, col=\"gray\", lty=2, add=TRUE) #Add True line to plot #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (True value:\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (True value:\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (True value:\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Non-normal Error Terms When the normality assumption for the error terms is violated, not all is lost. In fact, the estimate of the slope and intercept are still often fairly meaningful. However, it is unwise to put too much trust in the residual standard error as an estimate of the standard deviation \\(\\sigma\\). This is because the standard deviation in skewed distributions does not carry the same meaning it has in normal distributions. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rchisq(n, 1)*3 - 1 #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Unconstant Variance When variance of the error term changes across the regression, the regression approximates the â€œaverage varianceâ€ of the errors because the regression is still assuming the variance is constant across the regression. The estimates of the slope and intercept are still typically quite good, and can be used for interpretation. The residual standard error however should not be considered to be meaningful as it will be too large on one end of the regression and too small on the other end. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma + X_i) #normally distributed errors #with increasing variance Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Variance Varies (Non-Constant)\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 13.82 Slope 3.5 3.768 Sigma 2.5 13.02 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", round(mean(sigma + X_i), 2), \", mean)\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Normality Violated As silly as it sounds, if the only problem with the regression is the lack of normality of the error terms, it isnâ€™t all that big of a problem. Depending on how non-normal the residuals appear, there could be some skewing to the residual standard error, but otherwise, the slope and intercept are still interpretable and meaningful. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- runif(n, -sqrt(12*sigma^2)/2, sqrt(12*sigma^2)/2) #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 14.7 Slope 3.5 3.449 Sigma 2.5 2.317 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Independence Assumption Violated While the slope and intercept are often still meaningful when the independence assumption is violated, the residual standard error is unnecessarily large in this case. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, 2.5) + (1:n -n/2)*.5 #normal, but correlated errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Independence Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 16.47 Slope 3.5 3.296 Sigma 2.5 4.819 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Outliers Present While outliers do not violate any of the regression assumptions, they do pose substantial difficulties for the least squares regression estimates of the slope and intercept. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors epsilon_i[3] <- ifelse(X_i[3] < 10, runif(1,25,35), -runif(1,25,35)) #create outlier Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"An Outlier Present\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 13.18 Slope 3.5 3.447 Sigma 2.5 6.389 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) The material below this section is meant for Math 425 students only. Estimating the Model Parameters (Expand) How to get \\(b_0\\) and \\(b_1\\): least squares & maximum likelihoodâ€¦ There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical. The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas. Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 <- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\). When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\). Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i. \\label{exp} \\end{equation}\\] Least Squares{#leastSquares} To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\). \\[ Q = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2 \\] Then we use the function Q as if it were a function of \\(\\beta_0\\) and \\(\\beta_1\\). Ironically, the values of \\(Y\\) and \\(X\\) are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we donâ€™t know are the values of \\(\\beta_0\\) and \\(\\beta_1\\). This least squares applet is a good way to explore how various choices of the slope and intercept yield different values of the â€œsum of squared residualsâ€. But it turns out that there is one â€œbestâ€ choice of the slope and intercept that yields a â€œsmallestâ€ value of the â€œsum of squared residuals.â€ This best choice can actually be found using calculus by taking the partial derivatives of \\(Q\\) with respect to both \\(\\beta_0\\) and \\(\\beta_1\\). \\[ \\frac{\\partial Q}{\\partial \\beta_0} = -2\\sum (Y_i - \\beta_0 - \\beta_1X_i) \\] \\[ \\frac{\\partial Q}{\\partial \\beta_1} = -2\\sum X_i(Y_i-\\beta_0-\\beta_1X_i) \\] Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize \\(Q\\) for a given set of data. After all the calculations are completed we find the values of the parameter estimators \\(b_0\\) and \\(b_1\\) (of \\(\\beta_0\\) and \\(\\beta_1\\), respectively) are as stated previously. Maximum Likelihood{#mle} The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of \\(\\beta_0\\) and \\(\\beta_1\\) which minime the least squares \\(Q\\) function, we choose the values of \\(\\beta_0\\) and \\(\\beta_1\\) which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the \\(Y_i\\) for all observations \\(i=1,\\ldots,n\\). We can do this rather simply by using the assumption that the errors, \\(\\epsilon_i\\) are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if \\(f(Y_i)\\) denotes the probability density function for \\(Y_i\\), then the joint probability density for all \\(Y_i\\), \\(f(Y_1,\\ldots,Y_n)\\) is given by \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) \\] Since each \\(Y_i\\) is assumed to be normally distributed with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\) (see model (\\(\\ref{model}\\))) we have that \\[ f(Y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{Y_i-\\beta_0-\\beta_1X_i}{\\sigma}\\right)^2\\right]} \\] which provides the joint probability as \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] The likelihood function \\(L\\) is then given by consider the \\(Y_i\\) and \\(X_i\\) fixed and the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) as the variables in the function. \\[ L(\\beta_0,\\beta_1,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] Instead of taking partial derivatives of \\(L\\) directly (with respect to all parameters) we take the partial derivatives of the \\(\\log\\) of \\(L\\), which is easier to work with. In a similar, but more difficult calculation, to that of minimizing \\(Q\\), we obtain the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) which maximize the log of \\(L\\), and which therefore maximize \\(L\\). (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate \\(\\hat{\\sigma}^2\\) of \\(\\sigma^2\\). \\[ \\hat{\\sigma}^2 = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n} \\] Estimating the Model Variance (Expand) Estimating \\(\\sigma^2\\) with MSEâ€¦ As shown previously in the â€œEstimating Model Parametersâ€ section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation. Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\). This estimate turns out to be a biased estimator. This means that it is consistently wrong in its estimates of \\(\\sigma^2\\). If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong. This is bad. Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\). Without going into all the details, to â€œfixâ€ the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently. To find the correct degrees of freedom, we have to notice that the \\(\\hat{Y}_i\\) in the numerator of \\(\\widehat{\\sigma}^2\\) is defined by \\[\\begin{equation} \\widehat{Y}_i = b_0 + b_1X_i \\label{hatY} \\end{equation}\\] From this equation, we notice that two means, \\(\\bar{X}\\) and \\(\\bar{Y}\\), were estimated from the data in order to obtain \\(\\hat{Y}_i\\). (See the formulas for \\(b_0\\) and \\(b_1\\) above, and note how they use both \\(\\bar{X}\\) and \\(\\bar{Y}\\) in their calculation.) Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for \\(\\hat{\\sigma}^2\\) should be \\(n-2\\) instead of \\(n\\). Some incredibly long calculations will show that the â€œfixedâ€ estimator \\[\\begin{equation} s^2 = MSE = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n-2} \\quad \\text{(Unbiased Estimator of $\\sigma^2$)} \\end{equation}\\] is an unbiased estimator of \\(\\sigma^2\\). Here \\(MSE\\) stands for mean squared error, which is the most obvious name for a formula that squares the errors \\(Y_i-\\hat{Y}_i\\) then adds them up and divides by their degrees of freedom. Similarly, we call the numerator \\(\\sum(Y_i-\\hat{Y}_i)^2\\) the sum of the squared errors, denoted by \\(SSE\\). It is also important to note that the errors are often denoted by \\(r_i = Y_i-\\hat{Y}_i\\), the residuals. Putting this all together we get the following equivalent statements for \\(MSE\\). \\[\\begin{equation} s^2 = MSE = \\frac{SSE}{n-2} = \\frac{\\sum(Y_i-\\widehat{Y}_i)^2}{n-2} = \\frac{\\sum r_i^2}{n-2} \\end{equation}\\] As a final note, even though the expected value \\(E\\{MSE\\} = \\sigma^2\\), which shows \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\), it unfortunately isnâ€™t true that \\(\\sqrt{MSE}\\) is an unbiased estimator of \\(\\sigma\\). This presents a few problems later on, but these are minimal enough that we can overlook the issue and move forward. Transformations (Expand) \\(Y'\\), \\(X'\\), and returning to the original spaceâ€¦ Y transformations are denoted by y-prime, written \\(Y'\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\). \\[ Y' = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y' = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y' = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y' = \\log(Y)\\) lm(log(Y) ~ X) 0.25 \\(Y' = \\sqrt(\\sqrt(Y))\\) lm(sqrt(sqrt(Y)) ~ X) 0.5 \\(Y' = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y' = Y\\) lm(Y ~ X) 2 \\(Y' = Y^2\\) lm(Y^2 ~ X) Using â€œmaximum-likelihoodâ€ estimation, the Box-Cox procedure can actually automatically detect the â€œoptimalâ€ value of \\(\\lambda\\) to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise. Scatterplot Recognition Box-Cox Suggestion An Example Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\). set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Ya <- 1/sqrt(Y) #1/Y^2 Lam = -2 Yb <- 1/Y #1/Y Lam = -1 Yc <- exp(.02*Y) #log(Y) Lam = 0 Yd <- Y^2 #sqrt(Y) Lam = 0.5 Ye <- Y #Y Lam = 1 Yf <- sqrt(Y) #Y^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Ya ~ X, main=expression(paste(\"Use \", lambda == -2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ya^-2 ~ X)) curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yb ~ X, main=expression(paste(\"Use \", lambda == -1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yb^-1 ~ X)) curve(1/(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yc ~ X, main=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(log(Yc) ~ X)) curve(exp(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yd ~ X, main=expression(paste(\"Use \", lambda == 0.5)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(sqrt(Yd) ~ X)) curve((b[1] + b[2]*x)^2, add=TRUE, col=\"green\", lwd=2) plot(Ye ~ X, main=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ye ~ X)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yf ~ X, main=expression(paste(\"Use \", lambda == 2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yf^2 ~ X)) curve(sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try. par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0)) boxCox(lm(Ya ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -2)), line=.5) boxCox(lm(Yb ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -1)), line=.5) boxCox(lm(Yc ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), line=.5) boxCox(lm(Yd ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0.5)), line=.5) boxCox(lm(Ye ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), line=.5) boxCox(lm(Yf ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 2)), line=.5) An Example Suppose we were running a simple linear regression on the cars dataset. This would be done with the code cars.lm <- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesnâ€™t quite fit the data as well as we would hope. Instead, the data looks a little curved. cars.lm <-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") abline(cars.lm, col=\"gray\") Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful. library(car) boxCox(cars.lm) The output from the boxCox(...) function looks as follows. This plot tells use to use the \\(\\lambda = 0.5\\) transformation, so that \\(Y' = Y^0.5 = \\sqrt{Y}\\). (To see this yourself, click on the â€œBox-Cox Suggestionâ€ tab above, as well as on the â€œScatterplot Recognitionâ€ tab.) Now, a transformation regression is performed using sqrt(Y) in place of Y as follows: cars.lm.t <- lm(sqrt(dist) ~ speed, data=cars) summary(cars.lm.t) Â  Estimate Std. Error t value Pr(> (Intercept) 1.277 0.4844 2.636 0.01126 speed 0.3224 0.02978 10.83 1.773e-14 Then, \\[ \\widehat{Y}_i' = 1.277 + 0.3224 X_i \\] And replacing \\(\\hat{Y}_i' = \\sqrt{\\hat{Y}_i}\\) we have \\[ \\sqrt{\\widehat{Y}_i} = 1.277 + 0.3224 X_i \\] Solving for \\(\\hat{Y}_i\\) gives \\[ \\widehat{Y}_i = (1.277 + 0.3224 X_i)^2 \\] Which, using curve((1.277 + 0.3224*x)^2, add=TRUE) (see code for details) looks like this: plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") curve( (1.277 + 0.3224*x)^2, add=TRUE, col=\"firebrick\") X-Transformations X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them. The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated. The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation. set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Xa <- 1/sqrt(X) #1/X^2 Lam = -2 Xb <- 1/X #1/X Lam = -1 Xc <- exp(.02*X) #log(X) Lam = 0 Xd <- X^2 #sqrt(X) Lam = 0.5 Xe <- X #X Lam = 1 Xf <- sqrt(X) #X^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Y ~ Xa, main=expression(paste(\"Use \", X*minute == X^-2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xa^-2))) curve(b[1] + b[2]*x^-2, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xb, main=expression(paste(\"Use \", X*minute == X^-1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xb^-1))) curve(b[1] + b[2]*x^-1, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xc, main=expression(paste(\"Use \", X*minute == log(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ log(Xc))) curve(b[1] + b[2]*log(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xd, main=expression(paste(\"Use \", X*minute == sqrt(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ sqrt(Xd))) curve(b[1] + b[2]*sqrt(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xe, main=expression(paste(\"Use \", X*minute == X, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ Xe)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xf, main=expression(paste(\"Use \", X*minute == X^2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xf^2))) curve(b[1] + b[2]*x^2, add=TRUE, col=\"green\", lwd=2) Inference for the Model Parameters (Expand) t test formulas, sampling distributions, confidence intervals, and F testsâ€¦ When fitting the regression model given by the equation \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] to a sample of data, we typically test hypotheses about the parameters \\(\\beta_0\\), \\(\\beta_1\\), or both. Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_0 - \\overbrace{0}^\\text{a number}}{s_{b_0}}\\] This is the formula for the test statistic. It measures how far the estimated y-intercept \\(b_0\\) is from the null hypothesis for \\(\\beta_0\\) in units of â€œstandard errors of \\(b_0\\)â€. Thus the division by \\(s_{b_0}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0, it could be any number. \\(H_0: \\beta_1 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_1\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_1 - \\overbrace{0}^\\text{a number}}{s_{b_1}}\\] This is the formula for the test statistic. It measures how far the estimated slope \\(b_1\\) is from the null hypothesis for \\(\\beta_1\\) in units of â€œstandard errors of \\(b_1\\)â€. Thus the division by \\(s_{b_1}\\). Though the hypothesized value of \\(\\beta_1\\) is typically 0, it could be any number. Left-tailed p-value = pt(-abs(tvalue), degrees of freedom). Double it to get the two-sided p-value. In R, these values correspond to the output summary of an lm as follows. (Show Example) Consider the cars data in R. Suppose we used the regression model given by \\[ \\underbrace{Y_i}_\\text{Feet to Stop} = \\beta_0 + \\beta_1 \\underbrace{X_{i}}_\\text{mph} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\quad \\sim N(0,\\sigma^2) \\] to model the feet a vehicle (from the 1920â€™s) takes to stop when traveling at a certain speed (in miles per hour, mph) prior to stopping. When the regression is performed and summarized in R, it is always testing the following two hypotheses: \\[ H_0: \\beta_0 = 0 \\quad\\quad H_0: \\beta_1 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\quad\\quad H_a: \\beta_1 \\neq 0 \\] To perform the test of these hypotheses for the regression stated above, we would run the following codes in R. cars.lm <- lm(dist ~ speed, data=cars) pander(summary(cars.lm)$coefficients) These would produce summary output like the following, but the following output has been labeled with the math notation corresponding to each value. Letâ€™s emphasize what is happening in this summary output table. First, here is how the â€œt valueâ€ is calculated for the â€œ(Intercept)â€ in the summary table above. \\[ t = \\frac{b_0-0}{s_{b_0}} = \\frac{-17.58 - 0}{6.758} = -2.601 \\] Second, here is a visual representation of how the P-value, the â€œPr(>|t|)â€ as it is called in the summary table above, is calculated for this test statistic. (Click the graph to view an interactive applet showing this calculation.) Notice both ends of the t-distribution are being shaded to compute the P-value because the alternative hypothesis was \\(H_a: \\beta_0 \\neq 0\\). To compute the P-value in R, we use the â€œpercentile function for the t-distributionâ€ called pt( ). This function requires two things, the t-value and the degrees of freedom, in our case pt(-2.601, 48). Note the degrees of freedom (df) are 48 because the sample size is \\(n=50\\) and there are two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) in our regression model. Running this code in R gives: pt(-2.601, 48) = 0.00616 However, note that this value is only half of the actual P-value of 0.0123. To get the â€œtwo-sidedâ€ P-value (note that our alternative hypothesis used a \\(\\neq\\) symbol) we need to double this left-tailed P-value. 2*pt(-2.601, 48)) = 0.0123 Finally, note that the same procedure can be used to test hypotheses that use a value other than 0 in the null and alternative. For example, to test the hypotheses: \\[ H_0: \\beta_1 = 3 \\\\ H_a: \\beta_1 \\neq 3 \\] Use the t-formula \\[ t = \\frac{b_1 - 3}{s_{b_1}} = \\frac{\\overbrace{3.932}^{b_1} - \\overbrace{3}^{H_0}}{\\underbrace{0.4155}_{s_{b_1}}} = 2.243 \\] then the P-value is calculated in R by 2*pt(-abs(2.243), 48) = 0.0295495 To obtain confidence intervals in R use confint(mylm). Confidence Interval Formula Standard Error \\(\\beta_0\\) \\(b_0 \\pm\\) \\(t^*\\) This is called the â€œcritical valueâ€ and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\). Use qt(0.975, df) to get \\(t*\\) in R. \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_0\\). \\(s_{b_0}\\) The standard error of \\(b_0\\), denoted by \\(s_{b_0}\\) is provided in the regression summary output under the column header called â€œStd. Errorâ€ for the â€œ(Intercept)â€ row of the output. It is calculated using the formula shown below. \\[s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\] This is called the â€œestimated variance of \\(b_0\\)â€. Taking the square root of this number gives the â€œstandard error of \\(b_0\\)â€. \\(\\beta_1\\) \\(b_1 \\pm\\) \\(t^*\\) This is called the â€œcritical valueâ€ and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\) (sample size - number of parameters in the regression model). Use qt(0.975, df) to get \\(t*\\) in R \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_1\\). \\(s_{b_1}\\) The standard error of \\(b_1\\), denoted by \\(s_{b_1}\\) is provided in the regression summary output under the column header called â€œStd. It is calculated using the formula shown below. \\[s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\] This is called the â€œestimated variance of \\(b_1\\)â€. Taking the square root of this number gives the â€œstandard error of \\(b_1\\)â€. To be more exact, the types of inference we are interested in are the following. Determine if there is evidence of a meaningful linear relationship in the data. If \\(\\beta_1 = 0\\), then there is no relation between \\(X\\) and \\(E\\{Y\\}\\). Hence we might be interested in testing the hypotheses \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) is some hypothesized number. To provide a confidence interval for the true value of \\(\\beta_1\\). Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the sampling distribution of the estimate \\(b_1\\) of the parameter \\(\\beta_1\\). And, while we are at it, we may as well come to understand the sampling distribution of the estimate \\(b_0\\) of the parameter \\(\\beta_0\\). Review sampling distributions from Math 221. Since \\(b_1\\) is an estimate, it will vary from sample to sample, even though the truth, \\(\\beta_1\\), remains fixed. (The same holds for \\(b_0\\) and \\(\\beta_0\\).) It turns out that the sampling distribution of \\(b_1\\) (where the \\(X\\) values remain fixed from study to study) is normal with mean and variance: \\[ \\mu_{b_1} = \\beta_1 \\] \\[ \\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} \\] ## Simulation to Show relationship between Standard Errors ##----------------------------------------------- ## Edit anything in this area... n <- 100 #sample size Xstart <- 30 #lower-bound for x-axis Xstop <- 100 #upper-bound for x-axis beta_0 <- 2 #choice of true y-intercept beta_1 <- 3.5 #choice of true slope sigma <- 13.8 #choice of st. deviation of error terms ## End of Editable area. ##----------------------------------------------- # Create X, which will be used in the next R-chunk. X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) ## After playing this chunk, play the next chunk as well. To see that this is true, consider the regression model with values specified for each parameter as follows. \\[ Y_i = \\overbrace{\\beta_0}^{2} + \\overbrace{\\beta_1}^{3.5} X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\overbrace{\\sigma^2}^{\\sigma=13.8}) \\] Using the equations above for \\(\\mu_{b_1}\\) and \\(\\sigma^2_{b_1}\\) we obtain that the mean of the sampling distribution of \\(b_1\\) will be \\(\\mu_{b_1} = \\beta_1 = 3.5\\) Further, we see that the variance of the sampling distribution of \\(b_1\\) will be \\(\\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} = \\frac{13.8^2}{4.25\\times 10^{4}}\\) Taking the square root of the variance, the standard deviation of the sampling distribution of \\(b_1\\) will be \\(\\sigma_{b_1} = 0.067\\). Thatâ€™s very nice. But to really believe it, letâ€™s run a simulation ourselves. The â€œCodeâ€ below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of \\(b_1\\) and \\(b_0\\) for that regression. After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of \\(b_0\\) is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of \\(b_1\\) is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of \\(\\beta_0\\) or \\(\\beta_1\\), respectively. Also, the â€œStd. Errorâ€ of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of \\(\\sigma_{b_0}\\) and \\(\\sigma_{b_1}\\), respectively. N <- 5000 #number of times to pull a random sample storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N) for (i in 1:N){ Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model mylm <- lm(Y ~ X) storage_b0[i] <- coef(mylm)[1] storage_b1[i] <- coef(mylm)[2] storage_rmse[i] <- summary(mylm)$sigma } layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3)) Ystart <- 0 #min(0,min(Y)) Ystop <- 500 #max(max(Y), 0) Yrange <- Ystop - Ystart plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), ylim=c(Ystart, Ystop), pch=16, col=\"gray\", main=\"Regression Lines from many Samples (gray lines) \\n Plus Residual Standard Deviation Lines (green lines)\") text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1) text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1) text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1) for (i in 1:N){ abline(storage_b0[i], storage_b1[i], col=\"darkgray\") } abline(beta_0, beta_1, col=\"green\", lwd=3) abline(beta_0+sigma, beta_1, col=\"green\", lwd=2) abline(beta_0-sigma, beta_1, col=\"green\", lwd=2) abline(beta_0+2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0-2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0+3*sigma, beta_1, col=\"green\", lwd=.5) abline(beta_0-3*sigma, beta_1, col=\"green\", lwd=.5) par(mai=c(1,.6,.5,.01)) addnorm <- function(m,s, col=\"firebrick\"){ curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2) lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col) lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col) lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col) lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col) lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col) lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col) lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col) legend(\"topleft\", legend=paste(\"Std. Error = \", round(s,3)), cex=0.7, bty=\"n\") } h0 <- hist(storage_b0, col=\"skyblue3\", main=\"Sampling Distribution\\n Y-intercept\", xlab=expression(paste(\"Estimates of \", beta[0], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m0 <- mean(storage_b0) s0 <- sd(storage_b0) addnorm(m0,s0, col=\"green\") h1 <- hist(storage_b1, col=\"skyblue3\", main=\"Sampling Distribution\\n Slope\", xlab=expression(paste(\"Estimates of \", beta[1], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m1 <- mean(storage_b1) s1 <- sd(storage_b1) addnorm(m1,s1, col=\"green\") t Tests{#tTests} Using the information above about the sampling distributions of \\(b_1\\) and \\(b_0\\), an immediate choice of statistical test to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) can be zero, or any other value, is a t test given by \\[ t = \\frac{b_1 - \\beta_{10}}{s_{b_1}} \\] where \\(s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\). (You may want to review the section â€œEstimating the Model Varianceâ€ of this file to know where MSE came from.) With quite a bit of work it has been shown that \\(t\\) is distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. The nearly identical test statistic for testing \\[ H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_0 \\neq \\beta_{00} \\] is given by \\[ t = \\frac{b_0 - \\beta_{00}}{s_{b_0}} \\] where \\(s^2_{b_0} = MSE\\left[\\frac{1}{n}+\\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\). This version of \\(t\\) has also been shown to be distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. Confidence Intervals Creating a confidence interval for either \\(\\beta_1\\) or \\(\\beta_0\\) follows immediately from these results using the formulas \\[ b_1 \\pm t^*_{n-2}\\cdot s_{b_1} \\] \\[ b_0 \\pm t^*_{n-2}\\cdot s_{b_0} \\] where \\(t^*_{n-2}\\) is the critical value from a t distribution with \\(n-2\\) degrees of freedom corresponding to the chosen confidence level. F tests{#Ftests} Another way to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\quad\\quad \\text{or} \\quad\\quad H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\quad\\quad \\ \\ \\quad \\quad H_a: \\beta_0 \\neq \\beta_{00} \\] is with an \\(F\\) Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an \\(F\\) test is very general and can be used in many places that a t test cannot. In its most general form, the \\(F\\) test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that \\(H_0:\\beta_1=0\\) against the alternative that \\(H_a: \\beta_1\\neq 0\\), we are essentially comparing two models against each other. If \\(\\beta_1=0\\), then the corresponding model would be \\(E\\{Y_i\\} = \\beta_0\\). If \\(\\beta_1\\neq0\\), then the model remains \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the \\(F\\) Test, that the null model (reduced model) have fewer â€œfreeâ€ parameters than the alternative model (full model). To demonstrate what we mean by â€œfreeâ€ parameters, consider the following example. Say we wanted to test the hypothesis that \\(H_0:\\beta_1 = 2.5\\) against the alternative that \\(\\beta_1\\neq2.5\\). Then the null, or reduced model, would be \\(E\\{Y_i\\}=\\beta_0+2.5X_i\\). The alternative, or full model, would be \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). Thus, the null (reduced) model contains only one â€œfreeâ€ parameter because \\(\\beta_1\\) has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two â€œfreeâ€ parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model. Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing \\(H_0: \\beta_1=0\\) against \\(H_a:\\beta_1\\neq0\\) we have the partition \\[ \\underbrace{Y_i-\\bar{Y}}_{Total} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{Regression} + \\underbrace{Y_i-\\hat{Y}_i}_{Error} \\] The reason we use \\(\\bar{Y}\\) for the null model is that \\(\\bar{Y}\\) is the unbiased estimator of \\(\\beta_0\\) for the null model, \\(E\\{Y_i\\} = \\beta_0\\). Thus we would compute the following sums of squares: \\[ SSTO = \\sum(Y_i-\\bar{Y})^2 \\] \\[ SSR = \\sum(\\hat{Y}_i-\\bar{Y})^2 \\] \\[ SSE = \\sum(Y_i-\\hat{Y}_i)^2 \\] and note that \\(SSTO = SSR + SSE\\). Important to note is that \\(SSTO\\) uses the difference between the observations \\(Y_i\\) and the null (reduced) model. The \\(SSR\\) uses the diffences between the alternative (full) and null (reduced) model. The \\(SSE\\) uses the differences between the observations \\(Y_i\\) and the alternative (full) model. From these we could set up a General \\(F\\) table of the form Â  Sum Sq Df Mean Sq F Value Model Error \\(SSR\\) \\(df_R-df_F\\) \\(\\frac{SSR}{df_R-df_F}\\) \\(\\frac{SSR}{df_R-df_F}\\cdot\\frac{df_F}{SSE}\\) Residual Error \\(SSE\\) \\(df_F\\) \\(\\frac{SSE}{df_F}\\) Total Error \\(SSTO\\) \\(df_R\\) Prediction and Confidence Intervals for \\(\\hat{Y}_h\\) (Expand) predict(â€¦, interval=â€œpredictionâ€)â€¦ It is a common mistake to assume that averages (means) describe individuals. They do not. So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line. Interval R Code Math Equation When to Use Prediction predict(..., interval=\"prediction\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\text{Pred}\\ Y}\\) Predict an individualâ€™s value. Confidence predict(..., interval=\"confidence\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\hat{Y}}\\) Estimate location of the mean y-value. predict(mylm, data.frame(XvarName = number), interval=...) For example, consider this graph. Then click here to read about the graph. Notice the three dots above 15 mph in the graph. Each of these dots show a car that was going 15 mph when it applied the brakes. However, stopping distances of the three individual cars differ with one at 20 feet, one at 26 feet and one at 54 feet. The regression line represents the average stopping distance of cars. In this case, cars going 15 mph are estimated to have an average stopping distance of about 40 feet, as shown by the line. But individual vehicles, all going the same speed of 15 mph, varied from stopping distances of 20 feet up to 54 feet! So, to predict that a car going 15 mph will take 41.4 feet to stop, doesnâ€™t tell the whole story. Far more revealing is the complete statement, â€œCars going 15 mph are predicted to take anywhere from 10.2 to 72.6 feet to stop, with an average stopping distance of 41.4 feet.â€ This is called the â€œprediction intervalâ€ and is shown in the graph in blue. It is obtained in R with the codes: cars.lm <- lm(dist ~ speed, data=cars) predict(cars.lm, data.frame(speed=15), interval=\"prediction\") cars.lm <- lm(dist ~ speed, data=cars) pander(predict(cars.lm, data.frame(speed=15), interval=\"prediction\")) fit lwr upr 41.41 10.17 72.64 plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=\"firebrick2\", cex=1.5) cars.lm <- lm(dist ~ speed, data=cars) abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3)) abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2)) abline(v=15, lty=2, col=\"firebrick\") preds <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12) lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8)) lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8)) Now, for the details behind prediction intervals and confidence intervals. Letâ€™s begin by recalling some details (from the section â€œInference for the Model Parametersâ€) about the standard error of the y-intercept, \\(b_0\\). Recall that the y-intercept is the average y-value for the given x-value of \\(x=0\\). Recall further that the formula for the standard error of \\(b_0\\) is given by the formula \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] If we wanted to be more exact with this formula, we would write it as \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{(0-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice the addition of \\((0 - \\bar{X})^2\\) instead of just \\(\\bar{X}^2\\) in the numerator of the right-most part of the equation? This more complete statement obviously would reduce to just \\(\\bar{X}^2\\), but that is only because \\(X=0\\) when we are working with the y-intercept, \\(b_0\\). We could be working with other values of \\(X\\) than just zero. Letâ€™s take a quick detour and talk notation for a second. Typically, \\(X_i\\) and \\(Y_i\\) are used to denote the x-value and y-value of points that are contained in our data set. When we want to reference a point that wasnâ€™t within our original data set, we use the notation \\(X_h\\) and \\(Y_h\\). (The letter h is close to i, but different from i, so why not. There is really no other reason to use h.) Thus, \\(Y_h\\) is the y-value for the \\(X_h\\) x-value, neither of which were included in our original regression of \\(X_i\\)â€™s and \\(Y_i\\)â€™s. Now, back to the previous discussion. If \\(X_h = 0\\), then \\(\\hat{Y}_h\\) is the y-intercept, so \\(\\hat{Y}_h = b_0\\) when \\(X_h=0\\). So, we could write, \\[ s^2_{\\hat{Y}_h} = MSE\\left[\\frac{1}{n} + \\frac{(X_h-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice how the \\(b_0\\) in \\(s_{b_0}\\) was replaced with \\(\\hat{Y}_h\\) to get \\(s_{\\hat{Y}_h}\\) and the 0 in \\((0 - \\bar{X})^2\\) was replaced with \\(X_h\\) to get \\((X_h - \\bar{X})^2\\)? Interesting. We now have a formula that would give us the standard error of \\(\\hat{Y}_h\\) for any \\(X_h\\) value, not just \\(X_h = 0\\), or the y-intercept, \\(b_0\\). That is fantastic. It would look like this if plotted. Notice how the gray region is showing the standard error for each \\(\\hat{Y}_h\\) value? (It is technically showing the confidence interval for \\(E\\{Y_h\\}\\) at every possible \\(X_h\\) value, but that is just \\(\\hat{Y}_h \\pm t^* \\cdot s_{\\hat{Y}_h}\\).) ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + theme_bw() ## `geom_smooth()` using formula = 'y ~ x' Confidence Interval for \\(\\hat{Y}_h\\) \\[ \\hat{Y}_h \\pm t^* s_{\\hat{Y}_h} \\quad \\text{where} \\ s_{\\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] The confidence interval is a wonderful tool for estimating \\(E\\{Y_h\\}\\), the â€œtrueâ€ average y-value for a given x-value of \\(X_h\\). However, it is not valuable for predicting an individual dot, or \\(Y_h\\) value. Notice how few of the dots of the regression are actually contained within the confidence interval band in the plot? The confidence interval does not really predict where the dots will land, just where the average y-value is located for each x-value. Remember the 68-95-99.7 Rule of the normal distribution? If not, here is a link back to that concept in the Math 221 textbook. This rule states that roughly 95% of data, when normally distributed, will be between \\(z=-2\\) and \\(z=2\\) standard deviations from the mean. So, is going two â€œresidual standard errorsâ€ to both sides of the regression line enough to capture 95% of the data? The answer is, not quite. The reason for this is because our knowledge of where the true mean lies is uncertain. (Notice the confidence interval band shown in the plot.) However, adding two standard errors to the edges of the confidence band would get us in the right place. In other words, there are two sources of variability at play here, (1) our uncertaintity in where the regression line is sitting, and (2) the natural variability of the data points around the line. Thus, the â€œprediction intervalâ€ requires accounting for both of these sources of variability to produce the following equation. Prediction Interval for \\(Y_h\\) \\[ \\hat{Y}_h \\pm t^* s_{Pred \\hat{Y}_h} \\quad \\text{where} \\ s_{Pred \\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + 1 + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] This formula provides a useful band for identifying a region where we are 95% confident that a new observation for \\(Y_h\\) will land, given the value of \\(X_h\\). It looks as follows. Notice the prediction interval is much wider than the confidence interval. This is because data varies far more than do means. Prediction is for where the individual data points will land, confidence is for where the mean will land. cars.lm <- lm(dist ~ speed, data=cars) predy <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + geom_segment(aes(x=15, xend=15, y=predy[2], yend=predy[3]), lwd=4, color=rgb(.5,.7,.5,.01)) + geom_point(aes(x=15, y=predy[1]), cex=2, color=\"skyblue\", pch=15) + theme_bw() Lowess (and Loess) Curves (Expand) A non-parametric approach to estimating \\(E\\{Y_i\\}\\)â€¦ Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value. Using Base R # Select the relevant columns and remove NA values air2 <- airquality %>% dplyr::select(Temp, Ozone) %>% # Use dplyr's select function explicitly drop_na() # Drop rows with NA values in Temp or Ozone plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") lines(lowess(air2$Ozone, air2$Temp), col=\"firebrick\") Using ggplot2 # Your dataset (air2) should have been created earlier, ensuring no NAs air2 <- na.omit(airquality[c(\"Temp\", \"Ozone\")]) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=\"darkgray\") + # Add points with dark gray color geom_smooth(se=FALSE, method=\"loess\", method.args = list(degree=1)) + # Lowess curve, no standard error shading theme_bw() # Black and white theme ## `geom_smooth()` using formula = 'y ~ x' # Optionally, allow for predictions as well as the graph: # Uncomment the following lines if you want predictions as well: # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fitted, x=Ozone)) # Use fitted values from loess Advantages Disadvantages Quick. Good at ignoring outliers. Good at capturing the general pattern in the data. Good for making predictions within the scope of the data. No mathematical model. Not interpretable. No p-values. No adjusted R-squared. How it Works The Lowess curve localizes the regression model to a â€œneighborhoodâ€ of points, and then joins these localized regressions together into a smooth line. It minimizes the effect of outliers, and letâ€™s the data â€œspeak for itselfâ€. As a downside, it is not interpretable, and has no final way to write the model mathematically. All the same, it is a very powerful tool for identifying an appropriate model, or verifying the fit of a model, or making predictions when no reasonable model does an adequate job. Study this graphic and the explanations below to learn how it works. Recommendation: run the code in this â€œCodeâ€ chunk to the right in your Console, and flip through the resulting graphics. X <- cars$speed Y <- cars$dist X <- X[!is.na(X) & !is.na(Y)] Y <- Y[!is.na(X) & !is.na(Y)] f <- 1/2 n <- length(X) lfit <- rep(NA,n) for (xh in 1:n){ xdists <- X - X[xh] nn <- floor(n*f) r <- sort(abs(xdists))[nn] xdists.nbrhd <- which(abs(xdists) < r) w <- rep(0, length(xdists)) w[xdists.nbrhd] <- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w), col=rgb(.2,.2,.2,.3), cex=1.5, yaxt='n', xaxt='n', xlab=\"\", ylab=\"\") points(Y[xh] ~ X[xh], pch=16, col=\"orange\") lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2)) cat(\"\\n\\n\") readline(prompt=paste0(\"Center point is point #\", xh, \"... Press [enter] to continue...\")) MADnotThereYet <- TRUE count <- 0 while(MADnotThereYet){ readline(prompt=paste0(\"\\n Adjusting line to account for outliers in the y-direction... Press [enter] to continue...\")) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"wheat\", add=TRUE) MAD <- median(abs(lmc$res)) resm <- lmc$res/(6*MAD) resm[resm>1] <- 1 bisq <- (1-resm^2)^2 w <- w*bisq obs <- coef(lmc) lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) count <- count + 1 if ( (sum(abs(obs-lmc$coef))<.1) | (count > 3)) MADnotThereYet <- FALSE } curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"green\", add=TRUE) points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=\"green\") readline(prompt=paste0(\"\\n Use final line to get fitted value for this point... Press [enter] to continue to next point...\")) lfit[xh] <- predict(lmc, data.frame(X=X[xh])) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") if (xh == n){ readline(prompt=paste0(\"\\n Press [enter] to see actual Lowess curve...\")) lines(lowess(X,Y, f=f), col=\"firebrick\") legend(\"topleft\", bty=\"n\", legend=\"Actual lowess Curve using lowess(...)\", col=\"firebrick\", lty=1) } } Select a fraction of the data to use for the â€œneighborhoodâ€ of points (shown in blue in the graph above). The lowess function in R uses â€œf=2/3â€ and the loess function uses â€œspan=0.75â€ for this value, which selects the nearest two-thirds or 75% of the data, respectively, depending on which function you use. For this example, we set the fraction of points at 50%. Both functions can be set to whatever you want. Pick any point in the regression, eventually selecting all points one at a time. The selected point becomes the â€œcenterâ€ of a â€œneighborhoodâ€ of points surrounding it. In this example, the center point is in orange, and the neighboring points are in blue. Use the points within the neighborhood to fit a regression line. However, make the regression depend most on points closest to â€œcenterâ€ and least on points furthest from â€œcenter.â€ This is called a weighted regression. Weights are decided according to what is called the tricubic weight function, so that the weight \\(w\\) given to point \\(j\\) of the neighborhood of points is defined by \\[ w_j = \\left(1- \\left( \\frac{|X_c - X_j|}{\\max_k |X_c - X_k|}\\right)^3\\right)^3 \\] where \\(X_c\\) is the x-value of the â€œcenterâ€ dot and \\(X_j\\) is the x-value of any other dot in the neighborhood. The fitted-value of \\(\\hat{Y}_c\\) is obtained for the center point \\(X_c\\) of the current regression. This point is used as the Lowess (or Loess) curveâ€™s value at that particular x-value. Well, almost. Itâ€™s a first guess at where this value will end up, but thereâ€™s a little more to the algorithm before we are done. Initial guesses for each of these fitted values are obtained for each point in the regression. Now each local regression for each neighborhood is re-run a few times in such a way the the effect of outliers is minimized. The final line for each neighborhood is obtained by the following steps. Compute all residuals for points in the neighborhood of the current regression, denoted by \\(r_i\\). Then compute the MAD, median absolute deviation, of the residuals \\(MAD = \\text{median} (|r_1|, |r_2|, \\ldots)\\). Divide all residuals by 6 times the MAD: \\(u_i = r_i/(6\\cdot MAD)\\) (If \\(r_i > 6\\cdot MAD\\) then set \\(u_i = 0\\).) Compute what are called bisquare weights using the formula: \\(b_i = (1 - u_i^2)^2\\) Perform a regression using the weights \\(w_i = w_i b_i\\) Repeat the above process with the new weights \\(w_i\\) until the weights stop changing very much. The final fitted values for each \\(X\\)-value in the regression are obtained from the final regression line for each neighborhood. These fitted values make up the Lowess (or loess) curve. Note that the default of the loess function in R is to use quadratic regressions in each neighborhood instead of linear regressions. This can be controlled with the loess option of â€œdegree=2â€ (quadratic fits) or â€œdegree = 1â€. In the lowess function only a linear regression in each neighborhood is allowed. Examples: bodyweight, cars Quantitative Y | Multiple X Graphics Y is a single quantitative variable of interest, like height. While we could use an X1 of â€œshoe-sizeâ€ to explain height, we might also want to use a second x-variable, X2, like â€œgenderâ€ to help explain height. Further x-variables could also be used. Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a studentâ€™s final exam grade? How do factors like age, weight, exercise level, and diet impact a personâ€™s blood pressure? Examples: Is one Navy base more prone to heatwaves than the other? The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH) Tests Multiple Linear Regression Multiple regression is a way to understand how different factors affect something weâ€™re interested in. Itâ€™s like having more than one clue to solve a mystery. Hereâ€™s what you need to know: We can use several different pieces of information (called variables) to predict an outcome. There are many ways to combine these pieces of information. We start with simple combinations and build up to more complex ones, kind of like building with Lego blocks. This helps us make better predictions and understand complex situations more clearly. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model is a bit like using the same ingredient twice in a recipe, but in different ways. It uses the same factor (\\(X_i^2\\)) in two different forms: Once in its regular form (\\(\\beta_1 X_i\\)) Once squared (\\(\\beta_2 X_i^2\\)) This squared part is what we call the â€œquadraticâ€ term. It helps us describe relationships that arenâ€™t just straight lines, but have curves to them. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and â€œsteepnessâ€ of the Model: negative values face down, positive values face up; large values imply â€œsteeperâ€ parabolas and low values imply â€œflatterâ€ parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)â€™s explanation. An Example Using the airquality data set, we run the following â€œquadraticâ€ regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our â€œquadraticâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. Temp Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These â€œEstimatesâ€ can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(Month^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of Month from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, ) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will â€œopen downâ€ (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be â€œmonth zero.â€ Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the â€œcubicâ€ term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the â€œbase slope coefficientâ€ and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following â€œcubicâ€ regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our â€œcubicâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. uptake Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^3) \\(X_{i}^3\\), where the function I(â€¦) protects the cubing of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will â€œopen upâ€. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The â€œtwo-linesâ€ model is a way to compare two different groups in statistics using numbers. Hereâ€™s how it works: It uses two types of numbers: A regular number that can be any value (\\(X_{1i}\\)) A special number thatâ€™s either 0 or 1 (called an â€œindicator variableâ€, \\(X_{2i}\\)) The special number (\\(X_{2i}\\)) helps us turn group information (like â€œGroup Aâ€ or â€œGroup Bâ€) into numbers we can use in calculations. This model creates two separate lines on a graph: One line for Group A One line for Group B By using this method, we can easily see and compare how the two groups are different from each other. Think of it like having two recipes for the same dish, but with slightly different ingredients. This model helps us see how those small changes affect the final result. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the â€œbase-lineâ€ of the model, the â€œGroup 0â€ line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the â€œbase-lineâ€ line. \\(\\beta_3\\) Called the â€œinteractionâ€ term. Controls the change in the slope for the second line in the model as compared to the slope of the â€œbase-lineâ€ line. An Example Using the mtcars data set, we run the following â€œtwo-linesâ€ regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our â€œtwo-linesâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. mpg Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) (beta zero): This is like the starting point. Itâ€™s estimated by something called the â€œInterceptâ€. \\(\\beta_1\\) (beta one): This shows how one thing changes when another thing (called â€œqsecâ€) changes. Itâ€™s estimated to be 1.439. \\(\\beta_2\\) (beta two): This shows how something else (called â€œamâ€) affects the overall result. Itâ€™s estimated to be -14.51. \\(\\beta_3\\) (beta three): This shows how â€œqsecâ€ and â€œamâ€ work together to affect the result. Itâ€™s estimated to be 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called â€œ3Dâ€ regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to â€œbendâ€. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the â€œTempâ€ direction is estimated to be 2.659 and the slope in the â€œMonthâ€ direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction termâ€™s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the â€œslopesâ€ in each direction to change, creating a â€œcurvedâ€ surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called â€œHDâ€, or â€œHigh Dimensionalâ€, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isnâ€™t really even possible to â€œmentally connectâ€ with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)â€™s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the â€œWindâ€ direction is estimated to be -3.334. The slope in the â€œTempâ€ direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the â€œSolar.Râ€ direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here. R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the â€œOverviewâ€ section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the â€œcolumn (c) bindâ€ function and it joins together things as columns. Res =Â  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,Â  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals. Â panel=panel.smooth,Â  This places a lowess smoothing line on each scatterplot. col =Â  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(â€œcolor1â€,â€œcolor2â€, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for â€œlinear model.â€ Y Y must be a â€œnumericâ€ vector of the quantitative response variable. Â ~Â  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats â€œnumericâ€ variables as quantitative and â€œcharacterâ€ or â€œfactorâ€ variables as qualitative. R will automatcially recode qualitative variables to become â€œnumericâ€ variables using a 0,1 encoding. See the Explanation tab for details. Â + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details. Â + â€¦, * ... emphasizes that as many explanatory variables as are desired can be included in the model. Â data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(â€¦) function displays the results of an lm(â€¦) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(â€¦) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(â€¦). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min Â  -4.3818 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -2.2696 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.1344 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  1.7058 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  5.8752 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  t value To learn more about the â€œt valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  Pr(>|t|) The â€œPrâ€ stands for â€œProbabilityâ€ and the â€œ(> |t|)â€ stands for â€œmore extreme than the observed t-valueâ€. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the â€œp-valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. (Intercept) This always says â€œInterceptâ€ for any lm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero. Â  2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the intercept (26.6248479) by its standard error (2.1829432). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a â€œstarâ€. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + â€¦). Â  -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model. Â  0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the â€œEstimateâ€ by its standard error. It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + â€¦). Â  5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\). Â  2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot â€œ.â€ implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like. Â  0.0004029 This is the estimate of the coefficient of the interaction term. Â  0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE. Â 2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\). Â on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO. Â 0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car. Â Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data. Â 0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that â€œat least one is not.â€ Â 34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom. Â on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero. Â p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that â€œat least oneâ€ of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the â€œOverviewâ€ sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the â€œfitted-valueâ€ or â€œpredicted-valueâ€ for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œpredictionâ€) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance canâ€™t go negative) feet to 60.96915 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œconfidenceâ€) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BICâ€¦ There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (â€œR-squaredâ€). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The â€œsimplestâ€ but â€œbestâ€ model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it â€œan information criterion (AIC)â€ when he published the method and other people later began calling it the â€œAkaike Information Criterion.â€) Model Selection (Expand) pairs plots, added variable plots, and pattern recognitionâ€¦ Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots {.tabset .tabset-pills} A useful visualization tool for model selection is the â€œpairs plot.â€ This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice thatâ€¦ the y-axis of each plot is found by locating the variable name (like â€œmpgâ€) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like â€œdispâ€) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model {.tabset .tabset-pills} Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldnâ€™t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Letâ€™s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander() Â  Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a modelâ€™s ability to generalize to new dataâ€¦ The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesnâ€™t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, letâ€™s remind ourselves why we use regression models in the first place. The main goal is to capture the â€œessenceâ€ of the data. In other words, the general pattern is what we are after. We want a model that tells us how â€œall suchâ€ data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the â€œcomplicated modelâ€ is overfitting the original data. It does not capture the â€œessenceâ€ of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-valueâ€¦ The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the modelâ€¦ The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions{#check} The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read moreâ€¦) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) Outlier Analysis (Expand) Cookâ€™s Distances and Leverage Valuesâ€¦ The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs.Â fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cookâ€™s Distances The idea behind Cookâ€™s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article â€œDetection of Influential Observation in Linear Regressionâ€ (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, letâ€™s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2)) Â  1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the â€œdifferencesâ€ in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cookâ€™s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cookâ€™s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cookâ€™s Distances using the code cooks.distance(lmObject). Also, a graph of Cookâ€™s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of â€œleverageâ€ and is â€œpullingâ€ the regression toward itself. A value near 0 implies the point is just â€œone of manyâ€ and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that â€œminimizeâ€ the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)â€™s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the â€œhat matrixâ€ \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the â€œleverage valuesâ€ and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a â€œlot of pull,â€ and values close to 0 represent â€œlittle pull.â€ In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with â€œlots of leverageâ€ and a large â€œCookâ€™s Distanceâ€ are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regressionâ€¦ Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class. Examples: Civic Vs Corolla cadillacs Binomial Y | Quantitative X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t. We might use an explanatory variable X of â€œheightâ€ to see if taller students are more likely to get an A in Math 325 than shorter students. (They arenâ€™t, if you were wondering.) Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1â€™s and 0â€™s (this does happen or this doesnâ€™t happen): Does the width of a childâ€™s foot predict whether or not they are right-handed? What is the probability the cat adopted is a male based on how many days it took them to get adopted? Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted? (Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\") Tests Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The â€œPâ€ stands for â€œProbability thatâ€¦â€ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1â€¦ This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)â€¦ in other words, the â€œ|â€ says â€œgivenâ€ and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the â€œnatural constantâ€ number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14â€¦) Part What it means \\(P(Y_i = 1| x_i)\\) - The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s) - The symbol used to represent the notation in logisitc regression is \\(\\pi_i\\) \\(Y_i\\) (Responsive Variable) Describes the probability of $Y_i = $ 1 or 0 for a given scenario. \\(x_i\\) (Explanatory variable) Describes the observed value and how it effects the \\(Y_i\\) \\(\\frac{e^{\\text{stuff}}}{1+e^{\\text{stuff}}}\\) The content for â€œstuffâ€ being simple linear regression of the form \\(\\beta_0 + \\beta_{1x_i}\\) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -1.5651 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.6648 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -0.2460 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  0.7276 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  2.2691 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The test statistic is a regular old z-score. It is most reliable when the sample size is â€œlarge.â€ It is a measurement of the number of standard errors the estimate is from 0. Â  Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds. Â  1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds. Â  0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a â€œstarâ€. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about. Â  Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\). Â 43.230 Â on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates. Â 29.732 This can be calculated by sum(log(myglm$res^2)). Â on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, â€œA version of Akaikeâ€™s An Information Criterionâ€¦â€ The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model. Â 33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 * ## disp -0.014604 0.005168 -2.826 0.00471 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0â€™s or 1â€™s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == â€œBâ€ or height < 68. In other words, Y needs to be a collection of 0â€™s and 1â€™s. Â ~Â  The tilde is read â€œY on X.â€ X, X is some quantitative variable. Â data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(â€¦) function. curve( A function in R that draws a â€œcurveâ€ on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the â€œInterceptâ€ estimate from your logistic regression summary output. Â \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case â€œxâ€ in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula. Â exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator. Â add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(â€¦) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set. Â aes( The aes(â€¦) function stands for â€œaestheticsâ€ and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The plus sign adds a new layer to the ggplot. Â  geom_point( Tells the plot to add the physical geometry of â€œpointsâ€ to the plot. ) Closing parenthesis for the geom_point() function. Â + Add another layer to the plot. Â  geom_smooth( Add a smoothing line to the graph. method=â€œglmâ€, This adds a general linear model to the graph. method.args = list(family=â€œbinomialâ€), This tells the method=â€œglmâ€ to choose specifically the â€œbinomialâ€ model, otherwise known as the â€œlogistic regressionâ€ model. Â se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function. Â + Add another layer to the ggplot. Â  theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),Â  The xVariableName is the same one you used in your glm(y ~ x, â€¦) statement for â€œxâ€. Input any desired value for the â€œsomeNumericValueâ€ spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016 Explanation Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal). The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\). Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page. Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful. Checking Model Assumptions {#diagnostics} The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit. Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease. Binomial Y | Multiple X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t. We might use an explanatory variable X1 of â€œheightâ€ and a second explanatory variable X2 of â€œgenderâ€ to try to predict whether or not a student will get an A in Math 325. Test(s) used for this category: Multiple Logistic Regression Model Tests Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two. Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2Â  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. â€¦, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -2.9446 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.2364 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.0000 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q Â  0.2776 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  1.9968 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The Z-score testing the hypothesis that \\(\\beta_j=0\\) Â  Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\). Â  1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\). Â  4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\). Â  3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\). Â  0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\). Â  0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a â€œstarâ€. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\). Â  471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option. Â  Null Deviance: The deviance of the null model. Â 800.44 In this case, the null deviance is 800.44 Â on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression. Â 256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit. Â on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model. Â 272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -4.97603 0.65316 -7.618 2.57e-14 *** ## Time 0.39111 0.04979 7.854 4.02e-15 *** ## Diet2 0.58283 1.04061 0.560 0.5754 ## Diet3 -8.44476 4.32324 -1.953 0.0508 . ## Diet4 -67.41995 3768.10023 -0.018 0.9857 ## Time:Diet2 0.02391 0.08679 0.275 0.7830 ## Time:Diet3 1.20955 0.51249 2.360 0.0183 * ## Time:Diet4 8.83168 471.01259 0.019 0.9850 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,Â  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, â€¦), X2 = c(Value 1, Value 2, â€¦), â€¦) command. You should see the GSS example file for an example of how to use this function. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the â€œbâ€ vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(â€œSomeColorâ€,â€œDifferentColorâ€,â€¦) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(â€¦) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==â€œBâ€. Â ~Â  The formula operator in R. X1, The first x-variable in your glm code. Â data = YourDataSet, Specify the name of your data set. Â pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(â€¦) function computes e^(stuff) in R and stands for the â€œexponential function.â€ (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(â€¦) function requires that you call the x-variable â€œxâ€ although you can change this behavior using xname=â€œSomeOtherNameâ€ if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)). Â col = palette()[1], Pull the first color from the color palette for the first curve. Â add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters. Â col = palette()[2], Set the color of this second curve to the second color in the palette. Â add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. â€œtoprightâ€, Typically the legend is placed in the top-right corner of the graph. But it could be placed â€œtopâ€, â€œtopleftâ€, â€œleftâ€, â€œbottomleftâ€, â€œbottomâ€, â€œcenterâ€, â€œrightâ€, or â€œbottomrightâ€. Â legend = Why you have to write legend again, no one knows, but you do. c(â€œLable 1â€, â€œLabel 2â€), These are the values that will appear in the legend, one entry on each line of the legend. Â col = palette(), This specifies the colors of the symbols in the legend. First color goes with â€œLabel 1â€, and second goes with â€œLabel 2â€ and so on. Â lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist. Â bty = Specifies the â€œbox typeâ€ (bty) that should be drawn around the legend. â€˜nâ€™) By placing a â€œnoâ€ option (â€˜nâ€™) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot. Â aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + Add a layer to the ggplot. Â  geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function. Â + Add a layer to the ggplot. Â  geom_smooth(method=â€œglmâ€, method.args=list(family=â€œbinomialâ€), se=FALSE, Add the logistic regression curve to the plot. Â  aes(color=â€œX2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function. Â + Add a layer to the plot. Â  theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x' Explanation Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative. The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations. Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant. Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression. Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\). Categorical Y | Categorical X Graphics Y is a single categorical variable of interest, like gender. X is another categorical variable of interest, like â€œhair colorâ€. This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender. Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year? Whats the probability that you have brown hair and blue eyes? Tests Add your own notes about appropriate inferential procedures for this type of data here.",
    "sentences": ["Graphics Y is a single quantitative variable of interest, like â€œheightâ€.", "X is another single quantitative variable of interest, like â€œshoe-sizeâ€.", "This would imply we are using â€œshoe-sizeâ€ (X) to explain â€œheightâ€ (Y).", "Test(s) used for this category: Simple Linear Regression Analysis : Simple Linear Regression: BYU-Idaho Math Tutoring Used to find the relationship between one independent variable (predictor) and one dependent variable(outcome): Is there a relationship between the length of childrenâ€™s feet and the width of their foot?", "Example: Does the amount of tutoring a student receives correlate with their chapter exam scores?", "(Simple Linear Regression) ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams)", "Tests Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\).", "Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$\\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$$ \\(Y_i\\) The response variable.", "The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size.", "\\(=\\) This states that we are assuming \\(Y_i\\) was created, or is â€œequal toâ€ the formula that will follow on the right-hand-side of the equation."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Quantitative Y | Quantitative X"
  },
  {
    "id": 38,
    "title": "ğŸ“ Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single quantitative variable of interest, like â€œheightâ€. X is another single quantitative variable of interest, like â€œshoe-sizeâ€. This would imply we are using â€œshoe-sizeâ€ (X) to explain â€œheightâ€ (Y). Test(s) used for this category: Simple Linear Regression Analysis : Simple Linear Regression: BYU-Idaho Math Tutoring Used to find the relationship between one independent variable (predictor) and one dependent variable(outcome): Is there a relationship between the length of childrenâ€™s feet and the width of their foot? Example: Does the amount of tutoring a student receives correlate with their chapter exam scores? (Simple Linear Regression) ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams)",
    "sentences": ["Y is a single quantitative variable of interest, like â€œheightâ€.", "X is another single quantitative variable of interest, like â€œshoe-sizeâ€.", "This would imply we are using â€œshoe-sizeâ€ (X) to explain â€œheightâ€ (Y).", "Test(s) used for this category: Simple Linear Regression Analysis : Simple Linear Regression: BYU-Idaho Math Tutoring Used to find the relationship between one independent variable (predictor) and one dependent variable(outcome): Is there a relationship between the length of childrenâ€™s feet and the width of their foot?", "Example: Does the amount of tutoring a student receives correlate with their chapter exam scores?", "(Simple Linear Regression) ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams)"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 39,
    "title": "ğŸ“ Tests",
    "url": "index.html#tests",
    "content": "Tests Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\). Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$\\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$$ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is â€œequal toâ€ the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to â€œGodâ€™s Lawâ€ or â€œNatural Lawâ€. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are â€œrandomâ€ and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)â€¦ \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced â€œy-hatâ€, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what followsâ€¦ \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, â€œthe change in the average y-value for a one unit change in the x-value.â€ It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, â€œthe average y-value when x is zero.â€ It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful. R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your lm() code into mylm name. lm( lm(â€¦) is an R function that stands for â€œLinear Modelâ€. It performs a linear regression analysis for Y ~ X. YÂ  Y is your quantitative response variable. It is the name of one of the columns in your data set. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X. X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value. Â data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X. ) Closing parenthesis for the lm(â€¦) function. summary(mylm) The summary command allows you to print the results of your linear regression that were previously saved in mylm name. Â Click to Show OutputÂ  Click to View Output. Example output from a regression. Hover each piece to learn more. Call: lm(formula = dist ~ speed, data = cars) This is simply a statement of your original lm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(â€¦). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min Â  -29.069 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -9.525 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -2.272 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -2.272 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-2.272) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  9.215 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  43.201 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  t value To learn more about the â€œt valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  Pr(>|t|) The â€œPrâ€ stands for â€œProbabilityâ€ and the â€œ(> |t|)â€ stands for â€œmore extreme than the observed t-valueâ€. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the â€œp-valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. (Intercept) This always says â€œInterceptâ€ for any lm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -17.5791 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  6.7584 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -2.601 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the intercept (-17.5791) by its standard error (6.7584). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of -17.5791 is -2.601 standard errors (6.7584) from zero, which is a fairly surprising distance as shown by the p-value. 0.0123 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). speed This is always the name of your X-variable in your lm(Y ~ X, â€¦). Â  3.9324 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.4155 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 9.464 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the slope (3.9324) by its standard error (0.4155). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from zero, which is a really surprising distance as shown by the smallness of the p-value. 1.49e-12 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE. Â 15.38 For this particular regression, the estimate of \\(\\sigma\\) is 15.38. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\). Â on 48 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 50 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 50-2 = 48. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO. Â 0.6511, In this particular regression, 65.11% of the variation in stopping distance dist is explained by the regression model using speed of the car. Â Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data. Â 0.6438 In this case, the value of 0.6438 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is found as the ratio of the MSR/MSE where MSR = SSR/(p-1) and MSE = SSE/(n-p) where n is the sample size and p is the number of parameters in the regression model. Â 89.57 This is the value of the F-statistic for the lm(dist ~ speed, data=cars) regression. Note that SSE = sum( cars.lm$res^2 ) = 11353.52 with n - p = 50 - 2 = 48 degrees of freedom for this data. Further, SSR = sum( (cars.lm$fit - mean(cars$dist))^2 ) = 21185.46 with p - 1 = 1 degree of freedom. So MSR = 21185.46 and MSE = 11353.52 / 48 = 236.5317. So MSR / MSE = 21185.46 / 236.5317 = 89.56711. Â on 1 and 48 DF, The 1 degree of freedom is the SSR degrees of freedom (p-1). The 48 is the SSE degrees of freedom (n-p). Â p-value: 1.49e-12 The p-value for an F-statistic is found by the code pf(89.56711, 1, 48, lower.tail=FALSE), which gives the probability of being more extreme than the observed F-statistic in an F distribution with 1 and 48 degrees of freedom. Check Assumptions 1, 2, 3, and 5 When making it in an r chunk make your height 3! { r, fig.height = 3 } par( The par(â€¦) command stands for â€œGraphical PARametersâ€. It allows you to control various aspects of graphics in Base R. mfrow= This stands for â€œmultiple frames filled by rowâ€, which means, put lots of plots on the same row, starting with the plot on the left, then working towards the right as more plots are created. c( The combine function c(â€¦) is used to specify how many rows and columns of graphics should be placed together. 1, This specifies that 1 row of graphics should be produced. 3 This states that 3 columns of graphics should be produced. ) Closing parenthesis for c(â€¦) function. ) Closing parenthesis for par(â€¦) function. plot( This version of plot(â€¦) will actually create several regression diagnostic plots by default. mylm, This is the name of an lm object that you created previously. which= This allows you to select â€œwhichâ€ regression diagnostic plots should be drawn. 1 Selecting 1, would give the residuals vs.Â fitted values plot only. : The colon allows you to select more than just one plot. 2 Selecting 2 also gives the Q-Q Plot of residuals. If you wanted to instead you could just use which=1 to get the residuals vs fitted values plot, then you could use qqPlot(mylm$residuals) to create a fancier Q-Q Plot of the residuals. ) Closing parenthesis for plot(â€¦) function. plot( This version of plot(â€¦) will be used to create a time-ordered plot of the residuals. The order of the residuals is the original order of the x-values in the original data set. If the original data set doesnâ€™t have an order, then this plot is not interesting. mylm The lm object that you created previously. $ This allows you to access various elements from the regression that was performed. residuals This grabs the residuals for each observation in the regression. ) Closing parenthesis for plot(â€¦) function. Â Click to Show OutputÂ  Click to View Output. Which regression assumption(s) does the residuals versus fitted values plot diagnose? Linear Relation Constance Variance Which regression assumption(s) does the Q-Q Plot of the residuals diagnose? Normal Errors Which regression assumption(s) does the Residuals versus Order plot diagnose? (Remember, this plot can only be created when the data was collected in a specific order.) Independent Errors Plotting the Regression Line Base R ggplot2 To add the regression line to a scatterplot use the abline(...) command: plot( The plot(â€¦) function is used to create a scatterplot with a y-axis (the vertical axis) and an x-axis (the horizontal axis). YÂ  This is the â€œresponse variableâ€ of your regression. The thing you are interested in predicting. This is the name of a â€œnumericâ€ column of data from the data set called YourDataSet. ~Â  The tilde â€œ~â€ is used to relate Y to X and can be found on the top-left key of your keyboard. X,Â  This is the explanatory variable of your regression. It is the name of a â€œnumericâ€ column of data from YourDataSet. data= The data= statement is used to specify the name of the data set where the columns of â€œXâ€ and â€œYâ€ are located. YourDataSet This is the name of your data set, like KidsFeet or cars or airquality. ) Closing parenthesis for plot(â€¦) function. abline( This stands for â€œaâ€ (intercept) â€œbâ€ (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm This is the name of an lm(â€¦) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(â€¦) function will locate these two values from within mylm and use them to add a line to your current plot(â€¦). ) Closing parenthesis for abline(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm) You can customize the look of the regression line with abline( This stands for â€œaâ€ (intercept) â€œbâ€ (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm, This is the name of an lm(â€¦) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(â€¦) function will locate these two values from within mylm and use them to add a line to your current plot(â€¦). lty= The lty= stands for â€œline typeâ€ and allows you to select between 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. 1, This creates a solid line. Remember, other options include: 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. lwd= The lwd= allows you to specify the width of the line. The default width is 1. Using lwd=2 would double the thickness, and so on. Any positive value is allowed. 1, Default line width. To make a thicker line, us 2 or 3â€¦ To make a thinner line, try 0.5, but 1 is already pretty thin. col= This allows you to specify the color of the line using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). â€œsomeColorâ€ Type colors() in R for options. ) Closing parenthesis for abline(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm, lty=1, lwd=1, col=\"firebrick\") You can add points to the regression withâ€¦ points( This is like plot(â€¦) but adds points to the current plot(â€¦) instead of creating a new plot. newYÂ  newY should be a column of values from some data set. Or, use points(newX, newY) to add a single point to a graph. ~Â  This links Y to X in the plot. newX,Â  newX should be a column of values from some data set. It should be the same length as newY. If just a single value, use points(newX, newY) instead. data=YourDataSet,Â  If newY and newX come from a dataset, then use data= to tell the points(â€¦) function what data set they come from. If newY and newX are just single values, then data= is not needed. col=â€œskyblueâ€, This allows you to specify the color of the points using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). pch=16 This allows you to specify the type of plotting symbol to be used for the points. Type ?pch and scroll half way down in the help file that appears to learn about other possible symbols. ) Closing parenthesis for points(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) points(7,40, pch=16, col=\"skyblue\", cex=2) text(7,40, \"New Dot\", pos=3, cex=0.5) points(dist ~ speed, data=filter(cars, mylm$res > 2), cex=.8, col=\"red\") abline(mylm, lty=1, lwd=1, col=\"firebrick\") To add the regression line to a scatterplot using the ggplot2 approach, first ensure: library(ggplot2) or library(tidyverse) is loaded. Then, use the geom_smooth(method = lm) command: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. YourDataSet,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. X, This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. Â y =Â  â€œy=Â â€ declares which variable will become the y-axis of the graphic. Y This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. + Here the + is used to add yet another layer to ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. There are several other methods that could be used here. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(â€¦) to place more complicated models on the plot. Â se = FALSE se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. ) Closing parenthesis for the geom_smooth() function. Â Click to Show OutputÂ  Click to View Output. ggplot(cars, aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = \"lm\", formula=y~x, se=FALSE) There are a number of ways to customize the appearance of the regression line: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. speed,Â  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. y =Â  â€œy=Â â€ declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. Â + Here the + is used to add yet another layer to ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(â€¦) to place more complicated models on the plot. Â se = FALSE, se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. Â size = 2, Use size = 2 to adjust the thickness of the line to size 2. Â color = â€œorangeâ€, Use color = â€œorangeâ€ to change the color of the line to orange. Â Â linetype = â€œdashedâ€ Use linetype = â€œdashedâ€ to change the solid line to a dashed line. Some linetype options include â€œdashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. ) Closing parenthesis for the geom_smooth() function. Â Click to Show OutputÂ  Click to View Output. In addition to customizing the regression line, you can customize the points, add points, add lines, and much more. ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. speed,Â  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. y =Â  â€œy=Â â€ declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point( geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. size = 1.5, Use size = 1.5 to change the size of the points. Â color = â€œskyblueâ€ Use color = â€œskyblueâ€ to change the color of the points to Brother Saundersâ€™ favorite color. Â alpha = 0.5 Use alpha = 0.5 to change the transparency of the points to 0.5. ) Closing parenthesis of geom_point() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in ways similar to lm(â€¦) to place more complicated models on the plot. Â se = FALSE, se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. Â color = â€œnavyâ€, Use color = â€œnavyâ€ to change the color of the line to navy blue. Â size = 1.5 Use size = 1.5 to adjust the thickness of the line to 1.5. ) Closing parenthesis of geom_smooth() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_hline( Use geom_hline() to add a horizontal line at a specified y-intercept. You can also use geom_vline(xintercept = some_number) to add a vertical line to the graph. yintercept = Use â€œyintercept =â€ to tell geom_hline() that you are going to declare a y intercept for the horizontal line. Â 75 75 is the value of the y-intercept. , color = â€œfirebrickâ€ Use color = â€œfirebrickâ€ to change the color of the horizontal line to firebrick red. , size = 1, Use size = 1 to adjust the thickness of the horizontal line to size 1. Â Â Â Â Â Â Â Â Â Â Â Â Â linetype = â€œlongdashâ€ Use linetype = â€œlongdashâ€ to change the solid line to a dashed line with longer dashes. Some linetype options include â€œdashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. , alpha = 0.5 Use alpha = 0.5 to change the transparency of the horizontal line to 0.5. ) Closing parenthesis of geom_hline function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_segment( geom_segment() allows you to add a line segment to ggplot() by using specified start and end points. x = â€œx =â€ tells geom_segment() that you are going to declare the x-coordinate for the starting point of the line segment. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the starting point of the line segment. Â y =â€œy =â€ tells geom_segment() that you are going to declare the y-coordinate for the starting point of the line segment. Â 75, 75 is a number on the y-axis of your graph. It is the y-coordinate of the starting point of the line segment. Â xend = â€œxend =â€ tells geom_segment() that you are going to declare the x-coordinate for the end point of the line segment. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the end point of the line segment. Â yend = â€œyend =â€ tells geom_segment() that you are going to declare the y-coordinate for the end point of the line segment. Â 38, 38 is a number on the y-axis of your graph. It is the y-coordinate of the end point of the line segment. Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â size = 1 Use size = 1 to adjust the thickness of the line segment. , color = â€œlightgrayâ€ Use color = â€œlightgrayâ€ to change the color of the line segment to light gray. , linetype = â€œlongdashâ€ Use *linetype = â€œlongdash* to change the solid line segment to a dashed one. Some linetype options includeâ€dashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. ) Closing parenthesis for geom_segment() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_point( geom_point() can also be used to add individual points to the graph. Simply declare the x and y coordinates of the point you want to plot. x = â€œx =â€ tells geom_point() that you are going to declare the x-coordinate for the point. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the point. Â y = â€œy =â€ tells geom_point() that you are going to declare the y-coordinate for the point. Â 75 75 is a number on the y-axis of your graph. It is the y-coordinate of the point. , size = 3 Use size = 3 to make the point stand out more. , color = â€œfirebrickâ€ Use color = â€œfirebrickâ€ to change the color of the point to firebrick red. ) Closing parenthesis of the geom_point() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_text( geom_text() allows you to add customized text anywhere on the graph. It is very similar to the base R equivalent, text(â€¦). x = â€œx =â€ tells geom_text() that you are going to declare the x-coordinate for the text. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the text. Â y = â€œy =â€ tells geom_text() that you are going to declare the y-coordinate for the text. Â 84, 84 is a number on the y-axis of your graph. It is the y-coordinate of the text. Â label = â€œlabel =â€ tells geom_text() that you are going to give it the label. Â â€œMy Point (14, 75)â€, â€œMy Point (14, 75)â€ is the text that will appear on the graph. Â Â Â Â Â Â Â Â Â Â Â Â color = â€œnavyâ€ Use color = â€œnavyâ€ to change the color of the text to navy blue. , size = 3 Use size = 3 to change the size of the text. ) Closing parenthesis of the geom_text() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â theme_minimal() Add a minimalistic theme to the graph. There are many other themes that you can try out. Â Click to Show OutputÂ  Click to View Output. ## `geom_smooth()` using formula = 'y ~ x' Accessing Parts of the Regression Finally, note that the mylm object contains the names(mylm) of mylm$coefficients Contains two values. The first is the estimated \\(y\\)-intercept. The second is the estimated slope. ## (Intercept) speed ## -17.579095 3.932409 mylm$residuals Contains the residuals from the regression in the same order as the actual dataset. ## 1 2 3 4 5 6 7 8 9 ## 3.849460 11.849460 -5.947766 12.052234 2.119825 -7.812584 -3.744993 4.255007 12.255007 ## 10 11 12 13 14 15 16 17 18 ## -8.677401 2.322599 -15.609810 -9.609810 -5.609810 -1.609810 -7.542219 0.457781 0.457781 ## 19 20 21 22 23 24 25 26 27 ## 12.457781 -11.474628 -1.474628 22.525372 42.525372 -21.407036 -15.407036 12.592964 -13.339445 ## 28 29 30 31 32 33 34 35 36 ## -5.339445 -17.271854 -9.271854 0.728146 -11.204263 2.795737 22.795737 30.795737 -21.136672 ## 37 38 39 40 41 42 43 44 45 ## -11.136672 10.863328 -29.069080 -13.069080 -9.069080 -5.069080 2.930920 -2.933898 -18.866307 ## 46 47 48 49 50 ## -6.798715 15.201285 16.201285 43.201285 4.268876 mylm$fitted.values The values of \\(\\hat{Y}\\) in the same order as the original dataset. ## 1 2 3 4 5 6 7 8 9 10 ## -1.849460 -1.849460 9.947766 9.947766 13.880175 17.812584 21.744993 21.744993 21.744993 25.677401 ## 11 12 13 14 15 16 17 18 19 20 ## 25.677401 29.609810 29.609810 29.609810 29.609810 33.542219 33.542219 33.542219 33.542219 37.474628 ## 21 22 23 24 25 26 27 28 29 30 ## 37.474628 37.474628 37.474628 41.407036 41.407036 41.407036 45.339445 45.339445 49.271854 49.271854 ## 31 32 33 34 35 36 37 38 39 40 ## 49.271854 53.204263 53.204263 53.204263 53.204263 57.136672 57.136672 57.136672 61.069080 61.069080 ## 41 42 43 44 45 46 47 48 49 50 ## 61.069080 61.069080 61.069080 68.933898 72.866307 76.798715 76.798715 76.798715 76.798715 80.731124 mylm$â€¦ several other things that will not be explained here. Making Predictions predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ) Closing parenthesis for the data.frame(â€¦) function. ) Closing parenthesis for the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12)) 1 Â  29.60981 The value given is the â€œfitted-valueâ€ or â€œpredicted-valueâ€ for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(â€¦) function. Â interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. â€œpredictionâ€ This specifies that a prediction interval will be included with the predicted value. A prediction interval gives you a 95% confidence interval that captures 95% of the data, or \\(Y_i\\) values for the specific \\(X\\)-value specified in the prediction. ) Closing parenthesis of the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"prediction\") Â  fit The â€œfitâ€ is the predicted value. Â  lwr The â€œlwrâ€ is the lower bound. Â  upr The â€œuprâ€ is the upper bound. 1 29.60981 In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance canâ€™t go negative) feet to 60.96915 feet. -1.749529 This is the lower bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us the stopping distance could be as quick as -1.749529 feet (or 0 because distance canâ€™t go negative). 60.96915 This is the upper bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us that the actual stopping distance could be as high as 60.96915 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(â€¦) function. Â interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. â€œconfidenceâ€ This specifies that a confidence interval for the prediction should be provided. This is of use whenever your interest is in just estimating the average y-value, not the actual y-values. ) Closing parenthesis of the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"confidence\") Â  fit The â€œfitâ€ is the predicted value. Â  lwr The â€œlwrâ€ is the lower bound. Â  upr The â€œuprâ€ is the upper bound. 1 29.60981 In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. 24.39514 This is the lower bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is greater than this value. 34.82448 This is the upper bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is less than this value. Finding Confidence Intervals for Model Parameters confint( The R function confint(â€¦) allows you to use an lm(â€¦) object to compute confidence intervals for one or more parameters (like \\(\\beta_0\\) or \\(\\beta_1\\)) in your model. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â level = â€œlevel =â€ tells the confint(â€¦) function that you are going to declare at what level of confidence you want the interval. The default is â€œlevel = 0.95.â€ If you want to find 95% confidence intervals for your parameters, then just run confint(mylm). Â someConfidenceLevel someConfidenceLevel is simply a confidence level you choose when you want something other than a 95% confidence interval. Some examples of appropriate levels include 0.90 and 0.99. ) Closing parenthesis for confint(..) function. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.90) Â  Â  5 % The lower bound of a 90% confidence interval occurs at the 5th percentile. This is because at 90% confidence, 10% is left in the tails, with 5% on each end. The upper bound will thus end at the 95th percentile, hence the 5% and 95% as the column names. Â  95 % The upper bound of a 90% confidence interval ends at the 95th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -28.914514 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -6.243676 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.235501 and 4.629317. 3.235501 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. 4.629317 This is the upper bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.95) Â  Â  2.5 % The lower bound of a 95% confidence interval occurs at the 2.5th percentile. This is because at 95% confidence, 5% is left in the tails, with 2.5% on each end. The upper bound will thus end at the 97.5th percentile, hence the 2.5% and 97.5% as the column names for the lower and upper bounds, respectively. Â  97.5 % The upper bound of a 95% confidence interval ends at the 97.5th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -31.167850 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -3.990340 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.096964 and 4.767853. 3.096964 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.096964 and 4.767853 4.767853 This is the upper bound of the confidence interval. In this case, you can be 95% confident that the slope lies between 3.096964 and 4.767853 Explanation Linear regression has a rich mathematical theory behind it. This is because it uses a mathematical function and a random error term to describe the regression relation between a response variable \\(Y\\) and an explanatory variable called \\(X\\). Expand each element below to learn more. Regression Cheat Sheet (Expand) Term Pronunciation Meaning Math R Code \\(Y_i\\) $Y_i$ â€œwhy-eyeâ€ The data \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)\\) $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$ YourDataSet$YourYvariable \\(\\hat{Y}_i\\) $\\hat{Y}_i$ â€œwhy-hat-eyeâ€ The fitted line \\(\\hat{Y}_i = b_0 + b_1 X_i\\) $\\hat{Y}_i = b_0 + b_1 X_i$ lmObject$fitted.values \\(E\\{Y_i\\}\\) $E\\{Y_i\\}$ â€œexpected value of why-eyeâ€ True mean y-value \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) $E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i$ <none> \\(\\beta_0\\) $\\beta_0$ â€œbeta-zeroâ€ True y-intercept <none> <none> \\(\\beta_1\\) $\\beta_1$ â€œbeta-oneâ€ True slope <none> <none> \\(b_0\\) $b_0$ â€œb-zeroâ€ Estimated y-intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) $b_0 = \\bar{Y} - b_1\\bar{X} b_0 <- mean(Y) - b_1*mean(X)$ \\(b_1\\) $b_1$ â€œb-oneâ€ Estimated slope \\(b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\) $b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})} {\\sum(X_i - \\bar{X})^2}$ b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) \\(\\epsilon_i\\) $\\epsilon_i$ â€œepsilon-eyeâ€ Distance of dot to true line \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) $\\epsilon_i = Y_i - E\\{Y_i\\}$ <none> \\(r_i\\) $r_i$ â€œr-eyeâ€ or â€œresidual-eyeâ€ Distance of dot to estimated line \\(r_i = Y_i - \\hat{Y}_i\\) $r_i = Y_i - \\hat{Y}_i$ lmObject$residuals \\(\\sigma^2\\) $\\sigma^2$ â€œsigma-squaredâ€ Variance of the \\(\\epsilon_i\\) \\(Var\\{\\epsilon_i\\} = \\sigma^2\\)$Var\\{\\epsilon_i\\} = \\sigma^2$ <none> \\(MSE\\) $MSE$ â€œmean squared errorâ€ Estimate of \\(\\sigma^2\\) \\(MSE = \\frac{SSE}{n-p}\\)$MSE = \\frac{SSE}{n-p}$ sum( lmObject$res^2 ) / (n - p) \\(SSE\\) $SSE$ â€œsum of squared errorâ€ (residuals) Measure of dotâ€™s total deviation from the line \\(SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\)$SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$ sum( lmObject$res^2 ) \\(SSR\\) $SSR$ â€œsum of squared regression errorâ€ Measure of lineâ€™s deviation from y-bar \\(SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\)$SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2$ sum( (lmObject$fit - mean(YourData$Y))^2 ) \\(SSTO\\) $SSTO$ â€œtotal sum of squaresâ€ Measure of total variation in Y \\(SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\)$SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2$ sum( (YourData$Y - mean(YourData$Y))^2 ) \\(R^2\\) $R^2$ â€œR-squaredâ€ Proportion of variation in Y explained by the regression \\(R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\)$R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}$ SSR/SSTO \\(r\\) $r$ â€œrâ€ Correlation between X and Y. \\(r = \\sqrt{R^2}\\)$r = \\sqrt{R^2}$ sqrt(R^2) \\(\\hat{Y}_h\\) $\\hat{Y}_h$ â€œwhy-hat-aitchâ€ Estimated mean y-value for some x-value called \\(X_h\\) \\(\\hat{Y}_h = b_0 + b_1 X_h\\)$\\hat{Y}_h = b_0 + b_1 X_h$ predict(lmObject, data.frame(XvarName=#)) \\(X_h\\) $X_h$ â€œex-aitchâ€ Some x-value, not necessarily one of the \\(X_i\\) values used in the regression \\(X_h =\\) some number$X_h = $ Xh = # Confidence Interval â€œconfidence intervalâ€ Estimated bounds at a certain level of confidence for a parameter \\(b_0 \\pm t^* \\cdot s_{b_0}\\)b_0 \\pm t^* \\cdot s_{b_0} or \\(b_1 \\pm t^* \\cdot s_{b_1}\\)b_1 \\pm t^* \\cdot s_{b_1} confint(mylm, level = someConfidenceLevel) Parameter Estimate \\(\\beta_0\\) \\(b_0\\) \\(\\beta_1\\) \\(b_1\\) \\(\\epsilon_i\\) \\(r_i\\) \\(\\sigma^2\\) \\(MSE\\) \\(\\sigma\\) \\(\\sqrt{MSE}\\), the Residual standard error The Mathematical Model (Expand) \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)â€¦ There are three main elements to the mathematical model of regression. Each of these three elements is pictured below in the â€œRegression Relation Diagram.â€ Study both the three bullet points and their visual representations in the plot below for a clearer understanding. The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read moreâ€¦) The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as â€œnatural lawâ€ or â€œGodâ€™s lawâ€. It is some true line that is unknown to us. The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line. The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\). Note: \\(E\\{Y\\}\\) is pronounced â€œthe expected value of yâ€ because, wellâ€¦ the mean is the typical, average, or â€œexpectedâ€ value. The dots, i.e., the regression relation plus an error term: \\(Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}} + \\underbrace{\\epsilon_i}_\\text{error term} \\quad \\text{where} \\ \\epsilon_i\\sim N(0,\\sigma^2)\\) (Read moreâ€¦) This is shown by the dots in the graph below. This is the data. In regression, the assumption is that the y-value for individual \\(i\\), denoted by \\(Y_i\\), was â€œcreatedâ€ by adding an error term \\(\\epsilon_i\\) to each individualâ€™s â€œexpectedâ€ value \\(\\beta_0 + \\beta_1 X_i\\). Note the â€œorder of creationâ€ would require first knowing an indivualâ€™s x-value, \\(X_i\\), then their expected value from the regression relation \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) and then adding their \\(\\epsilon_i\\) value to the result. The \\(\\epsilon_i\\) allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance \\(\\epsilon_i\\) from the line. Note: \\(Y_i\\) is pronounced â€œwhy-eyeâ€ because it is the y-value for individual \\(i\\). Sometimes also called â€œwhy-sub-eyeâ€ because \\(i\\) is in the subscript of \\(Y\\). The estimated line, i.e., the line we get from a sample of data. \\(\\underbrace{\\hat{Y}_i}_{\\substack{\\text{estimated mean} \\\\ \\text{y-value}}} = \\underbrace{b_0 + b_1 X_i}_\\text{estimated regression equation}\\) (Read moreâ€¦) The estimated line is shown by the solid line in the graph below. \\(\\hat{Y}\\) is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation \\(E\\{Y\\}\\). So \\(\\hat{Y}\\) is interpreted as the estimated average (or mean) \\(Y\\)-value for any given \\(X\\)-value. Thus, \\(b_0\\) is the estimated y-intercept and \\(b_1\\) is the estimated slope. The bâ€™s are sample statistics, like \\(\\bar{x}\\) and the \\(\\beta\\)â€™s are population parameters like \\(\\mu\\). The \\(b\\)â€™s estimate the \\(\\beta\\)â€™s. Note: \\(\\hat{Y}_i\\) is pronounced â€œwhy-hat-eyeâ€ and is known as the â€œestimated y-valueâ€ or â€œfitted y-valueâ€ because it is the y-value you get from \\(b_0 + b_1 X_i\\). It is always different from \\(Y_i\\) because dots are rarely if ever exactly on the estimated regression line. This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line). Something to ponder: The true line, when coupled with the error terms, â€œcreatesâ€ the data. The estimated (or fitted) line uses the sampled data to try to â€œre-createâ€ the true line. We could loosely call this the â€œorder of creationâ€ as shown by the following diagram. par(mfrow=c(1,3), mai=c(.2,.2,.4,.1)) plot(y ~ x, col=\"white\", main=\"A Law is Given\", yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, main=\"Data is Created\", xaxt='n', yaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, xaxt='n', yaxt='n', main=\"The Law is Estimated\") curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) A Law is Given Data is Created The Law is Estimated \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) \\(Y_i = E\\{Y_i\\} + \\epsilon_i\\) \\(\\hat{Y}_i = b_0 + b_1 X_i\\) The true line is the â€œlawâ€. The \\(Y_i\\) are created by adding \\(\\epsilon_i\\) to \\(E\\{Y_i\\}\\) where \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\). The law is estimated with \\(\\hat{Y}_i\\) which is given with lm(...). Click open the â€œCodeâ€ buttom below to the right to find code that runs a simulation demonstrating this â€œorder of creationâ€. ## Simulating Data from a Regression Model ## This R-chunk is meant to be played in your R Console. ## It allows you to explore how the various elements ## of the regression model combine together to \"create\" ## data and then use the data to \"re-create\" the line. set.seed(101) #Allows us to always get the same \"random\" sample #Change to a new number to get a new sample n <- 3 #set the sample size X_i <- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45. beta0 <- 3 #Our choice for the y-intercept. beta1 <- .1 #Our choice for the slope. sigma <- 12.5 #Our choice for the std. deviation of the error terms. epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data View(fabData) #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it. fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData. summary(fab.lm) #Summarize your model. plot(y ~ x, data=fabData) #Plot the data. abline(fab.lm) #Add the estimated regression line to your plot. # Now for something you can't do in real life... but since we created the data... abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). legend(\"topleft\", legend=c(\"True Line\", \"Estimated Line\"), lty=c(2,1), bty=\"n\") #Add a legend to your plot specifying which line is which. Interpreting the Model Parameters (Expand) \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted asâ€¦ The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model. If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\). The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\). It is often misunderstood to be the â€œaverage change in yâ€ or just â€œthe change in yâ€ but it is more correctly referred to as the â€œchange in the average yâ€. To better see this, consider the three graphics shown below. par(mfrow=c(1,3)) hist(mtcars$mpg, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Number of Vehicles\", xlab=\"Gas Mileage (mpg)\", col=\"skyblue\") boxplot(mpg ~ cyl, data=mtcars, border=\"skyblue\", boxwex=0.5, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Number of Cylinders of Engine (cyl)\") plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=\"skyblue\", main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Quarter Mile Time (qsec)\") abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=\"darkgray\") mtext(side=3, text=\"Automatic Transmissions Only (am==0)\", cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=\"gray\") The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09. The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg. The scatterplot on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value. Residuals and Errors (Expand) \\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true errorâ€¦ Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\). The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\). We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summaryâ€¦ Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\). \\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms. Keep in mind the idea that the errors \\(\\epsilon_i\\) â€œcreatedâ€ the data and that the residuals \\(r_i\\) are computed after using the data to â€œre-createâ€ the line. Residuals have many uses in regression analysis. They allow us to diagnose the regression assumptions, See the â€œAssumptionsâ€ section below for more details. estimate the regression relation, See the â€œEstimating the Model Parametersâ€ section below for more details. estimate the variance of the error terms, See the â€œEstimating the Model Varianceâ€ section below for more details. and assess the fit of the regression relation. See the â€œAssessing the Fit of a Regressionâ€ section below for more details. Assessing the Fit of a Regression (Expand) \\(R^2\\), SSTO, SSR, and SSEâ€¦ Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line. par(mfrow=c(1,3), mai=c(.1,.1,.5,.1)) set.seed(2) x <- runif(30,0,20) y1 <- 2 + 3.5*x + rnorm(30,0,2) y2 <- 2 + 3.5*x + rnorm(30,0,8) y3 <- 2 + 3.5*x + rnorm(30,0,27) plot(y1 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Excellent Fit\") abline(lm(y1 ~ x), col=\"gray\") plot(y2 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Good Fit\") abline(lm(y2 ~ x), col=\"gray\") plot(y3 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Poor Fit\") abline(lm(y3 ~ x), col=\"gray\") A common way to measure the fit of a regression is with correlation. While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\). (If you are a Math 325 student, just stick with correlation for now and skip on to the next section of this Explanation tab. If you are a Math 425 student, it is critical that you come to understand \\(R^2\\) deeply, so read on.) Before you can understand \\(R^2\\), you must understand three important â€œsums of squaresâ€. (Read more about sumsâ€¦) A sum is just a fancy word for adding things together. \\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand. So we use the symbol \\(\\Sigma\\) to denote the word â€œsumâ€. Further, we use a subscript \\(\\Sigma_{i=1}\\) to state what value the sum is beginning with, and a superscript \\(\\Sigma_{i=1}^6\\) to state the value we are ending at. This gives \\[ \\sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Test your knowledge, do you see why the answer is 6 to the sum below? \\[ \\sum_{i=1}^3 i = 6 \\] Computing sums in R is fairly easy. Type the following codes in your R Console. sum(1:6) #gives the answer of 21 sum(1:3) #gives the answer of 6 However, sums really become useful when used with a data set. Each row of a data set represents an â€œindividualâ€™sâ€ data. We can reference each individual with a row number. In the data below, individual 3, denoted by \\(i=3\\), has a speed of 7 and a dist of 4. pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3) Individual speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 To compute the sum of the speed column, use sum(speed). If we divided this sum by 6, we would get the mean of speed mean(speed). In fact, the two most used statistics mean(...) and sd(...) both use sums. Take a moment to review the formulas for mean and standard deviation. It is strongly recommended that you study the Explanation tab for both as well. Weâ€™ll wait. See you back here shortly. â€¦ Welcome back. Suppose we let X = speed and Y = dist. Then \\(X_3 = 7\\) and \\(Y_3 = 4\\) because we are accessing row 3 of both the \\(X\\) (or speed) column and \\(Y\\) (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, sum(speed) would be written mathematically as \\(\\sum_{i=1}^6 X_i\\) and sum(dist) would be written as \\(\\sum_{i=1}^6 Y_i\\). Sum of Squared Errors Sum of Squares Regression Total Sum of Squares \\(\\text{SSE} = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\) \\(\\text{SSR} = \\sum_{i=1}^n \\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\) \\(\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\\) Measures how much the residuals deviate from the line. Measures how much the regression line deviates from the average y-value. Measures how much the y-values deviate from the average y-value. Equals SSTO - SSR Equals SSTO - SSE Equals SSE + SSR sum( (Y - mylm$fit)^2 ) sum( (mylm$fit - mean(Y))^2 ) sum( (Y - mean(Y))^2 ) It is important to remember that SSE and SSR split up SSTO, so that \\[ \\text{SSTO} = \\text{SSE} + \\text{SSR} \\] This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works. The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called \\(R^2\\) (â€œr-squaredâ€). R-Squared (\\(R^2\\)) \\[ \\underbrace{R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}}_\\text{Interpretation: Proportion of variation in Y explained by the regression.} \\] The smallest \\(R^2\\) can be is zero, and the largest it can be is 1. This is because \\(SSR\\) must be between 0 and SSTO, inclusive. Residual Plots & Regression Assumptions (Expand) Residuals vs.Â fitted-values, Q-Q Plot of the residuals, and residuals vs.Â order plotsâ€¦ There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate. Each assumption is labeled in the regression equation below. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions. (Read moreâ€¦) As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not. Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3 The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(\\hat{Y}_i\\). The residuals are the \\(r_i\\). This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable. | Show Examples | (Read moreâ€¦) The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption. The linear relation is assumed to be satisfied if there are no apparent trends in the plot. The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values. The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance. set.seed(2) X <- rnorm(30,15,3) notLin <- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8)) notLin.lm <- lm(Y~X, data=notLin) set.seed(15) Lin <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) Lin.lm <- lm(Y~X, data=Lin) par(mfrow=c(3,3), mai=c(.25,.25,.25,.25), mgp=c(1,.75,0)) plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Not Linear\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(notLin.lm$fitted.values,notLin.lm$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Good: Linear, Constant Variance\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) set.seed(6) notCon <- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5)) notCon.lm <- lm(Y~X, data=notCon) LinO <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) LinO[1] <- LinO[1]^2 LinO.lm <- lm(Y~X, data=LinO) plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Unconstant Variance\", cex.main=0.95, yaxt='n', xaxt='n', col=\"firebrick\") polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) # plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Outliers\", cex.main=0.95) # abline(h=0) tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(Girth ~ Volume, data=trees[-31,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(Height ~ Volume, data=trees) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) abline(h=0) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals, f=.4) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(weight ~ repwt, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(weight ~ repht, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) abline(h=0) Q-Q Plot of the Residuals: Checks Assumption #2 The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption. | Show Examples | (Read moreâ€¦) There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot. Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) set.seed(123) tmp <- rnorm(100) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"skyblue\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Normal\", col=\"skyblue\") tmp <- Davis$weight qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Right-skewed\", breaks=15, col=\"firebrick\") par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- rbeta(100, 5,1) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Left-skewed\", breaks=seq(min(tmp),max(tmp), length.out=13), col=\"firebrick\") tmp <- rbeta(100,2,2) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Heavy-tailed\", col=\"firebrick\") Residuals versus Order Plot: Checks Assumption #5 When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated. | Show Examples | (Read moreâ€¦) Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Good: No Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: General Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ qsec, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: Interesting Patterns\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),]) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Bad: Obvious Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") Problems from Failed Assumptions There are various problems that can arise when certain of the regression assupmtions are not satisfied. Lack of Linearity When the linearity assumption is violated, pretty much everything we obtain from the regression summary is no longer meaningful. The y-intercept estimate can be drastically off from its actual true value. Important model information is lost by trying to use a simple slope term \\(\\beta_1\\) to describe the model with respect to \\(X\\). The residual standard error will be much higher than it otherwise would be because of curvature patterns in the data that the line cannot capture. Thus, R-squared will be lower than it otherwise should be. P-values can become non-significant, when in fact there is a strong pattern in the data, but that pattern just cannot be captured by a simple line. *Normality of the errors is often put into question as well when a simplified line is used to try to capture a more complicated curved model. The plot below demonstrate these difficulties. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 7.5 #True slope beta_2 <- -0.25 #True bend X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors Y_i <- beta_0 + beta_1*X_i + beta_2*X_i^2 + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Non-Linear Relation\") abline(mylm, col=\"gray\") #Add fitted line to plot curve(beta_0 + beta_1*x + beta_2*x^2, col=\"gray\", lty=2, add=TRUE) #Add True line to plot #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (True value:\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (True value:\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (True value:\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Non-normal Error Terms When the normality assumption for the error terms is violated, not all is lost. In fact, the estimate of the slope and intercept are still often fairly meaningful. However, it is unwise to put too much trust in the residual standard error as an estimate of the standard deviation \\(\\sigma\\). This is because the standard deviation in skewed distributions does not carry the same meaning it has in normal distributions. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rchisq(n, 1)*3 - 1 #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Unconstant Variance When variance of the error term changes across the regression, the regression approximates the â€œaverage varianceâ€ of the errors because the regression is still assuming the variance is constant across the regression. The estimates of the slope and intercept are still typically quite good, and can be used for interpretation. The residual standard error however should not be considered to be meaningful as it will be too large on one end of the regression and too small on the other end. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma + X_i) #normally distributed errors #with increasing variance Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Variance Varies (Non-Constant)\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 13.82 Slope 3.5 3.768 Sigma 2.5 13.02 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", round(mean(sigma + X_i), 2), \", mean)\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Normality Violated As silly as it sounds, if the only problem with the regression is the lack of normality of the error terms, it isnâ€™t all that big of a problem. Depending on how non-normal the residuals appear, there could be some skewing to the residual standard error, but otherwise, the slope and intercept are still interpretable and meaningful. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- runif(n, -sqrt(12*sigma^2)/2, sqrt(12*sigma^2)/2) #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 14.7 Slope 3.5 3.449 Sigma 2.5 2.317 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Independence Assumption Violated While the slope and intercept are often still meaningful when the independence assumption is violated, the residual standard error is unnecessarily large in this case. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, 2.5) + (1:n -n/2)*.5 #normal, but correlated errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Independence Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 16.47 Slope 3.5 3.296 Sigma 2.5 4.819 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Outliers Present While outliers do not violate any of the regression assumptions, they do pose substantial difficulties for the least squares regression estimates of the slope and intercept. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors epsilon_i[3] <- ifelse(X_i[3] < 10, runif(1,25,35), -runif(1,25,35)) #create outlier Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"An Outlier Present\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 13.18 Slope 3.5 3.447 Sigma 2.5 6.389 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) The material below this section is meant for Math 425 students only. Estimating the Model Parameters (Expand) How to get \\(b_0\\) and \\(b_1\\): least squares & maximum likelihoodâ€¦ There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical. The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas. Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 <- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\). When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\). Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i. \\label{exp} \\end{equation}\\] Least Squares{#leastSquares} To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\). \\[ Q = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2 \\] Then we use the function Q as if it were a function of \\(\\beta_0\\) and \\(\\beta_1\\). Ironically, the values of \\(Y\\) and \\(X\\) are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we donâ€™t know are the values of \\(\\beta_0\\) and \\(\\beta_1\\). This least squares applet is a good way to explore how various choices of the slope and intercept yield different values of the â€œsum of squared residualsâ€. But it turns out that there is one â€œbestâ€ choice of the slope and intercept that yields a â€œsmallestâ€ value of the â€œsum of squared residuals.â€ This best choice can actually be found using calculus by taking the partial derivatives of \\(Q\\) with respect to both \\(\\beta_0\\) and \\(\\beta_1\\). \\[ \\frac{\\partial Q}{\\partial \\beta_0} = -2\\sum (Y_i - \\beta_0 - \\beta_1X_i) \\] \\[ \\frac{\\partial Q}{\\partial \\beta_1} = -2\\sum X_i(Y_i-\\beta_0-\\beta_1X_i) \\] Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize \\(Q\\) for a given set of data. After all the calculations are completed we find the values of the parameter estimators \\(b_0\\) and \\(b_1\\) (of \\(\\beta_0\\) and \\(\\beta_1\\), respectively) are as stated previously. Maximum Likelihood{#mle} The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of \\(\\beta_0\\) and \\(\\beta_1\\) which minime the least squares \\(Q\\) function, we choose the values of \\(\\beta_0\\) and \\(\\beta_1\\) which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the \\(Y_i\\) for all observations \\(i=1,\\ldots,n\\). We can do this rather simply by using the assumption that the errors, \\(\\epsilon_i\\) are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if \\(f(Y_i)\\) denotes the probability density function for \\(Y_i\\), then the joint probability density for all \\(Y_i\\), \\(f(Y_1,\\ldots,Y_n)\\) is given by \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) \\] Since each \\(Y_i\\) is assumed to be normally distributed with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\) (see model (\\(\\ref{model}\\))) we have that \\[ f(Y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{Y_i-\\beta_0-\\beta_1X_i}{\\sigma}\\right)^2\\right]} \\] which provides the joint probability as \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] The likelihood function \\(L\\) is then given by consider the \\(Y_i\\) and \\(X_i\\) fixed and the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) as the variables in the function. \\[ L(\\beta_0,\\beta_1,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] Instead of taking partial derivatives of \\(L\\) directly (with respect to all parameters) we take the partial derivatives of the \\(\\log\\) of \\(L\\), which is easier to work with. In a similar, but more difficult calculation, to that of minimizing \\(Q\\), we obtain the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) which maximize the log of \\(L\\), and which therefore maximize \\(L\\). (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate \\(\\hat{\\sigma}^2\\) of \\(\\sigma^2\\). \\[ \\hat{\\sigma}^2 = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n} \\] Estimating the Model Variance (Expand) Estimating \\(\\sigma^2\\) with MSEâ€¦ As shown previously in the â€œEstimating Model Parametersâ€ section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation. Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\). This estimate turns out to be a biased estimator. This means that it is consistently wrong in its estimates of \\(\\sigma^2\\). If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong. This is bad. Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\). Without going into all the details, to â€œfixâ€ the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently. To find the correct degrees of freedom, we have to notice that the \\(\\hat{Y}_i\\) in the numerator of \\(\\widehat{\\sigma}^2\\) is defined by \\[\\begin{equation} \\widehat{Y}_i = b_0 + b_1X_i \\label{hatY} \\end{equation}\\] From this equation, we notice that two means, \\(\\bar{X}\\) and \\(\\bar{Y}\\), were estimated from the data in order to obtain \\(\\hat{Y}_i\\). (See the formulas for \\(b_0\\) and \\(b_1\\) above, and note how they use both \\(\\bar{X}\\) and \\(\\bar{Y}\\) in their calculation.) Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for \\(\\hat{\\sigma}^2\\) should be \\(n-2\\) instead of \\(n\\). Some incredibly long calculations will show that the â€œfixedâ€ estimator \\[\\begin{equation} s^2 = MSE = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n-2} \\quad \\text{(Unbiased Estimator of $\\sigma^2$)} \\end{equation}\\] is an unbiased estimator of \\(\\sigma^2\\). Here \\(MSE\\) stands for mean squared error, which is the most obvious name for a formula that squares the errors \\(Y_i-\\hat{Y}_i\\) then adds them up and divides by their degrees of freedom. Similarly, we call the numerator \\(\\sum(Y_i-\\hat{Y}_i)^2\\) the sum of the squared errors, denoted by \\(SSE\\). It is also important to note that the errors are often denoted by \\(r_i = Y_i-\\hat{Y}_i\\), the residuals. Putting this all together we get the following equivalent statements for \\(MSE\\). \\[\\begin{equation} s^2 = MSE = \\frac{SSE}{n-2} = \\frac{\\sum(Y_i-\\widehat{Y}_i)^2}{n-2} = \\frac{\\sum r_i^2}{n-2} \\end{equation}\\] As a final note, even though the expected value \\(E\\{MSE\\} = \\sigma^2\\), which shows \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\), it unfortunately isnâ€™t true that \\(\\sqrt{MSE}\\) is an unbiased estimator of \\(\\sigma\\). This presents a few problems later on, but these are minimal enough that we can overlook the issue and move forward. Transformations (Expand) \\(Y'\\), \\(X'\\), and returning to the original spaceâ€¦ Y transformations are denoted by y-prime, written \\(Y'\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\). \\[ Y' = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y' = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y' = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y' = \\log(Y)\\) lm(log(Y) ~ X) 0.25 \\(Y' = \\sqrt(\\sqrt(Y))\\) lm(sqrt(sqrt(Y)) ~ X) 0.5 \\(Y' = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y' = Y\\) lm(Y ~ X) 2 \\(Y' = Y^2\\) lm(Y^2 ~ X) Using â€œmaximum-likelihoodâ€ estimation, the Box-Cox procedure can actually automatically detect the â€œoptimalâ€ value of \\(\\lambda\\) to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise. Scatterplot Recognition Box-Cox Suggestion An Example Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\). set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Ya <- 1/sqrt(Y) #1/Y^2 Lam = -2 Yb <- 1/Y #1/Y Lam = -1 Yc <- exp(.02*Y) #log(Y) Lam = 0 Yd <- Y^2 #sqrt(Y) Lam = 0.5 Ye <- Y #Y Lam = 1 Yf <- sqrt(Y) #Y^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Ya ~ X, main=expression(paste(\"Use \", lambda == -2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ya^-2 ~ X)) curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yb ~ X, main=expression(paste(\"Use \", lambda == -1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yb^-1 ~ X)) curve(1/(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yc ~ X, main=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(log(Yc) ~ X)) curve(exp(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yd ~ X, main=expression(paste(\"Use \", lambda == 0.5)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(sqrt(Yd) ~ X)) curve((b[1] + b[2]*x)^2, add=TRUE, col=\"green\", lwd=2) plot(Ye ~ X, main=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ye ~ X)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yf ~ X, main=expression(paste(\"Use \", lambda == 2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yf^2 ~ X)) curve(sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try. par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0)) boxCox(lm(Ya ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -2)), line=.5) boxCox(lm(Yb ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -1)), line=.5) boxCox(lm(Yc ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), line=.5) boxCox(lm(Yd ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0.5)), line=.5) boxCox(lm(Ye ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), line=.5) boxCox(lm(Yf ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 2)), line=.5) An Example Suppose we were running a simple linear regression on the cars dataset. This would be done with the code cars.lm <- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesnâ€™t quite fit the data as well as we would hope. Instead, the data looks a little curved. cars.lm <-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") abline(cars.lm, col=\"gray\") Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful. library(car) boxCox(cars.lm) The output from the boxCox(...) function looks as follows. This plot tells use to use the \\(\\lambda = 0.5\\) transformation, so that \\(Y' = Y^0.5 = \\sqrt{Y}\\). (To see this yourself, click on the â€œBox-Cox Suggestionâ€ tab above, as well as on the â€œScatterplot Recognitionâ€ tab.) Now, a transformation regression is performed using sqrt(Y) in place of Y as follows: cars.lm.t <- lm(sqrt(dist) ~ speed, data=cars) summary(cars.lm.t) Â  Estimate Std. Error t value Pr(> (Intercept) 1.277 0.4844 2.636 0.01126 speed 0.3224 0.02978 10.83 1.773e-14 Then, \\[ \\widehat{Y}_i' = 1.277 + 0.3224 X_i \\] And replacing \\(\\hat{Y}_i' = \\sqrt{\\hat{Y}_i}\\) we have \\[ \\sqrt{\\widehat{Y}_i} = 1.277 + 0.3224 X_i \\] Solving for \\(\\hat{Y}_i\\) gives \\[ \\widehat{Y}_i = (1.277 + 0.3224 X_i)^2 \\] Which, using curve((1.277 + 0.3224*x)^2, add=TRUE) (see code for details) looks like this: plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") curve( (1.277 + 0.3224*x)^2, add=TRUE, col=\"firebrick\") X-Transformations X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them. The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated. The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation. set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Xa <- 1/sqrt(X) #1/X^2 Lam = -2 Xb <- 1/X #1/X Lam = -1 Xc <- exp(.02*X) #log(X) Lam = 0 Xd <- X^2 #sqrt(X) Lam = 0.5 Xe <- X #X Lam = 1 Xf <- sqrt(X) #X^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Y ~ Xa, main=expression(paste(\"Use \", X*minute == X^-2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xa^-2))) curve(b[1] + b[2]*x^-2, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xb, main=expression(paste(\"Use \", X*minute == X^-1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xb^-1))) curve(b[1] + b[2]*x^-1, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xc, main=expression(paste(\"Use \", X*minute == log(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ log(Xc))) curve(b[1] + b[2]*log(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xd, main=expression(paste(\"Use \", X*minute == sqrt(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ sqrt(Xd))) curve(b[1] + b[2]*sqrt(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xe, main=expression(paste(\"Use \", X*minute == X, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ Xe)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xf, main=expression(paste(\"Use \", X*minute == X^2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xf^2))) curve(b[1] + b[2]*x^2, add=TRUE, col=\"green\", lwd=2) Inference for the Model Parameters (Expand) t test formulas, sampling distributions, confidence intervals, and F testsâ€¦ When fitting the regression model given by the equation \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] to a sample of data, we typically test hypotheses about the parameters \\(\\beta_0\\), \\(\\beta_1\\), or both. Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_0 - \\overbrace{0}^\\text{a number}}{s_{b_0}}\\] This is the formula for the test statistic. It measures how far the estimated y-intercept \\(b_0\\) is from the null hypothesis for \\(\\beta_0\\) in units of â€œstandard errors of \\(b_0\\)â€. Thus the division by \\(s_{b_0}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0, it could be any number. \\(H_0: \\beta_1 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_1\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_1 - \\overbrace{0}^\\text{a number}}{s_{b_1}}\\] This is the formula for the test statistic. It measures how far the estimated slope \\(b_1\\) is from the null hypothesis for \\(\\beta_1\\) in units of â€œstandard errors of \\(b_1\\)â€. Thus the division by \\(s_{b_1}\\). Though the hypothesized value of \\(\\beta_1\\) is typically 0, it could be any number. Left-tailed p-value = pt(-abs(tvalue), degrees of freedom). Double it to get the two-sided p-value. In R, these values correspond to the output summary of an lm as follows. (Show Example) Consider the cars data in R. Suppose we used the regression model given by \\[ \\underbrace{Y_i}_\\text{Feet to Stop} = \\beta_0 + \\beta_1 \\underbrace{X_{i}}_\\text{mph} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\quad \\sim N(0,\\sigma^2) \\] to model the feet a vehicle (from the 1920â€™s) takes to stop when traveling at a certain speed (in miles per hour, mph) prior to stopping. When the regression is performed and summarized in R, it is always testing the following two hypotheses: \\[ H_0: \\beta_0 = 0 \\quad\\quad H_0: \\beta_1 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\quad\\quad H_a: \\beta_1 \\neq 0 \\] To perform the test of these hypotheses for the regression stated above, we would run the following codes in R. cars.lm <- lm(dist ~ speed, data=cars) pander(summary(cars.lm)$coefficients) These would produce summary output like the following, but the following output has been labeled with the math notation corresponding to each value. Letâ€™s emphasize what is happening in this summary output table. First, here is how the â€œt valueâ€ is calculated for the â€œ(Intercept)â€ in the summary table above. \\[ t = \\frac{b_0-0}{s_{b_0}} = \\frac{-17.58 - 0}{6.758} = -2.601 \\] Second, here is a visual representation of how the P-value, the â€œPr(>|t|)â€ as it is called in the summary table above, is calculated for this test statistic. (Click the graph to view an interactive applet showing this calculation.) Notice both ends of the t-distribution are being shaded to compute the P-value because the alternative hypothesis was \\(H_a: \\beta_0 \\neq 0\\). To compute the P-value in R, we use the â€œpercentile function for the t-distributionâ€ called pt( ). This function requires two things, the t-value and the degrees of freedom, in our case pt(-2.601, 48). Note the degrees of freedom (df) are 48 because the sample size is \\(n=50\\) and there are two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) in our regression model. Running this code in R gives: pt(-2.601, 48) = 0.00616 However, note that this value is only half of the actual P-value of 0.0123. To get the â€œtwo-sidedâ€ P-value (note that our alternative hypothesis used a \\(\\neq\\) symbol) we need to double this left-tailed P-value. 2*pt(-2.601, 48)) = 0.0123 Finally, note that the same procedure can be used to test hypotheses that use a value other than 0 in the null and alternative. For example, to test the hypotheses: \\[ H_0: \\beta_1 = 3 \\\\ H_a: \\beta_1 \\neq 3 \\] Use the t-formula \\[ t = \\frac{b_1 - 3}{s_{b_1}} = \\frac{\\overbrace{3.932}^{b_1} - \\overbrace{3}^{H_0}}{\\underbrace{0.4155}_{s_{b_1}}} = 2.243 \\] then the P-value is calculated in R by 2*pt(-abs(2.243), 48) = 0.0295495 To obtain confidence intervals in R use confint(mylm). Confidence Interval Formula Standard Error \\(\\beta_0\\) \\(b_0 \\pm\\) \\(t^*\\) This is called the â€œcritical valueâ€ and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\). Use qt(0.975, df) to get \\(t*\\) in R. \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_0\\). \\(s_{b_0}\\) The standard error of \\(b_0\\), denoted by \\(s_{b_0}\\) is provided in the regression summary output under the column header called â€œStd. Errorâ€ for the â€œ(Intercept)â€ row of the output. It is calculated using the formula shown below. \\[s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\] This is called the â€œestimated variance of \\(b_0\\)â€. Taking the square root of this number gives the â€œstandard error of \\(b_0\\)â€. \\(\\beta_1\\) \\(b_1 \\pm\\) \\(t^*\\) This is called the â€œcritical valueâ€ and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\) (sample size - number of parameters in the regression model). Use qt(0.975, df) to get \\(t*\\) in R \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_1\\). \\(s_{b_1}\\) The standard error of \\(b_1\\), denoted by \\(s_{b_1}\\) is provided in the regression summary output under the column header called â€œStd. It is calculated using the formula shown below. \\[s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\] This is called the â€œestimated variance of \\(b_1\\)â€. Taking the square root of this number gives the â€œstandard error of \\(b_1\\)â€. To be more exact, the types of inference we are interested in are the following. Determine if there is evidence of a meaningful linear relationship in the data. If \\(\\beta_1 = 0\\), then there is no relation between \\(X\\) and \\(E\\{Y\\}\\). Hence we might be interested in testing the hypotheses \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) is some hypothesized number. To provide a confidence interval for the true value of \\(\\beta_1\\). Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the sampling distribution of the estimate \\(b_1\\) of the parameter \\(\\beta_1\\). And, while we are at it, we may as well come to understand the sampling distribution of the estimate \\(b_0\\) of the parameter \\(\\beta_0\\). Review sampling distributions from Math 221. Since \\(b_1\\) is an estimate, it will vary from sample to sample, even though the truth, \\(\\beta_1\\), remains fixed. (The same holds for \\(b_0\\) and \\(\\beta_0\\).) It turns out that the sampling distribution of \\(b_1\\) (where the \\(X\\) values remain fixed from study to study) is normal with mean and variance: \\[ \\mu_{b_1} = \\beta_1 \\] \\[ \\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} \\] ## Simulation to Show relationship between Standard Errors ##----------------------------------------------- ## Edit anything in this area... n <- 100 #sample size Xstart <- 30 #lower-bound for x-axis Xstop <- 100 #upper-bound for x-axis beta_0 <- 2 #choice of true y-intercept beta_1 <- 3.5 #choice of true slope sigma <- 13.8 #choice of st. deviation of error terms ## End of Editable area. ##----------------------------------------------- # Create X, which will be used in the next R-chunk. X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) ## After playing this chunk, play the next chunk as well. To see that this is true, consider the regression model with values specified for each parameter as follows. \\[ Y_i = \\overbrace{\\beta_0}^{2} + \\overbrace{\\beta_1}^{3.5} X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\overbrace{\\sigma^2}^{\\sigma=13.8}) \\] Using the equations above for \\(\\mu_{b_1}\\) and \\(\\sigma^2_{b_1}\\) we obtain that the mean of the sampling distribution of \\(b_1\\) will be \\(\\mu_{b_1} = \\beta_1 = 3.5\\) Further, we see that the variance of the sampling distribution of \\(b_1\\) will be \\(\\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} = \\frac{13.8^2}{4.25\\times 10^{4}}\\) Taking the square root of the variance, the standard deviation of the sampling distribution of \\(b_1\\) will be \\(\\sigma_{b_1} = 0.067\\). Thatâ€™s very nice. But to really believe it, letâ€™s run a simulation ourselves. The â€œCodeâ€ below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of \\(b_1\\) and \\(b_0\\) for that regression. After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of \\(b_0\\) is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of \\(b_1\\) is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of \\(\\beta_0\\) or \\(\\beta_1\\), respectively. Also, the â€œStd. Errorâ€ of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of \\(\\sigma_{b_0}\\) and \\(\\sigma_{b_1}\\), respectively. N <- 5000 #number of times to pull a random sample storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N) for (i in 1:N){ Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model mylm <- lm(Y ~ X) storage_b0[i] <- coef(mylm)[1] storage_b1[i] <- coef(mylm)[2] storage_rmse[i] <- summary(mylm)$sigma } layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3)) Ystart <- 0 #min(0,min(Y)) Ystop <- 500 #max(max(Y), 0) Yrange <- Ystop - Ystart plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), ylim=c(Ystart, Ystop), pch=16, col=\"gray\", main=\"Regression Lines from many Samples (gray lines) \\n Plus Residual Standard Deviation Lines (green lines)\") text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1) text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1) text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1) for (i in 1:N){ abline(storage_b0[i], storage_b1[i], col=\"darkgray\") } abline(beta_0, beta_1, col=\"green\", lwd=3) abline(beta_0+sigma, beta_1, col=\"green\", lwd=2) abline(beta_0-sigma, beta_1, col=\"green\", lwd=2) abline(beta_0+2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0-2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0+3*sigma, beta_1, col=\"green\", lwd=.5) abline(beta_0-3*sigma, beta_1, col=\"green\", lwd=.5) par(mai=c(1,.6,.5,.01)) addnorm <- function(m,s, col=\"firebrick\"){ curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2) lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col) lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col) lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col) lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col) lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col) lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col) lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col) legend(\"topleft\", legend=paste(\"Std. Error = \", round(s,3)), cex=0.7, bty=\"n\") } h0 <- hist(storage_b0, col=\"skyblue3\", main=\"Sampling Distribution\\n Y-intercept\", xlab=expression(paste(\"Estimates of \", beta[0], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m0 <- mean(storage_b0) s0 <- sd(storage_b0) addnorm(m0,s0, col=\"green\") h1 <- hist(storage_b1, col=\"skyblue3\", main=\"Sampling Distribution\\n Slope\", xlab=expression(paste(\"Estimates of \", beta[1], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m1 <- mean(storage_b1) s1 <- sd(storage_b1) addnorm(m1,s1, col=\"green\") t Tests{#tTests} Using the information above about the sampling distributions of \\(b_1\\) and \\(b_0\\), an immediate choice of statistical test to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) can be zero, or any other value, is a t test given by \\[ t = \\frac{b_1 - \\beta_{10}}{s_{b_1}} \\] where \\(s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\). (You may want to review the section â€œEstimating the Model Varianceâ€ of this file to know where MSE came from.) With quite a bit of work it has been shown that \\(t\\) is distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. The nearly identical test statistic for testing \\[ H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_0 \\neq \\beta_{00} \\] is given by \\[ t = \\frac{b_0 - \\beta_{00}}{s_{b_0}} \\] where \\(s^2_{b_0} = MSE\\left[\\frac{1}{n}+\\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\). This version of \\(t\\) has also been shown to be distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. Confidence Intervals Creating a confidence interval for either \\(\\beta_1\\) or \\(\\beta_0\\) follows immediately from these results using the formulas \\[ b_1 \\pm t^*_{n-2}\\cdot s_{b_1} \\] \\[ b_0 \\pm t^*_{n-2}\\cdot s_{b_0} \\] where \\(t^*_{n-2}\\) is the critical value from a t distribution with \\(n-2\\) degrees of freedom corresponding to the chosen confidence level. F tests{#Ftests} Another way to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\quad\\quad \\text{or} \\quad\\quad H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\quad\\quad \\ \\ \\quad \\quad H_a: \\beta_0 \\neq \\beta_{00} \\] is with an \\(F\\) Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an \\(F\\) test is very general and can be used in many places that a t test cannot. In its most general form, the \\(F\\) test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that \\(H_0:\\beta_1=0\\) against the alternative that \\(H_a: \\beta_1\\neq 0\\), we are essentially comparing two models against each other. If \\(\\beta_1=0\\), then the corresponding model would be \\(E\\{Y_i\\} = \\beta_0\\). If \\(\\beta_1\\neq0\\), then the model remains \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the \\(F\\) Test, that the null model (reduced model) have fewer â€œfreeâ€ parameters than the alternative model (full model). To demonstrate what we mean by â€œfreeâ€ parameters, consider the following example. Say we wanted to test the hypothesis that \\(H_0:\\beta_1 = 2.5\\) against the alternative that \\(\\beta_1\\neq2.5\\). Then the null, or reduced model, would be \\(E\\{Y_i\\}=\\beta_0+2.5X_i\\). The alternative, or full model, would be \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). Thus, the null (reduced) model contains only one â€œfreeâ€ parameter because \\(\\beta_1\\) has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two â€œfreeâ€ parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model. Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing \\(H_0: \\beta_1=0\\) against \\(H_a:\\beta_1\\neq0\\) we have the partition \\[ \\underbrace{Y_i-\\bar{Y}}_{Total} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{Regression} + \\underbrace{Y_i-\\hat{Y}_i}_{Error} \\] The reason we use \\(\\bar{Y}\\) for the null model is that \\(\\bar{Y}\\) is the unbiased estimator of \\(\\beta_0\\) for the null model, \\(E\\{Y_i\\} = \\beta_0\\). Thus we would compute the following sums of squares: \\[ SSTO = \\sum(Y_i-\\bar{Y})^2 \\] \\[ SSR = \\sum(\\hat{Y}_i-\\bar{Y})^2 \\] \\[ SSE = \\sum(Y_i-\\hat{Y}_i)^2 \\] and note that \\(SSTO = SSR + SSE\\). Important to note is that \\(SSTO\\) uses the difference between the observations \\(Y_i\\) and the null (reduced) model. The \\(SSR\\) uses the diffences between the alternative (full) and null (reduced) model. The \\(SSE\\) uses the differences between the observations \\(Y_i\\) and the alternative (full) model. From these we could set up a General \\(F\\) table of the form Â  Sum Sq Df Mean Sq F Value Model Error \\(SSR\\) \\(df_R-df_F\\) \\(\\frac{SSR}{df_R-df_F}\\) \\(\\frac{SSR}{df_R-df_F}\\cdot\\frac{df_F}{SSE}\\) Residual Error \\(SSE\\) \\(df_F\\) \\(\\frac{SSE}{df_F}\\) Total Error \\(SSTO\\) \\(df_R\\) Prediction and Confidence Intervals for \\(\\hat{Y}_h\\) (Expand) predict(â€¦, interval=â€œpredictionâ€)â€¦ It is a common mistake to assume that averages (means) describe individuals. They do not. So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line. Interval R Code Math Equation When to Use Prediction predict(..., interval=\"prediction\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\text{Pred}\\ Y}\\) Predict an individualâ€™s value. Confidence predict(..., interval=\"confidence\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\hat{Y}}\\) Estimate location of the mean y-value. predict(mylm, data.frame(XvarName = number), interval=...) For example, consider this graph. Then click here to read about the graph. Notice the three dots above 15 mph in the graph. Each of these dots show a car that was going 15 mph when it applied the brakes. However, stopping distances of the three individual cars differ with one at 20 feet, one at 26 feet and one at 54 feet. The regression line represents the average stopping distance of cars. In this case, cars going 15 mph are estimated to have an average stopping distance of about 40 feet, as shown by the line. But individual vehicles, all going the same speed of 15 mph, varied from stopping distances of 20 feet up to 54 feet! So, to predict that a car going 15 mph will take 41.4 feet to stop, doesnâ€™t tell the whole story. Far more revealing is the complete statement, â€œCars going 15 mph are predicted to take anywhere from 10.2 to 72.6 feet to stop, with an average stopping distance of 41.4 feet.â€ This is called the â€œprediction intervalâ€ and is shown in the graph in blue. It is obtained in R with the codes: cars.lm <- lm(dist ~ speed, data=cars) predict(cars.lm, data.frame(speed=15), interval=\"prediction\") cars.lm <- lm(dist ~ speed, data=cars) pander(predict(cars.lm, data.frame(speed=15), interval=\"prediction\")) fit lwr upr 41.41 10.17 72.64 plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=\"firebrick2\", cex=1.5) cars.lm <- lm(dist ~ speed, data=cars) abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3)) abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2)) abline(v=15, lty=2, col=\"firebrick\") preds <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12) lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8)) lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8)) Now, for the details behind prediction intervals and confidence intervals. Letâ€™s begin by recalling some details (from the section â€œInference for the Model Parametersâ€) about the standard error of the y-intercept, \\(b_0\\). Recall that the y-intercept is the average y-value for the given x-value of \\(x=0\\). Recall further that the formula for the standard error of \\(b_0\\) is given by the formula \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] If we wanted to be more exact with this formula, we would write it as \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{(0-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice the addition of \\((0 - \\bar{X})^2\\) instead of just \\(\\bar{X}^2\\) in the numerator of the right-most part of the equation? This more complete statement obviously would reduce to just \\(\\bar{X}^2\\), but that is only because \\(X=0\\) when we are working with the y-intercept, \\(b_0\\). We could be working with other values of \\(X\\) than just zero. Letâ€™s take a quick detour and talk notation for a second. Typically, \\(X_i\\) and \\(Y_i\\) are used to denote the x-value and y-value of points that are contained in our data set. When we want to reference a point that wasnâ€™t within our original data set, we use the notation \\(X_h\\) and \\(Y_h\\). (The letter h is close to i, but different from i, so why not. There is really no other reason to use h.) Thus, \\(Y_h\\) is the y-value for the \\(X_h\\) x-value, neither of which were included in our original regression of \\(X_i\\)â€™s and \\(Y_i\\)â€™s. Now, back to the previous discussion. If \\(X_h = 0\\), then \\(\\hat{Y}_h\\) is the y-intercept, so \\(\\hat{Y}_h = b_0\\) when \\(X_h=0\\). So, we could write, \\[ s^2_{\\hat{Y}_h} = MSE\\left[\\frac{1}{n} + \\frac{(X_h-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice how the \\(b_0\\) in \\(s_{b_0}\\) was replaced with \\(\\hat{Y}_h\\) to get \\(s_{\\hat{Y}_h}\\) and the 0 in \\((0 - \\bar{X})^2\\) was replaced with \\(X_h\\) to get \\((X_h - \\bar{X})^2\\)? Interesting. We now have a formula that would give us the standard error of \\(\\hat{Y}_h\\) for any \\(X_h\\) value, not just \\(X_h = 0\\), or the y-intercept, \\(b_0\\). That is fantastic. It would look like this if plotted. Notice how the gray region is showing the standard error for each \\(\\hat{Y}_h\\) value? (It is technically showing the confidence interval for \\(E\\{Y_h\\}\\) at every possible \\(X_h\\) value, but that is just \\(\\hat{Y}_h \\pm t^* \\cdot s_{\\hat{Y}_h}\\).) ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + theme_bw() ## `geom_smooth()` using formula = 'y ~ x' Confidence Interval for \\(\\hat{Y}_h\\) \\[ \\hat{Y}_h \\pm t^* s_{\\hat{Y}_h} \\quad \\text{where} \\ s_{\\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] The confidence interval is a wonderful tool for estimating \\(E\\{Y_h\\}\\), the â€œtrueâ€ average y-value for a given x-value of \\(X_h\\). However, it is not valuable for predicting an individual dot, or \\(Y_h\\) value. Notice how few of the dots of the regression are actually contained within the confidence interval band in the plot? The confidence interval does not really predict where the dots will land, just where the average y-value is located for each x-value. Remember the 68-95-99.7 Rule of the normal distribution? If not, here is a link back to that concept in the Math 221 textbook. This rule states that roughly 95% of data, when normally distributed, will be between \\(z=-2\\) and \\(z=2\\) standard deviations from the mean. So, is going two â€œresidual standard errorsâ€ to both sides of the regression line enough to capture 95% of the data? The answer is, not quite. The reason for this is because our knowledge of where the true mean lies is uncertain. (Notice the confidence interval band shown in the plot.) However, adding two standard errors to the edges of the confidence band would get us in the right place. In other words, there are two sources of variability at play here, (1) our uncertaintity in where the regression line is sitting, and (2) the natural variability of the data points around the line. Thus, the â€œprediction intervalâ€ requires accounting for both of these sources of variability to produce the following equation. Prediction Interval for \\(Y_h\\) \\[ \\hat{Y}_h \\pm t^* s_{Pred \\hat{Y}_h} \\quad \\text{where} \\ s_{Pred \\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + 1 + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] This formula provides a useful band for identifying a region where we are 95% confident that a new observation for \\(Y_h\\) will land, given the value of \\(X_h\\). It looks as follows. Notice the prediction interval is much wider than the confidence interval. This is because data varies far more than do means. Prediction is for where the individual data points will land, confidence is for where the mean will land. cars.lm <- lm(dist ~ speed, data=cars) predy <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + geom_segment(aes(x=15, xend=15, y=predy[2], yend=predy[3]), lwd=4, color=rgb(.5,.7,.5,.01)) + geom_point(aes(x=15, y=predy[1]), cex=2, color=\"skyblue\", pch=15) + theme_bw() Lowess (and Loess) Curves (Expand) A non-parametric approach to estimating \\(E\\{Y_i\\}\\)â€¦ Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value. Using Base R # Select the relevant columns and remove NA values air2 <- airquality %>% dplyr::select(Temp, Ozone) %>% # Use dplyr's select function explicitly drop_na() # Drop rows with NA values in Temp or Ozone plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") lines(lowess(air2$Ozone, air2$Temp), col=\"firebrick\") Using ggplot2 # Your dataset (air2) should have been created earlier, ensuring no NAs air2 <- na.omit(airquality[c(\"Temp\", \"Ozone\")]) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=\"darkgray\") + # Add points with dark gray color geom_smooth(se=FALSE, method=\"loess\", method.args = list(degree=1)) + # Lowess curve, no standard error shading theme_bw() # Black and white theme ## `geom_smooth()` using formula = 'y ~ x' # Optionally, allow for predictions as well as the graph: # Uncomment the following lines if you want predictions as well: # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fitted, x=Ozone)) # Use fitted values from loess Advantages Disadvantages Quick. Good at ignoring outliers. Good at capturing the general pattern in the data. Good for making predictions within the scope of the data. No mathematical model. Not interpretable. No p-values. No adjusted R-squared. How it Works The Lowess curve localizes the regression model to a â€œneighborhoodâ€ of points, and then joins these localized regressions together into a smooth line. It minimizes the effect of outliers, and letâ€™s the data â€œspeak for itselfâ€. As a downside, it is not interpretable, and has no final way to write the model mathematically. All the same, it is a very powerful tool for identifying an appropriate model, or verifying the fit of a model, or making predictions when no reasonable model does an adequate job. Study this graphic and the explanations below to learn how it works. Recommendation: run the code in this â€œCodeâ€ chunk to the right in your Console, and flip through the resulting graphics. X <- cars$speed Y <- cars$dist X <- X[!is.na(X) & !is.na(Y)] Y <- Y[!is.na(X) & !is.na(Y)] f <- 1/2 n <- length(X) lfit <- rep(NA,n) for (xh in 1:n){ xdists <- X - X[xh] nn <- floor(n*f) r <- sort(abs(xdists))[nn] xdists.nbrhd <- which(abs(xdists) < r) w <- rep(0, length(xdists)) w[xdists.nbrhd] <- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w), col=rgb(.2,.2,.2,.3), cex=1.5, yaxt='n', xaxt='n', xlab=\"\", ylab=\"\") points(Y[xh] ~ X[xh], pch=16, col=\"orange\") lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2)) cat(\"\\n\\n\") readline(prompt=paste0(\"Center point is point #\", xh, \"... Press [enter] to continue...\")) MADnotThereYet <- TRUE count <- 0 while(MADnotThereYet){ readline(prompt=paste0(\"\\n Adjusting line to account for outliers in the y-direction... Press [enter] to continue...\")) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"wheat\", add=TRUE) MAD <- median(abs(lmc$res)) resm <- lmc$res/(6*MAD) resm[resm>1] <- 1 bisq <- (1-resm^2)^2 w <- w*bisq obs <- coef(lmc) lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) count <- count + 1 if ( (sum(abs(obs-lmc$coef))<.1) | (count > 3)) MADnotThereYet <- FALSE } curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"green\", add=TRUE) points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=\"green\") readline(prompt=paste0(\"\\n Use final line to get fitted value for this point... Press [enter] to continue to next point...\")) lfit[xh] <- predict(lmc, data.frame(X=X[xh])) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") if (xh == n){ readline(prompt=paste0(\"\\n Press [enter] to see actual Lowess curve...\")) lines(lowess(X,Y, f=f), col=\"firebrick\") legend(\"topleft\", bty=\"n\", legend=\"Actual lowess Curve using lowess(...)\", col=\"firebrick\", lty=1) } } Select a fraction of the data to use for the â€œneighborhoodâ€ of points (shown in blue in the graph above). The lowess function in R uses â€œf=2/3â€ and the loess function uses â€œspan=0.75â€ for this value, which selects the nearest two-thirds or 75% of the data, respectively, depending on which function you use. For this example, we set the fraction of points at 50%. Both functions can be set to whatever you want. Pick any point in the regression, eventually selecting all points one at a time. The selected point becomes the â€œcenterâ€ of a â€œneighborhoodâ€ of points surrounding it. In this example, the center point is in orange, and the neighboring points are in blue. Use the points within the neighborhood to fit a regression line. However, make the regression depend most on points closest to â€œcenterâ€ and least on points furthest from â€œcenter.â€ This is called a weighted regression. Weights are decided according to what is called the tricubic weight function, so that the weight \\(w\\) given to point \\(j\\) of the neighborhood of points is defined by \\[ w_j = \\left(1- \\left( \\frac{|X_c - X_j|}{\\max_k |X_c - X_k|}\\right)^3\\right)^3 \\] where \\(X_c\\) is the x-value of the â€œcenterâ€ dot and \\(X_j\\) is the x-value of any other dot in the neighborhood. The fitted-value of \\(\\hat{Y}_c\\) is obtained for the center point \\(X_c\\) of the current regression. This point is used as the Lowess (or Loess) curveâ€™s value at that particular x-value. Well, almost. Itâ€™s a first guess at where this value will end up, but thereâ€™s a little more to the algorithm before we are done. Initial guesses for each of these fitted values are obtained for each point in the regression. Now each local regression for each neighborhood is re-run a few times in such a way the the effect of outliers is minimized. The final line for each neighborhood is obtained by the following steps. Compute all residuals for points in the neighborhood of the current regression, denoted by \\(r_i\\). Then compute the MAD, median absolute deviation, of the residuals \\(MAD = \\text{median} (|r_1|, |r_2|, \\ldots)\\). Divide all residuals by 6 times the MAD: \\(u_i = r_i/(6\\cdot MAD)\\) (If \\(r_i > 6\\cdot MAD\\) then set \\(u_i = 0\\).) Compute what are called bisquare weights using the formula: \\(b_i = (1 - u_i^2)^2\\) Perform a regression using the weights \\(w_i = w_i b_i\\) Repeat the above process with the new weights \\(w_i\\) until the weights stop changing very much. The final fitted values for each \\(X\\)-value in the regression are obtained from the final regression line for each neighborhood. These fitted values make up the Lowess (or loess) curve. Note that the default of the loess function in R is to use quadratic regressions in each neighborhood instead of linear regressions. This can be controlled with the loess option of â€œdegree=2â€ (quadratic fits) or â€œdegree = 1â€. In the lowess function only a linear regression in each neighborhood is allowed. Examples: bodyweight, cars Quantitative Y | Multiple X Graphics Y is a single quantitative variable of interest, like height. While we could use an X1 of â€œshoe-sizeâ€ to explain height, we might also want to use a second x-variable, X2, like â€œgenderâ€ to help explain height. Further x-variables could also be used. Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a studentâ€™s final exam grade? How do factors like age, weight, exercise level, and diet impact a personâ€™s blood pressure? Examples: Is one Navy base more prone to heatwaves than the other? The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH) Tests Multiple Linear Regression Multiple regression is a way to understand how different factors affect something weâ€™re interested in. Itâ€™s like having more than one clue to solve a mystery. Hereâ€™s what you need to know: We can use several different pieces of information (called variables) to predict an outcome. There are many ways to combine these pieces of information. We start with simple combinations and build up to more complex ones, kind of like building with Lego blocks. This helps us make better predictions and understand complex situations more clearly. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model is a bit like using the same ingredient twice in a recipe, but in different ways. It uses the same factor (\\(X_i^2\\)) in two different forms: Once in its regular form (\\(\\beta_1 X_i\\)) Once squared (\\(\\beta_2 X_i^2\\)) This squared part is what we call the â€œquadraticâ€ term. It helps us describe relationships that arenâ€™t just straight lines, but have curves to them. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and â€œsteepnessâ€ of the Model: negative values face down, positive values face up; large values imply â€œsteeperâ€ parabolas and low values imply â€œflatterâ€ parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)â€™s explanation. An Example Using the airquality data set, we run the following â€œquadraticâ€ regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our â€œquadraticâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. Temp Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These â€œEstimatesâ€ can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(Month^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of Month from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, ) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will â€œopen downâ€ (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be â€œmonth zero.â€ Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the â€œcubicâ€ term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the â€œbase slope coefficientâ€ and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following â€œcubicâ€ regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our â€œcubicâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. uptake Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^3) \\(X_{i}^3\\), where the function I(â€¦) protects the cubing of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will â€œopen upâ€. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The â€œtwo-linesâ€ model is a way to compare two different groups in statistics using numbers. Hereâ€™s how it works: It uses two types of numbers: A regular number that can be any value (\\(X_{1i}\\)) A special number thatâ€™s either 0 or 1 (called an â€œindicator variableâ€, \\(X_{2i}\\)) The special number (\\(X_{2i}\\)) helps us turn group information (like â€œGroup Aâ€ or â€œGroup Bâ€) into numbers we can use in calculations. This model creates two separate lines on a graph: One line for Group A One line for Group B By using this method, we can easily see and compare how the two groups are different from each other. Think of it like having two recipes for the same dish, but with slightly different ingredients. This model helps us see how those small changes affect the final result. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the â€œbase-lineâ€ of the model, the â€œGroup 0â€ line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the â€œbase-lineâ€ line. \\(\\beta_3\\) Called the â€œinteractionâ€ term. Controls the change in the slope for the second line in the model as compared to the slope of the â€œbase-lineâ€ line. An Example Using the mtcars data set, we run the following â€œtwo-linesâ€ regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our â€œtwo-linesâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. mpg Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) (beta zero): This is like the starting point. Itâ€™s estimated by something called the â€œInterceptâ€. \\(\\beta_1\\) (beta one): This shows how one thing changes when another thing (called â€œqsecâ€) changes. Itâ€™s estimated to be 1.439. \\(\\beta_2\\) (beta two): This shows how something else (called â€œamâ€) affects the overall result. Itâ€™s estimated to be -14.51. \\(\\beta_3\\) (beta three): This shows how â€œqsecâ€ and â€œamâ€ work together to affect the result. Itâ€™s estimated to be 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called â€œ3Dâ€ regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to â€œbendâ€. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the â€œTempâ€ direction is estimated to be 2.659 and the slope in the â€œMonthâ€ direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction termâ€™s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the â€œslopesâ€ in each direction to change, creating a â€œcurvedâ€ surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called â€œHDâ€, or â€œHigh Dimensionalâ€, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isnâ€™t really even possible to â€œmentally connectâ€ with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)â€™s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the â€œWindâ€ direction is estimated to be -3.334. The slope in the â€œTempâ€ direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the â€œSolar.Râ€ direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here. R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the â€œOverviewâ€ section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the â€œcolumn (c) bindâ€ function and it joins together things as columns. Res =Â  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,Â  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals. Â panel=panel.smooth,Â  This places a lowess smoothing line on each scatterplot. col =Â  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(â€œcolor1â€,â€œcolor2â€, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for â€œlinear model.â€ Y Y must be a â€œnumericâ€ vector of the quantitative response variable. Â ~Â  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats â€œnumericâ€ variables as quantitative and â€œcharacterâ€ or â€œfactorâ€ variables as qualitative. R will automatcially recode qualitative variables to become â€œnumericâ€ variables using a 0,1 encoding. See the Explanation tab for details. Â + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details. Â + â€¦, * ... emphasizes that as many explanatory variables as are desired can be included in the model. Â data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(â€¦) function displays the results of an lm(â€¦) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(â€¦) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(â€¦). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min Â  -4.3818 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -2.2696 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.1344 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  1.7058 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  5.8752 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  t value To learn more about the â€œt valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  Pr(>|t|) The â€œPrâ€ stands for â€œProbabilityâ€ and the â€œ(> |t|)â€ stands for â€œmore extreme than the observed t-valueâ€. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the â€œp-valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. (Intercept) This always says â€œInterceptâ€ for any lm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero. Â  2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the intercept (26.6248479) by its standard error (2.1829432). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a â€œstarâ€. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + â€¦). Â  -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model. Â  0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the â€œEstimateâ€ by its standard error. It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + â€¦). Â  5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\). Â  2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot â€œ.â€ implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like. Â  0.0004029 This is the estimate of the coefficient of the interaction term. Â  0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE. Â 2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\). Â on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO. Â 0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car. Â Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data. Â 0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that â€œat least one is not.â€ Â 34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom. Â on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero. Â p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that â€œat least oneâ€ of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the â€œOverviewâ€ sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the â€œfitted-valueâ€ or â€œpredicted-valueâ€ for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œpredictionâ€) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance canâ€™t go negative) feet to 60.96915 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œconfidenceâ€) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BICâ€¦ There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (â€œR-squaredâ€). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The â€œsimplestâ€ but â€œbestâ€ model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it â€œan information criterion (AIC)â€ when he published the method and other people later began calling it the â€œAkaike Information Criterion.â€) Model Selection (Expand) pairs plots, added variable plots, and pattern recognitionâ€¦ Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots {.tabset .tabset-pills} A useful visualization tool for model selection is the â€œpairs plot.â€ This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice thatâ€¦ the y-axis of each plot is found by locating the variable name (like â€œmpgâ€) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like â€œdispâ€) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model {.tabset .tabset-pills} Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldnâ€™t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Letâ€™s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander() Â  Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a modelâ€™s ability to generalize to new dataâ€¦ The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesnâ€™t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, letâ€™s remind ourselves why we use regression models in the first place. The main goal is to capture the â€œessenceâ€ of the data. In other words, the general pattern is what we are after. We want a model that tells us how â€œall suchâ€ data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the â€œcomplicated modelâ€ is overfitting the original data. It does not capture the â€œessenceâ€ of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-valueâ€¦ The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the modelâ€¦ The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions{#check} The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read moreâ€¦) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) Outlier Analysis (Expand) Cookâ€™s Distances and Leverage Valuesâ€¦ The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs.Â fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cookâ€™s Distances The idea behind Cookâ€™s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article â€œDetection of Influential Observation in Linear Regressionâ€ (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, letâ€™s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2)) Â  1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the â€œdifferencesâ€ in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cookâ€™s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cookâ€™s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cookâ€™s Distances using the code cooks.distance(lmObject). Also, a graph of Cookâ€™s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of â€œleverageâ€ and is â€œpullingâ€ the regression toward itself. A value near 0 implies the point is just â€œone of manyâ€ and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that â€œminimizeâ€ the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)â€™s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the â€œhat matrixâ€ \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the â€œleverage valuesâ€ and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a â€œlot of pull,â€ and values close to 0 represent â€œlittle pull.â€ In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with â€œlots of leverageâ€ and a large â€œCookâ€™s Distanceâ€ are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regressionâ€¦ Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class. Examples: Civic Vs Corolla cadillacs Binomial Y | Quantitative X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t. We might use an explanatory variable X of â€œheightâ€ to see if taller students are more likely to get an A in Math 325 than shorter students. (They arenâ€™t, if you were wondering.) Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1â€™s and 0â€™s (this does happen or this doesnâ€™t happen): Does the width of a childâ€™s foot predict whether or not they are right-handed? What is the probability the cat adopted is a male based on how many days it took them to get adopted? Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted? (Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\") Tests Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The â€œPâ€ stands for â€œProbability thatâ€¦â€ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1â€¦ This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)â€¦ in other words, the â€œ|â€ says â€œgivenâ€ and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the â€œnatural constantâ€ number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14â€¦) Part What it means \\(P(Y_i = 1| x_i)\\) - The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s) - The symbol used to represent the notation in logisitc regression is \\(\\pi_i\\) \\(Y_i\\) (Responsive Variable) Describes the probability of $Y_i = $ 1 or 0 for a given scenario. \\(x_i\\) (Explanatory variable) Describes the observed value and how it effects the \\(Y_i\\) \\(\\frac{e^{\\text{stuff}}}{1+e^{\\text{stuff}}}\\) The content for â€œstuffâ€ being simple linear regression of the form \\(\\beta_0 + \\beta_{1x_i}\\) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -1.5651 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.6648 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -0.2460 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  0.7276 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  2.2691 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The test statistic is a regular old z-score. It is most reliable when the sample size is â€œlarge.â€ It is a measurement of the number of standard errors the estimate is from 0. Â  Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds. Â  1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds. Â  0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a â€œstarâ€. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about. Â  Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\). Â 43.230 Â on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates. Â 29.732 This can be calculated by sum(log(myglm$res^2)). Â on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, â€œA version of Akaikeâ€™s An Information Criterionâ€¦â€ The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model. Â 33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 * ## disp -0.014604 0.005168 -2.826 0.00471 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0â€™s or 1â€™s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == â€œBâ€ or height < 68. In other words, Y needs to be a collection of 0â€™s and 1â€™s. Â ~Â  The tilde is read â€œY on X.â€ X, X is some quantitative variable. Â data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(â€¦) function. curve( A function in R that draws a â€œcurveâ€ on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the â€œInterceptâ€ estimate from your logistic regression summary output. Â \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case â€œxâ€ in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula. Â exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator. Â add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(â€¦) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set. Â aes( The aes(â€¦) function stands for â€œaestheticsâ€ and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The plus sign adds a new layer to the ggplot. Â  geom_point( Tells the plot to add the physical geometry of â€œpointsâ€ to the plot. ) Closing parenthesis for the geom_point() function. Â + Add another layer to the plot. Â  geom_smooth( Add a smoothing line to the graph. method=â€œglmâ€, This adds a general linear model to the graph. method.args = list(family=â€œbinomialâ€), This tells the method=â€œglmâ€ to choose specifically the â€œbinomialâ€ model, otherwise known as the â€œlogistic regressionâ€ model. Â se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function. Â + Add another layer to the ggplot. Â  theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),Â  The xVariableName is the same one you used in your glm(y ~ x, â€¦) statement for â€œxâ€. Input any desired value for the â€œsomeNumericValueâ€ spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016 Explanation Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal). The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\). Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page. Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful. Checking Model Assumptions {#diagnostics} The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit. Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease. Binomial Y | Multiple X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t. We might use an explanatory variable X1 of â€œheightâ€ and a second explanatory variable X2 of â€œgenderâ€ to try to predict whether or not a student will get an A in Math 325. Test(s) used for this category: Multiple Logistic Regression Model Tests Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two. Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2Â  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. â€¦, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -2.9446 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.2364 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.0000 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q Â  0.2776 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  1.9968 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The Z-score testing the hypothesis that \\(\\beta_j=0\\) Â  Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\). Â  1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\). Â  4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\). Â  3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\). Â  0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\). Â  0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a â€œstarâ€. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\). Â  471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option. Â  Null Deviance: The deviance of the null model. Â 800.44 In this case, the null deviance is 800.44 Â on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression. Â 256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit. Â on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model. Â 272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -4.97603 0.65316 -7.618 2.57e-14 *** ## Time 0.39111 0.04979 7.854 4.02e-15 *** ## Diet2 0.58283 1.04061 0.560 0.5754 ## Diet3 -8.44476 4.32324 -1.953 0.0508 . ## Diet4 -67.41995 3768.10023 -0.018 0.9857 ## Time:Diet2 0.02391 0.08679 0.275 0.7830 ## Time:Diet3 1.20955 0.51249 2.360 0.0183 * ## Time:Diet4 8.83168 471.01259 0.019 0.9850 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,Â  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, â€¦), X2 = c(Value 1, Value 2, â€¦), â€¦) command. You should see the GSS example file for an example of how to use this function. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the â€œbâ€ vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(â€œSomeColorâ€,â€œDifferentColorâ€,â€¦) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(â€¦) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==â€œBâ€. Â ~Â  The formula operator in R. X1, The first x-variable in your glm code. Â data = YourDataSet, Specify the name of your data set. Â pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(â€¦) function computes e^(stuff) in R and stands for the â€œexponential function.â€ (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(â€¦) function requires that you call the x-variable â€œxâ€ although you can change this behavior using xname=â€œSomeOtherNameâ€ if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)). Â col = palette()[1], Pull the first color from the color palette for the first curve. Â add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters. Â col = palette()[2], Set the color of this second curve to the second color in the palette. Â add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. â€œtoprightâ€, Typically the legend is placed in the top-right corner of the graph. But it could be placed â€œtopâ€, â€œtopleftâ€, â€œleftâ€, â€œbottomleftâ€, â€œbottomâ€, â€œcenterâ€, â€œrightâ€, or â€œbottomrightâ€. Â legend = Why you have to write legend again, no one knows, but you do. c(â€œLable 1â€, â€œLabel 2â€), These are the values that will appear in the legend, one entry on each line of the legend. Â col = palette(), This specifies the colors of the symbols in the legend. First color goes with â€œLabel 1â€, and second goes with â€œLabel 2â€ and so on. Â lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist. Â bty = Specifies the â€œbox typeâ€ (bty) that should be drawn around the legend. â€˜nâ€™) By placing a â€œnoâ€ option (â€˜nâ€™) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot. Â aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + Add a layer to the ggplot. Â  geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function. Â + Add a layer to the ggplot. Â  geom_smooth(method=â€œglmâ€, method.args=list(family=â€œbinomialâ€), se=FALSE, Add the logistic regression curve to the plot. Â  aes(color=â€œX2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function. Â + Add a layer to the plot. Â  theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x' Explanation Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative. The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations. Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant. Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression. Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\). Categorical Y | Categorical X Graphics Y is a single categorical variable of interest, like gender. X is another categorical variable of interest, like â€œhair colorâ€. This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender. Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year? Whats the probability that you have brown hair and blue eyes? Tests Add your own notes about appropriate inferential procedures for this type of data here.",
    "sentences": ["Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\).", "Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$\\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$$ \\(Y_i\\) The response variable.", "The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size.", "\\(=\\) This states that we are assuming \\(Y_i\\) was created, or is â€œequal toâ€ the formula that will follow on the right-hand-side of the equation.", "\\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life.", "It can be likened to â€œGodâ€™s Lawâ€ or â€œNatural Lawâ€.", "Something that governs the way the data behaves, but is unkown to us.", "\\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\).", "\\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\).", "The error terms are â€œrandomâ€ and unique for each individual."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 40,
    "title": "ğŸ“ Quantitative Y | Multiple X",
    "url": "index.html#quantitativeymultiplex",
    "content": "Quantitative Y | Multiple X Graphics Y is a single quantitative variable of interest, like height. While we could use an X1 of â€œshoe-sizeâ€ to explain height, we might also want to use a second x-variable, X2, like â€œgenderâ€ to help explain height. Further x-variables could also be used. Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a studentâ€™s final exam grade? How do factors like age, weight, exercise level, and diet impact a personâ€™s blood pressure? Examples: Is one Navy base more prone to heatwaves than the other? The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH) Tests Multiple Linear Regression Multiple regression is a way to understand how different factors affect something weâ€™re interested in. Itâ€™s like having more than one clue to solve a mystery. Hereâ€™s what you need to know: We can use several different pieces of information (called variables) to predict an outcome. There are many ways to combine these pieces of information. We start with simple combinations and build up to more complex ones, kind of like building with Lego blocks. This helps us make better predictions and understand complex situations more clearly. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model is a bit like using the same ingredient twice in a recipe, but in different ways. It uses the same factor (\\(X_i^2\\)) in two different forms: Once in its regular form (\\(\\beta_1 X_i\\)) Once squared (\\(\\beta_2 X_i^2\\)) This squared part is what we call the â€œquadraticâ€ term. It helps us describe relationships that arenâ€™t just straight lines, but have curves to them. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and â€œsteepnessâ€ of the Model: negative values face down, positive values face up; large values imply â€œsteeperâ€ parabolas and low values imply â€œflatterâ€ parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)â€™s explanation. An Example Using the airquality data set, we run the following â€œquadraticâ€ regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our â€œquadraticâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. Temp Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These â€œEstimatesâ€ can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(Month^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of Month from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, ) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will â€œopen downâ€ (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be â€œmonth zero.â€ Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the â€œcubicâ€ term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the â€œbase slope coefficientâ€ and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following â€œcubicâ€ regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our â€œcubicâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. uptake Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^3) \\(X_{i}^3\\), where the function I(â€¦) protects the cubing of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will â€œopen upâ€. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The â€œtwo-linesâ€ model is a way to compare two different groups in statistics using numbers. Hereâ€™s how it works: It uses two types of numbers: A regular number that can be any value (\\(X_{1i}\\)) A special number thatâ€™s either 0 or 1 (called an â€œindicator variableâ€, \\(X_{2i}\\)) The special number (\\(X_{2i}\\)) helps us turn group information (like â€œGroup Aâ€ or â€œGroup Bâ€) into numbers we can use in calculations. This model creates two separate lines on a graph: One line for Group A One line for Group B By using this method, we can easily see and compare how the two groups are different from each other. Think of it like having two recipes for the same dish, but with slightly different ingredients. This model helps us see how those small changes affect the final result. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the â€œbase-lineâ€ of the model, the â€œGroup 0â€ line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the â€œbase-lineâ€ line. \\(\\beta_3\\) Called the â€œinteractionâ€ term. Controls the change in the slope for the second line in the model as compared to the slope of the â€œbase-lineâ€ line. An Example Using the mtcars data set, we run the following â€œtwo-linesâ€ regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our â€œtwo-linesâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. mpg Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) (beta zero): This is like the starting point. Itâ€™s estimated by something called the â€œInterceptâ€. \\(\\beta_1\\) (beta one): This shows how one thing changes when another thing (called â€œqsecâ€) changes. Itâ€™s estimated to be 1.439. \\(\\beta_2\\) (beta two): This shows how something else (called â€œamâ€) affects the overall result. Itâ€™s estimated to be -14.51. \\(\\beta_3\\) (beta three): This shows how â€œqsecâ€ and â€œamâ€ work together to affect the result. Itâ€™s estimated to be 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called â€œ3Dâ€ regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to â€œbendâ€. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the â€œTempâ€ direction is estimated to be 2.659 and the slope in the â€œMonthâ€ direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction termâ€™s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the â€œslopesâ€ in each direction to change, creating a â€œcurvedâ€ surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called â€œHDâ€, or â€œHigh Dimensionalâ€, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isnâ€™t really even possible to â€œmentally connectâ€ with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)â€™s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the â€œWindâ€ direction is estimated to be -3.334. The slope in the â€œTempâ€ direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the â€œSolar.Râ€ direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here. R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the â€œOverviewâ€ section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the â€œcolumn (c) bindâ€ function and it joins together things as columns. Res =Â  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,Â  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals. Â panel=panel.smooth,Â  This places a lowess smoothing line on each scatterplot. col =Â  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(â€œcolor1â€,â€œcolor2â€, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for â€œlinear model.â€ Y Y must be a â€œnumericâ€ vector of the quantitative response variable. Â ~Â  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats â€œnumericâ€ variables as quantitative and â€œcharacterâ€ or â€œfactorâ€ variables as qualitative. R will automatcially recode qualitative variables to become â€œnumericâ€ variables using a 0,1 encoding. See the Explanation tab for details. Â + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details. Â + â€¦, * ... emphasizes that as many explanatory variables as are desired can be included in the model. Â data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(â€¦) function displays the results of an lm(â€¦) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(â€¦) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(â€¦). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min Â  -4.3818 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -2.2696 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.1344 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  1.7058 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  5.8752 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  t value To learn more about the â€œt valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  Pr(>|t|) The â€œPrâ€ stands for â€œProbabilityâ€ and the â€œ(> |t|)â€ stands for â€œmore extreme than the observed t-valueâ€. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the â€œp-valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. (Intercept) This always says â€œInterceptâ€ for any lm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero. Â  2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the intercept (26.6248479) by its standard error (2.1829432). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a â€œstarâ€. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + â€¦). Â  -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model. Â  0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the â€œEstimateâ€ by its standard error. It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + â€¦). Â  5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\). Â  2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot â€œ.â€ implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like. Â  0.0004029 This is the estimate of the coefficient of the interaction term. Â  0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE. Â 2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\). Â on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO. Â 0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car. Â Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data. Â 0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that â€œat least one is not.â€ Â 34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom. Â on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero. Â p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that â€œat least oneâ€ of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the â€œOverviewâ€ sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the â€œfitted-valueâ€ or â€œpredicted-valueâ€ for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œpredictionâ€) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance canâ€™t go negative) feet to 60.96915 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œconfidenceâ€) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BICâ€¦ There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (â€œR-squaredâ€). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The â€œsimplestâ€ but â€œbestâ€ model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it â€œan information criterion (AIC)â€ when he published the method and other people later began calling it the â€œAkaike Information Criterion.â€) Model Selection (Expand) pairs plots, added variable plots, and pattern recognitionâ€¦ Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots {.tabset .tabset-pills} A useful visualization tool for model selection is the â€œpairs plot.â€ This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice thatâ€¦ the y-axis of each plot is found by locating the variable name (like â€œmpgâ€) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like â€œdispâ€) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model {.tabset .tabset-pills} Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldnâ€™t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Letâ€™s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander() Â  Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a modelâ€™s ability to generalize to new dataâ€¦ The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesnâ€™t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, letâ€™s remind ourselves why we use regression models in the first place. The main goal is to capture the â€œessenceâ€ of the data. In other words, the general pattern is what we are after. We want a model that tells us how â€œall suchâ€ data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the â€œcomplicated modelâ€ is overfitting the original data. It does not capture the â€œessenceâ€ of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-valueâ€¦ The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the modelâ€¦ The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions{#check} The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read moreâ€¦) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) Outlier Analysis (Expand) Cookâ€™s Distances and Leverage Valuesâ€¦ The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs.Â fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cookâ€™s Distances The idea behind Cookâ€™s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article â€œDetection of Influential Observation in Linear Regressionâ€ (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, letâ€™s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2)) Â  1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the â€œdifferencesâ€ in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cookâ€™s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cookâ€™s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cookâ€™s Distances using the code cooks.distance(lmObject). Also, a graph of Cookâ€™s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of â€œleverageâ€ and is â€œpullingâ€ the regression toward itself. A value near 0 implies the point is just â€œone of manyâ€ and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that â€œminimizeâ€ the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)â€™s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the â€œhat matrixâ€ \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the â€œleverage valuesâ€ and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a â€œlot of pull,â€ and values close to 0 represent â€œlittle pull.â€ In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with â€œlots of leverageâ€ and a large â€œCookâ€™s Distanceâ€ are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regressionâ€¦ Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class. Examples: Civic Vs Corolla cadillacs",
    "sentences": ["Graphics Y is a single quantitative variable of interest, like height.", "While we could use an X1 of â€œshoe-sizeâ€ to explain height, we might also want to use a second x-variable, X2, like â€œgenderâ€ to help explain height.", "Further x-variables could also be used.", "Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a studentâ€™s final exam grade?", "How do factors like age, weight, exercise level, and diet impact a personâ€™s blood pressure?", "Examples: Is one Navy base more prone to heatwaves than the other?", "The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH)", "Tests Multiple Linear Regression Multiple regression is a way to understand how different factors affect something weâ€™re interested in.", "Itâ€™s like having more than one clue to solve a mystery.", "Hereâ€™s what you need to know: We can use several different pieces of information (called variables) to predict an outcome."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Quantitative Y | Multiple X"
  },
  {
    "id": 41,
    "title": "ğŸ“ Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single quantitative variable of interest, like height. While we could use an X1 of â€œshoe-sizeâ€ to explain height, we might also want to use a second x-variable, X2, like â€œgenderâ€ to help explain height. Further x-variables could also be used. Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a studentâ€™s final exam grade? How do factors like age, weight, exercise level, and diet impact a personâ€™s blood pressure? Examples: Is one Navy base more prone to heatwaves than the other? The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH)",
    "sentences": ["Y is a single quantitative variable of interest, like height.", "While we could use an X1 of â€œshoe-sizeâ€ to explain height, we might also want to use a second x-variable, X2, like â€œgenderâ€ to help explain height.", "Further x-variables could also be used.", "Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a studentâ€™s final exam grade?", "How do factors like age, weight, exercise level, and diet impact a personâ€™s blood pressure?", "Examples: Is one Navy base more prone to heatwaves than the other?", "The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH)"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 42,
    "title": "ğŸ“ Tests",
    "url": "index.html#tests",
    "content": "Tests Multiple Linear Regression Multiple regression is a way to understand how different factors affect something weâ€™re interested in. Itâ€™s like having more than one clue to solve a mystery. Hereâ€™s what you need to know: We can use several different pieces of information (called variables) to predict an outcome. There are many ways to combine these pieces of information. We start with simple combinations and build up to more complex ones, kind of like building with Lego blocks. This helps us make better predictions and understand complex situations more clearly. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model is a bit like using the same ingredient twice in a recipe, but in different ways. It uses the same factor (\\(X_i^2\\)) in two different forms: Once in its regular form (\\(\\beta_1 X_i\\)) Once squared (\\(\\beta_2 X_i^2\\)) This squared part is what we call the â€œquadraticâ€ term. It helps us describe relationships that arenâ€™t just straight lines, but have curves to them. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and â€œsteepnessâ€ of the Model: negative values face down, positive values face up; large values imply â€œsteeperâ€ parabolas and low values imply â€œflatterâ€ parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)â€™s explanation. An Example Using the airquality data set, we run the following â€œquadraticâ€ regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our â€œquadraticâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. Temp Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These â€œEstimatesâ€ can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(Month^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of Month from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, ) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will â€œopen downâ€ (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be â€œmonth zero.â€ Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the â€œcubicâ€ term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the â€œbase slope coefficientâ€ and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following â€œcubicâ€ regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our â€œcubicâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. uptake Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^3) \\(X_{i}^3\\), where the function I(â€¦) protects the cubing of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will â€œopen upâ€. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The â€œtwo-linesâ€ model is a way to compare two different groups in statistics using numbers. Hereâ€™s how it works: It uses two types of numbers: A regular number that can be any value (\\(X_{1i}\\)) A special number thatâ€™s either 0 or 1 (called an â€œindicator variableâ€, \\(X_{2i}\\)) The special number (\\(X_{2i}\\)) helps us turn group information (like â€œGroup Aâ€ or â€œGroup Bâ€) into numbers we can use in calculations. This model creates two separate lines on a graph: One line for Group A One line for Group B By using this method, we can easily see and compare how the two groups are different from each other. Think of it like having two recipes for the same dish, but with slightly different ingredients. This model helps us see how those small changes affect the final result. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the â€œbase-lineâ€ of the model, the â€œGroup 0â€ line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the â€œbase-lineâ€ line. \\(\\beta_3\\) Called the â€œinteractionâ€ term. Controls the change in the slope for the second line in the model as compared to the slope of the â€œbase-lineâ€ line. An Example Using the mtcars data set, we run the following â€œtwo-linesâ€ regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our â€œtwo-linesâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. mpg Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) (beta zero): This is like the starting point. Itâ€™s estimated by something called the â€œInterceptâ€. \\(\\beta_1\\) (beta one): This shows how one thing changes when another thing (called â€œqsecâ€) changes. Itâ€™s estimated to be 1.439. \\(\\beta_2\\) (beta two): This shows how something else (called â€œamâ€) affects the overall result. Itâ€™s estimated to be -14.51. \\(\\beta_3\\) (beta three): This shows how â€œqsecâ€ and â€œamâ€ work together to affect the result. Itâ€™s estimated to be 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called â€œ3Dâ€ regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to â€œbendâ€. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the â€œTempâ€ direction is estimated to be 2.659 and the slope in the â€œMonthâ€ direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction termâ€™s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the â€œslopesâ€ in each direction to change, creating a â€œcurvedâ€ surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called â€œHDâ€, or â€œHigh Dimensionalâ€, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isnâ€™t really even possible to â€œmentally connectâ€ with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)â€™s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the â€œWindâ€ direction is estimated to be -3.334. The slope in the â€œTempâ€ direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the â€œSolar.Râ€ direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here. R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the â€œOverviewâ€ section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the â€œcolumn (c) bindâ€ function and it joins together things as columns. Res =Â  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,Â  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals. Â panel=panel.smooth,Â  This places a lowess smoothing line on each scatterplot. col =Â  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(â€œcolor1â€,â€œcolor2â€, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for â€œlinear model.â€ Y Y must be a â€œnumericâ€ vector of the quantitative response variable. Â ~Â  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats â€œnumericâ€ variables as quantitative and â€œcharacterâ€ or â€œfactorâ€ variables as qualitative. R will automatcially recode qualitative variables to become â€œnumericâ€ variables using a 0,1 encoding. See the Explanation tab for details. Â + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details. Â + â€¦, * ... emphasizes that as many explanatory variables as are desired can be included in the model. Â data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(â€¦) function displays the results of an lm(â€¦) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(â€¦) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(â€¦). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min Â  -4.3818 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -2.2696 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.1344 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  1.7058 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  5.8752 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  t value To learn more about the â€œt valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  Pr(>|t|) The â€œPrâ€ stands for â€œProbabilityâ€ and the â€œ(> |t|)â€ stands for â€œmore extreme than the observed t-valueâ€. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the â€œp-valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. (Intercept) This always says â€œInterceptâ€ for any lm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero. Â  2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the intercept (26.6248479) by its standard error (2.1829432). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a â€œstarâ€. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + â€¦). Â  -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model. Â  0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the â€œEstimateâ€ by its standard error. It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + â€¦). Â  5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\). Â  2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot â€œ.â€ implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like. Â  0.0004029 This is the estimate of the coefficient of the interaction term. Â  0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE. Â 2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\). Â on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO. Â 0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car. Â Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data. Â 0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that â€œat least one is not.â€ Â 34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom. Â on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero. Â p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that â€œat least oneâ€ of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the â€œOverviewâ€ sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the â€œfitted-valueâ€ or â€œpredicted-valueâ€ for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œpredictionâ€) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance canâ€™t go negative) feet to 60.96915 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œconfidenceâ€) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BICâ€¦ There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (â€œR-squaredâ€). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The â€œsimplestâ€ but â€œbestâ€ model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it â€œan information criterion (AIC)â€ when he published the method and other people later began calling it the â€œAkaike Information Criterion.â€) Model Selection (Expand) pairs plots, added variable plots, and pattern recognitionâ€¦ Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots {.tabset .tabset-pills} A useful visualization tool for model selection is the â€œpairs plot.â€ This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice thatâ€¦ the y-axis of each plot is found by locating the variable name (like â€œmpgâ€) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like â€œdispâ€) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model {.tabset .tabset-pills} Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldnâ€™t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Letâ€™s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander() Â  Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a modelâ€™s ability to generalize to new dataâ€¦ The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesnâ€™t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, letâ€™s remind ourselves why we use regression models in the first place. The main goal is to capture the â€œessenceâ€ of the data. In other words, the general pattern is what we are after. We want a model that tells us how â€œall suchâ€ data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the â€œcomplicated modelâ€ is overfitting the original data. It does not capture the â€œessenceâ€ of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-valueâ€¦ The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the modelâ€¦ The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions{#check} The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read moreâ€¦) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) Outlier Analysis (Expand) Cookâ€™s Distances and Leverage Valuesâ€¦ The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs.Â fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cookâ€™s Distances The idea behind Cookâ€™s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article â€œDetection of Influential Observation in Linear Regressionâ€ (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, letâ€™s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2)) Â  1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the â€œdifferencesâ€ in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cookâ€™s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cookâ€™s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cookâ€™s Distances using the code cooks.distance(lmObject). Also, a graph of Cookâ€™s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of â€œleverageâ€ and is â€œpullingâ€ the regression toward itself. A value near 0 implies the point is just â€œone of manyâ€ and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that â€œminimizeâ€ the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)â€™s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the â€œhat matrixâ€ \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the â€œleverage valuesâ€ and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a â€œlot of pull,â€ and values close to 0 represent â€œlittle pull.â€ In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with â€œlots of leverageâ€ and a large â€œCookâ€™s Distanceâ€ are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regressionâ€¦ Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class. Examples: Civic Vs Corolla cadillacs",
    "sentences": ["Multiple Linear Regression Multiple regression is a way to understand how different factors affect something weâ€™re interested in.", "Itâ€™s like having more than one clue to solve a mystery.", "Hereâ€™s what you need to know: We can use several different pieces of information (called variables) to predict an outcome.", "There are many ways to combine these pieces of information.", "We start with simple combinations and build up to more complex ones, kind of like building with Lego blocks.", "This helps us make better predictions and understand complex situations more clearly.", "Overview Select a model to see interpretation details, an example, and R Code help.", "Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\).", "Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model is a bit like using the same ingredient twice in a recipe, but in different ways.", "It uses the same factor (\\(X_i^2\\)) in two different forms: Once in its regular form (\\(\\beta_1 X_i\\)) Once squared (\\(\\beta_2 X_i^2\\)) This squared part is what we call the â€œquadraticâ€ term."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 43,
    "title": "ğŸ“ Binomial Y | Quantitative X",
    "url": "index.html#binomialyquantitativex",
    "content": "Binomial Y | Quantitative X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t. We might use an explanatory variable X of â€œheightâ€ to see if taller students are more likely to get an A in Math 325 than shorter students. (They arenâ€™t, if you were wondering.) Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1â€™s and 0â€™s (this does happen or this doesnâ€™t happen): Does the width of a childâ€™s foot predict whether or not they are right-handed? What is the probability the cat adopted is a male based on how many days it took them to get adopted? Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted? (Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\") Tests Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The â€œPâ€ stands for â€œProbability thatâ€¦â€ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1â€¦ This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)â€¦ in other words, the â€œ|â€ says â€œgivenâ€ and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the â€œnatural constantâ€ number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14â€¦) Part What it means \\(P(Y_i = 1| x_i)\\) - The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s) - The symbol used to represent the notation in logisitc regression is \\(\\pi_i\\) \\(Y_i\\) (Responsive Variable) Describes the probability of $Y_i = $ 1 or 0 for a given scenario. \\(x_i\\) (Explanatory variable) Describes the observed value and how it effects the \\(Y_i\\) \\(\\frac{e^{\\text{stuff}}}{1+e^{\\text{stuff}}}\\) The content for â€œstuffâ€ being simple linear regression of the form \\(\\beta_0 + \\beta_{1x_i}\\) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -1.5651 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.6648 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -0.2460 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  0.7276 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  2.2691 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The test statistic is a regular old z-score. It is most reliable when the sample size is â€œlarge.â€ It is a measurement of the number of standard errors the estimate is from 0. Â  Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds. Â  1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds. Â  0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a â€œstarâ€. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about. Â  Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\). Â 43.230 Â on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates. Â 29.732 This can be calculated by sum(log(myglm$res^2)). Â on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, â€œA version of Akaikeâ€™s An Information Criterionâ€¦â€ The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model. Â 33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 * ## disp -0.014604 0.005168 -2.826 0.00471 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0â€™s or 1â€™s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == â€œBâ€ or height < 68. In other words, Y needs to be a collection of 0â€™s and 1â€™s. Â ~Â  The tilde is read â€œY on X.â€ X, X is some quantitative variable. Â data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(â€¦) function. curve( A function in R that draws a â€œcurveâ€ on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the â€œInterceptâ€ estimate from your logistic regression summary output. Â \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case â€œxâ€ in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula. Â exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator. Â add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(â€¦) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set. Â aes( The aes(â€¦) function stands for â€œaestheticsâ€ and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The plus sign adds a new layer to the ggplot. Â  geom_point( Tells the plot to add the physical geometry of â€œpointsâ€ to the plot. ) Closing parenthesis for the geom_point() function. Â + Add another layer to the plot. Â  geom_smooth( Add a smoothing line to the graph. method=â€œglmâ€, This adds a general linear model to the graph. method.args = list(family=â€œbinomialâ€), This tells the method=â€œglmâ€ to choose specifically the â€œbinomialâ€ model, otherwise known as the â€œlogistic regressionâ€ model. Â se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function. Â + Add another layer to the ggplot. Â  theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),Â  The xVariableName is the same one you used in your glm(y ~ x, â€¦) statement for â€œxâ€. Input any desired value for the â€œsomeNumericValueâ€ spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016 Explanation Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal). The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\). Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page. Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful. Checking Model Assumptions {#diagnostics} The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit. Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease.",
    "sentences": ["Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y.", "This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t.", "We might use an explanatory variable X of â€œheightâ€ to see if taller students are more likely to get an A in Math 325 than shorter students.", "(They arenâ€™t, if you were wondering.) Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1â€™s and 0â€™s (this does happen or this doesnâ€™t happen): Does the width of a childâ€™s foot predict whether or not they are right-handed?", "What is the probability the cat adopted is a male based on how many days it took them to get adopted?", "Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted?", "(Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\")", "Tests Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable.", "Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The â€œPâ€ stands for â€œProbability thatâ€¦â€ \\(Y_i\\) The response variable.", "The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Binomial Y | Quantitative X"
  },
  {
    "id": 44,
    "title": "ğŸ“ Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t. We might use an explanatory variable X of â€œheightâ€ to see if taller students are more likely to get an A in Math 325 than shorter students. (They arenâ€™t, if you were wondering.) Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1â€™s and 0â€™s (this does happen or this doesnâ€™t happen): Does the width of a childâ€™s foot predict whether or not they are right-handed? What is the probability the cat adopted is a male based on how many days it took them to get adopted? Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted? (Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\")",
    "sentences": ["Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y.", "This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t.", "We might use an explanatory variable X of â€œheightâ€ to see if taller students are more likely to get an A in Math 325 than shorter students.", "(They arenâ€™t, if you were wondering.)", "Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1â€™s and 0â€™s (this does happen or this doesnâ€™t happen): Does the width of a childâ€™s foot predict whether or not they are right-handed?", "What is the probability the cat adopted is a male based on how many days it took them to get adopted?", "Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted?", "(Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\")"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 45,
    "title": "ğŸ“ Tests",
    "url": "index.html#tests",
    "content": "Tests Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The â€œPâ€ stands for â€œProbability thatâ€¦â€ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1â€¦ This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)â€¦ in other words, the â€œ|â€ says â€œgivenâ€ and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the â€œnatural constantâ€ number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14â€¦) Part What it means \\(P(Y_i = 1| x_i)\\) - The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s) - The symbol used to represent the notation in logisitc regression is \\(\\pi_i\\) \\(Y_i\\) (Responsive Variable) Describes the probability of $Y_i = $ 1 or 0 for a given scenario. \\(x_i\\) (Explanatory variable) Describes the observed value and how it effects the \\(Y_i\\) \\(\\frac{e^{\\text{stuff}}}{1+e^{\\text{stuff}}}\\) The content for â€œstuffâ€ being simple linear regression of the form \\(\\beta_0 + \\beta_{1x_i}\\) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -1.5651 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.6648 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -0.2460 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  0.7276 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  2.2691 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The test statistic is a regular old z-score. It is most reliable when the sample size is â€œlarge.â€ It is a measurement of the number of standard errors the estimate is from 0. Â  Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds. Â  1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds. Â  0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a â€œstarâ€. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about. Â  Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\). Â 43.230 Â on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates. Â 29.732 This can be calculated by sum(log(myglm$res^2)). Â on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, â€œA version of Akaikeâ€™s An Information Criterionâ€¦â€ The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model. Â 33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 * ## disp -0.014604 0.005168 -2.826 0.00471 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0â€™s or 1â€™s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == â€œBâ€ or height < 68. In other words, Y needs to be a collection of 0â€™s and 1â€™s. Â ~Â  The tilde is read â€œY on X.â€ X, X is some quantitative variable. Â data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(â€¦) function. curve( A function in R that draws a â€œcurveâ€ on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the â€œInterceptâ€ estimate from your logistic regression summary output. Â \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case â€œxâ€ in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula. Â exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator. Â add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(â€¦) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set. Â aes( The aes(â€¦) function stands for â€œaestheticsâ€ and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The plus sign adds a new layer to the ggplot. Â  geom_point( Tells the plot to add the physical geometry of â€œpointsâ€ to the plot. ) Closing parenthesis for the geom_point() function. Â + Add another layer to the plot. Â  geom_smooth( Add a smoothing line to the graph. method=â€œglmâ€, This adds a general linear model to the graph. method.args = list(family=â€œbinomialâ€), This tells the method=â€œglmâ€ to choose specifically the â€œbinomialâ€ model, otherwise known as the â€œlogistic regressionâ€ model. Â se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function. Â + Add another layer to the ggplot. Â  theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),Â  The xVariableName is the same one you used in your glm(y ~ x, â€¦) statement for â€œxâ€. Input any desired value for the â€œsomeNumericValueâ€ spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016 Explanation Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal). The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\). Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page. Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful. Checking Model Assumptions {#diagnostics} The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit. Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease.",
    "sentences": ["Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable.", "Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The â€œPâ€ stands for â€œProbability thatâ€¦â€ \\(Y_i\\) The response variable.", "The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size.", "\\(= 1\\) Equals 1â€¦ This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual.", "\\(| x_i)\\) Given \\(x_i\\)â€¦ in other words, the â€œ|â€ says â€œgivenâ€ and \\(x_i\\) means the x-value of the current individual.", "\\(=\\) Equals sign.", "\\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the â€œnatural constantâ€ number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope.", "\\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value.", "It is the short hand notation for \\(P(Y_i = 1 |x_i)\\).", "(It is NOT the number 3.14â€¦) Part What it means \\(P(Y_i = 1| x_i)\\) - The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s) - The symbol used to represent the notation in logisitc regression is \\(\\pi_i\\) \\(Y_i\\) (Responsive Variable) Describes the probability of $Y_i = $ 1 or 0 for a given scenario."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 46,
    "title": "ğŸ“ Binomial Y | Multiple X",
    "url": "index.html#binomialymultiplex",
    "content": "Binomial Y | Multiple X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t. We might use an explanatory variable X1 of â€œheightâ€ and a second explanatory variable X2 of â€œgenderâ€ to try to predict whether or not a student will get an A in Math 325. Test(s) used for this category: Multiple Logistic Regression Model Tests Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two. Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2Â  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. â€¦, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -2.9446 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.2364 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.0000 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q Â  0.2776 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  1.9968 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The Z-score testing the hypothesis that \\(\\beta_j=0\\) Â  Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\). Â  1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\). Â  4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\). Â  3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\). Â  0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\). Â  0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a â€œstarâ€. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\). Â  471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option. Â  Null Deviance: The deviance of the null model. Â 800.44 In this case, the null deviance is 800.44 Â on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression. Â 256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit. Â on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model. Â 272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -4.97603 0.65316 -7.618 2.57e-14 *** ## Time 0.39111 0.04979 7.854 4.02e-15 *** ## Diet2 0.58283 1.04061 0.560 0.5754 ## Diet3 -8.44476 4.32324 -1.953 0.0508 . ## Diet4 -67.41995 3768.10023 -0.018 0.9857 ## Time:Diet2 0.02391 0.08679 0.275 0.7830 ## Time:Diet3 1.20955 0.51249 2.360 0.0183 * ## Time:Diet4 8.83168 471.01259 0.019 0.9850 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,Â  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, â€¦), X2 = c(Value 1, Value 2, â€¦), â€¦) command. You should see the GSS example file for an example of how to use this function. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the â€œbâ€ vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(â€œSomeColorâ€,â€œDifferentColorâ€,â€¦) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(â€¦) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==â€œBâ€. Â ~Â  The formula operator in R. X1, The first x-variable in your glm code. Â data = YourDataSet, Specify the name of your data set. Â pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(â€¦) function computes e^(stuff) in R and stands for the â€œexponential function.â€ (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(â€¦) function requires that you call the x-variable â€œxâ€ although you can change this behavior using xname=â€œSomeOtherNameâ€ if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)). Â col = palette()[1], Pull the first color from the color palette for the first curve. Â add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters. Â col = palette()[2], Set the color of this second curve to the second color in the palette. Â add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. â€œtoprightâ€, Typically the legend is placed in the top-right corner of the graph. But it could be placed â€œtopâ€, â€œtopleftâ€, â€œleftâ€, â€œbottomleftâ€, â€œbottomâ€, â€œcenterâ€, â€œrightâ€, or â€œbottomrightâ€. Â legend = Why you have to write legend again, no one knows, but you do. c(â€œLable 1â€, â€œLabel 2â€), These are the values that will appear in the legend, one entry on each line of the legend. Â col = palette(), This specifies the colors of the symbols in the legend. First color goes with â€œLabel 1â€, and second goes with â€œLabel 2â€ and so on. Â lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist. Â bty = Specifies the â€œbox typeâ€ (bty) that should be drawn around the legend. â€˜nâ€™) By placing a â€œnoâ€ option (â€˜nâ€™) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot. Â aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + Add a layer to the ggplot. Â  geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function. Â + Add a layer to the ggplot. Â  geom_smooth(method=â€œglmâ€, method.args=list(family=â€œbinomialâ€), se=FALSE, Add the logistic regression curve to the plot. Â  aes(color=â€œX2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function. Â + Add a layer to the plot. Â  theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x' Explanation Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative. The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations. Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant. Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression. Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\).",
    "sentences": ["Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y.", "This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t.", "We might use an explanatory variable X1 of â€œheightâ€ and a second explanatory variable X2 of â€œgenderâ€ to try to predict whether or not a student will get an A in Math 325.", "Test(s) used for this category: Multiple Logistic Regression Model", "Tests Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two.", "Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly.", "Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead.", "The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\).", "The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\).", "Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Binomial Y | Multiple X"
  },
  {
    "id": 47,
    "title": "ğŸ“ Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t. We might use an explanatory variable X1 of â€œheightâ€ and a second explanatory variable X2 of â€œgenderâ€ to try to predict whether or not a student will get an A in Math 325. Test(s) used for this category: Multiple Logistic Regression Model",
    "sentences": ["Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y.", "This would be like â€œgetting an A in Math 325â€ where 1 means you got an A and 0 means you didnâ€™t.", "We might use an explanatory variable X1 of â€œheightâ€ and a second explanatory variable X2 of â€œgenderâ€ to try to predict whether or not a student will get an A in Math 325.", "Test(s) used for this category: Multiple Logistic Regression Model"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 48,
    "title": "ğŸ“ Tests",
    "url": "index.html#tests",
    "content": "Tests Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two. Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2Â  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. â€¦, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -2.9446 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.2364 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.0000 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q Â  0.2776 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  1.9968 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The Z-score testing the hypothesis that \\(\\beta_j=0\\) Â  Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\). Â  1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\). Â  4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\). Â  3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\). Â  0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\). Â  0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a â€œstarâ€. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\). Â  471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option. Â  Null Deviance: The deviance of the null model. Â 800.44 In this case, the null deviance is 800.44 Â on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression. Â 256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit. Â on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model. Â 272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -4.97603 0.65316 -7.618 2.57e-14 *** ## Time 0.39111 0.04979 7.854 4.02e-15 *** ## Diet2 0.58283 1.04061 0.560 0.5754 ## Diet3 -8.44476 4.32324 -1.953 0.0508 . ## Diet4 -67.41995 3768.10023 -0.018 0.9857 ## Time:Diet2 0.02391 0.08679 0.275 0.7830 ## Time:Diet3 1.20955 0.51249 2.360 0.0183 * ## Time:Diet4 8.83168 471.01259 0.019 0.9850 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,Â  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, â€¦), X2 = c(Value 1, Value 2, â€¦), â€¦) command. You should see the GSS example file for an example of how to use this function. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the â€œbâ€ vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(â€œSomeColorâ€,â€œDifferentColorâ€,â€¦) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(â€¦) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==â€œBâ€. Â ~Â  The formula operator in R. X1, The first x-variable in your glm code. Â data = YourDataSet, Specify the name of your data set. Â pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(â€¦) function computes e^(stuff) in R and stands for the â€œexponential function.â€ (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(â€¦) function requires that you call the x-variable â€œxâ€ although you can change this behavior using xname=â€œSomeOtherNameâ€ if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)). Â col = palette()[1], Pull the first color from the color palette for the first curve. Â add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters. Â col = palette()[2], Set the color of this second curve to the second color in the palette. Â add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. â€œtoprightâ€, Typically the legend is placed in the top-right corner of the graph. But it could be placed â€œtopâ€, â€œtopleftâ€, â€œleftâ€, â€œbottomleftâ€, â€œbottomâ€, â€œcenterâ€, â€œrightâ€, or â€œbottomrightâ€. Â legend = Why you have to write legend again, no one knows, but you do. c(â€œLable 1â€, â€œLabel 2â€), These are the values that will appear in the legend, one entry on each line of the legend. Â col = palette(), This specifies the colors of the symbols in the legend. First color goes with â€œLabel 1â€, and second goes with â€œLabel 2â€ and so on. Â lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist. Â bty = Specifies the â€œbox typeâ€ (bty) that should be drawn around the legend. â€˜nâ€™) By placing a â€œnoâ€ option (â€˜nâ€™) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot. Â aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + Add a layer to the ggplot. Â  geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function. Â + Add a layer to the ggplot. Â  geom_smooth(method=â€œglmâ€, method.args=list(family=â€œbinomialâ€), se=FALSE, Add the logistic regression curve to the plot. Â  aes(color=â€œX2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function. Â + Add a layer to the plot. Â  theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x' Explanation Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative. The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations. Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant. Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression. Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\).",
    "sentences": ["Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two.", "Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly.", "Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead.", "The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\).", "The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\).", "Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.", "Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName.", "glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€.", "It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command.", "YÂ  Y is your binary response variable."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 49,
    "title": "ğŸ“ Categorical Y | Categorical X",
    "url": "index.html#categoricalycategoricalx",
    "content": "Categorical Y | Categorical X Graphics Y is a single categorical variable of interest, like gender. X is another categorical variable of interest, like â€œhair colorâ€. This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender. Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year? Whats the probability that you have brown hair and blue eyes? Tests Add your own notes about appropriate inferential procedures for this type of data here.",
    "sentences": ["Graphics Y is a single categorical variable of interest, like gender.", "X is another categorical variable of interest, like â€œhair colorâ€.", "This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender.", "Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year?", "Whats the probability that you have brown hair and blue eyes?", "Tests Add your own notes about appropriate inferential procedures for this type of data here."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Categorical Y | Categorical X"
  },
  {
    "id": 50,
    "title": "ğŸ“ Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single categorical variable of interest, like gender. X is another categorical variable of interest, like â€œhair colorâ€. This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender. Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year? Whats the probability that you have brown hair and blue eyes?",
    "sentences": ["Y is a single categorical variable of interest, like gender.", "X is another categorical variable of interest, like â€œhair colorâ€.", "This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender.", "Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year?", "Whats the probability that you have brown hair and blue eyes?"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 51,
    "title": "ğŸ“ Tests",
    "url": "index.html#tests",
    "content": "Tests Add your own notes about appropriate inferential procedures for this type of data here.",
    "sentences": "Add your own notes about appropriate inferential procedures for this type of data here.",
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 52,
    "title": "Kruskal-Wallis Test",
    "url": "Kruskal.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Kruskal-Wallis Test The nonparametric equivalent to one-way ANOVA. Kruskal-Wallis Rank Sum Test Allows for deciding if several samples come from the same population or if at least one sample comes from a different population. Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated. Can also be used when data is ordered (ordinal) but does not have an exact measurement. Best used when the distribution of the data is not normal. ANOVA is appropriate when the data is normal. Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, â€œis a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly. In many cases, this is practically equivalent to the mean of at least one population differing from the others.â€ Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable. g is a qualitative grouping variable defining which groups each value in x belongs to. It must either be a character vector or a factor vector. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. weight â€˜weightâ€™ is a numeric variable from the chickwts dataset that represents the quantatitive response variable. Â ~Â  â€˜~â€™ is the tilde symbol used to separate the left- and right-hand side in a model formula. feed, â€˜feedâ€™ is a qualitative grouping variable in the chickwts dataset. Â dataÂ =Â chickwts) â€˜chickwtsâ€™ is a dataset in R. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to View OutputÂ  Click to View Output. ## ## Kruskal-Wallis rank sum test ## ## data: weight by feed ## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07 Alternatively, you could use the following approach as well. kruskal.test(list(x, y, z)) x is a numeric vector of data values that represents the quantatitive response variable for the first group. y is a numeric vector of data values that represents the quantatitive response variable for the second group. z is a numeric vector of data values that represents the quantatitive response variable for the third group. Note that more than three vectors of data could be included inside of list(). Example Code Hover your mouse over the example codes to learn more. feed1 <- filter(chickwts, feed == â€œhorsebeanâ€) This splits out the first group of feed (horsebean) from the chickwts dataset. feed2 <- filter(chickwts, feed == â€œlinseedâ€) This splits out the second group of feed (linseed) from the chickwts dataset. feed3 <- filter(chickwts, feed == â€œsoybeanâ€) This splits out the third group of feed (soybean) from the chickwts dataset. feed4 <- filter(chickwts, feed == â€œsunflowerâ€) This splits out the fourth group of feed (sunflower) from the chickwts dataset. feed5 <- filter(chickwts, feed == â€œmeatmealâ€) This splits out the fifth group of feed (meatmeal) from the chickwts dataset. feed6 <- filter(chickwts, feed == â€œcaseinâ€) This splits out the sixth group of feed (casein) from the chickwts dataset. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. list(feed1$weight,feed2$weight,feed3$weight,feed4$weight,feed5$weight,feed6$weight)) A list of numeric vecotrs that represent the quantitative response variable for each group. Â Click to View OutputÂ  Click to View Output. Load library(tidyverse) to run this code in R. ## ## Kruskal-Wallis rank sum test ## ## data: list(feed1$weight, feed2$weight, feed3$weight, feed4$weight, feed5$weight, feed6$weight) ## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07 Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population. The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples. The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity. Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated. (Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\). Note that the notation \\(n_i\\) allows for each sample to be a different size. In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\). Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest. Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\). Award any tied values the average of the ranks of those values that are tied. In the bottle-cap data we have the following. Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample. Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\). In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\). In the bottle-cap data, \\(\\bar{R}_1 = \\frac{R_1}{n_1} = \\frac{24}{5} = 4.8\\) \\(\\bar{R}_2 = \\frac{R_2}{n_2} = \\frac{14}{3} = 4.667\\) \\(\\bar{R}_3 = \\frac{R_3}{n_3} = \\frac{40}{4} = 10.0\\) Step 5: If there are no ties, calculate the test statistic \\(H\\) by the formula \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] See the original article for the formula to use when there are ties present in the data. In the bottle-cap data, \\(H = 5.656\\). To understand the logic behind the test statistic \\(H\\), note that the null hypothesis assumes that each of the \\(C\\) samples are taken from the same population. Under this assumption, the ranks assigned to each sample should represent a uniform sample of the ranks \\(1,\\ldots,N\\). If the null hypothesis were true, we would expect the average of the ranks from each group, the \\(\\bar{R}_i\\), to be roughly about the same. More precisely, using the properties of the discrete uniform distribution (see wikipedia), we expect each \\(\\bar{R}_i\\) to be close to \\(\\frac{1}{2}(N+1)\\), which is the mean of the discrete uniform distribution. That \\(\\bar{R}_i\\) should be close to this value comes from the fact that the mean of a distribution of means is equal to the population mean. Further, the variance of \\(\\bar{R}_i\\) is given by the population variance, \\(\\frac{1}{12}(N^2-1)\\) in this case, divided by the sample size \\(n_i\\). Finally, the \\(\\frac{N-1}{N}\\) is a correction factor that will not be explained here. The full article explains the details about how a multivariate normal distribution is used to uncover the formula for \\(H\\). Thus, understanding \\(H\\) completely will require an advanced degree in statistical theory. However, the short story is that the distribution of the test statistic \\(H\\) is approximately a chi squared distribution with \\(C-1\\) degrees of freedom and has the form of a sum of values consisting of the (observed mean - expected mean)^2/(variance of the observed mean). Step 6: Calculate the p-value of \\(H\\) using a chi squared distribution with \\(C-1\\) degrees of freedom. \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] In the bottle-cap data, \\(p = 0.059\\), which represents the probability of \\(H\\) being as extreme or more extreme than the observed value of 5.656, assuming the null hypothesis is true. Examples: wages, pigweights Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) Good Example Analysis Poor Example Analysis High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Outlier Theory Assignment The nonparametric equivalent to one-way ANOVA. Kruskal-Wallis Rank Sum Test Allows for deciding if several samples come from the same population or if at least one sample comes from a different population. Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated. Can also be used when data is ordered (ordinal) but does not have an exact measurement. Best used when the distribution of the data is not normal. ANOVA is appropriate when the data is normal. Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, â€œis a tendency for observations in at least one ",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Kruskal-Wallis Test The nonparametric equivalent to one-way ANOVA.", "Kruskal-Wallis Rank Sum Test Allows for deciding if several samples come from the same population or if at least one sample comes from a different population.", "Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated.", "Can also be used when data is ordered (ordinal) but does not have an exact measurement.", "Best used when the distribution of the data is not normal.", "ANOVA is appropriate when the data is normal.", "Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, â€œis a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly.", "In many cases, this is practically equivalent to the mean of at least one population differing from the others.â€ Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable.", "g is a qualitative grouping variable defining which groups each value in x belongs to.", "It must either be a character vector or a factor vector.", "YourDataSet is the name of your data set.", "Example Code Hover your mouse over the example codes to learn more.", "kruskal.test( The function that performs a Kruskal-Wallis rank sum test.", "weight â€˜weightâ€™ is a numeric variable from the chickwts dataset that represents the quantatitive response variable.", "Â ~Â  â€˜~â€™ is the tilde symbol used to separate the left- and right-hand side in a model formula.", "feed, â€˜feedâ€™ is a qualitative grouping variable in the chickwts dataset.", "Â dataÂ =Â chickwts) â€˜chickwtsâ€™ is a dataset in R.", "Â Â Â Â Press Enter to run the code if you have typed it in yourself.", "You can also click here to view the output.", "Â Click to View OutputÂ  Click to View Output.", "## ## Kruskal-Wallis rank sum test ## ## data: weight by feed ## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07 Alternatively, you could use the following approach as well.", "kruskal.test(list(x, y, z)) x is a numeric vector of data values that represents the quantatitive response variable for the first group.", "y is a numeric vector of data values that represents the quantatitive response variable for the second group.", "z is a numeric vector of data values that represents the quantatitive response variable for the third group.", "Note that more than three vectors of data could be included inside of list().", "Example Code Hover your mouse over the example codes to learn more.", "feed1 <- filter(chickwts, feed == â€œhorsebeanâ€) This splits out the first group of feed (horsebean) from the chickwts dataset.", "feed2 <- filter(chickwts, feed == â€œlinseedâ€) This splits out the second group of feed (linseed) from the chickwts dataset.", "feed3 <- filter(chickwts, feed == â€œsoybeanâ€) This splits out the third group of feed (soybean) from the chickwts dataset.", "feed4 <- filter(chickwts, feed == â€œsunflowerâ€) This splits out the fourth group of feed (sunflower) from the chickwts dataset.", "feed5 <- filter(chickwts, feed == â€œmeatmealâ€) This splits out the fifth group of feed (meatmeal) from the chickwts dataset.", "feed6 <- filter(chickwts, feed == â€œcaseinâ€) This splits out the sixth group of feed (casein) from the chickwts dataset.", "kruskal.test( The function that performs a Kruskal-Wallis rank sum test.", "list(feed1$weight,feed2$weight,feed3$weight,feed4$weight,feed5$weight,feed6$weight)) A list of numeric vecotrs that represent the quantitative response variable for each group.", "Â Click to View OutputÂ  Click to View Output.", "Load library(tidyverse) to run this code in R.", "## ## Kruskal-Wallis rank sum test ## ## data: list(feed1$weight, feed2$weight, feed3$weight, feed4$weight, feed5$weight, feed6$weight) ## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07 Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population.", "The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples.", "The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity.", "Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated.", "(Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\).", "Note that the notation \\(n_i\\) allows for each sample to be a different size.", "In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\).", "Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest.", "Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\).", "Award any tied values the average of the ranks of those values that are tied.", "In the bottle-cap data we have the following.", "Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample.", "Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\).", "In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\)."],
    "type": "page",
    "page_title": "Kruskal-Wallis Test"
  },
  {
    "id": 53,
    "title": "ğŸ“ Kruskal-Wallis Rank Sum\r\nTest",
    "url": "Kruskal.html#kruskalwallisranksumtest",
    "content": "Kruskal-Wallis Rank Sum\r\nTest Allows for deciding if several samples come from the same population or if at least one sample comes from a different population. Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated. Can also be used when data is ordered (ordinal) but does not have an exact measurement. Best used when the distribution of the data is not normal. ANOVA is appropriate when the data is normal. Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, â€œis a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly. In many cases, this is practically equivalent to the mean of at least one population differing from the others.â€ Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable. g is a qualitative grouping variable defining which groups each value in x belongs to. It must either be a character vector or a factor vector. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. weight â€˜weightâ€™ is a numeric variable from the chickwts dataset that represents the quantatitive response variable. Â ~Â  â€˜~â€™ is the tilde symbol used to separate the left- and right-hand side in a model formula. feed, â€˜feedâ€™ is a qualitative grouping variable in the chickwts dataset. Â dataÂ =Â chickwts) â€˜chickwtsâ€™ is a dataset in R. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to View OutputÂ  Click to View Output. ## ## Kruskal-Wallis rank sum test ## ## data: weight by feed ## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07 Alternatively, you could use the following approach as well. kruskal.test(list(x, y, z)) x is a numeric vector of data values that represents the quantatitive response variable for the first group. y is a numeric vector of data values that represents the quantatitive response variable for the second group. z is a numeric vector of data values that represents the quantatitive response variable for the third group. Note that more than three vectors of data could be included inside of list(). Example Code Hover your mouse over the example codes to learn more. feed1 <- filter(chickwts, feed == â€œhorsebeanâ€) This splits out the first group of feed (horsebean) from the chickwts dataset. feed2 <- filter(chickwts, feed == â€œlinseedâ€) This splits out the second group of feed (linseed) from the chickwts dataset. feed3 <- filter(chickwts, feed == â€œsoybeanâ€) This splits out the third group of feed (soybean) from the chickwts dataset. feed4 <- filter(chickwts, feed == â€œsunflowerâ€) This splits out the fourth group of feed (sunflower) from the chickwts dataset. feed5 <- filter(chickwts, feed == â€œmeatmealâ€) This splits out the fifth group of feed (meatmeal) from the chickwts dataset. feed6 <- filter(chickwts, feed == â€œcaseinâ€) This splits out the sixth group of feed (casein) from the chickwts dataset. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. list(feed1$weight,feed2$weight,feed3$weight,feed4$weight,feed5$weight,feed6$weight)) A list of numeric vecotrs that represent the quantitative response variable for each group. Â Click to View OutputÂ  Click to View Output. Load library(tidyverse) to run this code in R. ## ## Kruskal-Wallis rank sum test ## ## data: list(feed1$weight, feed2$weight, feed3$weight, feed4$weight, feed5$weight, feed6$weight) ## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07 Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population. The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples. The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity. Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated. (Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\). Note that the notation \\(n_i\\) allows for each sample to be a different size. In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\). Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest. Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\). Award any tied values the average of the ranks of those values that are tied. In the bottle-cap data we have the following. Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample. Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\). In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\). In the bottle-cap data, \\(\\bar{R}_1 = \\frac{R_1}{n_1} = \\frac{24}{5} = 4.8\\) \\(\\bar{R}_2 = \\frac{R_2}{n_2} = \\frac{14}{3} = 4.667\\) \\(\\bar{R}_3 = \\frac{R_3}{n_3} = \\frac{40}{4} = 10.0\\) Step 5: If there are no ties, calculate the test statistic \\(H\\) by the formula \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] See the original article for the formula to use when there are ties present in the data. In the bottle-cap data, \\(H = 5.656\\). To understand the logic behind the test statistic \\(H\\), note that the null hypothesis assumes that each of the \\(C\\) samples are taken from the same population. Under this assumption, the ranks assigned to each sample should represent a uniform sample of the ranks \\(1,\\ldots,N\\). If the null hypothesis were true, we would expect the average of the ranks from each group, the \\(\\bar{R}_i\\), to be roughly about the same. More precisely, using the properties of the discrete uniform distribution (see wikipedia), we expect each \\(\\bar{R}_i\\) to be close to \\(\\frac{1}{2}(N+1)\\), which is the mean of the discrete uniform distribution. That \\(\\bar{R}_i\\) should be close to this value comes from the fact that the mean of a distribution of means is equal to the population mean. Further, the variance of \\(\\bar{R}_i\\) is given by the population variance, \\(\\frac{1}{12}(N^2-1)\\) in this case, divided by the sample size \\(n_i\\). Finally, the \\(\\frac{N-1}{N}\\) is a correction factor that will not be explained here. The full article explains the details about how a multivariate normal distribution is used to uncover the formula for \\(H\\). Thus, understanding \\(H\\) completely will require an advanced degree in statistical theory. However, the short story is that the distribution of the test statistic \\(H\\) is approximately a chi squared distribution with \\(C-1\\) degrees of freedom and has the form of a sum of values consisting of the (observed mean - expected mean)^2/(variance of the observed mean). Step 6: Calculate the p-value of \\(H\\) using a chi squared distribution with \\(C-1\\) degrees of freedom. \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] In the bottle-cap data, \\(p = 0.059\\), which represents the probability of \\(H\\) being as extreme or more extreme than the observed value of 5.656, assuming the null hypothesis is true. Examples: wages, pigweights",
    "sentences": ["Allows for deciding if several samples come from the same population or if at least one sample comes from a different population.", "Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated.", "Can also be used when data is ordered (ordinal) but does not have an exact measurement.", "Best used when the distribution of the data is not normal.", "ANOVA is appropriate when the data is normal.", "Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, â€œis a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly.", "In many cases, this is practically equivalent to the mean of at least one population differing from the others.â€ Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\]", "R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable.", "g is a qualitative grouping variable defining which groups each value in x belongs to.", "It must either be a character vector or a factor vector."],
    "type": "section",
    "page_title": "Kruskal-Wallis Test",
    "section_title": "Kruskal-Wallis Rank Sum\r\nTest"
  },
  {
    "id": 54,
    "title": "ğŸ“ Overview",
    "url": "Kruskal.html#overview",
    "content": "Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated. Can also be used when data is ordered (ordinal) but does not have an exact measurement. Best used when the distribution of the data is not normal. ANOVA is appropriate when the data is normal. Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, â€œis a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly. In many cases, this is practically equivalent to the mean of at least one population differing from the others.â€ Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\]",
    "sentences": ["It is assumed that the various populations are of approximately the same form, but are shifted or translated.", "Can also be used when data is ordered (ordinal) but does not have an exact measurement.", "Best used when the distribution of the data is not normal.", "ANOVA is appropriate when the data is normal.", "Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, â€œis a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly.", "In many cases, this is practically equivalent to the mean of at least one population differing from the others.â€ Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\]"],
    "type": "section",
    "page_title": "Kruskal-Wallis Test",
    "section_title": "Overview"
  },
  {
    "id": 55,
    "title": "ğŸ“ R Instructions",
    "url": "Kruskal.html#rinstructions",
    "content": "R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable. g is a qualitative grouping variable defining which groups each value in x belongs to. It must either be a character vector or a factor vector. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. weight â€˜weightâ€™ is a numeric variable from the chickwts dataset that represents the quantatitive response variable. Â ~Â  â€˜~â€™ is the tilde symbol used to separate the left- and right-hand side in a model formula. feed, â€˜feedâ€™ is a qualitative grouping variable in the chickwts dataset. Â dataÂ =Â chickwts) â€˜chickwtsâ€™ is a dataset in R. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to View OutputÂ  Click to View Output. ## ## Kruskal-Wallis rank sum test ## ## data: weight by feed ## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07 Alternatively, you could use the following approach as well. kruskal.test(list(x, y, z)) x is a numeric vector of data values that represents the quantatitive response variable for the first group. y is a numeric vector of data values that represents the quantatitive response variable for the second group. z is a numeric vector of data values that represents the quantatitive response variable for the third group. Note that more than three vectors of data could be included inside of list(). Example Code Hover your mouse over the example codes to learn more. feed1 <- filter(chickwts, feed == â€œhorsebeanâ€) This splits out the first group of feed (horsebean) from the chickwts dataset. feed2 <- filter(chickwts, feed == â€œlinseedâ€) This splits out the second group of feed (linseed) from the chickwts dataset. feed3 <- filter(chickwts, feed == â€œsoybeanâ€) This splits out the third group of feed (soybean) from the chickwts dataset. feed4 <- filter(chickwts, feed == â€œsunflowerâ€) This splits out the fourth group of feed (sunflower) from the chickwts dataset. feed5 <- filter(chickwts, feed == â€œmeatmealâ€) This splits out the fifth group of feed (meatmeal) from the chickwts dataset. feed6 <- filter(chickwts, feed == â€œcaseinâ€) This splits out the sixth group of feed (casein) from the chickwts dataset. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. list(feed1$weight,feed2$weight,feed3$weight,feed4$weight,feed5$weight,feed6$weight)) A list of numeric vecotrs that represent the quantitative response variable for each group. Â Click to View OutputÂ  Click to View Output. Load library(tidyverse) to run this code in R. ## ## Kruskal-Wallis rank sum test ## ## data: list(feed1$weight, feed2$weight, feed3$weight, feed4$weight, feed5$weight, feed6$weight) ## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07 Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population. The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples. The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity. Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated. (Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\). Note that the notation \\(n_i\\) allows for each sample to be a different size. In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\). Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest. Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\). Award any tied values the average of the ranks of those values that are tied. In the bottle-cap data we have the following. Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample. Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\). In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\). In the bottle-cap data, \\(\\bar{R}_1 = \\frac{R_1}{n_1} = \\frac{24}{5} = 4.8\\) \\(\\bar{R}_2 = \\frac{R_2}{n_2} = \\frac{14}{3} = 4.667\\) \\(\\bar{R}_3 = \\frac{R_3}{n_3} = \\frac{40}{4} = 10.0\\) Step 5: If there are no ties, calculate the test statistic \\(H\\) by the formula \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] See the original article for the formula to use when there are ties present in the data. In the bottle-cap data, \\(H = 5.656\\). To understand the logic behind the test statistic \\(H\\), note that the null hypothesis assumes that each of the \\(C\\) samples are taken from the same population. Under this assumption, the ranks assigned to each sample should represent a uniform sample of the ranks \\(1,\\ldots,N\\). If the null hypothesis were true, we would expect the average of the ranks from each group, the \\(\\bar{R}_i\\), to be roughly about the same. More precisely, using the properties of the discrete uniform distribution (see wikipedia), we expect each \\(\\bar{R}_i\\) to be close to \\(\\frac{1}{2}(N+1)\\), which is the mean of the discrete uniform distribution. That \\(\\bar{R}_i\\) should be close to this value comes from the fact that the mean of a distribution of means is equal to the population mean. Further, the variance of \\(\\bar{R}_i\\) is given by the population variance, \\(\\frac{1}{12}(N^2-1)\\) in this case, divided by the sample size \\(n_i\\). Finally, the \\(\\frac{N-1}{N}\\) is a correction factor that will not be explained here. The full article explains the details about how a multivariate normal distribution is used to uncover the formula for \\(H\\). Thus, understanding \\(H\\) completely will require an advanced degree in statistical theory. However, the short story is that the distribution of the test statistic \\(H\\) is approximately a chi squared distribution with \\(C-1\\) degrees of freedom and has the form of a sum of values consisting of the (observed mean - expected mean)^2/(variance of the observed mean). Step 6: Calculate the p-value of \\(H\\) using a chi squared distribution with \\(C-1\\) degrees of freedom. \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] In the bottle-cap data, \\(p = 0.059\\), which represents the probability of \\(H\\) being as extreme or more extreme than the observed value of 5.656, assuming the null hypothesis is true. Examples: wages, pigweights",
    "sentences": ["Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable.", "g is a qualitative grouping variable defining which groups each value in x belongs to.", "It must either be a character vector or a factor vector.", "YourDataSet is the name of your data set.", "Example Code Hover your mouse over the example codes to learn more.", "kruskal.test( The function that performs a Kruskal-Wallis rank sum test.", "weight â€˜weightâ€™ is a numeric variable from the chickwts dataset that represents the quantatitive response variable.", "Â ~Â  â€˜~â€™ is the tilde symbol used to separate the left- and right-hand side in a model formula.", "feed, â€˜feedâ€™ is a qualitative grouping variable in the chickwts dataset.", "Â dataÂ =Â chickwts) â€˜chickwtsâ€™ is a dataset in R."],
    "type": "section",
    "page_title": "Kruskal-Wallis Test",
    "section_title": "R Instructions"
  },
  {
    "id": 56,
    "title": "ğŸ“ Explanation",
    "url": "Kruskal.html#explanation",
    "content": "Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population. The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples. The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity. Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated. (Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\). Note that the notation \\(n_i\\) allows for each sample to be a different size. In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\). Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest. Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\). Award any tied values the average of the ranks of those values that are tied. In the bottle-cap data we have the following. Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample. Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\). In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\). In the bottle-cap data, \\(\\bar{R}_1 = \\frac{R_1}{n_1} = \\frac{24}{5} = 4.8\\) \\(\\bar{R}_2 = \\frac{R_2}{n_2} = \\frac{14}{3} = 4.667\\) \\(\\bar{R}_3 = \\frac{R_3}{n_3} = \\frac{40}{4} = 10.0\\) Step 5: If there are no ties, calculate the test statistic \\(H\\) by the formula \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] See the original article for the formula to use when there are ties present in the data. In the bottle-cap data, \\(H = 5.656\\). To understand the logic behind the test statistic \\(H\\), note that the null hypothesis assumes that each of the \\(C\\) samples are taken from the same population. Under this assumption, the ranks assigned to each sample should represent a uniform sample of the ranks \\(1,\\ldots,N\\). If the null hypothesis were true, we would expect the average of the ranks from each group, the \\(\\bar{R}_i\\), to be roughly about the same. More precisely, using the properties of the discrete uniform distribution (see wikipedia), we expect each \\(\\bar{R}_i\\) to be close to \\(\\frac{1}{2}(N+1)\\), which is the mean of the discrete uniform distribution. That \\(\\bar{R}_i\\) should be close to this value comes from the fact that the mean of a distribution of means is equal to the population mean. Further, the variance of \\(\\bar{R}_i\\) is given by the population variance, \\(\\frac{1}{12}(N^2-1)\\) in this case, divided by the sample size \\(n_i\\). Finally, the \\(\\frac{N-1}{N}\\) is a correction factor that will not be explained here. The full article explains the details about how a multivariate normal distribution is used to uncover the formula for \\(H\\). Thus, understanding \\(H\\) completely will require an advanced degree in statistical theory. However, the short story is that the distribution of the test statistic \\(H\\) is approximately a chi squared distribution with \\(C-1\\) degrees of freedom and has the form of a sum of values consisting of the (observed mean - expected mean)^2/(variance of the observed mean). Step 6: Calculate the p-value of \\(H\\) using a chi squared distribution with \\(C-1\\) degrees of freedom. \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] In the bottle-cap data, \\(p = 0.059\\), which represents the probability of \\(H\\) being as extreme or more extreme than the observed value of 5.656, assuming the null hypothesis is true.",
    "sentences": ["In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population.", "The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples.", "The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity.", "Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated.", "(Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\).", "Note that the notation \\(n_i\\) allows for each sample to be a different size.", "In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\).", "Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest.", "Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\).", "Award any tied values the average of the ranks of those values that are tied."],
    "type": "section",
    "page_title": "Kruskal-Wallis Test",
    "section_title": "Explanation"
  },
  {
    "id": 57,
    "title": "Linear Regression",
    "url": "LinearRegression.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Linear Regression Determine which explanatory variables have a significant effect on the mean of the quantitative response variable. Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\). Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$ \\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) $$ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is â€œequal toâ€ the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to â€œGodâ€™s Lawâ€ or â€œNatural Lawâ€. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are â€œrandomâ€ and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)â€¦ \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced â€œy-hatâ€, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what followsâ€¦ \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, â€œthe change in the average y-value for a one unit change in the x-value.â€ It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, â€œthe average y-value when x is zero.â€ It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful. R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your lm() code into mylm name. lm( lm(â€¦) is an R function that stands for â€œLinear Modelâ€. It performs a linear regression analysis for Y ~ X. YÂ  Y is your quantitative response variable. It is the name of one of the columns in your data set. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X. X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value. Â data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X. ) Closing parenthesis for the lm(â€¦) function. summary(mylm) The summary command allows you to print the results of your linear regression that were previously saved in mylm name. Â Click to Show OutputÂ  Click to View Output. Example output from a regression. Hover each piece to learn more. Call: lm(formula = dist ~ speed, data = cars) This is simply a statement of your original lm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(â€¦). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min Â  -29.069 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -9.525 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -2.272 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -2.272 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-2.272) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  9.215 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  43.201 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  t value To learn more about the â€œt valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  Pr(>|t|) The â€œPrâ€ stands for â€œProbabilityâ€ and the â€œ(> |t|)â€ stands for â€œmore extreme than the observed t-valueâ€. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the â€œp-valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. (Intercept) This always says â€œInterceptâ€ for any lm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -17.5791 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  6.7584 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -2.601 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the intercept (-17.5791) by its standard error (6.7584). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of -17.5791 is -2.601 standard errors (6.7584) from zero, which is a fairly surprising distance as shown by the p-value. 0.0123 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). speed This is always the name of your X-variable in your lm(Y ~ X, â€¦). Â  3.9324 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.4155 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 9.464 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the slope (3.9324) by its standard error (0.4155). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from zero, which is a really surprising distance as shown by the smallness of the p-value. 1.49e-12 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE. Â 15.38 For this particular regression, the estimate of \\(\\sigma\\) is 15.38. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\). Â on 48 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 50 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 50-2 = 48. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO. Â 0.6511, In this particular regression, 65.11% of the variation in stopping distance dist is explained by the regression model using speed of the car. Â Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitt",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Linear Regression Determine which explanatory variables have a significant effect on the mean of the quantitative response variable.", "Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\).", "Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$ \\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) $$ \\(Y_i\\) The response variable.", "The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size.", "\\(=\\) This states that we are assuming \\(Y_i\\) was created, or is â€œequal toâ€ the formula that will follow on the right-hand-side of the equation.", "\\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life.", "It can be likened to â€œGodâ€™s Lawâ€ or â€œNatural Lawâ€.", "Something that governs the way the data behaves, but is unkown to us.", "\\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\).", "\\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\).", "The error terms are â€œrandomâ€ and unique for each individual.", "This provides the statistical relationship of the regression.", "It is what allows each dot to be different, while still coming from the same line, or underlying law.", "\\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)â€¦ \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance.", "Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual.", "In other words, the variance is constant.", "The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line.", "The estimated regression line obtained from a regression analysis, pronounced â€œy-hatâ€, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est.", "y-int} + \\overbrace{b_1}^\\text{est.", "slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\).", "It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value.", "\\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what followsâ€¦ \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation.", "First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\).", "This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\).", "Second, this equation does not include \\(\\epsilon_i\\).", "In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values.", "Note: see the Explanation tab The Mathematical Model for details about these equations.", "Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line.", "This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true.", "If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin.", "This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true.", "Assumptions This regression model is appropriate for the data when five assumptions can be made.", "Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear.", "Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero.", "Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values.", "Fixed X: the \\(X_i\\) values can be considered fixed and measured without error.", "Independent Errors: the error terms \\(\\epsilon_i\\) are independent.", "Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions.", "Interpretation The slope is interpreted as, â€œthe change in the average y-value for a one unit change in the x-value.â€ It is not the average change in y.", "It is the change in the average y-value.", "The y-intercept is interpreted as, â€œthe average y-value when x is zero.â€ It is often not meaningful, but is sometimes useful.", "It just depends if x being zero is meaningful or not within the context of your analysis.", "For example, knowing the average price of a car with zero miles is useful.", "However, pretending to know the average height of adult males that weigh zero pounds, is not useful.", "R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command.", "Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your lm() code into mylm name.", "lm( lm(â€¦) is an R function that stands for â€œLinear Modelâ€.", "It performs a linear regression analysis for Y ~ X.", "YÂ  Y is your quantitative response variable.", "It is the name of one of the columns in your data set."],
    "type": "page",
    "page_title": "Linear Regression"
  },
  {
    "id": 58,
    "title": "ğŸ“ Simple Linear\r\nRegression",
    "url": "LinearRegression.html#simplelinearregression",
    "content": "Simple Linear\r\nRegression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\). Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$ \\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) $$ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is â€œequal toâ€ the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to â€œGodâ€™s Lawâ€ or â€œNatural Lawâ€. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are â€œrandomâ€ and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)â€¦ \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced â€œy-hatâ€, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what followsâ€¦ \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, â€œthe change in the average y-value for a one unit change in the x-value.â€ It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, â€œthe average y-value when x is zero.â€ It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful.",
    "sentences": ["Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\).", "Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$ \\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) $$ \\(Y_i\\) The response variable.", "The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size.", "\\(=\\) This states that we are assuming \\(Y_i\\) was created, or is â€œequal toâ€ the formula that will follow on the right-hand-side of the equation.", "\\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life.", "It can be likened to â€œGodâ€™s Lawâ€ or â€œNatural Lawâ€.", "Something that governs the way the data behaves, but is unkown to us.", "\\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\).", "\\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\).", "The error terms are â€œrandomâ€ and unique for each individual."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Simple Linear\r\nRegression"
  },
  {
    "id": 59,
    "title": "ğŸ“ Overview",
    "url": "LinearRegression.html#overview",
    "content": "Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$ \\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) $$ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is â€œequal toâ€ the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to â€œGodâ€™s Lawâ€ or â€œNatural Lawâ€. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are â€œrandomâ€ and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)â€¦ \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced â€œy-hatâ€, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what followsâ€¦ \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, â€œthe change in the average y-value for a one unit change in the x-value.â€ It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, â€œthe average y-value when x is zero.â€ It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful.",
    "sentences": ["Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$ \\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) $$ \\(Y_i\\) The response variable.", "The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size.", "\\(=\\) This states that we are assuming \\(Y_i\\) was created, or is â€œequal toâ€ the formula that will follow on the right-hand-side of the equation.", "\\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life.", "It can be likened to â€œGodâ€™s Lawâ€ or â€œNatural Lawâ€.", "Something that governs the way the data behaves, but is unkown to us.", "\\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\).", "\\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\).", "The error terms are â€œrandomâ€ and unique for each individual.", "This provides the statistical relationship of the regression."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Overview"
  },
  {
    "id": 60,
    "title": "ğŸ“ R Instructions",
    "url": "LinearRegression.html#rinstructions",
    "content": "R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your lm() code into mylm name. lm( lm(â€¦) is an R function that stands for â€œLinear Modelâ€. It performs a linear regression analysis for Y ~ X. YÂ  Y is your quantitative response variable. It is the name of one of the columns in your data set. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X. X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value. Â data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X. ) Closing parenthesis for the lm(â€¦) function. summary(mylm) The summary command allows you to print the results of your linear regression that were previously saved in mylm name. Â Click to Show OutputÂ  Click to View Output. Example output from a regression. Hover each piece to learn more. Call: lm(formula = dist ~ speed, data = cars) This is simply a statement of your original lm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(â€¦). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min Â  -29.069 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -9.525 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -2.272 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -2.272 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-2.272) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  9.215 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  43.201 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  t value To learn more about the â€œt valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  Pr(>|t|) The â€œPrâ€ stands for â€œProbabilityâ€ and the â€œ(> |t|)â€ stands for â€œmore extreme than the observed t-valueâ€. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the â€œp-valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. (Intercept) This always says â€œInterceptâ€ for any lm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -17.5791 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  6.7584 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -2.601 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the intercept (-17.5791) by its standard error (6.7584). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of -17.5791 is -2.601 standard errors (6.7584) from zero, which is a fairly surprising distance as shown by the p-value. 0.0123 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). speed This is always the name of your X-variable in your lm(Y ~ X, â€¦). Â  3.9324 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.4155 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 9.464 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the slope (3.9324) by its standard error (0.4155). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from zero, which is a really surprising distance as shown by the smallness of the p-value. 1.49e-12 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE. Â 15.38 For this particular regression, the estimate of \\(\\sigma\\) is 15.38. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\). Â on 48 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 50 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 50-2 = 48. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO. Â 0.6511, In this particular regression, 65.11% of the variation in stopping distance dist is explained by the regression model using speed of the car. Â Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data. Â 0.6438 In this case, the value of 0.6438 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is found as the ratio of the MSR/MSE where MSR = SSR/(p-1) and MSE = SSE/(n-p) where n is the sample size and p is the number of parameters in the regression model. Â 89.57 This is the value of the F-statistic for the lm(dist ~ speed, data=cars) regression. Note that SSE = sum( cars.lm$res^2 ) = 11353.52 with n - p = 50 - 2 = 48 degrees of freedom for this data. Further, SSR = sum( (cars.lm$fit - mean(cars$dist))^2 ) = 21185.46 with p - 1 = 1 degree of freedom. So MSR = 21185.46 and MSE = 11353.52 / 48 = 236.5317. So MSR / MSE = 21185.46 / 236.5317 = 89.56711. Â on 1 and 48 DF, The 1 degree of freedom is the SSR degrees of freedom (p-1). The 48 is the SSE degrees of freedom (n-p). Â p-value: 1.49e-12 The p-value for an F-statistic is found by the code pf(89.56711, 1, 48, lower.tail=FALSE), which gives the probability of being more extreme than the observed F-statistic in an F distribution with 1 and 48 degrees of freedom. Check Assumptions 1, 2, 3, and 5 par( The par(â€¦) command stands for â€œGraphical PARametersâ€. It allows you to control various aspects of graphics in Base R. mfrow= This stands for â€œmultiple frames filled by rowâ€, which means, put lots of plots on the same row, starting with the plot on the left, then working towards the right as more plots are created. c( The combine function c(â€¦) is used to specify how many rows and columns of graphics should be placed together. 1, This specifies that 1 row of graphics should be produced. 3 This states that 3 columns of graphics should be produced. ) Closing parenthesis for c(â€¦) function. ) Closing parenthesis for par(â€¦) function. plot( This version of plot(â€¦) will actually create several regression diagnostic plots by default. mylm, This is the name of an lm object that you created previously. which= This allows you to select â€œwhichâ€ regression diagnostic plots should be drawn. 1 Selecting 1, would give the residuals vs.Â fitted values plot only. : The colon allows you to select more than just one plot. 2 Selecting 2 also gives the Q-Q Plot of residuals. If you wanted to instead you could just use which=1 to get the residuals vs fitted values plot, then you could use qqPlot(mylm$residuals) to create a fancier Q-Q Plot of the residuals. ) Closing parenthesis for plot(â€¦) function. plot( This version of plot(â€¦) will be used to create a time-ordered plot of the residuals. The order of the residuals is the original order of the x-values in the original data set. If the original data set doesnâ€™t have an order, then this plot is not interesting. mylm The lm object that you created previously. $ This allows you to access various elements from the regression that was performed. residuals This grabs the residuals for each observation in the regression. ) Closing parenthesis for plot(â€¦) function. Â Click to Show OutputÂ  Click to View Output. Plotting the Regression Line Base R ggplot2 To add the regression line to a scatterplot use the abline(...) command: plot( The plot(â€¦) function is used to create a scatterplot with a y-axis (the vertical axis) and an x-axis (the horizontal axis). YÂ  This is the â€œresponse variableâ€ of your regression. The thing you are interested in predicting. This is the name of a â€œnumericâ€ column of data from the data set called YourDataSet. ~Â  The tilde â€œ~â€ is used to relate Y to X and can be found on the top-left key of your keyboard. X,Â  This is the explanatory variable of your regression. It is the name of a â€œnumericâ€ column of data from YourDataSet. data= The data= statement is used to specify the name of the data set where the columns of â€œXâ€ and â€œYâ€ are located. YourDataSet This is the name of your data set, like KidsFeet or cars or airquality. ) Closing parenthesis for plot(â€¦) function. abline( This stands for â€œaâ€ (intercept) â€œbâ€ (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm This is the name of an lm(â€¦) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(â€¦) function will locate these two values from within mylm and use them to add a line to your current plot(â€¦). ) Closing parenthesis for abline(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm) You can customize the look of the regression line with abline( This stands for â€œaâ€ (intercept) â€œbâ€ (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm, This is the name of an lm(â€¦) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(â€¦) function will locate these two values from within mylm and use them to add a line to your current plot(â€¦). lty= The lty= stands for â€œline typeâ€ and allows you to select between 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. 1, This creates a solid line. Remember, other options include: 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. lwd= The lwd= allows you to specify the width of the line. The default width is 1. Using lwd=2 would double the thickness, and so on. Any positive value is allowed. 1, Default line width. To make a thicker line, us 2 or 3â€¦ To make a thinner line, try 0.5, but 1 is already pretty thin. col= This allows you to specify the color of the line using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). â€œsomeColorâ€ Type colors() in R for options. ) Closing parenthesis for abline(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm, lty=1, lwd=1, col=\"firebrick\") You can add points to the regression withâ€¦ points( This is like plot(â€¦) but adds points to the current plot(â€¦) instead of creating a new plot. newYÂ  newY should be a column of values from some data set. Or, use points(newX, newY) to add a single point to a graph. ~Â  This links Y to X in the plot. newX,Â  newX should be a column of values from some data set. It should be the same length as newY. If just a single value, use points(newX, newY) instead. data=YourDataSet,Â  If newY and newX come from a dataset, then use data= to tell the points(â€¦) function what data set they come from. If newY and newX are just single values, then data= is not needed. col=â€œskyblueâ€, This allows you to specify the color of the points using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). pch=16 This allows you to specify the type of plotting symbol to be used for the points. Type ?pch and scroll half way down in the help file that appears to learn about other possible symbols. ) Closing parenthesis for points(â€¦) function. Â Click to Show OutputÂ  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) points(7,40, pch=16, col=\"skyblue\", cex=2) text(7,40, \"New Dot\", pos=3, cex=0.5) points(dist ~ speed, data=filter(cars, mylm$res > 2), cex=.8, col=\"red\") abline(mylm, lty=1, lwd=1, col=\"firebrick\") To add the regression line to a scatterplot using the ggplot2 approach, first ensure: library(ggplot2) or library(tidyverse) is loaded. Then, use the geom_smooth(method = lm) command: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. YourDataSet,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. X, This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. Â y =Â  â€œy=Â â€ declares which variable will become the y-axis of the graphic. Y This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. + Here the + is used to add yet another layer to ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. There are several other methods that could be used here. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(â€¦) to place more complicated models on the plot. Â se = FALSE se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. ) Closing parenthesis for the geom_smooth() function. Â Click to Show OutputÂ  Click to View Output. ggplot(cars, aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = \"lm\", formula=y~x, se=FALSE) There are a number of ways to customize the appearance of the regression line: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. speed,Â  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. y =Â  â€œy=Â â€ declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. Â + Here the + is used to add yet another layer to ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(â€¦) to place more complicated models on the plot. Â se = FALSE, se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. Â size = 2, Use size = 2 to adjust the thickness of the line to size 2. Â color = â€œorangeâ€, Use color = â€œorangeâ€ to change the color of the line to orange. Â Â linetype = â€œdashedâ€ Use linetype = â€œdashedâ€ to change the solid line to a dashed line. Some linetype options include â€œdashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. ) Closing parenthesis for the geom_smooth() function. Â Click to Show OutputÂ  Click to View Output. In addition to customizing the regression line, you can customize the points, add points, add lines, and much more. ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,Â  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =Â  â€œx =Â â€ declares which variable will become the x-axis of the graphic, your explanatory variable. Both â€œx=Â â€ and â€œy=Â â€ are optional phrasesin the ggplot2 syntax. speed,Â  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the â€œnumericâ€ column of YourDataSet. y =Â  â€œy=Â â€ declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a â€œnumericâ€ column of YourDataSet. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line. Â Â geom_point( geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. size = 1.5, Use size = 1.5 to change the size of the points. Â color = â€œskyblueâ€ Use color = â€œskyblueâ€ to change the color of the points to Brother Saundersâ€™ favorite color. Â alpha = 0.5 Use alpha = 0.5 to change the transparency of the points to 0.5. ) Closing parenthesis of geom_point() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =Â  Use â€œmethod =Â â€ to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. â€œlmâ€, lm stands for linear model. Using method = â€œlmâ€ tells geom_smooth() to fit a least-squares regression line onto the graphic. Â formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in ways similar to lm(â€¦) to place more complicated models on the plot. Â se = FALSE, se stands for â€œstandard errorâ€. Specifying FALSE turns this feature off. When TRUE, a gray band showing the â€œconfidence bandâ€ for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. Â color = â€œnavyâ€, Use color = â€œnavyâ€ to change the color of the line to navy blue. Â size = 1.5 Use size = 1.5 to adjust the thickness of the line to 1.5. ) Closing parenthesis of geom_smooth() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_hline( Use geom_hline() to add a horizontal line at a specified y-intercept. You can also use geom_vline(xintercept = some_number) to add a vertical line to the graph. yintercept = Use â€œyintercept =â€ to tell geom_hline() that you are going to declare a y intercept for the horizontal line. Â 75 75 is the value of the y-intercept. , color = â€œfirebrickâ€ Use color = â€œfirebrickâ€ to change the color of the horizontal line to firebrick red. , size = 1, Use size = 1 to adjust the thickness of the horizontal line to size 1. Â Â Â Â Â Â Â Â Â Â Â Â Â linetype = â€œlongdashâ€ Use linetype = â€œlongdashâ€ to change the solid line to a dashed line with longer dashes. Some linetype options include â€œdashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. , alpha = 0.5 Use alpha = 0.5 to change the transparency of the horizontal line to 0.5. ) Closing parenthesis of geom_hline function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_segment( geom_segment() allows you to add a line segment to ggplot() by using specified start and end points. x = â€œx =â€ tells geom_segment() that you are going to declare the x-coordinate for the starting point of the line segment. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the starting point of the line segment. Â y =â€œy =â€ tells geom_segment() that you are going to declare the y-coordinate for the starting point of the line segment. Â 75, 75 is a number on the y-axis of your graph. It is the y-coordinate of the starting point of the line segment. Â xend = â€œxend =â€ tells geom_segment() that you are going to declare the x-coordinate for the end point of the line segment. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the end point of the line segment. Â yend = â€œyend =â€ tells geom_segment() that you are going to declare the y-coordinate for the end point of the line segment. Â 38, 38 is a number on the y-axis of your graph. It is the y-coordinate of the end point of the line segment. Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â size = 1 Use size = 1 to adjust the thickness of the line segment. , color = â€œlightgrayâ€ Use color = â€œlightgrayâ€ to change the color of the line segment to light gray. , linetype = â€œlongdashâ€ Use *linetype = â€œlongdash* to change the solid line segment to a dashed one. Some linetype options includeâ€dashedâ€, â€œdottedâ€, â€œlongdashâ€, â€œdotdashâ€, etc. ) Closing parenthesis for geom_segment() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_point( geom_point() can also be used to add individual points to the graph. Simply declare the x and y coordinates of the point you want to plot. x = â€œx =â€ tells geom_point() that you are going to declare the x-coordinate for the point. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the point. Â y = â€œy =â€ tells geom_point() that you are going to declare the y-coordinate for the point. Â 75 75 is a number on the y-axis of your graph. It is the y-coordinate of the point. , size = 3 Use size = 3 to make the point stand out more. , color = â€œfirebrickâ€ Use color = â€œfirebrickâ€ to change the color of the point to firebrick red. ) Closing parenthesis of the geom_point() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â geom_text( geom_text() allows you to add customized text anywhere on the graph. It is very similar to the base R equivalent, text(â€¦). x = â€œx =â€ tells geom_text() that you are going to declare the x-coordinate for the text. Â 14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the text. Â y = â€œy =â€ tells geom_text() that you are going to declare the y-coordinate for the text. Â 84, 84 is a number on the y-axis of your graph. It is the y-coordinate of the text. Â label = â€œlabel =â€ tells geom_text() that you are going to give it the label. Â â€œMy Point (14, 75)â€, â€œMy Point (14, 75)â€ is the text that will appear on the graph. Â Â Â Â Â Â Â Â Â Â Â Â color = â€œnavyâ€ Use color = â€œnavyâ€ to change the color of the text to navy blue. , size = 3 Use size = 3 to change the size of the text. ) Closing parenthesis of the geom_text() function. Â + The + allows you to add more layers to the framework provided by ggplot(). Â Â theme_minimal() Add a minimalistic theme to the graph. There are many other themes that you can try out. Â Click to Show OutputÂ  Click to View Output. ## `geom_smooth()` using formula = 'y ~ x' Accessing Parts of the Regression Finally, note that the mylm object contains the names(mylm) of mylm$coefficients Contains two values. The first is the estimated \\(y\\)-intercept. The second is the estimated slope. ## (Intercept) speed ## -17.579095 3.932409 mylm$residuals Contains the residuals from the regression in the same order as the actual dataset. ## 1 2 3 4 5 6 7 8 9 ## 3.849460 11.849460 -5.947766 12.052234 2.119825 -7.812584 -3.744993 4.255007 12.255007 ## 10 11 12 13 14 15 16 17 18 ## -8.677401 2.322599 -15.609810 -9.609810 -5.609810 -1.609810 -7.542219 0.457781 0.457781 ## 19 20 21 22 23 24 25 26 27 ## 12.457781 -11.474628 -1.474628 22.525372 42.525372 -21.407036 -15.407036 12.592964 -13.339445 ## 28 29 30 31 32 33 34 35 36 ## -5.339445 -17.271854 -9.271854 0.728146 -11.204263 2.795737 22.795737 30.795737 -21.136672 ## 37 38 39 40 41 42 43 44 45 ## -11.136672 10.863328 -29.069080 -13.069080 -9.069080 -5.069080 2.930920 -2.933898 -18.866307 ## 46 47 48 49 50 ## -6.798715 15.201285 16.201285 43.201285 4.268876 mylm$fitted.values The values of \\(\\hat{Y}\\) in the same order as the original dataset. ## 1 2 3 4 5 6 7 8 9 10 ## -1.849460 -1.849460 9.947766 9.947766 13.880175 17.812584 21.744993 21.744993 21.744993 25.677401 ## 11 12 13 14 15 16 17 18 19 20 ## 25.677401 29.609810 29.609810 29.609810 29.609810 33.542219 33.542219 33.542219 33.542219 37.474628 ## 21 22 23 24 25 26 27 28 29 30 ## 37.474628 37.474628 37.474628 41.407036 41.407036 41.407036 45.339445 45.339445 49.271854 49.271854 ## 31 32 33 34 35 36 37 38 39 40 ## 49.271854 53.204263 53.204263 53.204263 53.204263 57.136672 57.136672 57.136672 61.069080 61.069080 ## 41 42 43 44 45 46 47 48 49 50 ## 61.069080 61.069080 61.069080 68.933898 72.866307 76.798715 76.798715 76.798715 76.798715 80.731124 mylm$â€¦ several other things that will not be explained here. Making Predictions predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ) Closing parenthesis for the data.frame(â€¦) function. ) Closing parenthesis for the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12)) 1 Â  29.60981 The value given is the â€œfitted-valueâ€ or â€œpredicted-valueâ€ for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(â€¦) function. Â interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. â€œpredictionâ€ This specifies that a prediction interval will be included with the predicted value. A prediction interval gives you a 95% confidence interval that captures 95% of the data, or \\(Y_i\\) values for the specific \\(X\\)-value specified in the prediction. ) Closing parenthesis of the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"prediction\") Â  fit The â€œfitâ€ is the predicted value. Â  lwr The â€œlwrâ€ is the lower bound. Â  upr The â€œuprâ€ is the upper bound. 1 29.60981 In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance canâ€™t go negative) feet to 60.96915 feet. -1.749529 This is the lower bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us the stopping distance could be as quick as -1.749529 feet (or 0 because distance canâ€™t go negative). 60.96915 This is the upper bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us that the actual stopping distance could be as high as 60.96915 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=â€¦ Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(â€¦) function. Â interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. â€œconfidenceâ€ This specifies that a confidence interval for the prediction should be provided. This is of use whenever your interest is in just estimating the average y-value, not the actual y-values. ) Closing parenthesis of the predict(â€¦) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"confidence\") Â  fit The â€œfitâ€ is the predicted value. Â  lwr The â€œlwrâ€ is the lower bound. Â  upr The â€œuprâ€ is the upper bound. 1 29.60981 In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. 24.39514 This is the lower bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is greater than this value. 34.82448 This is the upper bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is less than this value. Finding Confidence Intervals for Model Parameters confint( The R function confint(â€¦) allows you to use an lm(â€¦) object to compute confidence intervals for one or more parameters (like \\(\\beta_0\\) or \\(\\beta_1\\)) in your model. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â level = â€œlevel =â€ tells the confint(â€¦) function that you are going to declare at what level of confidence you want the interval. The default is â€œlevel = 0.95.â€ If you want to find 95% confidence intervals for your parameters, then just run confint(mylm). Â someConfidenceLevel someConfidenceLevel is simply a confidence level you choose when you want something other than a 95% confidence interval. Some examples of appropriate levels include 0.90 and 0.99. ) Closing parenthesis for confint(..) function. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.90) Â  Â  5 % The lower bound of a 90% confidence interval occurs at the 5th percentile. This is because at 90% confidence, 10% is left in the tails, with 5% on each end. The upper bound will thus end at the 95th percentile, hence the 5% and 95% as the column names. Â  95 % The upper bound of a 90% confidence interval ends at the 95th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -28.914514 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -6.243676 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.235501 and 4.629317. 3.235501 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. 4.629317 This is the upper bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.95) Â  Â  2.5 % The lower bound of a 95% confidence interval occurs at the 2.5th percentile. This is because at 95% confidence, 5% is left in the tails, with 2.5% on each end. The upper bound will thus end at the 97.5th percentile, hence the 2.5% and 97.5% as the column names for the lower and upper bounds, respectively. Â  97.5 % The upper bound of a 95% confidence interval ends at the 97.5th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -31.167850 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -3.990340 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.096964 and 4.767853. 3.096964 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.096964 and 4.767853 4.767853 This is the upper bound of the confidence interval. In this case, you can be 95% confident that the slope lies between 3.096964 and 4.767853",
    "sentences": ["Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command.", "Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your lm() code into mylm name.", "lm( lm(â€¦) is an R function that stands for â€œLinear Modelâ€.", "It performs a linear regression analysis for Y ~ X.", "YÂ  Y is your quantitative response variable.", "It is the name of one of the columns in your data set.", "~Â  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X.", "X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value.", "Â data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X.", "In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "R Instructions"
  },
  {
    "id": 61,
    "title": "ğŸ“ Explanation",
    "url": "LinearRegression.html#explanation",
    "content": "Explanation Linear regression has a rich mathematical theory behind it. This is because it uses a mathematical function and a random error term to describe the regression relation between a response variable \\(Y\\) and an explanatory variable called \\(X\\). Expand each element below to learn more. Regression Cheat Sheet (Expand) Term Pronunciation Meaning Math R Code \\(Y_i\\) $Y_i$ â€œwhy-eyeâ€ The data \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)\\) $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$ YourDataSet$YourYvariable \\(\\hat{Y}_i\\) $\\hat{Y}_i$ â€œwhy-hat-eyeâ€ The fitted line \\(\\hat{Y}_i = b_0 + b_1 X_i\\) $\\hat{Y}_i = b_0 + b_1 X_i$ lmObject$fitted.values \\(E\\{Y_i\\}\\) $E\\{Y_i\\}$ â€œexpected value of why-eyeâ€ True mean y-value \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) $E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i$ <none> \\(\\beta_0\\) $\\beta_0$ â€œbeta-zeroâ€ True y-intercept <none> <none> \\(\\beta_1\\) $\\beta_1$ â€œbeta-oneâ€ True slope <none> <none> \\(b_0\\) $b_0$ â€œb-zeroâ€ Estimated y-intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) $b_0 = \\bar{Y} - b_1\\bar{X} b_0 <- mean(Y) - b_1*mean(X)$ \\(b_1\\) $b_1$ â€œb-oneâ€ Estimated slope \\(b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\) $b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})} {\\sum(X_i - \\bar{X})^2}$ b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) \\(\\epsilon_i\\) $\\epsilon_i$ â€œepsilon-eyeâ€ Distance of dot to true line \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) $\\epsilon_i = Y_i - E\\{Y_i\\}$ <none> \\(r_i\\) $r_i$ â€œr-eyeâ€ or â€œresidual-eyeâ€ Distance of dot to estimated line \\(r_i = Y_i - \\hat{Y}_i\\) $r_i = Y_i - \\hat{Y}_i$ lmObject$residuals \\(\\sigma^2\\) $\\sigma^2$ â€œsigma-squaredâ€ Variance of the \\(\\epsilon_i\\) \\(Var\\{\\epsilon_i\\} = \\sigma^2\\)$Var\\{\\epsilon_i\\} = \\sigma^2$ <none> \\(MSE\\) $MSE$ â€œmean squared errorâ€ Estimate of \\(\\sigma^2\\) \\(MSE = \\frac{SSE}{n-p}\\)$MSE = \\frac{SSE}{n-p}$ sum( lmObject$res^2 ) / (n - p) \\(SSE\\) $SSE$ â€œsum of squared errorâ€ (residuals) Measure of dotâ€™s total deviation from the line \\(SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\)$SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$ sum( lmObject$res^2 ) \\(SSR\\) $SSR$ â€œsum of squared regression errorâ€ Measure of lineâ€™s deviation from y-bar \\(SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\)$SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2$ sum( (lmObject$fit - mean(YourData$Y))^2 ) \\(SSTO\\) $SSTO$ â€œtotal sum of squaresâ€ Measure of total variation in Y \\(SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\)$SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2$ sum( (YourData$Y - mean(YourData$Y))^2 ) \\(R^2\\) $R^2$ â€œR-squaredâ€ Proportion of variation in Y explained by the regression \\(R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\)$R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}$ SSR/SSTO \\(r\\) $r$ â€œrâ€ Correlation between X and Y. \\(r = \\sqrt{R^2}\\)$r = \\sqrt{R^2}$ sqrt(R^2) \\(\\hat{Y}_h\\) $\\hat{Y}_h$ â€œwhy-hat-aitchâ€ Estimated mean y-value for some x-value called \\(X_h\\) \\(\\hat{Y}_h = b_0 + b_1 X_h\\)$\\hat{Y}_h = b_0 + b_1 X_h$ predict(lmObject, data.frame(XvarName=#)) \\(X_h\\) $X_h$ â€œex-aitchâ€ Some x-value, not necessarily one of the \\(X_i\\) values used in the regression \\(X_h =\\) some number$X_h = $ Xh = # Confidence Interval â€œconfidence intervalâ€ Estimated bounds at a certain level of confidence for a parameter \\(b_0 \\pm t^* \\cdot s_{b_0}\\)b_0 \\pm t^* \\cdot s_{b_0} or \\(b_1 \\pm t^* \\cdot s_{b_1}\\)b_1 \\pm t^* \\cdot s_{b_1} confint(mylm, level = someConfidenceLevel) Parameter Estimate \\(\\beta_0\\) \\(b_0\\) \\(\\beta_1\\) \\(b_1\\) \\(\\epsilon_i\\) \\(r_i\\) \\(\\sigma^2\\) \\(MSE\\) \\(\\sigma\\) \\(\\sqrt{MSE}\\), the Residual standard error The Mathematical Model (Expand) \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)â€¦ There are three main elements to the mathematical model of regression. Each of these three elements is pictured below in the â€œRegression Relation Diagram.â€ Study both the three bullet points and their visual representations in the plot below for a clearer understanding. The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read moreâ€¦) The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as â€œnatural lawâ€ or â€œGodâ€™s lawâ€. It is some true line that is unknown to us. The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line. The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\). Note: \\(E\\{Y\\}\\) is pronounced â€œthe expected value of yâ€ because, wellâ€¦ the mean is the typical, average, or â€œexpectedâ€ value. The dots, i.e., the regression relation plus an error term: \\(Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}} + \\underbrace{\\epsilon_i}_\\text{error term} \\quad \\text{where} \\ \\epsilon_i\\sim N(0,\\sigma^2)\\) (Read moreâ€¦) This is shown by the dots in the graph below. This is the data. In regression, the assumption is that the y-value for individual \\(i\\), denoted by \\(Y_i\\), was â€œcreatedâ€ by adding an error term \\(\\epsilon_i\\) to each individualâ€™s â€œexpectedâ€ value \\(\\beta_0 + \\beta_1 X_i\\). Note the â€œorder of creationâ€ would require first knowing an indivualâ€™s x-value, \\(X_i\\), then their expected value from the regression relation \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) and then adding their \\(\\epsilon_i\\) value to the result. The \\(\\epsilon_i\\) allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance \\(\\epsilon_i\\) from the line. Note: \\(Y_i\\) is pronounced â€œwhy-eyeâ€ because it is the y-value for individual \\(i\\). Sometimes also called â€œwhy-sub-eyeâ€ because \\(i\\) is in the subscript of \\(Y\\). The estimated line, i.e., the line we get from a sample of data. \\(\\underbrace{\\hat{Y}_i}_{\\substack{\\text{estimated mean} \\\\ \\text{y-value}}} = \\underbrace{b_0 + b_1 X_i}_\\text{estimated regression equation}\\) (Read moreâ€¦) The estimated line is shown by the solid line in the graph below. \\(\\hat{Y}\\) is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation \\(E\\{Y\\}\\). So \\(\\hat{Y}\\) is interpreted as the estimated average (or mean) \\(Y\\)-value for any given \\(X\\)-value. Thus, \\(b_0\\) is the estimated y-intercept and \\(b_1\\) is the estimated slope. The bâ€™s are sample statistics, like \\(\\bar{x}\\) and the \\(\\beta\\)â€™s are population parameters like \\(\\mu\\). The \\(b\\)â€™s estimate the \\(\\beta\\)â€™s. Note: \\(\\hat{Y}_i\\) is pronounced â€œwhy-hat-eyeâ€ and is known as the â€œestimated y-valueâ€ or â€œfitted y-valueâ€ because it is the y-value you get from \\(b_0 + b_1 X_i\\). It is always different from \\(Y_i\\) because dots are rarely if ever exactly on the estimated regression line. This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line). Something to ponder: The true line, when coupled with the error terms, â€œcreatesâ€ the data. The estimated (or fitted) line uses the sampled data to try to â€œre-createâ€ the true line. We could loosely call this the â€œorder of creationâ€ as shown by the following diagram. par(mfrow=c(1,3), mai=c(.2,.2,.4,.1)) plot(y ~ x, col=\"white\", main=\"A Law is Given\", yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, main=\"Data is Created\", xaxt='n', yaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, xaxt='n', yaxt='n', main=\"The Law is Estimated\") curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) A Law is Given Data is Created The Law is Estimated \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) \\(Y_i = E\\{Y_i\\} + \\epsilon_i\\) \\(\\hat{Y}_i = b_0 + b_1 X_i\\) The true line is the â€œlawâ€. The \\(Y_i\\) are created by adding \\(\\epsilon_i\\) to \\(E\\{Y_i\\}\\) where \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\). The law is estimated with \\(\\hat{Y}_i\\) which is given with lm(...). Click open the â€œCodeâ€ buttom below to the right to find code that runs a simulation demonstrating this â€œorder of creationâ€. ## Simulating Data from a Regression Model ## This R-chunk is meant to be played in your R Console. ## It allows you to explore how the various elements ## of the regression model combine together to \"create\" ## data and then use the data to \"re-create\" the line. set.seed(101) #Allows us to always get the same \"random\" sample #Change to a new number to get a new sample n <- 153 #set the sample size X_i <- runif(n, 0, 22) #Gives n random values from a uniform distribution between 15 to 45. beta0 <- 89 #Our choice for the y-intercept. beta1 <- -1.25 #Our choice for the slope. sigma <- 8 #Our choice for the std. deviation of the error terms. epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data View(fabData) #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it. fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData. summary(fab.lm) #Summarize your model. plot(y ~ x, data=fabData) #Plot the data. abline(fab.lm) #Add the estimated regression line to your plot. # Now for something you can't do in real life... but since we created the data... abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). legend(\"topleft\", legend=c(\"True Line\", \"Estimated Line\"), lty=c(2,1), bty=\"n\") #Add a legend to your plot specifying which line is which. Interpreting the Model Parameters (Expand) \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted asâ€¦ The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model. If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\). The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\). It is often misunderstood to be the â€œaverage change in yâ€ or just â€œthe change in yâ€ but it is more correctly referred to as the â€œchange in the average yâ€. To better see this, consider the three graphics shown below. par(mfrow=c(1,3)) hist(mtcars$mpg, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Number of Vehicles\", xlab=\"Gas Mileage (mpg)\", col=\"skyblue\") boxplot(mpg ~ cyl, data=mtcars, border=\"skyblue\", boxwex=0.5, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Number of Cylinders of Engine (cyl)\") plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=\"skyblue\", main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Quarter Mile Time (qsec)\") abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=\"darkgray\") mtext(side=3, text=\"Automatic Transmissions Only (am==0)\", cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=\"gray\") The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09. The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg. The scatterplot on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value. Residuals and Errors (Expand) \\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true errorâ€¦ Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\). The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\). We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summaryâ€¦ Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\). \\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms. Keep in mind the idea that the errors \\(\\epsilon_i\\) â€œcreatedâ€ the data and that the residuals \\(r_i\\) are computed after using the data to â€œre-createâ€ the line. Residuals have many uses in regression analysis. They allow us to diagnose the regression assumptions, See the â€œAssumptionsâ€ section below for more details. estimate the regression relation, See the â€œEstimating the Model Parametersâ€ section below for more details. estimate the variance of the error terms, See the â€œEstimating the Model Varianceâ€ section below for more details. and assess the fit of the regression relation. See the â€œAssessing the Fit of a Regressionâ€ section below for more details. Assessing the Fit of a Regression (Expand) \\(R^2\\), SSTO, SSR, and SSEâ€¦ Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line. par(mfrow=c(1,3), mai=c(.1,.1,.5,.1)) set.seed(2) x <- runif(30,0,20) y1 <- 2 + 3.5*x + rnorm(30,0,2) y2 <- 2 + 3.5*x + rnorm(30,0,8) y3 <- 2 + 3.5*x + rnorm(30,0,27) plot(y1 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Excellent Fit\") abline(lm(y1 ~ x), col=\"gray\") plot(y2 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Good Fit\") abline(lm(y2 ~ x), col=\"gray\") plot(y3 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Poor Fit\") abline(lm(y3 ~ x), col=\"gray\") A common way to measure the fit of a regression is with correlation. While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\). (If you are a Math 325 student, just stick with correlation for now and skip on to the next section of this Explanation tab. If you are a Math 425 student, it is critical that you come to understand \\(R^2\\) deeply, so read on.) Before you can understand \\(R^2\\), you must understand three important â€œsums of squaresâ€. (Read more about sumsâ€¦) A sum is just a fancy word for adding things together. \\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand. So we use the symbol \\(\\Sigma\\) to denote the word â€œsumâ€. Further, we use a subscript \\(\\Sigma_{i=1}\\) to state what value the sum is beginning with, and a superscript \\(\\Sigma_{i=1}^6\\) to state the value we are ending at. This gives \\[ \\sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Test your knowledge, do you see why the answer is 6 to the sum below? \\[ \\sum_{i=1}^3 i = 6 \\] Computing sums in R is fairly easy. Type the following codes in your R Console. sum(1:6) #gives the answer of 21 sum(1:3) #gives the answer of 6 However, sums really become useful when used with a data set. Each row of a data set represents an â€œindividualâ€™sâ€ data. We can reference each individual with a row number. In the data below, individual 3, denoted by \\(i=3\\), has a speed of 7 and a dist of 4. pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3) Individual speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 To compute the sum of the speed column, use sum(speed). If we divided this sum by 6, we would get the mean of speed mean(speed). In fact, the two most used statistics mean(...) and sd(...) both use sums. Take a moment to review the formulas for mean and standard deviation. It is strongly recommended that you study the Explanation tab for both as well. Weâ€™ll wait. See you back here shortly. â€¦ Welcome back. Suppose we let X = speed and Y = dist. Then \\(X_3 = 7\\) and \\(Y_3 = 4\\) because we are accessing row 3 of both the \\(X\\) (or speed) column and \\(Y\\) (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, sum(speed) would be written mathematically as \\(\\sum_{i=1}^6 X_i\\) and sum(dist) would be written as \\(\\sum_{i=1}^6 Y_i\\). Sum of Squared Errors Sum of Squares Regression Total Sum of Squares \\(\\text{SSE} = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\) \\(\\text{SSR} = \\sum_{i=1}^n \\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\) \\(\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\\) Measures how much the residuals deviate from the line. Measures how much the regression line deviates from the average y-value. Measures how much the y-values deviate from the average y-value. Equals SSTO - SSR Equals SSTO - SSE Equals SSE + SSR sum( (Y - mylm$fit)^2 ) sum( (mylm$fit - mean(Y))^2 ) sum( (Y - mean(Y))^2 ) It is important to remember that SSE and SSR split up SSTO, so that \\[ \\text{SSTO} = \\text{SSE} + \\text{SSR} \\] This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works. The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called \\(R^2\\) (â€œr-squaredâ€). R-Squared (\\(R^2\\)) \\[ \\underbrace{R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}}_\\text{Interpretation: Proportion of variation in Y explained by the regression.} \\] The smallest \\(R^2\\) can be is zero, and the largest it can be is 1. This is because \\(SSR\\) must be between 0 and SSTO, inclusive. Residual Plots & Regression Assumptions (Expand) Residuals vs.Â fitted-values, Q-Q Plot of the residuals, and residuals vs.Â order plotsâ€¦ There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate. Each assumption is labeled in the regression equation below. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions. (Read moreâ€¦) As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not. Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3 The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(\\hat{Y}_i\\). The residuals are the \\(r_i\\). This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable. | Show Examples | (Read moreâ€¦) The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption. The linear relation is assumed to be satisfied if there are no apparent trends in the plot. The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values. The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance. set.seed(2) X <- rnorm(30,15,3) notLin <- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8)) notLin.lm <- lm(Y~X, data=notLin) set.seed(15) Lin <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) Lin.lm <- lm(Y~X, data=Lin) par(mfrow=c(3,3), mai=c(.25,.25,.25,.25), mgp=c(1,.75,0)) plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Not Linear\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(notLin.lm$fitted.values,notLin.lm$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Good: Linear, Constant Variance\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) set.seed(6) notCon <- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5)) notCon.lm <- lm(Y~X, data=notCon) LinO <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) LinO[1] <- LinO[1]^2 LinO.lm <- lm(Y~X, data=LinO) plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Unconstant Variance\", cex.main=0.95, yaxt='n', xaxt='n', col=\"firebrick\") polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) # plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Outliers\", cex.main=0.95) # abline(h=0) tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(Girth ~ Volume, data=trees[-31,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(Height ~ Volume, data=trees) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) abline(h=0) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals, f=.4) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(weight ~ repwt, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(weight ~ repht, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) abline(h=0) Q-Q Plot of the Residuals: Checks Assumption #2 The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption. | Show Examples | (Read moreâ€¦) There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot. Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) set.seed(123) tmp <- rnorm(100) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"skyblue\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Normal\", col=\"skyblue\") tmp <- Davis$weight qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Right-skewed\", breaks=15, col=\"firebrick\") par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- rbeta(100, 5,1) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Left-skewed\", breaks=seq(min(tmp),max(tmp), length.out=13), col=\"firebrick\") tmp <- rbeta(100,2,2) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Heavy-tailed\", col=\"firebrick\") Residuals versus Order Plot: Checks Assumption #5 When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated. | Show Examples | (Read moreâ€¦) Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Good: No Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: General Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ qsec, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: Interesting Patterns\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),]) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Bad: Obvious Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") Problems from Failed Assumptions There are various problems that can arise when certain of the regression assupmtions are not satisfied. Lack of Linearity When the linearity assumption is violated, pretty much everything we obtain from the regression summary is no longer meaningful. The y-intercept estimate can be drastically off from its actual true value. Important model information is lost by trying to use a simple slope term \\(\\beta_1\\) to describe the model with respect to \\(X\\). The residual standard error will be much higher than it otherwise would be because of curvature patterns in the data that the line cannot capture. Thus, R-squared will be lower than it otherwise should be. P-values can become non-significant, when in fact there is a strong pattern in the data, but that pattern just cannot be captured by a simple line. *Normality of the errors is often put into question as well when a simplified line is used to try to capture a more complicated curved model. The plot below demonstrate these difficulties. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 7.5 #True slope beta_2 <- -0.25 #True bend X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors Y_i <- beta_0 + beta_1*X_i + beta_2*X_i^2 + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Non-Linear Relation\") abline(mylm, col=\"gray\") #Add fitted line to plot curve(beta_0 + beta_1*x + beta_2*x^2, col=\"gray\", lty=2, add=TRUE) #Add True line to plot #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (True value:\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (True value:\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (True value:\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Non-normal Error Terms When the normality assumption for the error terms is violated, not all is lost. In fact, the estimate of the slope and intercept are still often fairly meaningful. However, it is unwise to put too much trust in the residual standard error as an estimate of the standard deviation \\(\\sigma\\). This is because the standard deviation in skewed distributions does not carry the same meaning it has in normal distributions. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rchisq(n, 1)*3 - 1 #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Unconstant Variance When variance of the error term changes across the regression, the regression approximates the â€œaverage varianceâ€ of the errors because the regression is still assuming the variance is constant across the regression. The estimates of the slope and intercept are still typically quite good, and can be used for interpretation. The residual standard error however should not be considered to be meaningful as it will be too large on one end of the regression and too small on the other end. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma + X_i) #normally distributed errors #with increasing variance Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Variance Varies (Non-Constant)\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 13.82 Slope 3.5 3.768 Sigma 2.5 13.02 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", round(mean(sigma + X_i), 2), \", mean)\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Normality Violated As silly as it sounds, if the only problem with the regression is the lack of normality of the error terms, it isnâ€™t all that big of a problem. Depending on how non-normal the residuals appear, there could be some skewing to the residual standard error, but otherwise, the slope and intercept are still interpretable and meaningful. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- runif(n, -sqrt(12*sigma^2)/2, sqrt(12*sigma^2)/2) #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 14.7 Slope 3.5 3.449 Sigma 2.5 2.317 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Independence Assumption Violated While the slope and intercept are often still meaningful when the independence assumption is violated, the residual standard error is unnecessarily large in this case. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, 2.5) + (1:n -n/2)*.5 #normal, but correlated errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Independence Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 16.47 Slope 3.5 3.296 Sigma 2.5 4.819 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Outliers Present While outliers do not violate any of the regression assumptions, they do pose substantial difficulties for the least squares regression estimates of the slope and intercept. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors epsilon_i[3] <- ifelse(X_i[3] < 10, runif(1,25,35), -runif(1,25,35)) #create outlier Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"An Outlier Present\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 13.18 Slope 3.5 3.447 Sigma 2.5 6.389 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) The material below this section is meant for Math 425 students only. Estimating the Model Parameters (Expand) How to get \\(b_0\\) and \\(b_1\\): least squares & maximum likelihoodâ€¦ There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical. The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas. Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 <- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\). When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\). Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i. \\label{exp} \\end{equation}\\] Least Squares To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\). \\[ Q = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2 \\] Then we use the function Q as if it were a function of \\(\\beta_0\\) and \\(\\beta_1\\). Ironically, the values of \\(Y\\) and \\(X\\) are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we donâ€™t know are the values of \\(\\beta_0\\) and \\(\\beta_1\\). This least squares applet is a good way to explore how various choices of the slope and intercept yield different values of the â€œsum of squared residualsâ€. But it turns out that there is one â€œbestâ€ choice of the slope and intercept that yields a â€œsmallestâ€ value of the â€œsum of squared residuals.â€ This best choice can actually be found using calculus by taking the partial derivatives of \\(Q\\) with respect to both \\(\\beta_0\\) and \\(\\beta_1\\). \\[ \\frac{\\partial Q}{\\partial \\beta_0} = -2\\sum (Y_i - \\beta_0 - \\beta_1X_i) \\] \\[ \\frac{\\partial Q}{\\partial \\beta_1} = -2\\sum X_i(Y_i-\\beta_0-\\beta_1X_i) \\] Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize \\(Q\\) for a given set of data. After all the calculations are completed we find the values of the parameter estimators \\(b_0\\) and \\(b_1\\) (of \\(\\beta_0\\) and \\(\\beta_1\\), respectively) are as stated previously. Maximum Likelihood The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of \\(\\beta_0\\) and \\(\\beta_1\\) which minime the least squares \\(Q\\) function, we choose the values of \\(\\beta_0\\) and \\(\\beta_1\\) which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the \\(Y_i\\) for all observations \\(i=1,\\ldots,n\\). We can do this rather simply by using the assumption that the errors, \\(\\epsilon_i\\) are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if \\(f(Y_i)\\) denotes the probability density function for \\(Y_i\\), then the joint probability density for all \\(Y_i\\), \\(f(Y_1,\\ldots,Y_n)\\) is given by \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) \\] Since each \\(Y_i\\) is assumed to be normally distributed with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\) (see model (\\(\\ref{model}\\))) we have that \\[ f(Y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{Y_i-\\beta_0-\\beta_1X_i}{\\sigma}\\right)^2\\right]} \\] which provides the joint probability as \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] The likelihood function \\(L\\) is then given by consider the \\(Y_i\\) and \\(X_i\\) fixed and the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) as the variables in the function. \\[ L(\\beta_0,\\beta_1,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] Instead of taking partial derivatives of \\(L\\) directly (with respect to all parameters) we take the partial derivatives of the \\(\\log\\) of \\(L\\), which is easier to work with. In a similar, but more difficult calculation, to that of minimizing \\(Q\\), we obtain the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) which maximize the log of \\(L\\), and which therefore maximize \\(L\\). (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate \\(\\hat{\\sigma}^2\\) of \\(\\sigma^2\\). \\[ \\hat{\\sigma}^2 = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n} \\] Estimating the Model Variance (Expand) Estimating \\(\\sigma^2\\) with MSEâ€¦ As shown previously in the â€œEstimating Model Parametersâ€ section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation. Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\). This estimate turns out to be a biased estimator. This means that it is consistently wrong in its estimates of \\(\\sigma^2\\). If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong. This is bad. Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\). Without going into all the details, to â€œfixâ€ the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently. To find the correct degrees of freedom, we have to notice that the \\(\\hat{Y}_i\\) in the numerator of \\(\\widehat{\\sigma}^2\\) is defined by \\[\\begin{equation} \\widehat{Y}_i = b_0 + b_1X_i \\label{hatY} \\end{equation}\\] From this equation, we notice that two means, \\(\\bar{X}\\) and \\(\\bar{Y}\\), were estimated from the data in order to obtain \\(\\hat{Y}_i\\). (See the formulas for \\(b_0\\) and \\(b_1\\) above, and note how they use both \\(\\bar{X}\\) and \\(\\bar{Y}\\) in their calculation.) Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for \\(\\hat{\\sigma}^2\\) should be \\(n-2\\) instead of \\(n\\). Some incredibly long calculations will show that the â€œfixedâ€ estimator \\[\\begin{equation} s^2 = MSE = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n-2} \\quad \\text{(Unbiased Estimator of $\\sigma^2$)} \\end{equation}\\] is an unbiased estimator of \\(\\sigma^2\\). Here \\(MSE\\) stands for mean squared error, which is the most obvious name for a formula that squares the errors \\(Y_i-\\hat{Y}_i\\) then adds them up and divides by their degrees of freedom. Similarly, we call the numerator \\(\\sum(Y_i-\\hat{Y}_i)^2\\) the sum of the squared errors, denoted by \\(SSE\\). It is also important to note that the errors are often denoted by \\(r_i = Y_i-\\hat{Y}_i\\), the residuals. Putting this all together we get the following equivalent statements for \\(MSE\\). \\[\\begin{equation} s^2 = MSE = \\frac{SSE}{n-2} = \\frac{\\sum(Y_i-\\widehat{Y}_i)^2}{n-2} = \\frac{\\sum r_i^2}{n-2} \\end{equation}\\] As a final note, even though the expected value \\(E\\{MSE\\} = \\sigma^2\\), which shows \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\), it unfortunately isnâ€™t true that \\(\\sqrt{MSE}\\) is an unbiased estimator of \\(\\sigma\\). This presents a few problems later on, but these are minimal enough that we can overlook the issue and move forward. Transformations (Expand) \\(Y'\\), \\(X'\\), and returning to the original spaceâ€¦ Y transformations are denoted by y-prime, written \\(Y'\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\). \\[ Y' = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y' = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y' = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y' = \\log(Y)\\) lm(log(Y) ~ X) 0.25 \\(Y' = \\sqrt(\\sqrt(Y))\\) lm(sqrt(sqrt(Y)) ~ X) 0.5 \\(Y' = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y' = Y\\) lm(Y ~ X) 2 \\(Y' = Y^2\\) lm(Y^2 ~ X) Using â€œmaximum-likelihoodâ€ estimation, the Box-Cox procedure can actually automatically detect the â€œoptimalâ€ value of \\(\\lambda\\) to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise. Scatterplot Recognition Box-Cox Suggestion An Example Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\). set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Ya <- 1/sqrt(Y) #1/Y^2 Lam = -2 Yb <- 1/Y #1/Y Lam = -1 Yc <- exp(.02*Y) #log(Y) Lam = 0 Yd <- Y^2 #sqrt(Y) Lam = 0.5 Ye <- Y #Y Lam = 1 Yf <- sqrt(Y) #Y^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Ya ~ X, main=expression(paste(\"Use \", lambda == -2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ya^-2 ~ X)) curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yb ~ X, main=expression(paste(\"Use \", lambda == -1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yb^-1 ~ X)) curve(1/(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yc ~ X, main=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(log(Yc) ~ X)) curve(exp(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yd ~ X, main=expression(paste(\"Use \", lambda == 0.5)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(sqrt(Yd) ~ X)) curve((b[1] + b[2]*x)^2, add=TRUE, col=\"green\", lwd=2) plot(Ye ~ X, main=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ye ~ X)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yf ~ X, main=expression(paste(\"Use \", lambda == 2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yf^2 ~ X)) curve(sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try. par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0)) boxCox(lm(Ya ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -2)), line=.5) boxCox(lm(Yb ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -1)), line=.5) boxCox(lm(Yc ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), line=.5) boxCox(lm(Yd ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0.5)), line=.5) boxCox(lm(Ye ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), line=.5) boxCox(lm(Yf ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 2)), line=.5) An Example Suppose we were running a simple linear regression on the cars dataset. This would be done with the code cars.lm <- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesnâ€™t quite fit the data as well as we would hope. Instead, the data looks a little curved. cars.lm <-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") abline(cars.lm, col=\"gray\") Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful. library(car) boxCox(cars.lm) The output from the boxCox(...) function looks as follows. This plot tells use to use the \\(\\lambda = 0.5\\) transformation, so that \\(Y' = Y^0.5 = \\sqrt{Y}\\). (To see this yourself, click on the â€œBox-Cox Suggestionâ€ tab above, as well as on the â€œScatterplot Recognitionâ€ tab.) Now, a transformation regression is performed using sqrt(Y) in place of Y as follows: cars.lm.t <- lm(sqrt(dist) ~ speed, data=cars) summary(cars.lm.t) Â  Estimate Std. Error t value Pr(> (Intercept) 1.277 0.4844 2.636 0.01126 speed 0.3224 0.02978 10.83 1.773e-14 Then, \\[ \\widehat{Y}_i' = 1.277 + 0.3224 X_i \\] And replacing \\(\\hat{Y}_i' = \\sqrt{\\hat{Y}_i}\\) we have \\[ \\sqrt{\\widehat{Y}_i} = 1.277 + 0.3224 X_i \\] Solving for \\(\\hat{Y}_i\\) gives \\[ \\widehat{Y}_i = (1.277 + 0.3224 X_i)^2 \\] Which, using curve((1.277 + 0.3224*x)^2, add=TRUE) (see code for details) looks like this: plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") curve( (1.277 + 0.3224*x)^2, add=TRUE, col=\"firebrick\") X-Transformations X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them. The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated. The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation. set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Xa <- 1/sqrt(X) #1/X^2 Lam = -2 Xb <- 1/X #1/X Lam = -1 Xc <- exp(.02*X) #log(X) Lam = 0 Xd <- X^2 #sqrt(X) Lam = 0.5 Xe <- X #X Lam = 1 Xf <- sqrt(X) #X^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Y ~ Xa, main=expression(paste(\"Use \", X*minute == X^-2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xa^-2))) curve(b[1] + b[2]*x^-2, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xb, main=expression(paste(\"Use \", X*minute == X^-1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xb^-1))) curve(b[1] + b[2]*x^-1, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xc, main=expression(paste(\"Use \", X*minute == log(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ log(Xc))) curve(b[1] + b[2]*log(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xd, main=expression(paste(\"Use \", X*minute == sqrt(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ sqrt(Xd))) curve(b[1] + b[2]*sqrt(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xe, main=expression(paste(\"Use \", X*minute == X, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ Xe)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xf, main=expression(paste(\"Use \", X*minute == X^2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xf^2))) curve(b[1] + b[2]*x^2, add=TRUE, col=\"green\", lwd=2) Inference for the Model Parameters (Expand) t test formulas, sampling distributions, confidence intervals, and F testsâ€¦ When fitting the regression model given by the equation \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] to a sample of data, we typically test hypotheses about the parameters \\(\\beta_0\\), \\(\\beta_1\\), or both. Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_0 - \\overbrace{0}^\\text{a number}}{s_{b_0}}\\] This is the formula for the test statistic. It measures how far the estimated y-intercept \\(b_0\\) is from the null hypothesis for \\(\\beta_0\\) in units of â€œstandard errors of \\(b_0\\)â€. Thus the division by \\(s_{b_0}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0, it could be any number. \\(H_0: \\beta_1 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_1\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_1 - \\overbrace{0}^\\text{a number}}{s_{b_1}}\\] This is the formula for the test statistic. It measures how far the estimated slope \\(b_1\\) is from the null hypothesis for \\(\\beta_1\\) in units of â€œstandard errors of \\(b_1\\)â€. Thus the division by \\(s_{b_1}\\). Though the hypothesized value of \\(\\beta_1\\) is typically 0, it could be any number. Left-tailed p-value = pt(-abs(tvalue), degrees of freedom). Double it to get the two-sided p-value. In R, these values correspond to the output summary of an lm as follows. (Show Example) Consider the cars data in R. Suppose we used the regression model given by \\[ \\underbrace{Y_i}_\\text{Feet to Stop} = \\beta_0 + \\beta_1 \\underbrace{X_{i}}_\\text{mph} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\quad \\sim N(0,\\sigma^2) \\] to model the feet a vehicle (from the 1920â€™s) takes to stop when traveling at a certain speed (in miles per hour, mph) prior to stopping. When the regression is performed and summarized in R, it is always testing the following two hypotheses: \\[ H_0: \\beta_0 = 0 \\quad\\quad H_0: \\beta_1 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\quad\\quad H_a: \\beta_1 \\neq 0 \\] To perform the test of these hypotheses for the regression stated above, we would run the following codes in R. cars.lm <- lm(dist ~ speed, data=cars) pander(summary(cars.lm)$coefficients) These would produce summary output like the following, but the following output has been labeled with the math notation corresponding to each value. Letâ€™s emphasize what is happening in this summary output table. First, here is how the â€œt valueâ€ is calculated for the â€œ(Intercept)â€ in the summary table above. \\[ t = \\frac{b_0-0}{s_{b_0}} = \\frac{-17.58 - 0}{6.758} = -2.601 \\] Second, here is a visual representation of how the P-value, the â€œPr(>|t|)â€ as it is called in the summary table above, is calculated for this test statistic. (Click the graph to view an interactive applet showing this calculation.) Notice both ends of the t-distribution are being shaded to compute the P-value because the alternative hypothesis was \\(H_a: \\beta_0 \\neq 0\\). To compute the P-value in R, we use the â€œpercentile function for the t-distributionâ€ called pt( ). This function requires two things, the t-value and the degrees of freedom, in our case pt(-2.601, 48). Note the degrees of freedom (df) are 48 because the sample size is \\(n=50\\) and there are two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) in our regression model. Running this code in R gives: pt(-2.601, 48) = 0.00616 However, note that this value is only half of the actual P-value of 0.0123. To get the â€œtwo-sidedâ€ P-value (note that our alternative hypothesis used a \\(\\neq\\) symbol) we need to double this left-tailed P-value. 2*pt(-2.601, 48)) = 0.0123 Finally, note that the same procedure can be used to test hypotheses that use a value other than 0 in the null and alternative. For example, to test the hypotheses: \\[ H_0: \\beta_1 = 3 \\\\ H_a: \\beta_1 \\neq 3 \\] Use the t-formula \\[ t = \\frac{b_1 - 3}{s_{b_1}} = \\frac{\\overbrace{3.932}^{b_1} - \\overbrace{3}^{H_0}}{\\underbrace{0.4155}_{s_{b_1}}} = 2.243 \\] then the P-value is calculated in R by 2*pt(-abs(2.243), 48) = 0.0295495 To obtain confidence intervals in R use confint(mylm). Confidence Interval Formula Standard Error \\(\\beta_0\\) \\(b_0 \\pm\\) \\(t^*\\) This is called the â€œcritical valueâ€ and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\). Use qt(0.975, df) to get \\(t*\\) in R. \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_0\\). \\(s_{b_0}\\) The standard error of \\(b_0\\), denoted by \\(s_{b_0}\\) is provided in the regression summary output under the column header called â€œStd. Errorâ€ for the â€œ(Intercept)â€ row of the output. It is calculated using the formula shown below. \\[s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\] This is called the â€œestimated variance of \\(b_0\\)â€. Taking the square root of this number gives the â€œstandard error of \\(b_0\\)â€. \\(\\beta_1\\) \\(b_1 \\pm\\) \\(t^*\\) This is called the â€œcritical valueâ€ and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\) (sample size - number of parameters in the regression model). Use qt(0.975, df) to get \\(t*\\) in R \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_1\\). \\(s_{b_1}\\) The standard error of \\(b_1\\), denoted by \\(s_{b_1}\\) is provided in the regression summary output under the column header called â€œStd. It is calculated using the formula shown below. \\[s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\] This is called the â€œestimated variance of \\(b_1\\)â€. Taking the square root of this number gives the â€œstandard error of \\(b_1\\)â€. To be more exact, the types of inference we are interested in are the following. Determine if there is evidence of a meaningful linear relationship in the data. If \\(\\beta_1 = 0\\), then there is no relation between \\(X\\) and \\(E\\{Y\\}\\). Hence we might be interested in testing the hypotheses \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) is some hypothesized number. To provide a confidence interval for the true value of \\(\\beta_1\\). Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the sampling distribution of the estimate \\(b_1\\) of the parameter \\(\\beta_1\\). And, while we are at it, we may as well come to understand the sampling distribution of the estimate \\(b_0\\) of the parameter \\(\\beta_0\\). Review sampling distributions from Math 221. Since \\(b_1\\) is an estimate, it will vary from sample to sample, even though the truth, \\(\\beta_1\\), remains fixed. (The same holds for \\(b_0\\) and \\(\\beta_0\\).) It turns out that the sampling distribution of \\(b_1\\) (where the \\(X\\) values remain fixed from study to study) is normal with mean and variance: \\[ \\mu_{b_1} = \\beta_1 \\] \\[ \\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} \\] ## Simulation to Show relationship between Standard Errors ##----------------------------------------------- ## Edit anything in this area... n <- 100 #sample size Xstart <- 30 #lower-bound for x-axis Xstop <- 100 #upper-bound for x-axis beta_0 <- 2 #choice of true y-intercept beta_1 <- 3.5 #choice of true slope sigma <- 13.8 #choice of st. deviation of error terms ## End of Editable area. ##----------------------------------------------- # Create X, which will be used in the next R-chunk. X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) ## After playing this chunk, play the next chunk as well. To see that this is true, consider the regression model with values specified for each parameter as follows. \\[ Y_i = \\overbrace{\\beta_0}^{2} + \\overbrace{\\beta_1}^{3.5} X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\overbrace{\\sigma^2}^{\\sigma=13.8}) \\] Using the equations above for \\(\\mu_{b_1}\\) and \\(\\sigma^2_{b_1}\\) we obtain that the mean of the sampling distribution of \\(b_1\\) will be \\(\\mu_{b_1} = \\beta_1 = 3.5\\) Further, we see that the variance of the sampling distribution of \\(b_1\\) will be \\(\\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} = \\frac{13.8^2}{4.25\\times 10^{4}}\\) Taking the square root of the variance, the standard deviation of the sampling distribution of \\(b_1\\) will be \\(\\sigma_{b_1} = 0.067\\). Thatâ€™s very nice. But to really believe it, letâ€™s run a simulation ourselves. The â€œCodeâ€ below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of \\(b_1\\) and \\(b_0\\) for that regression. After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of \\(b_0\\) is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of \\(b_1\\) is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of \\(\\beta_0\\) or \\(\\beta_1\\), respectively. Also, the â€œStd. Errorâ€ of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of \\(\\sigma_{b_0}\\) and \\(\\sigma_{b_1}\\), respectively. N <- 5000 #number of times to pull a random sample storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N) for (i in 1:N){ Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model mylm <- lm(Y ~ X) storage_b0[i] <- coef(mylm)[1] storage_b1[i] <- coef(mylm)[2] storage_rmse[i] <- summary(mylm)$sigma } layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3)) Ystart <- 0 #min(0,min(Y)) Ystop <- 500 #max(max(Y), 0) Yrange <- Ystop - Ystart plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), ylim=c(Ystart, Ystop), pch=16, col=\"gray\", main=\"Regression Lines from many Samples (gray lines) \\n Plus Residual Standard Deviation Lines (green lines)\") text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1) text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1) text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1) for (i in 1:N){ abline(storage_b0[i], storage_b1[i], col=\"darkgray\") } abline(beta_0, beta_1, col=\"green\", lwd=3) abline(beta_0+sigma, beta_1, col=\"green\", lwd=2) abline(beta_0-sigma, beta_1, col=\"green\", lwd=2) abline(beta_0+2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0-2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0+3*sigma, beta_1, col=\"green\", lwd=.5) abline(beta_0-3*sigma, beta_1, col=\"green\", lwd=.5) par(mai=c(1,.6,.5,.01)) addnorm <- function(m,s, col=\"firebrick\"){ curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2) lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col) lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col) lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col) lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col) lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col) lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col) lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col) legend(\"topleft\", legend=paste(\"Std. Error = \", round(s,3)), cex=0.7, bty=\"n\") } h0 <- hist(storage_b0, col=\"skyblue3\", main=\"Sampling Distribution\\n Y-intercept\", xlab=expression(paste(\"Estimates of \", beta[0], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m0 <- mean(storage_b0) s0 <- sd(storage_b0) addnorm(m0,s0, col=\"green\") h1 <- hist(storage_b1, col=\"skyblue3\", main=\"Sampling Distribution\\n Slope\", xlab=expression(paste(\"Estimates of \", beta[1], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m1 <- mean(storage_b1) s1 <- sd(storage_b1) addnorm(m1,s1, col=\"green\") t Tests Using the information above about the sampling distributions of \\(b_1\\) and \\(b_0\\), an immediate choice of statistical test to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) can be zero, or any other value, is a t test given by \\[ t = \\frac{b_1 - \\beta_{10}}{s_{b_1}} \\] where \\(s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\). (You may want to review the section â€œEstimating the Model Varianceâ€ of this file to know where MSE came from.) With quite a bit of work it has been shown that \\(t\\) is distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. The nearly identical test statistic for testing \\[ H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_0 \\neq \\beta_{00} \\] is given by \\[ t = \\frac{b_0 - \\beta_{00}}{s_{b_0}} \\] where \\(s^2_{b_0} = MSE\\left[\\frac{1}{n}+\\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\). This version of \\(t\\) has also been shown to be distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. Confidence Intervals Creating a confidence interval for either \\(\\beta_1\\) or \\(\\beta_0\\) follows immediately from these results using the formulas \\[ b_1 \\pm t^*_{n-2}\\cdot s_{b_1} \\] \\[ b_0 \\pm t^*_{n-2}\\cdot s_{b_0} \\] where \\(t^*_{n-2}\\) is the critical value from a t distribution with \\(n-2\\) degrees of freedom corresponding to the chosen confidence level. F tests Another way to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\quad\\quad \\text{or} \\quad\\quad H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\quad\\quad \\ \\ \\quad \\quad H_a: \\beta_0 \\neq \\beta_{00} \\] is with an \\(F\\) Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an \\(F\\) test is very general and can be used in many places that a t test cannot. In its most general form, the \\(F\\) test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that \\(H_0:\\beta_1=0\\) against the alternative that \\(H_a: \\beta_1\\neq 0\\), we are essentially comparing two models against each other. If \\(\\beta_1=0\\), then the corresponding model would be \\(E\\{Y_i\\} = \\beta_0\\). If \\(\\beta_1\\neq0\\), then the model remains \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the \\(F\\) Test, that the null model (reduced model) have fewer â€œfreeâ€ parameters than the alternative model (full model). To demonstrate what we mean by â€œfreeâ€ parameters, consider the following example. Say we wanted to test the hypothesis that \\(H_0:\\beta_1 = 2.5\\) against the alternative that \\(\\beta_1\\neq2.5\\). Then the null, or reduced model, would be \\(E\\{Y_i\\}=\\beta_0+2.5X_i\\). The alternative, or full model, would be \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). Thus, the null (reduced) model contains only one â€œfreeâ€ parameter because \\(\\beta_1\\) has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two â€œfreeâ€ parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model. Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing \\(H_0: \\beta_1=0\\) against \\(H_a:\\beta_1\\neq0\\) we have the partition \\[ \\underbrace{Y_i-\\bar{Y}}_{Total} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{Regression} + \\underbrace{Y_i-\\hat{Y}_i}_{Error} \\] The reason we use \\(\\bar{Y}\\) for the null model is that \\(\\bar{Y}\\) is the unbiased estimator of \\(\\beta_0\\) for the null model, \\(E\\{Y_i\\} = \\beta_0\\). Thus we would compute the following sums of squares: \\[ SSTO = \\sum(Y_i-\\bar{Y})^2 \\] \\[ SSR = \\sum(\\hat{Y}_i-\\bar{Y})^2 \\] \\[ SSE = \\sum(Y_i-\\hat{Y}_i)^2 \\] and note that \\(SSTO = SSR + SSE\\). Important to note is that \\(SSTO\\) uses the difference between the observations \\(Y_i\\) and the null (reduced) model. The \\(SSR\\) uses the diffences between the alternative (full) and null (reduced) model. The \\(SSE\\) uses the differences between the observations \\(Y_i\\) and the alternative (full) model. From these we could set up a General \\(F\\) table of the form Â  Sum Sq Df Mean Sq F Value Model Error \\(SSR\\) \\(df_R-df_F\\) \\(\\frac{SSR}{df_R-df_F}\\) \\(\\frac{SSR}{df_R-df_F}\\cdot\\frac{df_F}{SSE}\\) Residual Error \\(SSE\\) \\(df_F\\) \\(\\frac{SSE}{df_F}\\) Total Error \\(SSTO\\) \\(df_R\\) Prediction and Confidence Intervals for \\(\\hat{Y}_h\\) (Expand) predict(â€¦, interval=â€œpredictionâ€)â€¦ It is a common mistake to assume that averages (means) describe individuals. They do not. So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line. Interval R Code Math Equation When to Use Prediction predict(..., interval=\"prediction\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\text{Pred}\\ Y}\\) Predict an individualâ€™s value. Confidence predict(..., interval=\"confidence\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\hat{Y}}\\) Estimate location of the mean y-value. predict(mylm, data.frame(XvarName = number), interval=...) For example, consider this graph. Then click here to read about the graph. Notice the three dots above 15 mph in the graph. Each of these dots show a car that was going 15 mph when it applied the brakes. However, stopping distances of the three individual cars differ with one at 20 feet, one at 26 feet and one at 54 feet. The regression line represents the average stopping distance of cars. In this case, cars going 15 mph are estimated to have an average stopping distance of about 40 feet, as shown by the line. But individual vehicles, all going the same speed of 15 mph, varied from stopping distances of 20 feet up to 54 feet! So, to predict that a car going 15 mph will take 41.4 feet to stop, doesnâ€™t tell the whole story. Far more revealing is the complete statement, â€œCars going 15 mph are predicted to take anywhere from 10.2 to 72.6 feet to stop, with an average stopping distance of 41.4 feet.â€ This is called the â€œprediction intervalâ€ and is shown in the graph in blue. It is obtained in R with the codes: cars.lm <- lm(dist ~ speed, data=cars) predict(cars.lm, data.frame(speed=15), interval=\"prediction\") cars.lm <- lm(dist ~ speed, data=cars) pander(predict(cars.lm, data.frame(speed=15), interval=\"prediction\")) fit lwr upr 41.41 10.17 72.64 plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=\"firebrick2\", cex=1.5) cars.lm <- lm(dist ~ speed, data=cars) abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3)) abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2)) abline(v=15, lty=2, col=\"firebrick\") preds <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12) lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8)) lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8)) Now, for the details behind prediction intervals and confidence intervals. Letâ€™s begin by recalling some details (from the section â€œInference for the Model Parametersâ€) about the standard error of the y-intercept, \\(b_0\\). Recall that the y-intercept is the average y-value for the given x-value of \\(x=0\\). Recall further that the formula for the standard error of \\(b_0\\) is given by the formula \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] If we wanted to be more exact with this formula, we would write it as \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{(0-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice the addition of \\((0 - \\bar{X})^2\\) instead of just \\(\\bar{X}^2\\) in the numerator of the right-most part of the equation? This more complete statement obviously would reduce to just \\(\\bar{X}^2\\), but that is only because \\(X=0\\) when we are working with the y-intercept, \\(b_0\\). We could be working with other values of \\(X\\) than just zero. Letâ€™s take a quick detour and talk notation for a second. Typically, \\(X_i\\) and \\(Y_i\\) are used to denote the x-value and y-value of points that are contained in our data set. When we want to reference a point that wasnâ€™t within our original data set, we use the notation \\(X_h\\) and \\(Y_h\\). (The letter h is close to i, but different from i, so why not. There is really no other reason to use h.) Thus, \\(Y_h\\) is the y-value for the \\(X_h\\) x-value, neither of which were included in our original regression of \\(X_i\\)â€™s and \\(Y_i\\)â€™s. Now, back to the previous discussion. If \\(X_h = 0\\), then \\(\\hat{Y}_h\\) is the y-intercept, so \\(\\hat{Y}_h = b_0\\) when \\(X_h=0\\). So, we could write, \\[ s^2_{\\hat{Y}_h} = MSE\\left[\\frac{1}{n} + \\frac{(X_h-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice how the \\(b_0\\) in \\(s_{b_0}\\) was replaced with \\(\\hat{Y}_h\\) to get \\(s_{\\hat{Y}_h}\\) and the 0 in \\((0 - \\bar{X})^2\\) was replaced with \\(X_h\\) to get \\((X_h - \\bar{X})^2\\)? Interesting. We now have a formula that would give us the standard error of \\(\\hat{Y}_h\\) for any \\(X_h\\) value, not just \\(X_h = 0\\), or the y-intercept, \\(b_0\\). That is fantastic. It would look like this if plotted. Notice how the gray region is showing the standard error for each \\(\\hat{Y}_h\\) value? (It is technically showing the confidence interval for \\(E\\{Y_h\\}\\) at every possible \\(X_h\\) value, but that is just \\(\\hat{Y}_h \\pm t^* \\cdot s_{\\hat{Y}_h}\\).) ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + theme_bw() ## `geom_smooth()` using formula = 'y ~ x' Confidence Interval for \\(\\hat{Y}_h\\) \\[ \\hat{Y}_h \\pm t^* s_{\\hat{Y}_h} \\quad \\text{where} \\ s_{\\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] The confidence interval is a wonderful tool for estimating \\(E\\{Y_h\\}\\), the â€œtrueâ€ average y-value for a given x-value of \\(X_h\\). However, it is not valuable for predicting an individual dot, or \\(Y_h\\) value. Notice how few of the dots of the regression are actually contained within the confidence interval band in the plot? The confidence interval does not really predict where the dots will land, just where the average y-value is located for each x-value. Remember the 68-95-99.7 Rule of the normal distribution? If not, here is a link back to that concept in the Math 221 textbook. This rule states that roughly 95% of data, when normally distributed, will be between \\(z=-2\\) and \\(z=2\\) standard deviations from the mean. So, is going two â€œresidual standard errorsâ€ to both sides of the regression line enough to capture 95% of the data? The answer is, not quite. The reason for this is because our knowledge of where the true mean lies is uncertain. (Notice the confidence interval band shown in the plot.) However, adding two standard errors to the edges of the confidence band would get us in the right place. In other words, there are two sources of variability at play here, (1) our uncertaintity in where the regression line is sitting, and (2) the natural variability of the data points around the line. Thus, the â€œprediction intervalâ€ requires accounting for both of these sources of variability to produce the following equation. Prediction Interval for \\(Y_h\\) \\[ \\hat{Y}_h \\pm t^* s_{Pred \\hat{Y}_h} \\quad \\text{where} \\ s_{Pred \\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + 1 + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] This formula provides a useful band for identifying a region where we are 95% confident that a new observation for \\(Y_h\\) will land, given the value of \\(X_h\\). It looks as follows. Notice the prediction interval is much wider than the confidence interval. This is because data varies far more than do means. Prediction is for where the individual data points will land, confidence is for where the mean will land. cars.lm <- lm(dist ~ speed, data=cars) predy <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + geom_segment(aes(x=15, xend=15, y=predy[2], yend=predy[3]), lwd=4, color=rgb(.5,.7,.5,.01)) + geom_point(aes(x=15, y=predy[1]), cex=2, color=\"skyblue\", pch=15) + theme_bw() ## Warning in geom_segment(aes(x = 15, xend = 15, y = predy[2], yend = predy[3]), : All aesthetics have length 1, but the data has 50 rows. ## â„¹ Please consider using `annotate()` or provide this layer with data containing a single row. ## Warning in geom_point(aes(x = 15, y = predy[1]), cex = 2, color = \"skyblue\", : All aesthetics have length 1, but the data has 50 rows. ## â„¹ Please consider using `annotate()` or provide this layer with data containing a single row. ## `geom_smooth()` using formula = 'y ~ x' Lowess (and Loess) Curves (Expand) A non-parametric approach to estimating \\(E\\{Y_i\\}\\)â€¦ Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value. Using Base R air2 <- airquality %>% na.omit(select(Temp, Ozone)) # Just quickly draw the lowess curve: plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") lines(lowess(air2$Ozone, air2$Temp), col=\"firebrick\") ## OR optionally, ## allow for predictions as well as the graph: # plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # lines(mylo$fit ~ Ozone, data=air2) Using ggplot2 air2 <- airquality %>% na.omit(select(Temp, Ozone)) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=\"darkgray\") + geom_smooth(se=F, method=\"loess\", method.args = list(degree=1)) + #Note, degree=2 by default. theme_bw() ## `geom_smooth()` using formula = 'y ~ x' ## OR optionally, ## allow for predictions as well as the graph: # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fit, x=Ozone)) Advantages Disadvantages Quick. Good at ignoring outliers. Good at capturing the general pattern in the data. Good for making predictions within the scope of the data. No mathematical model. Not interpretable. No p-values. No adjusted R-squared. How it Works The Lowess curve localizes the regression model to a â€œneighborhoodâ€ of points, and then joins these localized regressions together into a smooth line. It minimizes the effect of outliers, and letâ€™s the data â€œspeak for itselfâ€. As a downside, it is not interpretable, and has no final way to write the model mathematically. All the same, it is a very powerful tool for identifying an appropriate model, or verifying the fit of a model, or making predictions when no reasonable model does an adequate job. Study this graphic and the explanations below to learn how it works. Recommendation: run the code in this â€œCodeâ€ chunk to the right in your Console, and flip through the resulting graphics. X <- cars$speed Y <- cars$dist X <- X[!is.na(X) & !is.na(Y)] Y <- Y[!is.na(X) & !is.na(Y)] f <- 1/2 n <- length(X) lfit <- rep(NA,n) for (xh in 1:n){ xdists <- X - X[xh] nn <- floor(n*f) r <- sort(abs(xdists))[nn] xdists.nbrhd <- which(abs(xdists) < r) w <- rep(0, length(xdists)) w[xdists.nbrhd] <- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w), col=rgb(.2,.2,.2,.3), cex=1.5, yaxt='n', xaxt='n', xlab=\"\", ylab=\"\") points(Y[xh] ~ X[xh], pch=16, col=\"orange\") lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2)) cat(\"\\n\\n\") readline(prompt=paste0(\"Center point is point #\", xh, \"... Press [enter] to continue...\")) MADnotThereYet <- TRUE count <- 0 while(MADnotThereYet){ readline(prompt=paste0(\"\\n Adjusting line to account for outliers in the y-direction... Press [enter] to continue...\")) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"wheat\", add=TRUE) MAD <- median(abs(lmc$res)) resm <- lmc$res/(6*MAD) resm[resm>1] <- 1 bisq <- (1-resm^2)^2 w <- w*bisq obs <- coef(lmc) lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) count <- count + 1 if ( (sum(abs(obs-lmc$coef))<.1) | (count > 3)) MADnotThereYet <- FALSE } curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"green\", add=TRUE) points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=\"green\") readline(prompt=paste0(\"\\n Use final line to get fitted value for this point... Press [enter] to continue to next point...\")) lfit[xh] <- predict(lmc, data.frame(X=X[xh])) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") if (xh == n){ readline(prompt=paste0(\"\\n Press [enter] to see actual Lowess curve...\")) lines(lowess(X,Y, f=f), col=\"firebrick\") legend(\"topleft\", bty=\"n\", legend=\"Actual lowess Curve using lowess(...)\", col=\"firebrick\", lty=1) } } Select a fraction of the data to use for the â€œneighborhoodâ€ of points (shown in blue in the graph above). The lowess function in R uses â€œf=2/3â€ and the loess function uses â€œspan=0.75â€ for this value, which selects the nearest two-thirds or 75% of the data, respectively, depending on which function you use. For this example, we set the fraction of points at 50%. Both functions can be set to whatever you want. Pick any point in the regression, eventually selecting all points one at a time. The selected point becomes the â€œcenterâ€ of a â€œneighborhoodâ€ of points surrounding it. In this example, the center point is in orange, and the neighboring points are in blue. Use the points within the neighborhood to fit a regression line. However, make the regression depend most on points closest to â€œcenterâ€ and least on points furthest from â€œcenter.â€ This is called a weighted regression. Weights are decided according to what is called the tricubic weight function, so that the weight \\(w\\) given to point \\(j\\) of the neighborhood of points is defined by \\[ w_j = \\left(1- \\left( \\frac{|X_c - X_j|}{\\max_k |X_c - X_k|}\\right)^3\\right)^3 \\] where \\(X_c\\) is the x-value of the â€œcenterâ€ dot and \\(X_j\\) is the x-value of any other dot in the neighborhood. The fitted-value of \\(\\hat{Y}_c\\) is obtained for the center point \\(X_c\\) of the current regression. This point is used as the Lowess (or Loess) curveâ€™s value at that particular x-value. Well, almost. Itâ€™s a first guess at where this value will end up, but thereâ€™s a little more to the algorithm before we are done. Initial guesses for each of these fitted values are obtained for each point in the regression. Now each local regression for each neighborhood is re-run a few times in such a way the the effect of outliers is minimized. The final line for each neighborhood is obtained by the following steps. Compute all residuals for points in the neighborhood of the current regression, denoted by \\(r_i\\). Then compute the MAD, median absolute deviation, of the residuals \\(MAD = \\text{median} (|r_1|, |r_2|, \\ldots)\\). Divide all residuals by 6 times the MAD: \\(u_i = r_i/(6\\cdot MAD)\\) (If \\(r_i > 6\\cdot MAD\\) then set \\(u_i = 0\\).) Compute what are called bisquare weights using the formula: \\(b_i = (1 - u_i^2)^2\\) Perform a regression using the weights \\(w_i = w_i b_i\\) Repeat the above process with the new weights \\(w_i\\) until the weights stop changing very much. The final fitted values for each \\(X\\)-value in the regression are obtained from the final regression line for each neighborhood. These fitted values make up the Lowess (or loess) curve. Note that the default of the loess function in R is to use quadratic regressions in each neighborhood instead of linear regressions. This can be controlled with the loess option of â€œdegree=2â€ (quadratic fits) or â€œdegree = 1â€. In the lowess function only a linear regression in each neighborhood is allowed. Examples: bodyweight, cars Multiple Linear Regression Multiple regression allows for more than one explanatory variable to be included in the modeling of the expected value of the quantitative response variable \\(Y_i\\). There are infinitely many possible multiple regression models to choose from. Here are a few â€œbasicâ€ models that work as building blocks to more complicated models. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model uses the same \\(X\\)-variable twice, once with a \\(\\beta_1 X_i\\) term and once with a \\(\\beta_2 X_i^2\\) term. The \\(X_i^2\\) term is called the â€œquadraticâ€ term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and â€œsteepnessâ€ of the Model: negative values face down, positive values face up; large values imply â€œsteeperâ€ parabolas and low values imply â€œflatterâ€ parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)â€™s explanation. An Example Using the airquality data set, we run the following â€œquadraticâ€ regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our â€œquadraticâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. Temp Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These â€œEstimatesâ€ can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(Month^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of Month from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, ) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will â€œopen downâ€ (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be â€œmonth zero.â€ Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the â€œcubicâ€ term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the â€œbase slope coefficientâ€ and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following â€œcubicâ€ regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our â€œcubicâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. uptake Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^3) \\(X_{i}^3\\), where the function I(â€¦) protects the cubing of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will â€œopen upâ€. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The â€œtwo-linesâ€ model is a simple way to compare two groups in statistics. It uses two main parts: A regular number (\\(X_{1i}\\)) that can be any value. A special number (\\(X_{2i}\\)) thatâ€™s either 0 or 1. This special number (also called a â€œdummy variableâ€ or â€œindicator variableâ€) helps turn information about groups (like â€œGroup Aâ€ or â€œGroup Bâ€) into numbers we can use in math. By using these two parts, we can create two separate lines on a graph: One line for Group A One line for Group B This helps us see how the two groups are different from each other. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the â€œbase-lineâ€ of the model, the â€œGroup 0â€ line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the â€œbase-lineâ€ line. \\(\\beta_3\\) Called the â€œinteractionâ€ term. Controls the change in the slope for the second line in the model as compared to the slope of the â€œbase-lineâ€ line. An Example Using the mtcars data set, we run the following â€œtwo-linesâ€ regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our â€œtwo-linesâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. mpg Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept), \\(\\beta_1\\) is estimated by the qsec value of 1.439, \\(\\beta_2\\) is estimated by the am value of -14.51, and \\(\\beta_3\\) is estimated by the qsec:am value of 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called â€œ3Dâ€ regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to â€œbendâ€. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the â€œTempâ€ direction is estimated to be 2.659 and the slope in the â€œMonthâ€ direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction termâ€™s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the â€œslopesâ€ in each direction to change, creating a â€œcurvedâ€ surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called â€œHDâ€, or â€œHigh Dimensionalâ€, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isnâ€™t really even possible to â€œmentally connectâ€ with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)â€™s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the â€œWindâ€ direction is estimated to be -3.334. The slope in the â€œTempâ€ direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the â€œSolar.Râ€ direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here. R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the â€œOverviewâ€ section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the â€œcolumn (c) bindâ€ function and it joins together things as columns. Res =Â  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,Â  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals. Â panel=panel.smooth,Â  This places a lowess smoothing line on each scatterplot. col =Â  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(â€œcolor1â€,â€œcolor2â€, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for â€œlinear model.â€ Y Y must be a â€œnumericâ€ vector of the quantitative response variable. Â ~Â  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats â€œnumericâ€ variables as quantitative and â€œcharacterâ€ or â€œfactorâ€ variables as qualitative. R will automatcially recode qualitative variables to become â€œnumericâ€ variables using a 0,1 encoding. See the Explanation tab for details. Â + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details. Â + â€¦, * ... emphasizes that as many explanatory variables as are desired can be included in the model. Â data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(â€¦) function displays the results of an lm(â€¦) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(â€¦) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(â€¦). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min Â  -4.3818 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -2.2696 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.1344 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  1.7058 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  5.8752 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  t value To learn more about the â€œt valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  Pr(>|t|) The â€œPrâ€ stands for â€œProbabilityâ€ and the â€œ(> |t|)â€ stands for â€œmore extreme than the observed t-valueâ€. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the â€œp-valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. (Intercept) This always says â€œInterceptâ€ for any lm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero. Â  2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the intercept (26.6248479) by its standard error (2.1829432). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a â€œstarâ€. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + â€¦). Â  -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model. Â  0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the â€œEstimateâ€ by its standard error. It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + â€¦). Â  5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\). Â  2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot â€œ.â€ implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like. Â  0.0004029 This is the estimate of the coefficient of the interaction term. Â  0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE. Â 2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\). Â on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO. Â 0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car. Â Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data. Â 0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that â€œat least one is not.â€ Â 34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom. Â on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero. Â p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that â€œat least oneâ€ of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the â€œOverviewâ€ sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the â€œfitted-valueâ€ or â€œpredicted-valueâ€ for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œpredictionâ€) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance canâ€™t go negative) feet to 60.96915 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œconfidenceâ€) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BICâ€¦ There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (â€œR-squaredâ€). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The â€œsimplestâ€ but â€œbestâ€ model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it â€œan information criterion (AIC)â€ when he published the method and other people later began calling it the â€œAkaike Information Criterion.â€) Model Selection (Expand) pairs plots, added variable plots, and pattern recognitionâ€¦ Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots A useful visualization tool for model selection is the â€œpairs plot.â€ This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice thatâ€¦ the y-axis of each plot is found by locating the variable name (like â€œmpgâ€) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like â€œdispâ€) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldnâ€™t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Letâ€™s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander() Â  Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a modelâ€™s ability to generalize to new dataâ€¦ The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesnâ€™t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, letâ€™s remind ourselves why we use regression models in the first place. The main goal is to capture the â€œessenceâ€ of the data. In other words, the general pattern is what we are after. We want a model that tells us how â€œall suchâ€ data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the â€œcomplicated modelâ€ is overfitting the original data. It does not capture the â€œessenceâ€ of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-valueâ€¦ The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the modelâ€¦ The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read moreâ€¦) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) Outlier Analysis (Expand) Cookâ€™s Distances and Leverage Valuesâ€¦ The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs.Â fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cookâ€™s Distances The idea behind Cookâ€™s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article â€œDetection of Influential Observation in Linear Regressionâ€ (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, letâ€™s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2)) Â  1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the â€œdifferencesâ€ in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cookâ€™s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cookâ€™s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cookâ€™s Distances using the code cooks.distance(lmObject). Also, a graph of Cookâ€™s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of â€œleverageâ€ and is â€œpullingâ€ the regression toward itself. A value near 0 implies the point is just â€œone of manyâ€ and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that â€œminimizeâ€ the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)â€™s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the â€œhat matrixâ€ \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the â€œleverage valuesâ€ and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a â€œlot of pull,â€ and values close to 0 represent â€œlittle pull.â€ In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with â€œlots of leverageâ€ and a large â€œCookâ€™s Distanceâ€ are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regressionâ€¦ Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class. Examples: Civic Vs Corolla cadillacs",
    "sentences": ["Linear regression has a rich mathematical theory behind it.", "This is because it uses a mathematical function and a random error term to describe the regression relation between a response variable \\(Y\\) and an explanatory variable called \\(X\\).", "Expand each element below to learn more.", "Regression Cheat Sheet (Expand) Term Pronunciation Meaning Math R Code \\(Y_i\\) $Y_i$ â€œwhy-eyeâ€ The data \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)\\) $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$ YourDataSet$YourYvariable \\(\\hat{Y}_i\\) $\\hat{Y}_i$ â€œwhy-hat-eyeâ€ The fitted line \\(\\hat{Y}_i = b_0 + b_1 X_i\\) $\\hat{Y}_i = b_0 + b_1 X_i$ lmObject$fitted.values \\(E\\{Y_i\\}\\) $E\\{Y_i\\}$ â€œexpected value of why-eyeâ€ True mean y-value \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) $E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i$ <none> \\(\\beta_0\\) $\\beta_0$ â€œbeta-zeroâ€ True y-intercept <none> <none> \\(\\beta_1\\) $\\beta_1$ â€œbeta-oneâ€ True slope <none> <none> \\(b_0\\) $b_0$ â€œb-zeroâ€ Estimated y-intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) $b_0 = \\bar{Y} - b_1\\bar{X} b_0 <- mean(Y) - b_1*mean(X)$ \\(b_1\\) $b_1$ â€œb-oneâ€ Estimated slope \\(b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\) $b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})} {\\sum(X_i - \\bar{X})^2}$ b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) \\(\\epsilon_i\\) $\\epsilon_i$ â€œepsilon-eyeâ€ Distance of dot to true line \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) $\\epsilon_i = Y_i - E\\{Y_i\\}$ <none> \\(r_i\\) $r_i$ â€œr-eyeâ€ or â€œresidual-eyeâ€ Distance of dot to estimated line \\(r_i = Y_i - \\hat{Y}_i\\) $r_i = Y_i - \\hat{Y}_i$ lmObject$residuals \\(\\sigma^2\\) $\\sigma^2$ â€œsigma-squaredâ€ Variance of the \\(\\epsilon_i\\) \\(Var\\{\\epsilon_i\\} = \\sigma^2\\)$Var\\{\\epsilon_i\\} = \\sigma^2$ <none> \\(MSE\\) $MSE$ â€œmean squared errorâ€ Estimate of \\(\\sigma^2\\) \\(MSE = \\frac{SSE}{n-p}\\)$MSE = \\frac{SSE}{n-p}$ sum( lmObject$res^2 ) / (n - p) \\(SSE\\) $SSE$ â€œsum of squared errorâ€ (residuals) Measure of dotâ€™s total deviation from the line \\(SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\)$SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$ sum( lmObject$res^2 ) \\(SSR\\) $SSR$ â€œsum of squared regression errorâ€ Measure of lineâ€™s deviation from y-bar \\(SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\)$SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2$ sum( (lmObject$fit - mean(YourData$Y))^2 ) \\(SSTO\\) $SSTO$ â€œtotal sum of squaresâ€ Measure of total variation in Y \\(SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\)$SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2$ sum( (YourData$Y - mean(YourData$Y))^2 ) \\(R^2\\) $R^2$ â€œR-squaredâ€ Proportion of variation in Y explained by the regression \\(R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\)$R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}$ SSR/SSTO \\(r\\) $r$ â€œrâ€ Correlation between X and Y.", "\\(r = \\sqrt{R^2}\\)$r = \\sqrt{R^2}$ sqrt(R^2) \\(\\hat{Y}_h\\) $\\hat{Y}_h$ â€œwhy-hat-aitchâ€ Estimated mean y-value for some x-value called \\(X_h\\) \\(\\hat{Y}_h = b_0 + b_1 X_h\\)$\\hat{Y}_h = b_0 + b_1 X_h$ predict(lmObject, data.frame(XvarName=#)) \\(X_h\\) $X_h$ â€œex-aitchâ€ Some x-value, not necessarily one of the \\(X_i\\) values used in the regression \\(X_h =\\) some number$X_h = $ Xh = # Confidence Interval â€œconfidence intervalâ€ Estimated bounds at a certain level of confidence for a parameter \\(b_0 \\pm t^* \\cdot s_{b_0}\\)b_0 \\pm t^* \\cdot s_{b_0} or \\(b_1 \\pm t^* \\cdot s_{b_1}\\)b_1 \\pm t^* \\cdot s_{b_1} confint(mylm, level = someConfidenceLevel) Parameter Estimate \\(\\beta_0\\) \\(b_0\\) \\(\\beta_1\\) \\(b_1\\) \\(\\epsilon_i\\) \\(r_i\\) \\(\\sigma^2\\) \\(MSE\\) \\(\\sigma\\) \\(\\sqrt{MSE}\\), the Residual standard error The Mathematical Model (Expand) \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)â€¦ There are three main elements to the mathematical model of regression.", "Each of these three elements is pictured below in the â€œRegression Relation Diagram.â€ Study both the three bullet points and their visual representations in the plot below for a clearer understanding.", "The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read moreâ€¦) The true line is shown by the dotted line in the graph pictured below.", "This is typically unobservable.", "Think of it as â€œnatural lawâ€ or â€œGodâ€™s lawâ€.", "It is some true line that is unknown to us."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Explanation"
  },
  {
    "id": 62,
    "title": "ğŸ“ The Mathematical Model\r\n(Expand)",
    "url": "LinearRegression.html#themathematicalmodelexpand",
    "content": "The Mathematical Model\r\n(Expand) \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)â€¦ There are three main elements to the mathematical model of regression. Each of these three elements is pictured below in the â€œRegression Relation Diagram.â€ Study both the three bullet points and their visual representations in the plot below for a clearer understanding. The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read moreâ€¦) The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as â€œnatural lawâ€ or â€œGodâ€™s lawâ€. It is some true line that is unknown to us. The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line. The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\). Note: \\(E\\{Y\\}\\) is pronounced â€œthe expected value of yâ€ because, wellâ€¦ the mean is the typical, average, or â€œexpectedâ€ value. The dots, i.e., the regression relation plus an error term: \\(Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}} + \\underbrace{\\epsilon_i}_\\text{error term} \\quad \\text{where} \\ \\epsilon_i\\sim N(0,\\sigma^2)\\) (Read moreâ€¦) This is shown by the dots in the graph below. This is the data. In regression, the assumption is that the y-value for individual \\(i\\), denoted by \\(Y_i\\), was â€œcreatedâ€ by adding an error term \\(\\epsilon_i\\) to each individualâ€™s â€œexpectedâ€ value \\(\\beta_0 + \\beta_1 X_i\\). Note the â€œorder of creationâ€ would require first knowing an indivualâ€™s x-value, \\(X_i\\), then their expected value from the regression relation \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) and then adding their \\(\\epsilon_i\\) value to the result. The \\(\\epsilon_i\\) allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance \\(\\epsilon_i\\) from the line. Note: \\(Y_i\\) is pronounced â€œwhy-eyeâ€ because it is the y-value for individual \\(i\\). Sometimes also called â€œwhy-sub-eyeâ€ because \\(i\\) is in the subscript of \\(Y\\). The estimated line, i.e., the line we get from a sample of data. \\(\\underbrace{\\hat{Y}_i}_{\\substack{\\text{estimated mean} \\\\ \\text{y-value}}} = \\underbrace{b_0 + b_1 X_i}_\\text{estimated regression equation}\\) (Read moreâ€¦) The estimated line is shown by the solid line in the graph below. \\(\\hat{Y}\\) is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation \\(E\\{Y\\}\\). So \\(\\hat{Y}\\) is interpreted as the estimated average (or mean) \\(Y\\)-value for any given \\(X\\)-value. Thus, \\(b_0\\) is the estimated y-intercept and \\(b_1\\) is the estimated slope. The bâ€™s are sample statistics, like \\(\\bar{x}\\) and the \\(\\beta\\)â€™s are population parameters like \\(\\mu\\). The \\(b\\)â€™s estimate the \\(\\beta\\)â€™s. Note: \\(\\hat{Y}_i\\) is pronounced â€œwhy-hat-eyeâ€ and is known as the â€œestimated y-valueâ€ or â€œfitted y-valueâ€ because it is the y-value you get from \\(b_0 + b_1 X_i\\). It is always different from \\(Y_i\\) because dots are rarely if ever exactly on the estimated regression line. This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line). Something to ponder: The true line, when coupled with the error terms, â€œcreatesâ€ the data. The estimated (or fitted) line uses the sampled data to try to â€œre-createâ€ the true line. We could loosely call this the â€œorder of creationâ€ as shown by the following diagram. par(mfrow=c(1,3), mai=c(.2,.2,.4,.1)) plot(y ~ x, col=\"white\", main=\"A Law is Given\", yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, main=\"Data is Created\", xaxt='n', yaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, xaxt='n', yaxt='n', main=\"The Law is Estimated\") curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) A Law is Given Data is Created The Law is Estimated \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) \\(Y_i = E\\{Y_i\\} + \\epsilon_i\\) \\(\\hat{Y}_i = b_0 + b_1 X_i\\) The true line is the â€œlawâ€. The \\(Y_i\\) are created by adding \\(\\epsilon_i\\) to \\(E\\{Y_i\\}\\) where \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\). The law is estimated with \\(\\hat{Y}_i\\) which is given with lm(...). Click open the â€œCodeâ€ buttom below to the right to find code that runs a simulation demonstrating this â€œorder of creationâ€. ## Simulating Data from a Regression Model ## This R-chunk is meant to be played in your R Console. ## It allows you to explore how the various elements ## of the regression model combine together to \"create\" ## data and then use the data to \"re-create\" the line. set.seed(101) #Allows us to always get the same \"random\" sample #Change to a new number to get a new sample n <- 153 #set the sample size X_i <- runif(n, 0, 22) #Gives n random values from a uniform distribution between 15 to 45. beta0 <- 89 #Our choice for the y-intercept. beta1 <- -1.25 #Our choice for the slope. sigma <- 8 #Our choice for the std. deviation of the error terms. epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data View(fabData) #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it. fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData. summary(fab.lm) #Summarize your model. plot(y ~ x, data=fabData) #Plot the data. abline(fab.lm) #Add the estimated regression line to your plot. # Now for something you can't do in real life... but since we created the data... abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). legend(\"topleft\", legend=c(\"True Line\", \"Estimated Line\"), lty=c(2,1), bty=\"n\") #Add a legend to your plot specifying which line is which.",
    "sentences": ["\\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)â€¦", "There are three main elements to the mathematical model of regression.", "Each of these three elements is pictured below in the â€œRegression Relation Diagram.â€ Study both the three bullet points and their visual representations in the plot below for a clearer understanding.", "The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read moreâ€¦) The true line is shown by the dotted line in the graph pictured below.", "This is typically unobservable.", "Think of it as â€œnatural lawâ€ or â€œGodâ€™s lawâ€.", "It is some true line that is unknown to us.", "The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line.", "The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\).", "Note: \\(E\\{Y\\}\\) is pronounced â€œthe expected value of yâ€ because, wellâ€¦ the mean is the typical, average, or â€œexpectedâ€ value."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "The Mathematical Model\r\n(Expand)"
  },
  {
    "id": 63,
    "title": "ğŸ“ Interpreting the Model Parameters\r\n(Expand)",
    "url": "LinearRegression.html#interpretingthemodelparametersexpand",
    "content": "Interpreting the Model Parameters\r\n(Expand) \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted asâ€¦ The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model. If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\). The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\). It is often misunderstood to be the â€œaverage change in yâ€ or just â€œthe change in yâ€ but it is more correctly referred to as the â€œchange in the average yâ€. To better see this, consider the three graphics shown below. par(mfrow=c(1,3)) hist(mtcars$mpg, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Number of Vehicles\", xlab=\"Gas Mileage (mpg)\", col=\"skyblue\") boxplot(mpg ~ cyl, data=mtcars, border=\"skyblue\", boxwex=0.5, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Number of Cylinders of Engine (cyl)\") plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=\"skyblue\", main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Quarter Mile Time (qsec)\") abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=\"darkgray\") mtext(side=3, text=\"Automatic Transmissions Only (am==0)\", cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=\"gray\") The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09. The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg. The scatterplot on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value.",
    "sentences": ["\\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted asâ€¦", "The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model.", "If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\).", "The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\).", "It is often misunderstood to be the â€œaverage change in yâ€ or just â€œthe change in yâ€ but it is more correctly referred to as the â€œchange in the average yâ€.", "To better see this, consider the three graphics shown below.", "par(mfrow=c(1,3)) hist(mtcars$mpg, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Number of Vehicles\", xlab=\"Gas Mileage (mpg)\", col=\"skyblue\") boxplot(mpg ~ cyl, data=mtcars, border=\"skyblue\", boxwex=0.5, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Number of Cylinders of Engine (cyl)\") plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=\"skyblue\", main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Quarter Mile Time (qsec)\") abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=\"darkgray\") mtext(side=3, text=\"Automatic Transmissions Only (am==0)\", cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=\"gray\") The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set.", "The average gas mileage is 20.09.", "The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively.", "If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Interpreting the Model Parameters\r\n(Expand)"
  },
  {
    "id": 64,
    "title": "ğŸ“ Residuals and Errors\r\n(Expand)",
    "url": "LinearRegression.html#residualsanderrorsexpand",
    "content": "Residuals and Errors\r\n(Expand) \\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true errorâ€¦ Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\). The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\). We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summaryâ€¦ Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\). \\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms. Keep in mind the idea that the errors \\(\\epsilon_i\\) â€œcreatedâ€ the data and that the residuals \\(r_i\\) are computed after using the data to â€œre-createâ€ the line. Residuals have many uses in regression analysis. They allow us to diagnose the regression assumptions, See the â€œAssumptionsâ€ section below for more details. estimate the regression relation, See the â€œEstimating the Model Parametersâ€ section below for more details. estimate the variance of the error terms, See the â€œEstimating the Model Varianceâ€ section below for more details. and assess the fit of the regression relation. See the â€œAssessing the Fit of a Regressionâ€ section below for more details.",
    "sentences": ["\\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true errorâ€¦", "Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\).", "The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\).", "We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summaryâ€¦ Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\).", "\\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms.", "Keep in mind the idea that the errors \\(\\epsilon_i\\) â€œcreatedâ€ the data and that the residuals \\(r_i\\) are computed after using the data to â€œre-createâ€ the line.", "Residuals have many uses in regression analysis.", "They allow us to diagnose the regression assumptions, See the â€œAssumptionsâ€ section below for more details.", "estimate the regression relation, See the â€œEstimating the Model Parametersâ€ section below for more details.", "estimate the variance of the error terms, See the â€œEstimating the Model Varianceâ€ section below for more details."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Residuals and Errors\r\n(Expand)"
  },
  {
    "id": 65,
    "title": "ğŸ“ Assessing the Fit of a Regression\r\n(Expand)",
    "url": "LinearRegression.html#assessingthefitofaregressionexpand",
    "content": "Assessing the Fit of a Regression\r\n(Expand) \\(R^2\\), SSTO, SSR, and SSEâ€¦ Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line. par(mfrow=c(1,3), mai=c(.1,.1,.5,.1)) set.seed(2) x <- runif(30,0,20) y1 <- 2 + 3.5*x + rnorm(30,0,2) y2 <- 2 + 3.5*x + rnorm(30,0,8) y3 <- 2 + 3.5*x + rnorm(30,0,27) plot(y1 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Excellent Fit\") abline(lm(y1 ~ x), col=\"gray\") plot(y2 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Good Fit\") abline(lm(y2 ~ x), col=\"gray\") plot(y3 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Poor Fit\") abline(lm(y3 ~ x), col=\"gray\") A common way to measure the fit of a regression is with correlation. While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\). (If you are a Math 325 student, just stick with correlation for now and skip on to the next section of this Explanation tab. If you are a Math 425 student, it is critical that you come to understand \\(R^2\\) deeply, so read on.) Before you can understand \\(R^2\\), you must understand three important â€œsums of squaresâ€. (Read more about sumsâ€¦) A sum is just a fancy word for adding things together. \\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand. So we use the symbol \\(\\Sigma\\) to denote the word â€œsumâ€. Further, we use a subscript \\(\\Sigma_{i=1}\\) to state what value the sum is beginning with, and a superscript \\(\\Sigma_{i=1}^6\\) to state the value we are ending at. This gives \\[ \\sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Test your knowledge, do you see why the answer is 6 to the sum below? \\[ \\sum_{i=1}^3 i = 6 \\] Computing sums in R is fairly easy. Type the following codes in your R Console. sum(1:6) #gives the answer of 21 sum(1:3) #gives the answer of 6 However, sums really become useful when used with a data set. Each row of a data set represents an â€œindividualâ€™sâ€ data. We can reference each individual with a row number. In the data below, individual 3, denoted by \\(i=3\\), has a speed of 7 and a dist of 4. pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3) Individual speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 To compute the sum of the speed column, use sum(speed). If we divided this sum by 6, we would get the mean of speed mean(speed). In fact, the two most used statistics mean(...) and sd(...) both use sums. Take a moment to review the formulas for mean and standard deviation. It is strongly recommended that you study the Explanation tab for both as well. Weâ€™ll wait. See you back here shortly. â€¦ Welcome back. Suppose we let X = speed and Y = dist. Then \\(X_3 = 7\\) and \\(Y_3 = 4\\) because we are accessing row 3 of both the \\(X\\) (or speed) column and \\(Y\\) (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, sum(speed) would be written mathematically as \\(\\sum_{i=1}^6 X_i\\) and sum(dist) would be written as \\(\\sum_{i=1}^6 Y_i\\). Sum of Squared Errors Sum of Squares Regression Total Sum of Squares \\(\\text{SSE} = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\) \\(\\text{SSR} = \\sum_{i=1}^n \\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\) \\(\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\\) Measures how much the residuals deviate from the line. Measures how much the regression line deviates from the average y-value. Measures how much the y-values deviate from the average y-value. Equals SSTO - SSR Equals SSTO - SSE Equals SSE + SSR sum( (Y - mylm$fit)^2 ) sum( (mylm$fit - mean(Y))^2 ) sum( (Y - mean(Y))^2 ) It is important to remember that SSE and SSR split up SSTO, so that \\[ \\text{SSTO} = \\text{SSE} + \\text{SSR} \\] This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works. The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called \\(R^2\\) (â€œr-squaredâ€). R-Squared (\\(R^2\\)) \\[ \\underbrace{R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}}_\\text{Interpretation: Proportion of variation in Y explained by the regression.} \\] The smallest \\(R^2\\) can be is zero, and the largest it can be is 1. This is because \\(SSR\\) must be between 0 and SSTO, inclusive.",
    "sentences": ["\\(R^2\\), SSTO, SSR, and SSEâ€¦", "Not all regressions are created equally as the three plots below show.", "Sometimes the dots are a clustered very tightly to the line.", "At other times, the dots spread out fairly dramatically from the line.", "par(mfrow=c(1,3), mai=c(.1,.1,.5,.1)) set.seed(2) x <- runif(30,0,20) y1 <- 2 + 3.5*x + rnorm(30,0,2) y2 <- 2 + 3.5*x + rnorm(30,0,8) y3 <- 2 + 3.5*x + rnorm(30,0,27) plot(y1 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Excellent Fit\") abline(lm(y1 ~ x), col=\"gray\") plot(y2 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Good Fit\") abline(lm(y2 ~ x), col=\"gray\") plot(y3 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Poor Fit\") abline(lm(y3 ~ x), col=\"gray\") A common way to measure the fit of a regression is with correlation.", "While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\).", "(If you are a Math 325 student, just stick with correlation for now and skip on to the next section of this Explanation tab.", "If you are a Math 425 student, it is critical that you come to understand \\(R^2\\) deeply, so read on.) Before you can understand \\(R^2\\), you must understand three important â€œsums of squaresâ€.", "(Read more about sumsâ€¦) A sum is just a fancy word for adding things together.", "\\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Assessing the Fit of a Regression\r\n(Expand)"
  },
  {
    "id": 66,
    "title": "ğŸ“ Residual Plots & Regression Assumptions\r\n(Expand)",
    "url": "LinearRegression.html#residualplotsregressionassumptionsexpand",
    "content": "Residual Plots & Regression Assumptions\r\n(Expand) Residuals vs.Â fitted-values, Q-Q Plot of the residuals, and residuals vs.Â order plotsâ€¦ There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate. Each assumption is labeled in the regression equation below. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions. (Read moreâ€¦) As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not. Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3 The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(\\hat{Y}_i\\). The residuals are the \\(r_i\\). This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable. | Show Examples | (Read moreâ€¦) The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption. The linear relation is assumed to be satisfied if there are no apparent trends in the plot. The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values. The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance. set.seed(2) X <- rnorm(30,15,3) notLin <- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8)) notLin.lm <- lm(Y~X, data=notLin) set.seed(15) Lin <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) Lin.lm <- lm(Y~X, data=Lin) par(mfrow=c(3,3), mai=c(.25,.25,.25,.25), mgp=c(1,.75,0)) plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Not Linear\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(notLin.lm$fitted.values,notLin.lm$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Good: Linear, Constant Variance\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) set.seed(6) notCon <- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5)) notCon.lm <- lm(Y~X, data=notCon) LinO <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) LinO[1] <- LinO[1]^2 LinO.lm <- lm(Y~X, data=LinO) plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Unconstant Variance\", cex.main=0.95, yaxt='n', xaxt='n', col=\"firebrick\") polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) # plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Outliers\", cex.main=0.95) # abline(h=0) tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(Girth ~ Volume, data=trees[-31,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(Height ~ Volume, data=trees) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) abline(h=0) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals, f=.4) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(weight ~ repwt, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(weight ~ repht, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) abline(h=0) Q-Q Plot of the Residuals: Checks Assumption #2 The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption. | Show Examples | (Read moreâ€¦) There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot. Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) set.seed(123) tmp <- rnorm(100) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"skyblue\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Normal\", col=\"skyblue\") tmp <- Davis$weight qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Right-skewed\", breaks=15, col=\"firebrick\") par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- rbeta(100, 5,1) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Left-skewed\", breaks=seq(min(tmp),max(tmp), length.out=13), col=\"firebrick\") tmp <- rbeta(100,2,2) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Heavy-tailed\", col=\"firebrick\") Residuals versus Order Plot: Checks Assumption #5 When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated. | Show Examples | (Read moreâ€¦) Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Good: No Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: General Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ qsec, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: Interesting Patterns\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),]) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Bad: Obvious Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") Problems from Failed Assumptions There are various problems that can arise when certain of the regression assupmtions are not satisfied. Lack of Linearity When the linearity assumption is violated, pretty much everything we obtain from the regression summary is no longer meaningful. The y-intercept estimate can be drastically off from its actual true value. Important model information is lost by trying to use a simple slope term \\(\\beta_1\\) to describe the model with respect to \\(X\\). The residual standard error will be much higher than it otherwise would be because of curvature patterns in the data that the line cannot capture. Thus, R-squared will be lower than it otherwise should be. P-values can become non-significant, when in fact there is a strong pattern in the data, but that pattern just cannot be captured by a simple line. *Normality of the errors is often put into question as well when a simplified line is used to try to capture a more complicated curved model. The plot below demonstrate these difficulties. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 7.5 #True slope beta_2 <- -0.25 #True bend X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors Y_i <- beta_0 + beta_1*X_i + beta_2*X_i^2 + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Non-Linear Relation\") abline(mylm, col=\"gray\") #Add fitted line to plot curve(beta_0 + beta_1*x + beta_2*x^2, col=\"gray\", lty=2, add=TRUE) #Add True line to plot #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (True value:\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (True value:\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (True value:\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Non-normal Error Terms When the normality assumption for the error terms is violated, not all is lost. In fact, the estimate of the slope and intercept are still often fairly meaningful. However, it is unwise to put too much trust in the residual standard error as an estimate of the standard deviation \\(\\sigma\\). This is because the standard deviation in skewed distributions does not carry the same meaning it has in normal distributions. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rchisq(n, 1)*3 - 1 #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Unconstant Variance When variance of the error term changes across the regression, the regression approximates the â€œaverage varianceâ€ of the errors because the regression is still assuming the variance is constant across the regression. The estimates of the slope and intercept are still typically quite good, and can be used for interpretation. The residual standard error however should not be considered to be meaningful as it will be too large on one end of the regression and too small on the other end. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma + X_i) #normally distributed errors #with increasing variance Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Variance Varies (Non-Constant)\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 13.82 Slope 3.5 3.768 Sigma 2.5 13.02 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", round(mean(sigma + X_i), 2), \", mean)\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Normality Violated As silly as it sounds, if the only problem with the regression is the lack of normality of the error terms, it isnâ€™t all that big of a problem. Depending on how non-normal the residuals appear, there could be some skewing to the residual standard error, but otherwise, the slope and intercept are still interpretable and meaningful. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- runif(n, -sqrt(12*sigma^2)/2, sqrt(12*sigma^2)/2) #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 14.7 Slope 3.5 3.449 Sigma 2.5 2.317 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Independence Assumption Violated While the slope and intercept are often still meaningful when the independence assumption is violated, the residual standard error is unnecessarily large in this case. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, 2.5) + (1:n -n/2)*.5 #normal, but correlated errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Independence Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 16.47 Slope 3.5 3.296 Sigma 2.5 4.819 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Outliers Present While outliers do not violate any of the regression assumptions, they do pose substantial difficulties for the least squares regression estimates of the slope and intercept. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors epsilon_i[3] <- ifelse(X_i[3] < 10, runif(1,25,35), -runif(1,25,35)) #create outlier Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"An Outlier Present\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) Â  True Estimated Y-Intercept 14.2 13.18 Slope 3.5 3.447 Sigma 2.5 6.389 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3)",
    "sentences": ["Residuals vs.Â fitted-values, Q-Q Plot of the residuals, and residuals vs.Â order plotsâ€¦", "There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate.", "Each assumption is labeled in the regression equation below.", "The regression relation between \\(Y\\) and \\(X\\) is linear.", "The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\).", "The variance of the error terms is constant over all \\(X\\) values.", "The \\(X\\) values can be considered fixed and measured without error.", "The error terms are independent.", "Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions.", "(Read moreâ€¦) As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Residual Plots & Regression Assumptions\r\n(Expand)"
  },
  {
    "id": 67,
    "title": "ğŸ“ Estimating the Model Parameters\r\n(Expand)",
    "url": "LinearRegression.html#estimatingthemodelparametersexpand",
    "content": "Estimating the Model Parameters\r\n(Expand) How to get \\(b_0\\) and \\(b_1\\): least squares & maximum likelihoodâ€¦ There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical. The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas. Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 <- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\). When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\). Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i. \\label{exp} \\end{equation}\\] Least Squares To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\). \\[ Q = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2 \\] Then we use the function Q as if it were a function of \\(\\beta_0\\) and \\(\\beta_1\\). Ironically, the values of \\(Y\\) and \\(X\\) are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we donâ€™t know are the values of \\(\\beta_0\\) and \\(\\beta_1\\). This least squares applet is a good way to explore how various choices of the slope and intercept yield different values of the â€œsum of squared residualsâ€. But it turns out that there is one â€œbestâ€ choice of the slope and intercept that yields a â€œsmallestâ€ value of the â€œsum of squared residuals.â€ This best choice can actually be found using calculus by taking the partial derivatives of \\(Q\\) with respect to both \\(\\beta_0\\) and \\(\\beta_1\\). \\[ \\frac{\\partial Q}{\\partial \\beta_0} = -2\\sum (Y_i - \\beta_0 - \\beta_1X_i) \\] \\[ \\frac{\\partial Q}{\\partial \\beta_1} = -2\\sum X_i(Y_i-\\beta_0-\\beta_1X_i) \\] Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize \\(Q\\) for a given set of data. After all the calculations are completed we find the values of the parameter estimators \\(b_0\\) and \\(b_1\\) (of \\(\\beta_0\\) and \\(\\beta_1\\), respectively) are as stated previously. Maximum Likelihood The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of \\(\\beta_0\\) and \\(\\beta_1\\) which minime the least squares \\(Q\\) function, we choose the values of \\(\\beta_0\\) and \\(\\beta_1\\) which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the \\(Y_i\\) for all observations \\(i=1,\\ldots,n\\). We can do this rather simply by using the assumption that the errors, \\(\\epsilon_i\\) are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if \\(f(Y_i)\\) denotes the probability density function for \\(Y_i\\), then the joint probability density for all \\(Y_i\\), \\(f(Y_1,\\ldots,Y_n)\\) is given by \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) \\] Since each \\(Y_i\\) is assumed to be normally distributed with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\) (see model (\\(\\ref{model}\\))) we have that \\[ f(Y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{Y_i-\\beta_0-\\beta_1X_i}{\\sigma}\\right)^2\\right]} \\] which provides the joint probability as \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] The likelihood function \\(L\\) is then given by consider the \\(Y_i\\) and \\(X_i\\) fixed and the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) as the variables in the function. \\[ L(\\beta_0,\\beta_1,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] Instead of taking partial derivatives of \\(L\\) directly (with respect to all parameters) we take the partial derivatives of the \\(\\log\\) of \\(L\\), which is easier to work with. In a similar, but more difficult calculation, to that of minimizing \\(Q\\), we obtain the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) which maximize the log of \\(L\\), and which therefore maximize \\(L\\). (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate \\(\\hat{\\sigma}^2\\) of \\(\\sigma^2\\). \\[ \\hat{\\sigma}^2 = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n} \\]",
    "sentences": ["How to get \\(b_0\\) and \\(b_1\\): least squares & maximum likelihoodâ€¦", "There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model.", "The oldest and most tradiational approach is using the idea of least squares.", "A more general approach uses the idea of maximum likelihood (see below).", "Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical.", "The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas.", "Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 <- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\).", "When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\).", "Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i.", "\\label{exp} \\end{equation}\\] Least Squares To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\)."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Estimating the Model Parameters\r\n(Expand)"
  },
  {
    "id": 68,
    "title": "ğŸ“ Estimating the Model Variance\r\n(Expand)",
    "url": "LinearRegression.html#estimatingthemodelvarianceexpand",
    "content": "Estimating the Model Variance\r\n(Expand) Estimating \\(\\sigma^2\\) with MSEâ€¦ As shown previously in the â€œEstimating Model Parametersâ€ section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation. Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\). This estimate turns out to be a biased estimator. This means that it is consistently wrong in its estimates of \\(\\sigma^2\\). If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong. This is bad. Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\). Without going into all the details, to â€œfixâ€ the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently. To find the correct degrees of freedom, we have to notice that the \\(\\hat{Y}_i\\) in the numerator of \\(\\widehat{\\sigma}^2\\) is defined by \\[\\begin{equation} \\widehat{Y}_i = b_0 + b_1X_i \\label{hatY} \\end{equation}\\] From this equation, we notice that two means, \\(\\bar{X}\\) and \\(\\bar{Y}\\), were estimated from the data in order to obtain \\(\\hat{Y}_i\\). (See the formulas for \\(b_0\\) and \\(b_1\\) above, and note how they use both \\(\\bar{X}\\) and \\(\\bar{Y}\\) in their calculation.) Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for \\(\\hat{\\sigma}^2\\) should be \\(n-2\\) instead of \\(n\\). Some incredibly long calculations will show that the â€œfixedâ€ estimator \\[\\begin{equation} s^2 = MSE = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n-2} \\quad \\text{(Unbiased Estimator of $\\sigma^2$)} \\end{equation}\\] is an unbiased estimator of \\(\\sigma^2\\). Here \\(MSE\\) stands for mean squared error, which is the most obvious name for a formula that squares the errors \\(Y_i-\\hat{Y}_i\\) then adds them up and divides by their degrees of freedom. Similarly, we call the numerator \\(\\sum(Y_i-\\hat{Y}_i)^2\\) the sum of the squared errors, denoted by \\(SSE\\). It is also important to note that the errors are often denoted by \\(r_i = Y_i-\\hat{Y}_i\\), the residuals. Putting this all together we get the following equivalent statements for \\(MSE\\). \\[\\begin{equation} s^2 = MSE = \\frac{SSE}{n-2} = \\frac{\\sum(Y_i-\\widehat{Y}_i)^2}{n-2} = \\frac{\\sum r_i^2}{n-2} \\end{equation}\\] As a final note, even though the expected value \\(E\\{MSE\\} = \\sigma^2\\), which shows \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\), it unfortunately isnâ€™t true that \\(\\sqrt{MSE}\\) is an unbiased estimator of \\(\\sigma\\). This presents a few problems later on, but these are minimal enough that we can overlook the issue and move forward.",
    "sentences": ["Estimating \\(\\sigma^2\\) with MSEâ€¦", "As shown previously in the â€œEstimating Model Parametersâ€ section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation.", "Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators.", "Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\).", "This estimate turns out to be a biased estimator.", "This means that it is consistently wrong in its estimates of \\(\\sigma^2\\).", "If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong.", "This is bad.", "Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\).", "Without going into all the details, to â€œfixâ€ the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Estimating the Model Variance\r\n(Expand)"
  },
  {
    "id": 69,
    "title": "ğŸ“ Transformations\r\n(Expand)",
    "url": "LinearRegression.html#transformationsexpand",
    "content": "Transformations\r\n(Expand) \\(Y'\\), \\(X'\\), and returning to the original spaceâ€¦ Y transformations are denoted by y-prime, written \\(Y'\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\). \\[ Y' = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y' = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y' = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y' = \\log(Y)\\) lm(log(Y) ~ X) 0.25 \\(Y' = \\sqrt(\\sqrt(Y))\\) lm(sqrt(sqrt(Y)) ~ X) 0.5 \\(Y' = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y' = Y\\) lm(Y ~ X) 2 \\(Y' = Y^2\\) lm(Y^2 ~ X) Using â€œmaximum-likelihoodâ€ estimation, the Box-Cox procedure can actually automatically detect the â€œoptimalâ€ value of \\(\\lambda\\) to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise. Scatterplot Recognition Box-Cox Suggestion An Example Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\). set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Ya <- 1/sqrt(Y) #1/Y^2 Lam = -2 Yb <- 1/Y #1/Y Lam = -1 Yc <- exp(.02*Y) #log(Y) Lam = 0 Yd <- Y^2 #sqrt(Y) Lam = 0.5 Ye <- Y #Y Lam = 1 Yf <- sqrt(Y) #Y^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Ya ~ X, main=expression(paste(\"Use \", lambda == -2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ya^-2 ~ X)) curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yb ~ X, main=expression(paste(\"Use \", lambda == -1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yb^-1 ~ X)) curve(1/(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yc ~ X, main=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(log(Yc) ~ X)) curve(exp(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yd ~ X, main=expression(paste(\"Use \", lambda == 0.5)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(sqrt(Yd) ~ X)) curve((b[1] + b[2]*x)^2, add=TRUE, col=\"green\", lwd=2) plot(Ye ~ X, main=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ye ~ X)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yf ~ X, main=expression(paste(\"Use \", lambda == 2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yf^2 ~ X)) curve(sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try. par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0)) boxCox(lm(Ya ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -2)), line=.5) boxCox(lm(Yb ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -1)), line=.5) boxCox(lm(Yc ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), line=.5) boxCox(lm(Yd ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0.5)), line=.5) boxCox(lm(Ye ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), line=.5) boxCox(lm(Yf ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 2)), line=.5) An Example Suppose we were running a simple linear regression on the cars dataset. This would be done with the code cars.lm <- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesnâ€™t quite fit the data as well as we would hope. Instead, the data looks a little curved. cars.lm <-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") abline(cars.lm, col=\"gray\") Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful. library(car) boxCox(cars.lm) The output from the boxCox(...) function looks as follows. This plot tells use to use the \\(\\lambda = 0.5\\) transformation, so that \\(Y' = Y^0.5 = \\sqrt{Y}\\). (To see this yourself, click on the â€œBox-Cox Suggestionâ€ tab above, as well as on the â€œScatterplot Recognitionâ€ tab.) Now, a transformation regression is performed using sqrt(Y) in place of Y as follows: cars.lm.t <- lm(sqrt(dist) ~ speed, data=cars) summary(cars.lm.t) Â  Estimate Std. Error t value Pr(> (Intercept) 1.277 0.4844 2.636 0.01126 speed 0.3224 0.02978 10.83 1.773e-14 Then, \\[ \\widehat{Y}_i' = 1.277 + 0.3224 X_i \\] And replacing \\(\\hat{Y}_i' = \\sqrt{\\hat{Y}_i}\\) we have \\[ \\sqrt{\\widehat{Y}_i} = 1.277 + 0.3224 X_i \\] Solving for \\(\\hat{Y}_i\\) gives \\[ \\widehat{Y}_i = (1.277 + 0.3224 X_i)^2 \\] Which, using curve((1.277 + 0.3224*x)^2, add=TRUE) (see code for details) looks like this: plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") curve( (1.277 + 0.3224*x)^2, add=TRUE, col=\"firebrick\") X-Transformations X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them. The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated. The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation. set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Xa <- 1/sqrt(X) #1/X^2 Lam = -2 Xb <- 1/X #1/X Lam = -1 Xc <- exp(.02*X) #log(X) Lam = 0 Xd <- X^2 #sqrt(X) Lam = 0.5 Xe <- X #X Lam = 1 Xf <- sqrt(X) #X^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Y ~ Xa, main=expression(paste(\"Use \", X*minute == X^-2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xa^-2))) curve(b[1] + b[2]*x^-2, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xb, main=expression(paste(\"Use \", X*minute == X^-1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xb^-1))) curve(b[1] + b[2]*x^-1, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xc, main=expression(paste(\"Use \", X*minute == log(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ log(Xc))) curve(b[1] + b[2]*log(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xd, main=expression(paste(\"Use \", X*minute == sqrt(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ sqrt(Xd))) curve(b[1] + b[2]*sqrt(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xe, main=expression(paste(\"Use \", X*minute == X, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ Xe)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xf, main=expression(paste(\"Use \", X*minute == X^2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xf^2))) curve(b[1] + b[2]*x^2, add=TRUE, col=\"green\", lwd=2)",
    "sentences": ["\\(Y'\\), \\(X'\\), and returning to the original spaceâ€¦", "Y transformations are denoted by y-prime, written \\(Y'\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\).", "\\[ Y' = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y' = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y' = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y' = \\log(Y)\\) lm(log(Y) ~ X) 0.25 \\(Y' = \\sqrt(\\sqrt(Y))\\) lm(sqrt(sqrt(Y)) ~ X) 0.5 \\(Y' = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y' = Y\\) lm(Y ~ X) 2 \\(Y' = Y^2\\) lm(Y^2 ~ X) Using â€œmaximum-likelihoodâ€ estimation, the Box-Cox procedure can actually automatically detect the â€œoptimalâ€ value of \\(\\lambda\\) to consider for a Y-transformation.", "Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise.", "Scatterplot Recognition Box-Cox Suggestion An Example Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\).", "set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Ya <- 1/sqrt(Y) #1/Y^2 Lam = -2 Yb <- 1/Y #1/Y Lam = -1 Yc <- exp(.02*Y) #log(Y) Lam = 0 Yd <- Y^2 #sqrt(Y) Lam = 0.5 Ye <- Y #Y Lam = 1 Yf <- sqrt(Y) #Y^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Ya ~ X, main=expression(paste(\"Use \", lambda == -2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ya^-2 ~ X)) curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yb ~ X, main=expression(paste(\"Use \", lambda == -1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yb^-1 ~ X)) curve(1/(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yc ~ X, main=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(log(Yc) ~ X)) curve(exp(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yd ~ X, main=expression(paste(\"Use \", lambda == 0.5)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(sqrt(Yd) ~ X)) curve((b[1] + b[2]*x)^2, add=TRUE, col=\"green\", lwd=2) plot(Ye ~ X, main=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ye ~ X)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yf ~ X, main=expression(paste(\"Use \", lambda == 2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yf^2 ~ X)) curve(sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try.", "par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0)) boxCox(lm(Ya ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -2)), line=.5) boxCox(lm(Yb ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -1)), line=.5) boxCox(lm(Yc ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), line=.5) boxCox(lm(Yd ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0.5)), line=.5) boxCox(lm(Ye ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), line=.5) boxCox(lm(Yf ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 2)), line=.5) An Example Suppose we were running a simple linear regression on the cars dataset.", "This would be done with the code cars.lm <- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesnâ€™t quite fit the data as well as we would hope.", "Instead, the data looks a little curved.", "cars.lm <-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") abline(cars.lm, col=\"gray\") Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Transformations\r\n(Expand)"
  },
  {
    "id": 70,
    "title": "ğŸ“ Inference for the Model Parameters\r\n(Expand)",
    "url": "LinearRegression.html#inferenceforthemodelparametersexpand",
    "content": "Inference for the Model Parameters\r\n(Expand) t test formulas, sampling distributions, confidence intervals, and F testsâ€¦ When fitting the regression model given by the equation \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] to a sample of data, we typically test hypotheses about the parameters \\(\\beta_0\\), \\(\\beta_1\\), or both. Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_0 - \\overbrace{0}^\\text{a number}}{s_{b_0}}\\] This is the formula for the test statistic. It measures how far the estimated y-intercept \\(b_0\\) is from the null hypothesis for \\(\\beta_0\\) in units of â€œstandard errors of \\(b_0\\)â€. Thus the division by \\(s_{b_0}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0, it could be any number. \\(H_0: \\beta_1 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_1\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_1 - \\overbrace{0}^\\text{a number}}{s_{b_1}}\\] This is the formula for the test statistic. It measures how far the estimated slope \\(b_1\\) is from the null hypothesis for \\(\\beta_1\\) in units of â€œstandard errors of \\(b_1\\)â€. Thus the division by \\(s_{b_1}\\). Though the hypothesized value of \\(\\beta_1\\) is typically 0, it could be any number. Left-tailed p-value = pt(-abs(tvalue), degrees of freedom). Double it to get the two-sided p-value. In R, these values correspond to the output summary of an lm as follows. (Show Example) Consider the cars data in R. Suppose we used the regression model given by \\[ \\underbrace{Y_i}_\\text{Feet to Stop} = \\beta_0 + \\beta_1 \\underbrace{X_{i}}_\\text{mph} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\quad \\sim N(0,\\sigma^2) \\] to model the feet a vehicle (from the 1920â€™s) takes to stop when traveling at a certain speed (in miles per hour, mph) prior to stopping. When the regression is performed and summarized in R, it is always testing the following two hypotheses: \\[ H_0: \\beta_0 = 0 \\quad\\quad H_0: \\beta_1 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\quad\\quad H_a: \\beta_1 \\neq 0 \\] To perform the test of these hypotheses for the regression stated above, we would run the following codes in R. cars.lm <- lm(dist ~ speed, data=cars) pander(summary(cars.lm)$coefficients) These would produce summary output like the following, but the following output has been labeled with the math notation corresponding to each value. Letâ€™s emphasize what is happening in this summary output table. First, here is how the â€œt valueâ€ is calculated for the â€œ(Intercept)â€ in the summary table above. \\[ t = \\frac{b_0-0}{s_{b_0}} = \\frac{-17.58 - 0}{6.758} = -2.601 \\] Second, here is a visual representation of how the P-value, the â€œPr(>|t|)â€ as it is called in the summary table above, is calculated for this test statistic. (Click the graph to view an interactive applet showing this calculation.) Notice both ends of the t-distribution are being shaded to compute the P-value because the alternative hypothesis was \\(H_a: \\beta_0 \\neq 0\\). To compute the P-value in R, we use the â€œpercentile function for the t-distributionâ€ called pt( ). This function requires two things, the t-value and the degrees of freedom, in our case pt(-2.601, 48). Note the degrees of freedom (df) are 48 because the sample size is \\(n=50\\) and there are two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) in our regression model. Running this code in R gives: pt(-2.601, 48) = 0.00616 However, note that this value is only half of the actual P-value of 0.0123. To get the â€œtwo-sidedâ€ P-value (note that our alternative hypothesis used a \\(\\neq\\) symbol) we need to double this left-tailed P-value. 2*pt(-2.601, 48)) = 0.0123 Finally, note that the same procedure can be used to test hypotheses that use a value other than 0 in the null and alternative. For example, to test the hypotheses: \\[ H_0: \\beta_1 = 3 \\\\ H_a: \\beta_1 \\neq 3 \\] Use the t-formula \\[ t = \\frac{b_1 - 3}{s_{b_1}} = \\frac{\\overbrace{3.932}^{b_1} - \\overbrace{3}^{H_0}}{\\underbrace{0.4155}_{s_{b_1}}} = 2.243 \\] then the P-value is calculated in R by 2*pt(-abs(2.243), 48) = 0.0295495 To obtain confidence intervals in R use confint(mylm). Confidence Interval Formula Standard Error \\(\\beta_0\\) \\(b_0 \\pm\\) \\(t^*\\) This is called the â€œcritical valueâ€ and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\). Use qt(0.975, df) to get \\(t*\\) in R. \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_0\\). \\(s_{b_0}\\) The standard error of \\(b_0\\), denoted by \\(s_{b_0}\\) is provided in the regression summary output under the column header called â€œStd. Errorâ€ for the â€œ(Intercept)â€ row of the output. It is calculated using the formula shown below. \\[s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\] This is called the â€œestimated variance of \\(b_0\\)â€. Taking the square root of this number gives the â€œstandard error of \\(b_0\\)â€. \\(\\beta_1\\) \\(b_1 \\pm\\) \\(t^*\\) This is called the â€œcritical valueâ€ and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\) (sample size - number of parameters in the regression model). Use qt(0.975, df) to get \\(t*\\) in R \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_1\\). \\(s_{b_1}\\) The standard error of \\(b_1\\), denoted by \\(s_{b_1}\\) is provided in the regression summary output under the column header called â€œStd. It is calculated using the formula shown below. \\[s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\] This is called the â€œestimated variance of \\(b_1\\)â€. Taking the square root of this number gives the â€œstandard error of \\(b_1\\)â€. To be more exact, the types of inference we are interested in are the following. Determine if there is evidence of a meaningful linear relationship in the data. If \\(\\beta_1 = 0\\), then there is no relation between \\(X\\) and \\(E\\{Y\\}\\). Hence we might be interested in testing the hypotheses \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) is some hypothesized number. To provide a confidence interval for the true value of \\(\\beta_1\\). Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the sampling distribution of the estimate \\(b_1\\) of the parameter \\(\\beta_1\\). And, while we are at it, we may as well come to understand the sampling distribution of the estimate \\(b_0\\) of the parameter \\(\\beta_0\\). Review sampling distributions from Math 221. Since \\(b_1\\) is an estimate, it will vary from sample to sample, even though the truth, \\(\\beta_1\\), remains fixed. (The same holds for \\(b_0\\) and \\(\\beta_0\\).) It turns out that the sampling distribution of \\(b_1\\) (where the \\(X\\) values remain fixed from study to study) is normal with mean and variance: \\[ \\mu_{b_1} = \\beta_1 \\] \\[ \\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} \\] ## Simulation to Show relationship between Standard Errors ##----------------------------------------------- ## Edit anything in this area... n <- 100 #sample size Xstart <- 30 #lower-bound for x-axis Xstop <- 100 #upper-bound for x-axis beta_0 <- 2 #choice of true y-intercept beta_1 <- 3.5 #choice of true slope sigma <- 13.8 #choice of st. deviation of error terms ## End of Editable area. ##----------------------------------------------- # Create X, which will be used in the next R-chunk. X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) ## After playing this chunk, play the next chunk as well. To see that this is true, consider the regression model with values specified for each parameter as follows. \\[ Y_i = \\overbrace{\\beta_0}^{2} + \\overbrace{\\beta_1}^{3.5} X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\overbrace{\\sigma^2}^{\\sigma=13.8}) \\] Using the equations above for \\(\\mu_{b_1}\\) and \\(\\sigma^2_{b_1}\\) we obtain that the mean of the sampling distribution of \\(b_1\\) will be \\(\\mu_{b_1} = \\beta_1 = 3.5\\) Further, we see that the variance of the sampling distribution of \\(b_1\\) will be \\(\\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} = \\frac{13.8^2}{4.25\\times 10^{4}}\\) Taking the square root of the variance, the standard deviation of the sampling distribution of \\(b_1\\) will be \\(\\sigma_{b_1} = 0.067\\). Thatâ€™s very nice. But to really believe it, letâ€™s run a simulation ourselves. The â€œCodeâ€ below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of \\(b_1\\) and \\(b_0\\) for that regression. After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of \\(b_0\\) is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of \\(b_1\\) is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of \\(\\beta_0\\) or \\(\\beta_1\\), respectively. Also, the â€œStd. Errorâ€ of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of \\(\\sigma_{b_0}\\) and \\(\\sigma_{b_1}\\), respectively. N <- 5000 #number of times to pull a random sample storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N) for (i in 1:N){ Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model mylm <- lm(Y ~ X) storage_b0[i] <- coef(mylm)[1] storage_b1[i] <- coef(mylm)[2] storage_rmse[i] <- summary(mylm)$sigma } layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3)) Ystart <- 0 #min(0,min(Y)) Ystop <- 500 #max(max(Y), 0) Yrange <- Ystop - Ystart plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), ylim=c(Ystart, Ystop), pch=16, col=\"gray\", main=\"Regression Lines from many Samples (gray lines) \\n Plus Residual Standard Deviation Lines (green lines)\") text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1) text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1) text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1) for (i in 1:N){ abline(storage_b0[i], storage_b1[i], col=\"darkgray\") } abline(beta_0, beta_1, col=\"green\", lwd=3) abline(beta_0+sigma, beta_1, col=\"green\", lwd=2) abline(beta_0-sigma, beta_1, col=\"green\", lwd=2) abline(beta_0+2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0-2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0+3*sigma, beta_1, col=\"green\", lwd=.5) abline(beta_0-3*sigma, beta_1, col=\"green\", lwd=.5) par(mai=c(1,.6,.5,.01)) addnorm <- function(m,s, col=\"firebrick\"){ curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2) lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col) lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col) lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col) lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col) lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col) lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col) lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col) legend(\"topleft\", legend=paste(\"Std. Error = \", round(s,3)), cex=0.7, bty=\"n\") } h0 <- hist(storage_b0, col=\"skyblue3\", main=\"Sampling Distribution\\n Y-intercept\", xlab=expression(paste(\"Estimates of \", beta[0], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m0 <- mean(storage_b0) s0 <- sd(storage_b0) addnorm(m0,s0, col=\"green\") h1 <- hist(storage_b1, col=\"skyblue3\", main=\"Sampling Distribution\\n Slope\", xlab=expression(paste(\"Estimates of \", beta[1], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m1 <- mean(storage_b1) s1 <- sd(storage_b1) addnorm(m1,s1, col=\"green\") t Tests Using the information above about the sampling distributions of \\(b_1\\) and \\(b_0\\), an immediate choice of statistical test to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) can be zero, or any other value, is a t test given by \\[ t = \\frac{b_1 - \\beta_{10}}{s_{b_1}} \\] where \\(s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\). (You may want to review the section â€œEstimating the Model Varianceâ€ of this file to know where MSE came from.) With quite a bit of work it has been shown that \\(t\\) is distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. The nearly identical test statistic for testing \\[ H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_0 \\neq \\beta_{00} \\] is given by \\[ t = \\frac{b_0 - \\beta_{00}}{s_{b_0}} \\] where \\(s^2_{b_0} = MSE\\left[\\frac{1}{n}+\\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\). This version of \\(t\\) has also been shown to be distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. Confidence Intervals Creating a confidence interval for either \\(\\beta_1\\) or \\(\\beta_0\\) follows immediately from these results using the formulas \\[ b_1 \\pm t^*_{n-2}\\cdot s_{b_1} \\] \\[ b_0 \\pm t^*_{n-2}\\cdot s_{b_0} \\] where \\(t^*_{n-2}\\) is the critical value from a t distribution with \\(n-2\\) degrees of freedom corresponding to the chosen confidence level. F tests Another way to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\quad\\quad \\text{or} \\quad\\quad H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\quad\\quad \\ \\ \\quad \\quad H_a: \\beta_0 \\neq \\beta_{00} \\] is with an \\(F\\) Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an \\(F\\) test is very general and can be used in many places that a t test cannot. In its most general form, the \\(F\\) test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that \\(H_0:\\beta_1=0\\) against the alternative that \\(H_a: \\beta_1\\neq 0\\), we are essentially comparing two models against each other. If \\(\\beta_1=0\\), then the corresponding model would be \\(E\\{Y_i\\} = \\beta_0\\). If \\(\\beta_1\\neq0\\), then the model remains \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the \\(F\\) Test, that the null model (reduced model) have fewer â€œfreeâ€ parameters than the alternative model (full model). To demonstrate what we mean by â€œfreeâ€ parameters, consider the following example. Say we wanted to test the hypothesis that \\(H_0:\\beta_1 = 2.5\\) against the alternative that \\(\\beta_1\\neq2.5\\). Then the null, or reduced model, would be \\(E\\{Y_i\\}=\\beta_0+2.5X_i\\). The alternative, or full model, would be \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). Thus, the null (reduced) model contains only one â€œfreeâ€ parameter because \\(\\beta_1\\) has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two â€œfreeâ€ parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model. Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing \\(H_0: \\beta_1=0\\) against \\(H_a:\\beta_1\\neq0\\) we have the partition \\[ \\underbrace{Y_i-\\bar{Y}}_{Total} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{Regression} + \\underbrace{Y_i-\\hat{Y}_i}_{Error} \\] The reason we use \\(\\bar{Y}\\) for the null model is that \\(\\bar{Y}\\) is the unbiased estimator of \\(\\beta_0\\) for the null model, \\(E\\{Y_i\\} = \\beta_0\\). Thus we would compute the following sums of squares: \\[ SSTO = \\sum(Y_i-\\bar{Y})^2 \\] \\[ SSR = \\sum(\\hat{Y}_i-\\bar{Y})^2 \\] \\[ SSE = \\sum(Y_i-\\hat{Y}_i)^2 \\] and note that \\(SSTO = SSR + SSE\\). Important to note is that \\(SSTO\\) uses the difference between the observations \\(Y_i\\) and the null (reduced) model. The \\(SSR\\) uses the diffences between the alternative (full) and null (reduced) model. The \\(SSE\\) uses the differences between the observations \\(Y_i\\) and the alternative (full) model. From these we could set up a General \\(F\\) table of the form Â  Sum Sq Df Mean Sq F Value Model Error \\(SSR\\) \\(df_R-df_F\\) \\(\\frac{SSR}{df_R-df_F}\\) \\(\\frac{SSR}{df_R-df_F}\\cdot\\frac{df_F}{SSE}\\) Residual Error \\(SSE\\) \\(df_F\\) \\(\\frac{SSE}{df_F}\\) Total Error \\(SSTO\\) \\(df_R\\)",
    "sentences": ["t test formulas, sampling distributions, confidence intervals, and F testsâ€¦", "When fitting the regression model given by the equation \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] to a sample of data, we typically test hypotheses about the parameters \\(\\beta_0\\), \\(\\beta_1\\), or both.", "Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0.", "However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0.", "To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown.", "\\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis.", "By default, the p-value from summary(mylm) in R uses \\(\\neq\\).", "\\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0.", "However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0.", "To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Inference for the Model Parameters\r\n(Expand)"
  },
  {
    "id": 71,
    "title": "ğŸ“ Prediction and Confidence Intervals for \\(\\hat{Y}_h\\)\r\n(Expand)",
    "url": "LinearRegression.html#predictionandconfidenceintervalsfor\\\\hatyh\\expand",
    "content": "Prediction and Confidence Intervals for \\(\\hat{Y}_h\\)\r\n(Expand) predict(â€¦, interval=â€œpredictionâ€)â€¦ It is a common mistake to assume that averages (means) describe individuals. They do not. So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line. Interval R Code Math Equation When to Use Prediction predict(..., interval=\"prediction\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\text{Pred}\\ Y}\\) Predict an individualâ€™s value. Confidence predict(..., interval=\"confidence\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\hat{Y}}\\) Estimate location of the mean y-value. predict(mylm, data.frame(XvarName = number), interval=...) For example, consider this graph. Then click here to read about the graph. Notice the three dots above 15 mph in the graph. Each of these dots show a car that was going 15 mph when it applied the brakes. However, stopping distances of the three individual cars differ with one at 20 feet, one at 26 feet and one at 54 feet. The regression line represents the average stopping distance of cars. In this case, cars going 15 mph are estimated to have an average stopping distance of about 40 feet, as shown by the line. But individual vehicles, all going the same speed of 15 mph, varied from stopping distances of 20 feet up to 54 feet! So, to predict that a car going 15 mph will take 41.4 feet to stop, doesnâ€™t tell the whole story. Far more revealing is the complete statement, â€œCars going 15 mph are predicted to take anywhere from 10.2 to 72.6 feet to stop, with an average stopping distance of 41.4 feet.â€ This is called the â€œprediction intervalâ€ and is shown in the graph in blue. It is obtained in R with the codes: cars.lm <- lm(dist ~ speed, data=cars) predict(cars.lm, data.frame(speed=15), interval=\"prediction\") cars.lm <- lm(dist ~ speed, data=cars) pander(predict(cars.lm, data.frame(speed=15), interval=\"prediction\")) fit lwr upr 41.41 10.17 72.64 plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=\"firebrick2\", cex=1.5) cars.lm <- lm(dist ~ speed, data=cars) abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3)) abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2)) abline(v=15, lty=2, col=\"firebrick\") preds <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12) lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8)) lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8)) Now, for the details behind prediction intervals and confidence intervals. Letâ€™s begin by recalling some details (from the section â€œInference for the Model Parametersâ€) about the standard error of the y-intercept, \\(b_0\\). Recall that the y-intercept is the average y-value for the given x-value of \\(x=0\\). Recall further that the formula for the standard error of \\(b_0\\) is given by the formula \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] If we wanted to be more exact with this formula, we would write it as \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{(0-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice the addition of \\((0 - \\bar{X})^2\\) instead of just \\(\\bar{X}^2\\) in the numerator of the right-most part of the equation? This more complete statement obviously would reduce to just \\(\\bar{X}^2\\), but that is only because \\(X=0\\) when we are working with the y-intercept, \\(b_0\\). We could be working with other values of \\(X\\) than just zero. Letâ€™s take a quick detour and talk notation for a second. Typically, \\(X_i\\) and \\(Y_i\\) are used to denote the x-value and y-value of points that are contained in our data set. When we want to reference a point that wasnâ€™t within our original data set, we use the notation \\(X_h\\) and \\(Y_h\\). (The letter h is close to i, but different from i, so why not. There is really no other reason to use h.) Thus, \\(Y_h\\) is the y-value for the \\(X_h\\) x-value, neither of which were included in our original regression of \\(X_i\\)â€™s and \\(Y_i\\)â€™s. Now, back to the previous discussion. If \\(X_h = 0\\), then \\(\\hat{Y}_h\\) is the y-intercept, so \\(\\hat{Y}_h = b_0\\) when \\(X_h=0\\). So, we could write, \\[ s^2_{\\hat{Y}_h} = MSE\\left[\\frac{1}{n} + \\frac{(X_h-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice how the \\(b_0\\) in \\(s_{b_0}\\) was replaced with \\(\\hat{Y}_h\\) to get \\(s_{\\hat{Y}_h}\\) and the 0 in \\((0 - \\bar{X})^2\\) was replaced with \\(X_h\\) to get \\((X_h - \\bar{X})^2\\)? Interesting. We now have a formula that would give us the standard error of \\(\\hat{Y}_h\\) for any \\(X_h\\) value, not just \\(X_h = 0\\), or the y-intercept, \\(b_0\\). That is fantastic. It would look like this if plotted. Notice how the gray region is showing the standard error for each \\(\\hat{Y}_h\\) value? (It is technically showing the confidence interval for \\(E\\{Y_h\\}\\) at every possible \\(X_h\\) value, but that is just \\(\\hat{Y}_h \\pm t^* \\cdot s_{\\hat{Y}_h}\\).) ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + theme_bw() ## `geom_smooth()` using formula = 'y ~ x' Confidence Interval for \\(\\hat{Y}_h\\) \\[ \\hat{Y}_h \\pm t^* s_{\\hat{Y}_h} \\quad \\text{where} \\ s_{\\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] The confidence interval is a wonderful tool for estimating \\(E\\{Y_h\\}\\), the â€œtrueâ€ average y-value for a given x-value of \\(X_h\\). However, it is not valuable for predicting an individual dot, or \\(Y_h\\) value. Notice how few of the dots of the regression are actually contained within the confidence interval band in the plot? The confidence interval does not really predict where the dots will land, just where the average y-value is located for each x-value. Remember the 68-95-99.7 Rule of the normal distribution? If not, here is a link back to that concept in the Math 221 textbook. This rule states that roughly 95% of data, when normally distributed, will be between \\(z=-2\\) and \\(z=2\\) standard deviations from the mean. So, is going two â€œresidual standard errorsâ€ to both sides of the regression line enough to capture 95% of the data? The answer is, not quite. The reason for this is because our knowledge of where the true mean lies is uncertain. (Notice the confidence interval band shown in the plot.) However, adding two standard errors to the edges of the confidence band would get us in the right place. In other words, there are two sources of variability at play here, (1) our uncertaintity in where the regression line is sitting, and (2) the natural variability of the data points around the line. Thus, the â€œprediction intervalâ€ requires accounting for both of these sources of variability to produce the following equation. Prediction Interval for \\(Y_h\\) \\[ \\hat{Y}_h \\pm t^* s_{Pred \\hat{Y}_h} \\quad \\text{where} \\ s_{Pred \\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + 1 + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] This formula provides a useful band for identifying a region where we are 95% confident that a new observation for \\(Y_h\\) will land, given the value of \\(X_h\\). It looks as follows. Notice the prediction interval is much wider than the confidence interval. This is because data varies far more than do means. Prediction is for where the individual data points will land, confidence is for where the mean will land. cars.lm <- lm(dist ~ speed, data=cars) predy <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + geom_segment(aes(x=15, xend=15, y=predy[2], yend=predy[3]), lwd=4, color=rgb(.5,.7,.5,.01)) + geom_point(aes(x=15, y=predy[1]), cex=2, color=\"skyblue\", pch=15) + theme_bw() ## Warning in geom_segment(aes(x = 15, xend = 15, y = predy[2], yend = predy[3]), : All aesthetics have length 1, but the data has 50 rows. ## â„¹ Please consider using `annotate()` or provide this layer with data containing a single row. ## Warning in geom_point(aes(x = 15, y = predy[1]), cex = 2, color = \"skyblue\", : All aesthetics have length 1, but the data has 50 rows. ## â„¹ Please consider using `annotate()` or provide this layer with data containing a single row. ## `geom_smooth()` using formula = 'y ~ x'",
    "sentences": ["predict(â€¦, interval=â€œpredictionâ€)â€¦", "It is a common mistake to assume that averages (means) describe individuals.", "They do not.", "So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line.", "Interval R Code Math Equation When to Use Prediction predict(..., interval=\"prediction\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\text{Pred}\\ Y}\\) Predict an individualâ€™s value.", "Confidence predict(..., interval=\"confidence\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\hat{Y}}\\) Estimate location of the mean y-value.", "predict(mylm, data.frame(XvarName = number), interval=...) For example, consider this graph.", "Then click here to read about the graph.", "Notice the three dots above 15 mph in the graph.", "Each of these dots show a car that was going 15 mph when it applied the brakes."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Prediction and Confidence Intervals for \\(\\hat{Y}_h\\)\r\n(Expand)"
  },
  {
    "id": 72,
    "title": "ğŸ“ Lowess (and Loess) Curves\r\n(Expand)",
    "url": "LinearRegression.html#lowessandloesscurvesexpand",
    "content": "Lowess (and Loess) Curves\r\n(Expand) A non-parametric approach to estimating \\(E\\{Y_i\\}\\)â€¦ Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value. Using Base R air2 <- airquality %>% na.omit(select(Temp, Ozone)) # Just quickly draw the lowess curve: plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") lines(lowess(air2$Ozone, air2$Temp), col=\"firebrick\") ## OR optionally, ## allow for predictions as well as the graph: # plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # lines(mylo$fit ~ Ozone, data=air2) Using ggplot2 air2 <- airquality %>% na.omit(select(Temp, Ozone)) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=\"darkgray\") + geom_smooth(se=F, method=\"loess\", method.args = list(degree=1)) + #Note, degree=2 by default. theme_bw() ## `geom_smooth()` using formula = 'y ~ x' ## OR optionally, ## allow for predictions as well as the graph: # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fit, x=Ozone)) Advantages Disadvantages Quick. Good at ignoring outliers. Good at capturing the general pattern in the data. Good for making predictions within the scope of the data. No mathematical model. Not interpretable. No p-values. No adjusted R-squared. How it Works The Lowess curve localizes the regression model to a â€œneighborhoodâ€ of points, and then joins these localized regressions together into a smooth line. It minimizes the effect of outliers, and letâ€™s the data â€œspeak for itselfâ€. As a downside, it is not interpretable, and has no final way to write the model mathematically. All the same, it is a very powerful tool for identifying an appropriate model, or verifying the fit of a model, or making predictions when no reasonable model does an adequate job. Study this graphic and the explanations below to learn how it works. Recommendation: run the code in this â€œCodeâ€ chunk to the right in your Console, and flip through the resulting graphics. X <- cars$speed Y <- cars$dist X <- X[!is.na(X) & !is.na(Y)] Y <- Y[!is.na(X) & !is.na(Y)] f <- 1/2 n <- length(X) lfit <- rep(NA,n) for (xh in 1:n){ xdists <- X - X[xh] nn <- floor(n*f) r <- sort(abs(xdists))[nn] xdists.nbrhd <- which(abs(xdists) < r) w <- rep(0, length(xdists)) w[xdists.nbrhd] <- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w), col=rgb(.2,.2,.2,.3), cex=1.5, yaxt='n', xaxt='n', xlab=\"\", ylab=\"\") points(Y[xh] ~ X[xh], pch=16, col=\"orange\") lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2)) cat(\"\\n\\n\") readline(prompt=paste0(\"Center point is point #\", xh, \"... Press [enter] to continue...\")) MADnotThereYet <- TRUE count <- 0 while(MADnotThereYet){ readline(prompt=paste0(\"\\n Adjusting line to account for outliers in the y-direction... Press [enter] to continue...\")) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"wheat\", add=TRUE) MAD <- median(abs(lmc$res)) resm <- lmc$res/(6*MAD) resm[resm>1] <- 1 bisq <- (1-resm^2)^2 w <- w*bisq obs <- coef(lmc) lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) count <- count + 1 if ( (sum(abs(obs-lmc$coef))<.1) | (count > 3)) MADnotThereYet <- FALSE } curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"green\", add=TRUE) points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=\"green\") readline(prompt=paste0(\"\\n Use final line to get fitted value for this point... Press [enter] to continue to next point...\")) lfit[xh] <- predict(lmc, data.frame(X=X[xh])) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") if (xh == n){ readline(prompt=paste0(\"\\n Press [enter] to see actual Lowess curve...\")) lines(lowess(X,Y, f=f), col=\"firebrick\") legend(\"topleft\", bty=\"n\", legend=\"Actual lowess Curve using lowess(...)\", col=\"firebrick\", lty=1) } } Select a fraction of the data to use for the â€œneighborhoodâ€ of points (shown in blue in the graph above). The lowess function in R uses â€œf=2/3â€ and the loess function uses â€œspan=0.75â€ for this value, which selects the nearest two-thirds or 75% of the data, respectively, depending on which function you use. For this example, we set the fraction of points at 50%. Both functions can be set to whatever you want. Pick any point in the regression, eventually selecting all points one at a time. The selected point becomes the â€œcenterâ€ of a â€œneighborhoodâ€ of points surrounding it. In this example, the center point is in orange, and the neighboring points are in blue. Use the points within the neighborhood to fit a regression line. However, make the regression depend most on points closest to â€œcenterâ€ and least on points furthest from â€œcenter.â€ This is called a weighted regression. Weights are decided according to what is called the tricubic weight function, so that the weight \\(w\\) given to point \\(j\\) of the neighborhood of points is defined by \\[ w_j = \\left(1- \\left( \\frac{|X_c - X_j|}{\\max_k |X_c - X_k|}\\right)^3\\right)^3 \\] where \\(X_c\\) is the x-value of the â€œcenterâ€ dot and \\(X_j\\) is the x-value of any other dot in the neighborhood. The fitted-value of \\(\\hat{Y}_c\\) is obtained for the center point \\(X_c\\) of the current regression. This point is used as the Lowess (or Loess) curveâ€™s value at that particular x-value. Well, almost. Itâ€™s a first guess at where this value will end up, but thereâ€™s a little more to the algorithm before we are done. Initial guesses for each of these fitted values are obtained for each point in the regression. Now each local regression for each neighborhood is re-run a few times in such a way the the effect of outliers is minimized. The final line for each neighborhood is obtained by the following steps. Compute all residuals for points in the neighborhood of the current regression, denoted by \\(r_i\\). Then compute the MAD, median absolute deviation, of the residuals \\(MAD = \\text{median} (|r_1|, |r_2|, \\ldots)\\). Divide all residuals by 6 times the MAD: \\(u_i = r_i/(6\\cdot MAD)\\) (If \\(r_i > 6\\cdot MAD\\) then set \\(u_i = 0\\).) Compute what are called bisquare weights using the formula: \\(b_i = (1 - u_i^2)^2\\) Perform a regression using the weights \\(w_i = w_i b_i\\) Repeat the above process with the new weights \\(w_i\\) until the weights stop changing very much. The final fitted values for each \\(X\\)-value in the regression are obtained from the final regression line for each neighborhood. These fitted values make up the Lowess (or loess) curve. Note that the default of the loess function in R is to use quadratic regressions in each neighborhood instead of linear regressions. This can be controlled with the loess option of â€œdegree=2â€ (quadratic fits) or â€œdegree = 1â€. In the lowess function only a linear regression in each neighborhood is allowed.",
    "sentences": ["A non-parametric approach to estimating \\(E\\{Y_i\\}\\)â€¦", "Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value.", "Using Base R air2 <- airquality %>% na.omit(select(Temp, Ozone)) # Just quickly draw the lowess curve: plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") lines(lowess(air2$Ozone, air2$Temp), col=\"firebrick\") ## OR optionally, ## allow for predictions as well as the graph: # plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # lines(mylo$fit ~ Ozone, data=air2) Using ggplot2 air2 <- airquality %>% na.omit(select(Temp, Ozone)) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=\"darkgray\") + geom_smooth(se=F, method=\"loess\", method.args = list(degree=1)) + #Note, degree=2 by default.", "theme_bw() ## `geom_smooth()` using formula = 'y ~ x' ## OR optionally, ## allow for predictions as well as the graph: # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fit, x=Ozone)) Advantages Disadvantages Quick.", "Good at ignoring outliers.", "Good at capturing the general pattern in the data.", "Good for making predictions within the scope of the data.", "No mathematical model.", "Not interpretable.", "No p-values."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Lowess (and Loess) Curves\r\n(Expand)"
  },
  {
    "id": 73,
    "title": "ğŸ“ Multiple Linear\r\nRegression",
    "url": "LinearRegression.html#multiplelinearregression",
    "content": "Multiple Linear\r\nRegression Multiple regression allows for more than one explanatory variable to be included in the modeling of the expected value of the quantitative response variable \\(Y_i\\). There are infinitely many possible multiple regression models to choose from. Here are a few â€œbasicâ€ models that work as building blocks to more complicated models. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model uses the same \\(X\\)-variable twice, once with a \\(\\beta_1 X_i\\) term and once with a \\(\\beta_2 X_i^2\\) term. The \\(X_i^2\\) term is called the â€œquadraticâ€ term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and â€œsteepnessâ€ of the Model: negative values face down, positive values face up; large values imply â€œsteeperâ€ parabolas and low values imply â€œflatterâ€ parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)â€™s explanation. An Example Using the airquality data set, we run the following â€œquadraticâ€ regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our â€œquadraticâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. Temp Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These â€œEstimatesâ€ can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(Month^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of Month from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, ) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will â€œopen downâ€ (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be â€œmonth zero.â€ Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the â€œcubicâ€ term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the â€œbase slope coefficientâ€ and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following â€œcubicâ€ regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our â€œcubicâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. uptake Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^3) \\(X_{i}^3\\), where the function I(â€¦) protects the cubing of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will â€œopen upâ€. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The â€œtwo-linesâ€ model is a simple way to compare two groups in statistics. It uses two main parts: A regular number (\\(X_{1i}\\)) that can be any value. A special number (\\(X_{2i}\\)) thatâ€™s either 0 or 1. This special number (also called a â€œdummy variableâ€ or â€œindicator variableâ€) helps turn information about groups (like â€œGroup Aâ€ or â€œGroup Bâ€) into numbers we can use in math. By using these two parts, we can create two separate lines on a graph: One line for Group A One line for Group B This helps us see how the two groups are different from each other. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the â€œbase-lineâ€ of the model, the â€œGroup 0â€ line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the â€œbase-lineâ€ line. \\(\\beta_3\\) Called the â€œinteractionâ€ term. Controls the change in the slope for the second line in the model as compared to the slope of the â€œbase-lineâ€ line. An Example Using the mtcars data set, we run the following â€œtwo-linesâ€ regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our â€œtwo-linesâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. mpg Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept), \\(\\beta_1\\) is estimated by the qsec value of 1.439, \\(\\beta_2\\) is estimated by the am value of -14.51, and \\(\\beta_3\\) is estimated by the qsec:am value of 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called â€œ3Dâ€ regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to â€œbendâ€. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the â€œTempâ€ direction is estimated to be 2.659 and the slope in the â€œMonthâ€ direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction termâ€™s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the â€œslopesâ€ in each direction to change, creating a â€œcurvedâ€ surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called â€œHDâ€, or â€œHigh Dimensionalâ€, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isnâ€™t really even possible to â€œmentally connectâ€ with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)â€™s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the â€œWindâ€ direction is estimated to be -3.334. The slope in the â€œTempâ€ direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the â€œSolar.Râ€ direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here.",
    "sentences": ["Multiple regression allows for more than one explanatory variable to be included in the modeling of the expected value of the quantitative response variable \\(Y_i\\).", "There are infinitely many possible multiple regression models to choose from.", "Here are a few â€œbasicâ€ models that work as building blocks to more complicated models.", "Overview Select a model to see interpretation details, an example, and R Code help.", "Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\).", "Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model uses the same \\(X\\)-variable twice, once with a \\(\\beta_1 X_i\\) term and once with a \\(\\beta_2 X_i^2\\) term.", "The \\(X_i^2\\) term is called the â€œquadraticâ€ term.", "Parameter Effect \\(\\beta_0\\) Y-intercept of the Model.", "\\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\).", "\\(\\beta_2\\) Controls the concavity and â€œsteepnessâ€ of the Model: negative values face down, positive values face up; large values imply â€œsteeperâ€ parabolas and low values imply â€œflatterâ€ parabolas."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Multiple Linear\r\nRegression"
  },
  {
    "id": 74,
    "title": "ğŸ“ Overview",
    "url": "LinearRegression.html#overview",
    "content": "Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model uses the same \\(X\\)-variable twice, once with a \\(\\beta_1 X_i\\) term and once with a \\(\\beta_2 X_i^2\\) term. The \\(X_i^2\\) term is called the â€œquadraticâ€ term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and â€œsteepnessâ€ of the Model: negative values face down, positive values face up; large values imply â€œsteeperâ€ parabolas and low values imply â€œflatterâ€ parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)â€™s explanation. An Example Using the airquality data set, we run the following â€œquadraticâ€ regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our â€œquadraticâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. Temp Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These â€œEstimatesâ€ can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(Month^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of Month from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, ) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will â€œopen downâ€ (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be â€œmonth zero.â€ Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the â€œcubicâ€ term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the â€œbase slope coefficientâ€ and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following â€œcubicâ€ regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our â€œcubicâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. uptake Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^2) \\(X_{i}^2\\), where the function I(â€¦) protects the squaring of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. I(conc^3) \\(X_{i}^3\\), where the function I(â€¦) protects the cubing of conc from how lm(â€¦) would otherwise interpret that statement. The I(â€¦) function must be used anytime you raise an x-variable to a power in the lm(â€¦) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will â€œopen upâ€. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The â€œtwo-linesâ€ model is a simple way to compare two groups in statistics. It uses two main parts: A regular number (\\(X_{1i}\\)) that can be any value. A special number (\\(X_{2i}\\)) thatâ€™s either 0 or 1. This special number (also called a â€œdummy variableâ€ or â€œindicator variableâ€) helps turn information about groups (like â€œGroup Aâ€ or â€œGroup Bâ€) into numbers we can use in math. By using these two parts, we can create two separate lines on a graph: One line for Group A One line for Group B This helps us see how the two groups are different from each other. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the â€œbase-lineâ€ of the model, the â€œGroup 0â€ line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the â€œbase-lineâ€ line. \\(\\beta_3\\) Called the â€œinteractionâ€ term. Controls the change in the slope for the second line in the model as compared to the slope of the â€œbase-lineâ€ line. An Example Using the mtcars data set, we run the following â€œtwo-linesâ€ regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our â€œtwo-linesâ€ regression. lm( R function lm used to perform linear regressions in R. The lm stands for â€œlinear modelâ€. mpg Y-variable, should be quantitative. Â ~Â  The tilde ~ is what lm(â€¦) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)â€™s are going to be estimated by the lm(â€¦). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ. Â +Â  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(â€¦) from the \\(Y_i = ...\\) model. No betaâ€™s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(â€¦) function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Pay special attention to how the lm(â€¦) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)â€™s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)â€™s and \\(\\epsilon\\) are given by the output of the lm(â€¦) funtion in the â€œEstimatesâ€ column of summary(â€¦.) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients) Â  Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)â€™s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept), \\(\\beta_1\\) is estimated by the qsec value of 1.439, \\(\\beta_2\\) is estimated by the am value of -14.51, and \\(\\beta_3\\) is estimated by the qsec:am value of 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the â€œCodeâ€ to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called â€œ3Dâ€ regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to â€œbendâ€. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the â€œTempâ€ direction is estimated to be 2.659 and the slope in the â€œMonthâ€ direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction termâ€™s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the â€œslopesâ€ in each direction to change, creating a â€œcurvedâ€ surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called â€œHDâ€, or â€œHigh Dimensionalâ€, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isnâ€™t really even possible to â€œmentally connectâ€ with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)â€™s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the â€œWindâ€ direction is estimated to be -3.334. The slope in the â€œTempâ€ direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the â€œSolar.Râ€ direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here.",
    "sentences": ["Select a model to see interpretation details, an example, and R Code help.", "Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\).", "Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model uses the same \\(X\\)-variable twice, once with a \\(\\beta_1 X_i\\) term and once with a \\(\\beta_2 X_i^2\\) term.", "The \\(X_i^2\\) term is called the â€œquadraticâ€ term.", "Parameter Effect \\(\\beta_0\\) Y-intercept of the Model.", "\\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\).", "\\(\\beta_2\\) Controls the concavity and â€œsteepnessâ€ of the Model: negative values face down, positive values face up; large values imply â€œsteeperâ€ parabolas and low values imply â€œflatterâ€ parabolas.", "Also involved in the position of the vertex, see \\(\\beta_1\\)â€™s explanation.", "An Example Using the airquality data set, we run the following â€œquadraticâ€ regression.", "Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...)."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Overview"
  },
  {
    "id": 75,
    "title": "ğŸ“ R Instructions",
    "url": "LinearRegression.html#rinstructions",
    "content": "R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the â€œOverviewâ€ section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the â€œcolumn (c) bindâ€ function and it joins together things as columns. Res =Â  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,Â  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals. Â panel=panel.smooth,Â  This places a lowess smoothing line on each scatterplot. col =Â  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(â€œcolor1â€,â€œcolor2â€, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for â€œlinear model.â€ Y Y must be a â€œnumericâ€ vector of the quantitative response variable. Â ~Â  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats â€œnumericâ€ variables as quantitative and â€œcharacterâ€ or â€œfactorâ€ variables as qualitative. R will automatcially recode qualitative variables to become â€œnumericâ€ variables using a 0,1 encoding. See the Explanation tab for details. Â + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details. Â + â€¦, * ... emphasizes that as many explanatory variables as are desired can be included in the model. Â data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(â€¦) function displays the results of an lm(â€¦) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(â€¦) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(â€¦). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min Â  -4.3818 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -2.2696 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.1344 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  1.7058 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  5.8752 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  t value To learn more about the â€œt valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  Pr(>|t|) The â€œPrâ€ stands for â€œProbabilityâ€ and the â€œ(> |t|)â€ stands for â€œmore extreme than the observed t-valueâ€. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the â€œp-valueâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. (Intercept) This always says â€œInterceptâ€ for any lm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero. Â  2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the â€œEstimateâ€ of the intercept (26.6248479) by its standard error (2.1829432). It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a â€œstarâ€. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + â€¦). Â  -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model. Â  0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the â€œEstimateâ€ by its standard error. It gives the â€œnumber of standard errorsâ€ away from zero that the â€œestimateâ€ has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + â€¦). Â  5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\). Â  2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot â€œ.â€ implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like. Â  0.0004029 This is the estimate of the coefficient of the interaction term. Â  0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE. Â 2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\). Â on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO. Â 0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car. Â Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data. Â 0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that â€œat least one is not.â€ Â 34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom. Â on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero. Â p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that â€œat least oneâ€ of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the â€œOverviewâ€ sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the â€œfitted-valueâ€ or â€œpredicted-valueâ€ for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). Â newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œpredictionâ€) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance canâ€™t go negative) feet to 60.96915 feet. predict( The R function predict(â€¦) allows you to use an lm(â€¦) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(â€¦) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(â€¦). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=â€¦ Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + â€¦) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = â€œconfidenceâ€) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The â€œfitâ€ is the predicted value. The â€œlwrâ€ is the lower bound. The â€œuprâ€ is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet.",
    "sentences": ["NOTE: These are general R Commands for all types of multiple linear regressions.", "See the â€œOverviewâ€ section for R Commands details about a specific multiple linear regression model.", "Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set.", "It requires that all columns of the data set be either numeric or factor classes.", "(Character classes will throw an error.) cbind( This is the â€œcolumn (c) bindâ€ function and it joins together things as columns.", "Res =Â  This is just any name you come up with, but Res is a good abbreviation for Residuals.", "mylm$residuals,Â  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set.", "YourDataSet), This puts the original data set along side the residuals.", "Â panel=panel.smooth,Â  This places a lowess smoothing line on each scatterplot.", "col =Â  specifies the colors of the dots."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "R Instructions"
  },
  {
    "id": 76,
    "title": "ğŸ“ Explanation",
    "url": "LinearRegression.html#explanation",
    "content": "Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BICâ€¦ There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (â€œR-squaredâ€). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The â€œsimplestâ€ but â€œbestâ€ model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it â€œan information criterion (AIC)â€ when he published the method and other people later began calling it the â€œAkaike Information Criterion.â€) Model Selection (Expand) pairs plots, added variable plots, and pattern recognitionâ€¦ Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots A useful visualization tool for model selection is the â€œpairs plot.â€ This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice thatâ€¦ the y-axis of each plot is found by locating the variable name (like â€œmpgâ€) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like â€œdispâ€) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldnâ€™t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Letâ€™s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander() Â  Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a modelâ€™s ability to generalize to new dataâ€¦ The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesnâ€™t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, letâ€™s remind ourselves why we use regression models in the first place. The main goal is to capture the â€œessenceâ€ of the data. In other words, the general pattern is what we are after. We want a model that tells us how â€œall suchâ€ data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the â€œcomplicated modelâ€ is overfitting the original data. It does not capture the â€œessenceâ€ of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-valueâ€¦ The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the modelâ€¦ The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read moreâ€¦) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) Outlier Analysis (Expand) Cookâ€™s Distances and Leverage Valuesâ€¦ The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs.Â fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cookâ€™s Distances The idea behind Cookâ€™s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article â€œDetection of Influential Observation in Linear Regressionâ€ (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, letâ€™s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2)) Â  1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the â€œdifferencesâ€ in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cookâ€™s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cookâ€™s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cookâ€™s Distances using the code cooks.distance(lmObject). Also, a graph of Cookâ€™s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of â€œleverageâ€ and is â€œpullingâ€ the regression toward itself. A value near 0 implies the point is just â€œone of manyâ€ and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that â€œminimizeâ€ the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)â€™s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the â€œhat matrixâ€ \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the â€œleverage valuesâ€ and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a â€œlot of pull,â€ and values close to 0 represent â€œlittle pull.â€ In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with â€œlots of leverageâ€ and a large â€œCookâ€™s Distanceâ€ are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regressionâ€¦ Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class.",
    "sentences": ["Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BICâ€¦ There are many measures of the quality of a regression model.", "One of the most popular measurements is the \\(R^2\\) value (â€œR-squaredâ€).", "The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model.", "Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1.", "Values close to 1 imply a very good model.", "Values close to 0 imply a very poor model.", "One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model.", "Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty.", "The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\).", "Consider the models below."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Explanation"
  },
  {
    "id": 77,
    "title": "ğŸ“ Assessing the Model Fit\r\n(Expand)",
    "url": "LinearRegression.html#assessingthemodelfitexpand",
    "content": "Assessing the Model Fit\r\n(Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BICâ€¦ There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (â€œR-squaredâ€). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The â€œsimplestâ€ but â€œbestâ€ model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it â€œan information criterion (AIC)â€ when he published the method and other people later began calling it the â€œAkaike Information Criterion.â€)",
    "sentences": ["\\(R^2\\), adjusted \\(R^2\\), AIC, BICâ€¦", "There are many measures of the quality of a regression model.", "One of the most popular measurements is the \\(R^2\\) value (â€œR-squaredâ€).", "The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model.", "Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1.", "Values close to 1 imply a very good model.", "Values close to 0 imply a very poor model.", "One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model.", "Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty.", "The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\)."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Assessing the Model Fit\r\n(Expand)"
  },
  {
    "id": 78,
    "title": "ğŸ“ Model Selection\r\n(Expand)",
    "url": "LinearRegression.html#modelselectionexpand",
    "content": "Model Selection\r\n(Expand) pairs plots, added variable plots, and pattern recognitionâ€¦ Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots A useful visualization tool for model selection is the â€œpairs plot.â€ This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice thatâ€¦ the y-axis of each plot is found by locating the variable name (like â€œmpgâ€) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like â€œdispâ€) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldnâ€™t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Letâ€™s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander() Â  Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446",
    "sentences": ["pairs plots, added variable plots, and pattern recognitionâ€¦", "Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\).", "They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model.", "However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model.", "Pairs Plots A useful visualization tool for model selection is the â€œpairs plot.â€ This plot shows all possible 2D scatterplots that can be created from a given dataset.", "Here is a pairs plot of the mtcars data set in R.", "Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice thatâ€¦ the y-axis of each plot is found by locating the variable name (like â€œmpgâ€) that is found to the left or right of the current plot.", "the x-axis of each plot is found by locating the variable name (like â€œdispâ€) that is found above or below each plot.", "the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot.", "Selecting a Model Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Model Selection\r\n(Expand)"
  },
  {
    "id": 79,
    "title": "ğŸ“ Model Validation\r\n(Expand)",
    "url": "LinearRegression.html#modelvalidationexpand",
    "content": "Model Validation\r\n(Expand) Verifying a modelâ€™s ability to generalize to new dataâ€¦ The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesnâ€™t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, letâ€™s remind ourselves why we use regression models in the first place. The main goal is to capture the â€œessenceâ€ of the data. In other words, the general pattern is what we are after. We want a model that tells us how â€œall suchâ€ data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the â€œcomplicated modelâ€ is overfitting the original data. It does not capture the â€œessenceâ€ of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data.",
    "sentences": ["Verifying a modelâ€™s ability to generalize to new dataâ€¦", "The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesnâ€™t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model.", "set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, letâ€™s remind ourselves why we use regression models in the first place.", "The main goal is to capture the â€œessenceâ€ of the data.", "In other words, the general pattern is what we are after.", "We want a model that tells us how â€œall suchâ€ data is created, not just the specific data we have sampled.", "So, the great test of a model is to see how well it works on a new sample of data.", "This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data.", "set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data.", "yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Model Validation\r\n(Expand)"
  },
  {
    "id": 80,
    "title": "ğŸ“ Interpretation\r\n(Expand)",
    "url": "LinearRegression.html#interpretationexpand",
    "content": "Interpretation\r\n(Expand) \\(\\beta_j\\) is the change in the average y-valueâ€¦ The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant.",
    "sentences": ["\\(\\beta_j\\) is the change in the average y-valueâ€¦", "The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Interpretation\r\n(Expand)"
  },
  {
    "id": 81,
    "title": "ğŸ“ Added Variable Plots\r\n(Expand)",
    "url": "LinearRegression.html#addedvariableplotsexpand",
    "content": "Added Variable Plots\r\n(Expand) When to add another \\(X\\)-variable to the modelâ€¦ The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read moreâ€¦) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0)",
    "sentences": ["When to add another \\(X\\)-variable to the modelâ€¦", "The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption.", "The regression relation between \\(Y\\) and \\(X\\) is linear.", "The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\).", "The variance of the error terms is constant over all \\(X\\) values.", "The \\(X\\) values can be considered fixed and measured without error.", "The error terms are independent.", "All important variables are included in the model.", "Checking the Assumptions The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot.", "Added variable plots can be used to determine if a new variable should be included in the model."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Added Variable Plots\r\n(Expand)"
  },
  {
    "id": 82,
    "title": "ğŸ“ Checking the Assumptions",
    "url": "LinearRegression.html#checkingtheassumptions",
    "content": "Checking the Assumptions The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. (Read moreâ€¦) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=â€œResidualsâ€, main=â€œâ€œ, cex.main=0.95, xaxt=â€˜nâ€™, yaxt=â€˜nâ€™, col=â€firebrickâ€) abline(h=0)",
    "sentences": ["The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot.", "Added variable plots can be used to determine if a new variable should be included in the model.", "(Read moreâ€¦) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model.", "The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis).", "If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression.", "The new variable should be included in the model.", "If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model.", "The new variable should continue to be left out of the model.", "The left column of plots below show scenarios where the new explanatory variable should be included in the model.", "The right column of plots show scenarios where the new explanatory variable should not be included in the model."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Checking the Assumptions"
  },
  {
    "id": 83,
    "title": "ğŸ“ Outlier Analysis\r\n(Expand)",
    "url": "LinearRegression.html#outlieranalysisexpand",
    "content": "Outlier Analysis\r\n(Expand) Cookâ€™s Distances and Leverage Valuesâ€¦ The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs.Â fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cookâ€™s Distances The idea behind Cookâ€™s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article â€œDetection of Influential Observation in Linear Regressionâ€ (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, letâ€™s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2)) Â  1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the â€œdifferencesâ€ in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cookâ€™s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cookâ€™s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cookâ€™s Distances using the code cooks.distance(lmObject). Also, a graph of Cookâ€™s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of â€œleverageâ€ and is â€œpullingâ€ the regression toward itself. A value near 0 implies the point is just â€œone of manyâ€ and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that â€œminimizeâ€ the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)â€™s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the â€œhat matrixâ€ \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the â€œleverage valuesâ€ and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a â€œlot of pull,â€ and values close to 0 represent â€œlittle pull.â€ In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with â€œlots of leverageâ€ and a large â€œCookâ€™s Distanceâ€ are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression.",
    "sentences": ["Cookâ€™s Distances and Leverage Valuesâ€¦", "The presence of outlying points in a regression can bias the regression estimates substantially.", "In simple linear regressions, the outlier are usually quite visible in a residuals vs.Â fitted-values plot.", "However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression.", "Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model.", "Cookâ€™s Distances The idea behind Cookâ€™s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\).", "As found in the original article â€œDetection of Influential Observation in Linear Regressionâ€ (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression.", "To understand this formula, letâ€™s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\).", "Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression.", "See the image below for a visual explanation."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Outlier Analysis\r\n(Expand)"
  },
  {
    "id": 84,
    "title": "ğŸ“ Inference for the Model Parameters\r\n(Expand)",
    "url": "LinearRegression.html#inferenceforthemodelparametersexpand",
    "content": "Inference for the Model Parameters\r\n(Expand) t Tests and F tests in multiple regressionâ€¦ Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class.",
    "sentences": ["t Tests and F tests in multiple regressionâ€¦", "Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously.", "t Tests The most typical tests for multiple regression are t Tests for a single coefficient.", "The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model.", "The significance of the single variable is thus assessed after accounting for the effect of all other variables.", "If a t Test of a single coefficient is significant, then that variable should remain in the model.", "If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides.", "Removing it from the model may be appropriate.", "However, whenever a single variable is removed from the model the other variables can change in their significance.", "F Tests Another approach to testing hypotheses about coefficients is to use an F Test."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Inference for the Model Parameters\r\n(Expand)"
  },
  {
    "id": 85,
    "title": "Logistic Regression",
    "url": "LogisticRegression.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Logistic Regression Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\). The explanatory variables can be either quantitative or qualitative. Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The â€œPâ€ stands for â€œProbability thatâ€¦â€ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1â€¦ This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)â€¦ in other words, the â€œ|â€ says â€œgivenâ€ and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the â€œnatural constantâ€ number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14â€¦) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -1.5651 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.6648 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -0.2460 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  0.7276 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  2.2691 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The test statistic is a regular old z-score. It is most reliable when the sample size is â€œlarge.â€ It is a measurement of the number of standard errors the estimate is from 0. Â  Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds. Â  1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds. Â  0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a â€œstarâ€. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about. Â  Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\). Â 43.230 Â on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates. Â 29.732 This can be calculated by sum(log(myglm$res^2)). Â on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, â€œA version of Akaikeâ€™s An Information Criterionâ€¦â€ The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model. Â 33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 * ## disp -0.014604 0.005168 -2.826 0.00471 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit ",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Logistic Regression Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\).", "The explanatory variables can be either quantitative or qualitative.", "Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable.", "Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The â€œPâ€ stands for â€œProbability thatâ€¦â€ \\(Y_i\\) The response variable.", "The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size.", "\\(= 1\\) Equals 1â€¦ This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual.", "\\(| x_i)\\) Given \\(x_i\\)â€¦ in other words, the â€œ|â€ says â€œgivenâ€ and \\(x_i\\) means the x-value of the current individual.", "\\(=\\) Equals sign.", "\\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the â€œnatural constantâ€ number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope.", "\\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value.", "It is the short hand notation for \\(P(Y_i = 1 |x_i)\\).", "(It is NOT the number 3.14â€¦) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly.", "Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead.", "The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\).", "The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\).", "Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.", "Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName.", "glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€.", "It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command.", "YÂ  Y is your binary response variable.", "It must consist of only 0â€™s and 1â€™s.", "Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s.", "~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X.", "X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1.", "Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X.", "In other words, one column of your dataset would be called Y and another column would be called X.", "Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression.", "It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course.", "summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName.", "Example output from a regression.", "Hover each piece to learn more.", "Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression.", "It allows you to verify that you ran what you thought you ran in the glm(â€¦).", "Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space.", "(This is a fairly complicated idea.) Min Â  -1.5651 â€œminâ€ gives the value of the residual that is furthest below the regression line.", "Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line.", "1Q Â  -0.6648 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile.", "Median Â  -0.2460 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero.", "Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary.", "This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed.", "This can also be seen in the maximum being much larger in magnitude than the minimum.", "3Q Â  0.7276 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile.", "In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally.", "Max Â  2.2691 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual.", "In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed.", "Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\).", "You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model.", "These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values.", "Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details.", "Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section."],
    "type": "page",
    "page_title": "Logistic Regression"
  },
  {
    "id": 86,
    "title": "ğŸ“ Simple Logistic Regression\r\nModel",
    "url": "LogisticRegression.html#simplelogisticregressionmodel",
    "content": "Simple Logistic Regression\r\nModel Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The â€œPâ€ stands for â€œProbability thatâ€¦â€ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1â€¦ This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)â€¦ in other words, the â€œ|â€ says â€œgivenâ€ and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the â€œnatural constantâ€ number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14â€¦) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse",
    "sentences": ["Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable.", "Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The â€œPâ€ stands for â€œProbability thatâ€¦â€ \\(Y_i\\) The response variable.", "The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size.", "\\(= 1\\) Equals 1â€¦ This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual.", "\\(| x_i)\\) Given \\(x_i\\)â€¦ in other words, the â€œ|â€ says â€œgivenâ€ and \\(x_i\\) means the x-value of the current individual.", "\\(=\\) Equals sign.", "\\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the â€œnatural constantâ€ number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope.", "\\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value.", "It is the short hand notation for \\(P(Y_i = 1 |x_i)\\).", "(It is NOT the number 3.14â€¦) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Simple Logistic Regression\r\nModel"
  },
  {
    "id": 87,
    "title": "ğŸ“ Overview",
    "url": "LogisticRegression.html#overview",
    "content": "Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The â€œPâ€ stands for â€œProbability thatâ€¦â€ \\(Y_i\\) The response variable. The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1â€¦ This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)â€¦ in other words, the â€œ|â€ says â€œgivenâ€ and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the â€œnatural constantâ€ number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14â€¦) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse",
    "sentences": ["The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The â€œPâ€ stands for â€œProbability thatâ€¦â€ \\(Y_i\\) The response variable.", "The â€œiâ€ denotes that this is the y-value for individual â€œiâ€, where â€œiâ€ is 1, 2, 3,â€¦ and so on up to \\(n\\), the sample size.", "\\(= 1\\) Equals 1â€¦ This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual.", "\\(| x_i)\\) Given \\(x_i\\)â€¦ in other words, the â€œ|â€ says â€œgivenâ€ and \\(x_i\\) means the x-value of the current individual.", "\\(=\\) Equals sign.", "\\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the â€œnatural constantâ€ number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope.", "\\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value.", "It is the short hand notation for \\(P(Y_i = 1 |x_i)\\).", "(It is NOT the number 3.14â€¦) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly.", "Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Overview"
  },
  {
    "id": 88,
    "title": "ğŸ“ R Instructions",
    "url": "LogisticRegression.html#rinstructions",
    "content": "R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -1.5651 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.6648 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -0.2460 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  0.7276 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  2.2691 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The test statistic is a regular old z-score. It is most reliable when the sample size is â€œlarge.â€ It is a measurement of the number of standard errors the estimate is from 0. Â  Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds. Â  1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds. Â  0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a â€œstarâ€. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about. Â  Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\). Â 43.230 Â on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates. Â 29.732 This can be calculated by sum(log(myglm$res^2)). Â on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, â€œA version of Akaikeâ€™s An Information Criterionâ€¦â€ The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model. Â 33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 * ## disp -0.014604 0.005168 -2.826 0.00471 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0â€™s or 1â€™s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == â€œBâ€ or height < 68. In other words, Y needs to be a collection of 0â€™s and 1â€™s. Â ~Â  The tilde is read â€œY on X.â€ X, X is some quantitative variable. Â data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(â€¦) function. curve( A function in R that draws a â€œcurveâ€ on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the â€œInterceptâ€ estimate from your logistic regression summary output. Â \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case â€œxâ€ in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula. Â exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator. Â add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(â€¦) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set. Â aes( The aes(â€¦) function stands for â€œaestheticsâ€ and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + The plus sign adds a new layer to the ggplot. Â  geom_point( Tells the plot to add the physical geometry of â€œpointsâ€ to the plot. ) Closing parenthesis for the geom_point() function. Â + Add another layer to the plot. Â  geom_smooth( Add a smoothing line to the graph. method=â€œglmâ€, This adds a general linear model to the graph. method.args = list(family=â€œbinomialâ€), This tells the method=â€œglmâ€ to choose specifically the â€œbinomialâ€ model, otherwise known as the â€œlogistic regressionâ€ model. Â se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function. Â + Add another layer to the ggplot. Â  theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),Â  The xVariableName is the same one you used in your glm(y ~ x, â€¦) statement for â€œxâ€. Input any desired value for the â€œsomeNumericValueâ€ spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016",
    "sentences": ["Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.", "Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName.", "glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€.", "It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command.", "YÂ  Y is your binary response variable.", "It must consist of only 0â€™s and 1â€™s.", "Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s.", "~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X.", "X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1.", "Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "R Instructions"
  },
  {
    "id": 89,
    "title": "ğŸ“ Perform a Logistic Regression",
    "url": "LogisticRegression.html#performalogisticregression",
    "content": "Perform a Logistic Regression Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -1.5651 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.6648 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  -0.2460 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q Â  0.7276 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  2.2691 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The test statistic is a regular old z-score. It is most reliable when the sample size is â€œlarge.â€ It is a measurement of the number of standard errors the estimate is from 0. Â  Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds. Â  1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a â€œstarâ€. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds. Â  0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a â€œstarâ€. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about. Â  Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\). Â 43.230 Â on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates. Â 29.732 This can be calculated by sum(log(myglm$res^2)). Â on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, â€œA version of Akaikeâ€™s An Information Criterionâ€¦â€ The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model. Â 33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required.",
    "sentences": ["Example output from a regression.", "Hover each piece to learn more.", "Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression.", "It allows you to verify that you ran what you thought you ran in the glm(â€¦).", "Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space.", "(This is a fairly complicated idea.) Min Â  -1.5651 â€œminâ€ gives the value of the residual that is furthest below the regression line.", "Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line.", "1Q Â  -0.6648 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile.", "Median Â  -0.2460 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero.", "Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Perform a Logistic Regression"
  },
  {
    "id": 90,
    "title": "ğŸ“ Diagnose the Goodness-of-Fit",
    "url": "LogisticRegression.html#diagnosethegoodnessoffit",
    "content": "Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 * ## disp -0.014604 0.005168 -2.826 0.00471 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression.",
    "sentences": ["There are two ways to check the goodness of fit of a logistic regression model.", "Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test.", "library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function.", "You may need to run the code: install.packages(â€œResourceSelectionâ€) first.", "hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test.", "See the â€œExplanationâ€ file to learn about this test.", "YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously.", "$y,Â  ALWAYS type a â€œyâ€ here.", "This gives you the actual binary (0,1) y-values of your logistic regression.", "The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Diagnose the Goodness-of-Fit"
  },
  {
    "id": 91,
    "title": "ğŸ“ Plot the Regression",
    "url": "LogisticRegression.html#plottheregression",
    "content": "Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0â€™s or 1â€™s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == â€œBâ€ or height < 68. In other words, Y needs to be a collection of 0â€™s and 1â€™s. Â ~Â  The tilde is read â€œY on X.â€ X, X is some quantitative variable. Â data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(â€¦) function. curve( A function in R that draws a â€œcurveâ€ on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the â€œInterceptâ€ estimate from your logistic regression summary output. Â \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case â€œxâ€ in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula. Â exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator. Â add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot.",
    "sentences": ["Base R ggplot2", "plot( The plot function allows us to draw a scatterplot where the y-axis is only 0â€™s or 1â€™s and the x-axis is quantitative.", "Y Y should be some logical statement like Fail > 0 or sex == â€œBâ€ or height < 68.", "In other words, Y needs to be a collection of 0â€™s and 1â€™s.", "Â ~Â  The tilde is read â€œY on X.â€ X, X is some quantitative variable.", "Â data= Tell the plot which data set to use.", "X and Y are columns of that data set.", "YourDataSet The name of your data set.", ") Closing parenthesis for plot(â€¦) function.", "curve( A function in R that draws a â€œcurveâ€ on a plot."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Plot the Regression"
  },
  {
    "id": 92,
    "title": "ğŸ“ Predict Probabilities",
    "url": "LogisticRegression.html#predictprobabilities",
    "content": "Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016",
    "sentences": ["To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code", "myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016"],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Predict Probabilities"
  },
  {
    "id": 93,
    "title": "ğŸ“ Explanation",
    "url": "LogisticRegression.html#explanation",
    "content": "Explanation Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal). The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\). Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page. Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful. Checking Model Assumptions The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit. Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease.",
    "sentences": ["Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal).", "The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario.", "The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\).", "Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation.", "The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\).", "It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation.", "The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\).", "Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page.", "Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression.", "If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\)."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Explanation"
  },
  {
    "id": 94,
    "title": "ğŸ“ The Model",
    "url": "LogisticRegression.html#themodel",
    "content": "The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\).",
    "sentences": ["Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario.", "The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation", "\\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\]", "The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\)."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "The Model"
  },
  {
    "id": 95,
    "title": "ğŸ“ Interpretation",
    "url": "LogisticRegression.html#interpretation",
    "content": "Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page.",
    "sentences": ["This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation.", "The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\).", "It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation.", "The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\).", "Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Interpretation"
  },
  {
    "id": 96,
    "title": "ğŸ“ Hypothesis Testing",
    "url": "LogisticRegression.html#hypothesistesting",
    "content": "Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful.",
    "sentences": ["Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression.", "If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\).", "In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\).", "If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Hypothesis Testing"
  },
  {
    "id": 97,
    "title": "ğŸ“ Checking Model Assumptions",
    "url": "LogisticRegression.html#checkingmodelassumptions",
    "content": "Checking Model Assumptions The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit.",
    "sentences": ["The model assumptions are not as clear in logistic regression as they are in linear regression.", "For our purposes we will focus only on considering the goodness of fit of the logistic regression model.", "If the model appears to fit the data well, then it will be assumed to be appropriate.", "Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses.", "In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Checking Model Assumptions"
  },
  {
    "id": 98,
    "title": "ğŸ“ Prediction",
    "url": "LogisticRegression.html#prediction",
    "content": "Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease.",
    "sentences": ["One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\).", "This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual.", "For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Prediction"
  },
  {
    "id": 99,
    "title": "ğŸ“ Multiple Logistic Regression\r\nModel",
    "url": "LogisticRegression.html#multiplelogisticregressionmodel",
    "content": "Multiple Logistic Regression\r\nModel Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two. Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2Â  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. â€¦, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -2.9446 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.2364 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.0000 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q Â  0.2776 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  1.9968 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The Z-score testing the hypothesis that \\(\\beta_j=0\\) Â  Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\). Â  1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\). Â  4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\). Â  3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\). Â  0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\). Â  0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a â€œstarâ€. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\). Â  471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option. Â  Null Deviance: The deviance of the null model. Â 800.44 In this case, the null deviance is 800.44 Â on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression. Â 256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit. Â on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model. Â 272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -4.97603 0.65316 -7.618 2.57e-14 *** ## Time 0.39111 0.04979 7.854 4.02e-15 *** ## Diet2 0.58283 1.04061 0.560 0.5754 ## Diet3 -8.44476 4.32324 -1.953 0.0508 . ## Diet4 -67.41995 3768.10023 -0.018 0.9857 ## Time:Diet2 0.02391 0.08679 0.275 0.7830 ## Time:Diet3 1.20955 0.51249 2.360 0.0183 * ## Time:Diet4 8.83168 471.01259 0.019 0.9850 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,Â  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, â€¦), X2 = c(Value 1, Value 2, â€¦), â€¦) command. You should see the GSS example file for an example of how to use this function. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the â€œbâ€ vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(â€œSomeColorâ€,â€œDifferentColorâ€,â€¦) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(â€¦) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==â€œBâ€. Â ~Â  The formula operator in R. X1, The first x-variable in your glm code. Â data = YourDataSet, Specify the name of your data set. Â pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(â€¦) function computes e^(stuff) in R and stands for the â€œexponential function.â€ (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(â€¦) function requires that you call the x-variable â€œxâ€ although you can change this behavior using xname=â€œSomeOtherNameâ€ if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)). Â col = palette()[1], Pull the first color from the color palette for the first curve. Â add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters. Â col = palette()[2], Set the color of this second curve to the second color in the palette. Â add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. â€œtoprightâ€, Typically the legend is placed in the top-right corner of the graph. But it could be placed â€œtopâ€, â€œtopleftâ€, â€œleftâ€, â€œbottomleftâ€, â€œbottomâ€, â€œcenterâ€, â€œrightâ€, or â€œbottomrightâ€. Â legend = Why you have to write legend again, no one knows, but you do. c(â€œLable 1â€, â€œLabel 2â€), These are the values that will appear in the legend, one entry on each line of the legend. Â col = palette(), This specifies the colors of the symbols in the legend. First color goes with â€œLabel 1â€, and second goes with â€œLabel 2â€ and so on. Â lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist. Â bty = Specifies the â€œbox typeâ€ (bty) that should be drawn around the legend. â€˜nâ€™) By placing a â€œnoâ€ option (â€˜nâ€™) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot. Â aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + Add a layer to the ggplot. Â  geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function. Â + Add a layer to the ggplot. Â  geom_smooth(method=â€œglmâ€, method.args=list(family=â€œbinomialâ€), se=FALSE, Add the logistic regression curve to the plot. Â  aes(color=â€œX2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function. Â + Add a layer to the plot. Â  theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x'",
    "sentences": ["Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two.", "Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly.", "Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead.", "The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\).", "The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\).", "Examples: GSS", "R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.", "Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName.", "glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€.", "It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Multiple Logistic Regression\r\nModel"
  },
  {
    "id": 100,
    "title": "ğŸ“ Overview",
    "url": "LogisticRegression.html#overview",
    "content": "Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS",
    "sentences": ["The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly.", "Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead.", "The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\).", "The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\).", "Examples: GSS"],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Overview"
  },
  {
    "id": 101,
    "title": "ğŸ“ R Instructions",
    "url": "LogisticRegression.html#rinstructions",
    "content": "R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command. Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. YÂ  Y is your binary response variable. It must consist of only 0â€™s and 1â€™s. Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s. ~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2Â  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. â€¦, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model. Â data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X. Â family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -2.9446 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.2364 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.0000 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q Â  0.2776 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  1.9968 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The Z-score testing the hypothesis that \\(\\beta_j=0\\) Â  Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\). Â  1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\). Â  4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\). Â  3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\). Â  0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\). Â  0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a â€œstarâ€. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\). Â  471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option. Â  Null Deviance: The deviance of the null model. Â 800.44 In this case, the null deviance is 800.44 Â on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression. Â 256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit. Â on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model. Â 272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -4.97603 0.65316 -7.618 2.57e-14 *** ## Time 0.39111 0.04979 7.854 4.02e-15 *** ## Diet2 0.58283 1.04061 0.560 0.5754 ## Diet3 -8.44476 4.32324 -1.953 0.0508 . ## Diet4 -67.41995 3768.10023 -0.018 0.9857 ## Time:Diet2 0.02391 0.08679 0.275 0.7830 ## Time:Diet3 1.20955 0.51249 2.360 0.0183 * ## Time:Diet4 8.83168 471.01259 0.019 0.9850 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,Â  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =Â  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,Â  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, â€¦), X2 = c(Value 1, Value 2, â€¦), â€¦) command. You should see the GSS example file for an example of how to use this function. type = â€œresponseâ€) The type = â€œresponseâ€ options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the â€œbâ€ vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(â€œSomeColorâ€,â€œDifferentColorâ€,â€¦) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(â€¦) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==â€œBâ€. Â ~Â  The formula operator in R. X1, The first x-variable in your glm code. Â data = YourDataSet, Specify the name of your data set. Â pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(â€¦) function computes e^(stuff) in R and stands for the â€œexponential function.â€ (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(â€¦) function requires that you call the x-variable â€œxâ€ although you can change this behavior using xname=â€œSomeOtherNameâ€ if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)). Â col = palette()[1], Pull the first color from the color palette for the first curve. Â add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters. Â col = palette()[2], Set the color of this second curve to the second color in the palette. Â add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. â€œtoprightâ€, Typically the legend is placed in the top-right corner of the graph. But it could be placed â€œtopâ€, â€œtopleftâ€, â€œleftâ€, â€œbottomleftâ€, â€œbottomâ€, â€œcenterâ€, â€œrightâ€, or â€œbottomrightâ€. Â legend = Why you have to write legend again, no one knows, but you do. c(â€œLable 1â€, â€œLabel 2â€), These are the values that will appear in the legend, one entry on each line of the legend. Â col = palette(), This specifies the colors of the symbols in the legend. First color goes with â€œLabel 1â€, and second goes with â€œLabel 2â€ and so on. Â lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist. Â bty = Specifies the â€œbox typeâ€ (bty) that should be drawn around the legend. â€˜nâ€™) By placing a â€œnoâ€ option (â€˜nâ€™) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot. Â aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(â€¦) function. ) Closing parenthesis for ggplot(â€¦) function. Â + Add a layer to the ggplot. Â  geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function. Â + Add a layer to the ggplot. Â  geom_smooth(method=â€œglmâ€, method.args=list(family=â€œbinomialâ€), se=FALSE, Add the logistic regression curve to the plot. Â  aes(color=â€œX2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function. Â + Add a layer to the plot. Â  theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x'",
    "sentences": ["Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.", "Â <-Â  This is the â€œleft arrowâ€ assignment operator that stores the results of your glm() code into YourGlmName.", "glm( glm( is an R function that stands for â€œGeneral Linear Modelâ€.", "It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command.", "YÂ  Y is your binary response variable.", "It must consist of only 0â€™s and 1â€™s.", "Since TRUEâ€™s = 1â€™s and FALSEâ€™s = 0â€™s in R, Y could be a logical statement like (Price > 100) or (Animal == â€œCatâ€) if your Y-variable wasnâ€™t currently coded as 0â€™s and 1â€™s.", "~Â  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X.", "X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1.", "* The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "R Instructions"
  },
  {
    "id": 102,
    "title": "ğŸ“ Perform the Logistic Regression",
    "url": "LogisticRegression.html#performthelogisticregression",
    "content": "Perform the Logistic Regression To perform a logistic regression in R use the commands Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(â€¦). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min Â  -2.9446 â€œminâ€ gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q Â  -0.2364 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median Â  0.0000 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q Â  0.2776 â€œ3Qâ€ gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max Â  1.9968 â€œMaxâ€ gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(â€¦) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(â€¦) function and displayed in this part of the output along with standard errors, t-values, and p-values. Â  Estimate To learn more about the â€œEstimatesâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œEstimating the Model Parametersâ€ section for details. Error To learn more about the â€œStandard Errorsâ€ of the â€œCoefficientsâ€ see the â€œExplanationâ€ tab, â€œInference for the Model Parametersâ€ section. Â  z value The Z-score testing the hypothesis that \\(\\beta_j=0\\) Â  Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says â€œInterceptâ€ for any glm(â€¦) you run in R. That is because R always assumes there is a y-intercept for your regression function. Â  -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero. Â  0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit. Â  0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a â€œstarâ€. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\). Â  1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\). Â  4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\). Â  3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\). Â  0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\). Â  0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a â€œstarâ€. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, â€¦). Â  8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\). Â  471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€™*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 These â€œcodesâ€ explain what significance level the p-value is smaller than based on how many â€œstarsâ€ * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option. Â  Null Deviance: The deviance of the null model. Â 800.44 In this case, the null deviance is 800.44 Â on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression. Â 256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit. Â on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model. Â 272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the â€œEstimatesâ€ of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary. Â 20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required.",
    "sentences": ["To perform a logistic regression in R use the commands", "Example output from a regression.", "Hover each piece to learn more.", "Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(â€¦) â€œcallâ€ that you made when performing your regression.", "It allows you to verify that you ran what you thought you ran in the glm(â€¦).", "Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space.", "(This is a fairly complicated idea.) Min Â  -2.9446 â€œminâ€ gives the value of the residual that is furthest below the regression line.", "Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line.", "1Q Â  -0.2364 â€œ1Qâ€ gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile.", "Median Â  0.0000 â€œMedianâ€ gives the median of the residuals, which would ideally would be about equal to zero."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Perform the Logistic Regression"
  },
  {
    "id": 103,
    "title": "ğŸ“ Diagnose the Goodness-of-Fit",
    "url": "LogisticRegression.html#diagnosethegoodnessoffit",
    "content": "Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(â€œResourceSelectionâ€) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the â€œExplanationâ€ file to learn about this test. YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously. $y,Â  ALWAYS type a â€œyâ€ here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code. $fitted,Â  ALWAYS type â€œfittedâ€ here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The â€œg=10â€ is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that â€œthe logistic regression is a good fit.â€ So we actually donâ€™t want to â€œreject the nullâ€ in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,Â  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,Â  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -4.97603 0.65316 -7.618 2.57e-14 *** ## Time 0.39111 0.04979 7.854 4.02e-15 *** ## Diet2 0.58283 1.04061 0.560 0.5754 ## Diet3 -8.44476 4.32324 -1.953 0.0508 . ## Diet4 -67.41995 3768.10023 -0.018 0.9857 ## Time:Diet2 0.02391 0.08679 0.275 0.7830 ## Time:Diet3 1.20955 0.51249 2.360 0.0183 * ## Time:Diet4 8.83168 471.01259 0.019 0.9850 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must â€œreject the nullâ€ and conclude a poor fit, which implies that we should not trust the results of the logistic regression.",
    "sentences": ["There are two ways to check the goodness of fit of a logistic regression model.", "Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test.", "library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function.", "You may need to run the code: install.packages(â€œResourceSelectionâ€) first.", "hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test.", "See the â€œExplanationâ€ file to learn about this test.", "YourGlmName YourGlmName is the name of your glm(â€¦) code that you created previously.", "$y,Â  ALWAYS type a â€œyâ€ here.", "This gives you the actual binary (0,1) y-values of your logistic regression.", "The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a â€œgood fit.â€ YourGlmName YourGlmName is the name you used to save the results of your glm(â€¦) code."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Diagnose the Goodness-of-Fit"
  },
  {
    "id": 104,
    "title": "ğŸ“ Predict Probabilities",
    "url": "LogisticRegression.html#predictprobabilities",
    "content": "Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code ## 1 ## 0.7314498",
    "sentences": ["To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code", "## 1 ## 0.7314498"],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Predict Probabilities"
  },
  {
    "id": 105,
    "title": "ğŸ“ Plot the Regression",
    "url": "LogisticRegression.html#plottheregression",
    "content": "Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the â€œbâ€ vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(â€œSomeColorâ€,â€œDifferentColorâ€,â€¦) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(â€¦) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==â€œBâ€. Â ~Â  The formula operator in R. X1, The first x-variable in your glm code. Â data = YourDataSet, Specify the name of your data set. Â pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(â€¦) function computes e^(stuff) in R and stands for the â€œexponential function.â€ (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(â€¦) function requires that you call the x-variable â€œxâ€ although you can change this behavior using xname=â€œSomeOtherNameâ€ if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)). Â col = palette()[1], Pull the first color from the color palette for the first curve. Â add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters. Â col = palette()[2], Set the color of this second curve to the second color in the palette. Â add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. â€œtoprightâ€, Typically the legend is placed in the top-right corner of the graph. But it could be placed â€œtopâ€, â€œtopleftâ€, â€œleftâ€, â€œbottomleftâ€, â€œbottomâ€, â€œcenterâ€, â€œrightâ€, or â€œbottomrightâ€. Â legend = Why you have to write legend again, no one knows, but you do. c(â€œLable 1â€, â€œLabel 2â€), These are the values that will appear in the legend, one entry on each line of the legend. Â col = palette(), This specifies the colors of the symbols in the legend. First color goes with â€œLabel 1â€, and second goes with â€œLabel 2â€ and so on. Â lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist. Â bty = Specifies the â€œbox typeâ€ (bty) that should be drawn around the legend. â€˜nâ€™) By placing a â€œnoâ€ option (â€˜nâ€™) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n')",
    "sentences": ["Base R ggplot2", "b <- coef(myglm) This stores the estimated coefficients from the regression into the â€œbâ€ vector.", "palette( The palette() function allows you to specify the colors R chooses for the plot.", "c(â€œSomeColorâ€,â€œDifferentColorâ€,â€¦) Specify as many colors for the palette as you wish.", "R will choose a color for each group that you specify later on in your plot(â€¦) code.", ")) Closing parentheses.", "plot( Create the binary scatterplot for the logistic regression.", "Y Note that Y must be binary (0,1) values.", "If it is not, use a logical statement to make it binary, like height>60 or sex==â€œBâ€.", "Â ~Â  The formula operator in R."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Plot the Regression"
  },
  {
    "id": 106,
    "title": "ğŸ“ Explanation",
    "url": "LogisticRegression.html#explanation",
    "content": "Explanation Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative. The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations. Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant. Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression. Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\).",
    "sentences": ["Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative.", "The Model Very little changes in multiple logistic regression from Simple Logistic Regression.", "The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations.", "Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant.", "Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression.", "Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression.", "The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\)."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Explanation"
  },
  {
    "id": 107,
    "title": "ğŸ“ The Model",
    "url": "LogisticRegression.html#themodel",
    "content": "The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations.",
    "sentences": ["Very little changes in multiple logistic regression from Simple Logistic Regression.", "The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation", "\\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\]", "The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "The Model"
  },
  {
    "id": 108,
    "title": "ğŸ“ Interpretation",
    "url": "LogisticRegression.html#interpretation",
    "content": "Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant.",
    "sentences": "The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant.",
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Interpretation"
  },
  {
    "id": 109,
    "title": "ğŸ“ Checking the Model Assumptions",
    "url": "LogisticRegression.html#checkingthemodelassumptions",
    "content": "Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression.",
    "sentences": "Diagnostics are the same in multiple logistic regression as they are in simple logistic regression.",
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Checking the Model Assumptions"
  },
  {
    "id": 110,
    "title": "ğŸ“ Prediction",
    "url": "LogisticRegression.html#prediction",
    "content": "Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\).",
    "sentences": ["The idea behind prediction in multiple logistic regression is the same as in simple logistic regression.", "The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\)."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Prediction"
  },
  {
    "id": 111,
    "title": "Making Inference",
    "url": "MakingInference.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Making Inference It is common to only have a sample of data from some population of interest. Using the information from the sample to reach conclusions about the population is called making inference. When statistical inference is performed properly, the conclusions about the population are almost always correct. Hypothesis Testing One of the great focal points of statistics concerns hypothesis testing. Science generally agrees upon the principle that truth must be uncovered by the process of elimination. The process begins by establishing a starting assumption, or null hypothesis (\\(H_0\\)). Data is then collected and the evidence against the null hypothesis is measured, typically with the \\(p\\)-value. The \\(p\\)-value becomes small (gets close to zero) when the evidence is extremely different from what would be expected if the null hypothesis were true. When the \\(p\\)-value is below the significance level \\(\\alpha\\) (typically \\(\\alpha=0.05\\)) the null hypothesis is abandoned (rejected) in favor of a competing alternative hypothesis (\\(H_a\\)). Click for an Example The current hypothesis may be that the world is flat. Then someone who thinks otherwise sets sail in a boat, gathers some evidence, and when there is sufficient evidence in the data to disbelieve the current hypothesis, we conclude the world is not flat. In light of this new knowledge, we shift our belief to the next working hypothesis, that the world is round. After a while, someone gathers more evidence and shows that the world is not round, and we move to the next working hypothesis, that it is oblate spheroid, i.e., a sphere that is squashed at its poles and swollen at the equator. This process of elimination is called hypothesis testing. The process begins by establishing a null hypothesis (denoted symbolically by \\(H_0\\)) which represents the current opinion, status quo, or what we will believe if the evidence is not sufficient to suggest otherwise. The alternative hypothesis (denoted symbolically by \\(H_a\\)) designates what we will believe if there is sufficient evidence in the data to discredit, or â€œreject,â€ the null hypothesis. See the BYU-I Math 221 Stats Wiki for another example. Managing Decision Errors When the \\(p\\)-value approaches zero, one of two things must be occurring. Either an extremely rare event has happened or the null hypothesis is incorrect. Since the second option, that the null hypothesis is incorrect, is the more plausible option, we reject the null hypothesis in favor of the alternative whenever the \\(p\\)-value is close to zero. It is important to remember that rejecting the null hypothesis could however be a mistake. Â  \\(H_0\\) True \\(H_0\\) False Reject \\(H_0\\) Type I Error Correct Decision Accept \\(H_0\\) Correct Decision Type II Error Type I Error, Significance Level, Confidence and \\(\\alpha\\) A Type I Error is defined as rejecting the null hypothesis when it is actually true. (Throwing away truth.) The significance level, \\(\\alpha\\), of a hypothesis test controls the probability of a Type I Error. The typical value of \\(\\alpha = 0.05\\) came from tradition and is a somewhat arbitrary value. Any value from 0 to 1 could be used for \\(\\alpha\\). When deciding on the level of \\(\\alpha\\) for a particular study it is important to remember that as \\(\\alpha\\) increases, the probability of a Type I Error increases, and the probability of a Type II Error decreases. When \\(\\alpha\\) gets smaller, the probability of a Type I Error gets smaller, while the probability of a Type II Error increases. Confidence is defined as \\(1-\\alpha\\) or the opposite of a Type I error. That is the probability of accepting the NULL when it is in fact true. Type II Errors, \\(\\beta\\), and Power It is also possible to make a Type II Error, which is defined as failing to reject the null hypothesis when it is actually false. (Failing to move to truth.) The probability of a Type II Error, \\(\\beta\\), is often unknown. However, practitioners often make an assumption about a detectable difference that is desired which then allows \\(\\beta\\) to be prescribed much like \\(\\alpha\\). In essence, the detectable difference prescribes a fixed value for \\(H_a\\). We can then talk about the power of of a hypothesis test, which is 1 minus the probability of a Type II Error, \\(\\beta\\). See Statistical Power in Wikipedia for a starting source if your are interested. This website provides a novel interactive visualization to help you understand power. It does require a little background on Cohenâ€™s D. Sufficient Evidence Statistics comes in to play with hypothesis testing by defining the phrase â€œsufficient evidence.â€ When there is â€œsufficient evidenceâ€ in the data, the null hypothesis is rejected and the alternative hypothesis becomes the working hypothesis. There are many statistical approaches to this problem of measuring the significance of evidence, but in almost all cases, the final measurement of evidence is given by the \\(p\\)-value of the hypothesis test. The \\(p\\)-value of a test is defined as the probability of the evidence being as extreme or more extreme than what was observed assuming the null hypothesis is true. This is an interesting phrase that is at first difficult to understand. The â€œas extreme or more extremeâ€ part of the definition of the \\(p\\)-value comes from the idea that the null hypothesis will be rejected when the evidence in the data is extremely inconsistent with the null hypothesis. If the data is not extremely different from what we would expect under the null hypothesis, then we will continue to believe the null hypothesis. Although, it is worth emphasizing that this does not prove the null hypothesis to be true. Evidence not Proof Hypothesis testing allows us a formal way to decide if we should â€œconclude the alternativeâ€ or â€œcontinue to accept the null.â€ It is important to remember that statistics (and science) cannot prove anything, just show evidence towards. Thus we never really prove a hypothesis is true, we simply show evidence towards or against a hypothesis. Calculating the \\(p\\)-Value Recall that the \\(p\\)-value measures how extremely the data (the evidence) differs from what is expected under the null hypothesis. Small \\(p\\)-values lead us to discard (reject) the null hypothesis. A \\(p\\)-value can be calculated whenever we have two things. A test statistic, which is a way of measuring how â€œfarâ€ the observed data is from what is expected under the null hypothesis. The sampling distribution of the test statistic, which is the theoretical distribution of the test statistic over all possible samples, assuming the null hypothesis was true. Visit the Math 221 textbook for an explanation. A distribution describes how data is spread out. When we know the shape of a distribution, we know which values are possible, but more importantly which values are most plausible (likely) and which are the least plausible (unlikely). The \\(p\\)-value uses the sampling distribution of the test statistic to measure the probability of the observed test statistic being as extreme or more extreme than the one observed. All \\(p\\)-value computation methods can be classified into two broad categories, parametric methods and nonparametric methods. Parametric Methods Parametric methods assume that, under the null hypothesis, the test statistic follows a specific theoretical parametric distribution. Parametric methods are typically more statistically powerful than nonparametric methods, but necessarily force more assumptions on the data. Parametric distributions are theoretical distributions that can be described by a mathematical function. There are many theoretical distributions. (See the List of Probability Distributions in Wikipedia for details.) Four of the most widely used parametric distributions are: The Normal Distribution Click for Details One of the most important distributions in statistics is the normal distribution. It is a theoretical distribution that approximates the distributions of many real life data distributions, like heights of people, heights of corn plants, baseball batting averages, lengths of gestational periods for many species including humans, and so on. More importantly, the sampling distribution of the sample mean \\(\\bar{x}\\) is normally distributed in two important scenarios. The parent population is normally distributed. The sample size is sufficiently large. (Often \\(n\\geq 30\\) is sufficient, but this is a general rule of thumb that is sometimes insufficient.) Mathematical Formula \\[ f(x | \\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} \\] The symbols \\(\\mu\\) and \\(\\sigma\\) are the two parameters of this distribution. The parameter \\(\\mu\\) controls the center, or mean of the distribution. The parameter \\(\\sigma\\) controls the spread, or standard deviation of the distribution. Graphical Form Comments The usefulness of the normal distribution is that we know which values of data are likely and which are unlikely by just knowing three things: that the data is normally distributed, \\(\\mu\\), the mean of the distribution, and \\(\\sigma\\), the standard deviation of the distribution. For example, as shown in the plot above, a value of \\(x=-8\\) would be very probable for the normal distribution with \\(\\mu=-5\\) and \\(\\sigma=2\\) (light blue curve). However, the value of \\(x=-8\\) would be very unlikely to occur in the normal distribution with \\(\\mu=3\\) and \\(\\sigma=3\\) (gray curve). In fact, \\(x=-8\\) would be even more unlikely an occurance for the \\(\\mu=0\\) and \\(\\sigma=1\\) distribution (dark blue curve). The Chi Squared Distribution Click for Details The chi squared distribution only allows for values that are greater than or equal to zero. While it has a few real life applications, by far its greatest use is theoretical. The test statistic of the chi squared test is distributed according to a chi squared distribution. Mathematical Formula \\[ f(x|p) = \\frac{1}{\\Gamma(p/2)2^{p/2}}x^{(p/2)-1}e^{-x/2} \\] The only parameter of the chi squared distribution is \\(p\\), which is known as the degrees of freedom. Larger values of the parameter \\(p\\) move the center of the chi squared distribution farther to the right. As \\(p\\) goes to infinity, the chi squared distribution begins to look more and more normal in shape. Note that the symbol in the denominator of the chi squared distribution, \\(\\Gamma(p/2)\\), is the Gamma function of \\(p/2\\). (See Gamma Function in Wikipedia for details.) Graphical Form Comments It is important to remember that the chi squared distribution is only defined for \\(x\\geq 0\\) and for positive values of the parameter \\(p\\). This is unlike the normal distribution which is defined for all numbers \\(x\\) from negative infinity to positive infinity as well as for all values of \\(\\mu\\) from negative infinity to positive infinity. The t Distribution Click for Details A close friend of the normal distribution is the t distribution. Although the t distribution is seldom used to model real life data, the distribution is used extensively in hypothesis testing. For example, it is the sampling distribution of the one sample t statistic. It also shows up in many other places, like in regression, in the independent samples t test, and in the paired samples t test. Mathematical Formula \\[ f(x|p) = \\frac{\\Gamma\\left(\\frac{p+1}{2}\\right)}{\\Gamma\\left(\\frac{p}{2}\\right)}\\frac{1}{\\sqrt{p\\pi}}\\frac{1}{\\left(1 + \\left(\\frac{x^2}{p}\\right)\\right)^{(p+1)/2}} \\] Notice that, similar to the chi squared distribution, the t distribution has only one parameter, the degrees of freedom \\(p\\). As the single parameter \\(p\\) is varied from \\(p=1\\), to \\(p=2\\), â€¦, \\(p=5\\), and larger and larger numbers, the resulting distribution becomes more and more normal in shape. Note that the expressions \\(\\Gamma\\left(\\frac{p+1}{2}\\right)\\) and \\(\\Gamma(p/2)\\), refer to the Gamma function. (See Gamma Function in Wikipedia for details.) Graphical Form Comments When the degrees of freedom \\(p=30\\), the resulting t distribution is almost indistinguishable visually from the normal distribution. This is one of the reasons that a sample size of 30 is often used as a rule of thumb for the sample size being â€œlarge enoughâ€ to assume the sampling distribution of the sample mean is approximately normal. The F Distribution Click for Details Another commonly used distribution for test statistics, like in ANOVA and regression, is the F distribution. Technically speaking, the F distribution is the ratio of two chi squared random variables that are each divided by their respective degrees of freedom. Mathematical Formula \\[ f(x|p_1,p_2) = \\frac{\\Gamma\\left(\\frac{p_1+p_2}{2}\\right)}{\\Gamma\\left(\\frac{p_1}{2}\\right)\\Gamma\\left(\\frac{p_2}{2}\\right)}\\frac{\\left(\\frac{p_1}{p_2}\\right)^{p_1/2}x^{(p_1-2)/2}}{\\left(1+\\left(\\frac{p_1}{p_2}\\right)x\\right)^{(p_1+p_2)/2}} \\] where \\(x\\geq 0\\) and the parameters \\(p_1\\) and \\(p_2\\) are the â€œnumeratorâ€ and â€œdenominatorâ€ degrees of freedom, respectively. Graphical Form Comments The effects of the parameters \\(p_1\\) and \\(p_2\\) on the F distribution are complicated, but generally speaking, as they both increase the distribution becomes more and more normal in shape. Nonparametric Methods Nonparametric methods place minimal assumptions on the distribution of data. They allow the data to â€œspeak for itself.â€ They are typically less powerful than the parametric alternatives, but are more broadly applicable because fewer assumptions need to be satisfied. Nonparametric methods include Rank Sum Tests and Permutation Tests. Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Making Inference It is common to only have a sample of data from some population of interest.", "Using the information from the sample to reach conclusions about the population is called making inference.", "When statistical inference is performed properly, the conclusions about the population are almost always correct.", "Hypothesis Testing One of the great focal points of statistics concerns hypothesis testing.", "Science generally agrees upon the principle that truth must be uncovered by the process of elimination.", "The process begins by establishing a starting assumption, or null hypothesis (\\(H_0\\)).", "Data is then collected and the evidence against the null hypothesis is measured, typically with the \\(p\\)-value.", "The \\(p\\)-value becomes small (gets close to zero) when the evidence is extremely different from what would be expected if the null hypothesis were true.", "When the \\(p\\)-value is below the significance level \\(\\alpha\\) (typically \\(\\alpha=0.05\\)) the null hypothesis is abandoned (rejected) in favor of a competing alternative hypothesis (\\(H_a\\)).", "Click for an Example The current hypothesis may be that the world is flat.", "Then someone who thinks otherwise sets sail in a boat, gathers some evidence, and when there is sufficient evidence in the data to disbelieve the current hypothesis, we conclude the world is not flat.", "In light of this new knowledge, we shift our belief to the next working hypothesis, that the world is round.", "After a while, someone gathers more evidence and shows that the world is not round, and we move to the next working hypothesis, that it is oblate spheroid, i.e., a sphere that is squashed at its poles and swollen at the equator.", "This process of elimination is called hypothesis testing.", "The process begins by establishing a null hypothesis (denoted symbolically by \\(H_0\\)) which represents the current opinion, status quo, or what we will believe if the evidence is not sufficient to suggest otherwise.", "The alternative hypothesis (denoted symbolically by \\(H_a\\)) designates what we will believe if there is sufficient evidence in the data to discredit, or â€œreject,â€ the null hypothesis.", "See the BYU-I Math 221 Stats Wiki for another example.", "Managing Decision Errors When the \\(p\\)-value approaches zero, one of two things must be occurring.", "Either an extremely rare event has happened or the null hypothesis is incorrect.", "Since the second option, that the null hypothesis is incorrect, is the more plausible option, we reject the null hypothesis in favor of the alternative whenever the \\(p\\)-value is close to zero.", "It is important to remember that rejecting the null hypothesis could however be a mistake.", "Â  \\(H_0\\) True \\(H_0\\) False Reject \\(H_0\\) Type I Error Correct Decision Accept \\(H_0\\) Correct Decision Type II Error Type I Error, Significance Level, Confidence and \\(\\alpha\\) A Type I Error is defined as rejecting the null hypothesis when it is actually true.", "(Throwing away truth.) The significance level, \\(\\alpha\\), of a hypothesis test controls the probability of a Type I Error.", "The typical value of \\(\\alpha = 0.05\\) came from tradition and is a somewhat arbitrary value.", "Any value from 0 to 1 could be used for \\(\\alpha\\).", "When deciding on the level of \\(\\alpha\\) for a particular study it is important to remember that as \\(\\alpha\\) increases, the probability of a Type I Error increases, and the probability of a Type II Error decreases.", "When \\(\\alpha\\) gets smaller, the probability of a Type I Error gets smaller, while the probability of a Type II Error increases.", "Confidence is defined as \\(1-\\alpha\\) or the opposite of a Type I error.", "That is the probability of accepting the NULL when it is in fact true.", "Type II Errors, \\(\\beta\\), and Power It is also possible to make a Type II Error, which is defined as failing to reject the null hypothesis when it is actually false.", "(Failing to move to truth.) The probability of a Type II Error, \\(\\beta\\), is often unknown.", "However, practitioners often make an assumption about a detectable difference that is desired which then allows \\(\\beta\\) to be prescribed much like \\(\\alpha\\).", "In essence, the detectable difference prescribes a fixed value for \\(H_a\\).", "We can then talk about the power of of a hypothesis test, which is 1 minus the probability of a Type II Error, \\(\\beta\\).", "See Statistical Power in Wikipedia for a starting source if your are interested.", "This website provides a novel interactive visualization to help you understand power.", "It does require a little background on Cohenâ€™s D.", "Sufficient Evidence Statistics comes in to play with hypothesis testing by defining the phrase â€œsufficient evidence.â€ When there is â€œsufficient evidenceâ€ in the data, the null hypothesis is rejected and the alternative hypothesis becomes the working hypothesis.", "There are many statistical approaches to this problem of measuring the significance of evidence, but in almost all cases, the final measurement of evidence is given by the \\(p\\)-value of the hypothesis test.", "The \\(p\\)-value of a test is defined as the probability of the evidence being as extreme or more extreme than what was observed assuming the null hypothesis is true.", "This is an interesting phrase that is at first difficult to understand.", "The â€œas extreme or more extremeâ€ part of the definition of the \\(p\\)-value comes from the idea that the null hypothesis will be rejected when the evidence in the data is extremely inconsistent with the null hypothesis.", "If the data is not extremely different from what we would expect under the null hypothesis, then we will continue to believe the null hypothesis.", "Although, it is worth emphasizing that this does not prove the null hypothesis to be true.", "Evidence not Proof Hypothesis testing allows us a formal way to decide if we should â€œconclude the alternativeâ€ or â€œcontinue to accept the null.â€ It is important to remember that statistics (and science) cannot prove anything, just show evidence towards.", "Thus we never really prove a hypothesis is true, we simply show evidence towards or against a hypothesis.", "Calculating the \\(p\\)-Value Recall that the \\(p\\)-value measures how extremely the data (the evidence) differs from what is expected under the null hypothesis.", "Small \\(p\\)-values lead us to discard (reject) the null hypothesis.", "A \\(p\\)-value can be calculated whenever we have two things.", "A test statistic, which is a way of measuring how â€œfarâ€ the observed data is from what is expected under the null hypothesis."],
    "type": "page",
    "page_title": "Making Inference"
  },
  {
    "id": 112,
    "title": "ğŸ“ Hypothesis Testing",
    "url": "MakingInference.html#hypothesistesting",
    "content": "Hypothesis Testing One of the great focal points of statistics concerns hypothesis testing. Science generally agrees upon the principle that truth must be uncovered by the process of elimination. The process begins by establishing a starting assumption, or null hypothesis (\\(H_0\\)). Data is then collected and the evidence against the null hypothesis is measured, typically with the \\(p\\)-value. The \\(p\\)-value becomes small (gets close to zero) when the evidence is extremely different from what would be expected if the null hypothesis were true. When the \\(p\\)-value is below the significance level \\(\\alpha\\) (typically \\(\\alpha=0.05\\)) the null hypothesis is abandoned (rejected) in favor of a competing alternative hypothesis (\\(H_a\\)). Click for an Example The current hypothesis may be that the world is flat. Then someone who thinks otherwise sets sail in a boat, gathers some evidence, and when there is sufficient evidence in the data to disbelieve the current hypothesis, we conclude the world is not flat. In light of this new knowledge, we shift our belief to the next working hypothesis, that the world is round. After a while, someone gathers more evidence and shows that the world is not round, and we move to the next working hypothesis, that it is oblate spheroid, i.e., a sphere that is squashed at its poles and swollen at the equator. This process of elimination is called hypothesis testing. The process begins by establishing a null hypothesis (denoted symbolically by \\(H_0\\)) which represents the current opinion, status quo, or what we will believe if the evidence is not sufficient to suggest otherwise. The alternative hypothesis (denoted symbolically by \\(H_a\\)) designates what we will believe if there is sufficient evidence in the data to discredit, or â€œreject,â€ the null hypothesis. See the BYU-I Math 221 Stats Wiki for another example. Managing Decision Errors When the \\(p\\)-value approaches zero, one of two things must be occurring. Either an extremely rare event has happened or the null hypothesis is incorrect. Since the second option, that the null hypothesis is incorrect, is the more plausible option, we reject the null hypothesis in favor of the alternative whenever the \\(p\\)-value is close to zero. It is important to remember that rejecting the null hypothesis could however be a mistake. Â  \\(H_0\\) True \\(H_0\\) False Reject \\(H_0\\) Type I Error Correct Decision Accept \\(H_0\\) Correct Decision Type II Error Type I Error, Significance Level, Confidence and \\(\\alpha\\) A Type I Error is defined as rejecting the null hypothesis when it is actually true. (Throwing away truth.) The significance level, \\(\\alpha\\), of a hypothesis test controls the probability of a Type I Error. The typical value of \\(\\alpha = 0.05\\) came from tradition and is a somewhat arbitrary value. Any value from 0 to 1 could be used for \\(\\alpha\\). When deciding on the level of \\(\\alpha\\) for a particular study it is important to remember that as \\(\\alpha\\) increases, the probability of a Type I Error increases, and the probability of a Type II Error decreases. When \\(\\alpha\\) gets smaller, the probability of a Type I Error gets smaller, while the probability of a Type II Error increases. Confidence is defined as \\(1-\\alpha\\) or the opposite of a Type I error. That is the probability of accepting the NULL when it is in fact true. Type II Errors, \\(\\beta\\), and Power It is also possible to make a Type II Error, which is defined as failing to reject the null hypothesis when it is actually false. (Failing to move to truth.) The probability of a Type II Error, \\(\\beta\\), is often unknown. However, practitioners often make an assumption about a detectable difference that is desired which then allows \\(\\beta\\) to be prescribed much like \\(\\alpha\\). In essence, the detectable difference prescribes a fixed value for \\(H_a\\). We can then talk about the power of of a hypothesis test, which is 1 minus the probability of a Type II Error, \\(\\beta\\). See Statistical Power in Wikipedia for a starting source if your are interested. This website provides a novel interactive visualization to help you understand power. It does require a little background on Cohenâ€™s D. Sufficient Evidence Statistics comes in to play with hypothesis testing by defining the phrase â€œsufficient evidence.â€ When there is â€œsufficient evidenceâ€ in the data, the null hypothesis is rejected and the alternative hypothesis becomes the working hypothesis. There are many statistical approaches to this problem of measuring the significance of evidence, but in almost all cases, the final measurement of evidence is given by the \\(p\\)-value of the hypothesis test. The \\(p\\)-value of a test is defined as the probability of the evidence being as extreme or more extreme than what was observed assuming the null hypothesis is true. This is an interesting phrase that is at first difficult to understand. The â€œas extreme or more extremeâ€ part of the definition of the \\(p\\)-value comes from the idea that the null hypothesis will be rejected when the evidence in the data is extremely inconsistent with the null hypothesis. If the data is not extremely different from what we would expect under the null hypothesis, then we will continue to believe the null hypothesis. Although, it is worth emphasizing that this does not prove the null hypothesis to be true. Evidence not Proof Hypothesis testing allows us a formal way to decide if we should â€œconclude the alternativeâ€ or â€œcontinue to accept the null.â€ It is important to remember that statistics (and science) cannot prove anything, just show evidence towards. Thus we never really prove a hypothesis is true, we simply show evidence towards or against a hypothesis.",
    "sentences": ["One of the great focal points of statistics concerns hypothesis testing.", "Science generally agrees upon the principle that truth must be uncovered by the process of elimination.", "The process begins by establishing a starting assumption, or null hypothesis (\\(H_0\\)).", "Data is then collected and the evidence against the null hypothesis is measured, typically with the \\(p\\)-value.", "The \\(p\\)-value becomes small (gets close to zero) when the evidence is extremely different from what would be expected if the null hypothesis were true.", "When the \\(p\\)-value is below the significance level \\(\\alpha\\) (typically \\(\\alpha=0.05\\)) the null hypothesis is abandoned (rejected) in favor of a competing alternative hypothesis (\\(H_a\\)).", "Click for an Example The current hypothesis may be that the world is flat.", "Then someone who thinks otherwise sets sail in a boat, gathers some evidence, and when there is sufficient evidence in the data to disbelieve the current hypothesis, we conclude the world is not flat.", "In light of this new knowledge, we shift our belief to the next working hypothesis, that the world is round.", "After a while, someone gathers more evidence and shows that the world is not round, and we move to the next working hypothesis, that it is oblate spheroid, i.e., a sphere that is squashed at its poles and swollen at the equator."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Hypothesis Testing"
  },
  {
    "id": 113,
    "title": "ğŸ“ Managing Decision Errors",
    "url": "MakingInference.html#managingdecisionerrors",
    "content": "Managing Decision Errors When the \\(p\\)-value approaches zero, one of two things must be occurring. Either an extremely rare event has happened or the null hypothesis is incorrect. Since the second option, that the null hypothesis is incorrect, is the more plausible option, we reject the null hypothesis in favor of the alternative whenever the \\(p\\)-value is close to zero. It is important to remember that rejecting the null hypothesis could however be a mistake. Â  \\(H_0\\) True \\(H_0\\) False Reject \\(H_0\\) Type I Error Correct Decision Accept \\(H_0\\) Correct Decision Type II Error",
    "sentences": ["When the \\(p\\)-value approaches zero, one of two things must be occurring.", "Either an extremely rare event has happened or the null hypothesis is incorrect.", "Since the second option, that the null hypothesis is incorrect, is the more plausible option, we reject the null hypothesis in favor of the alternative whenever the \\(p\\)-value is close to zero.", "It is important to remember that rejecting the null hypothesis could however be a mistake.", "Â  \\(H_0\\) True \\(H_0\\) False Reject \\(H_0\\) Type I Error Correct Decision Accept \\(H_0\\) Correct Decision Type II Error"],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Managing Decision Errors"
  },
  {
    "id": 114,
    "title": "ğŸ“ Type I Error, Significance Level, Confidence and \\(\\alpha\\)",
    "url": "MakingInference.html#typeierrorsignificancelevelconfidenceand\\\\alpha\\",
    "content": "Type I Error, Significance Level, Confidence and \\(\\alpha\\) A Type I Error is defined as rejecting the null hypothesis when it is actually true. (Throwing away truth.) The significance level, \\(\\alpha\\), of a hypothesis test controls the probability of a Type I Error. The typical value of \\(\\alpha = 0.05\\) came from tradition and is a somewhat arbitrary value. Any value from 0 to 1 could be used for \\(\\alpha\\). When deciding on the level of \\(\\alpha\\) for a particular study it is important to remember that as \\(\\alpha\\) increases, the probability of a Type I Error increases, and the probability of a Type II Error decreases. When \\(\\alpha\\) gets smaller, the probability of a Type I Error gets smaller, while the probability of a Type II Error increases. Confidence is defined as \\(1-\\alpha\\) or the opposite of a Type I error. That is the probability of accepting the NULL when it is in fact true.",
    "sentences": ["A Type I Error is defined as rejecting the null hypothesis when it is actually true.", "(Throwing away truth.) The significance level, \\(\\alpha\\), of a hypothesis test controls the probability of a Type I Error.", "The typical value of \\(\\alpha = 0.05\\) came from tradition and is a somewhat arbitrary value.", "Any value from 0 to 1 could be used for \\(\\alpha\\).", "When deciding on the level of \\(\\alpha\\) for a particular study it is important to remember that as \\(\\alpha\\) increases, the probability of a Type I Error increases, and the probability of a Type II Error decreases.", "When \\(\\alpha\\) gets smaller, the probability of a Type I Error gets smaller, while the probability of a Type II Error increases.", "Confidence is defined as \\(1-\\alpha\\) or the opposite of a Type I error.", "That is the probability of accepting the NULL when it is in fact true."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Type I Error, Significance Level, Confidence and \\(\\alpha\\)"
  },
  {
    "id": 115,
    "title": "ğŸ“ Type II Errors, \\(\\beta\\), and\r\nPower",
    "url": "MakingInference.html#typeiierrors\\\\beta\\andpower",
    "content": "Type II Errors, \\(\\beta\\), and\r\nPower It is also possible to make a Type II Error, which is defined as failing to reject the null hypothesis when it is actually false. (Failing to move to truth.) The probability of a Type II Error, \\(\\beta\\), is often unknown. However, practitioners often make an assumption about a detectable difference that is desired which then allows \\(\\beta\\) to be prescribed much like \\(\\alpha\\). In essence, the detectable difference prescribes a fixed value for \\(H_a\\). We can then talk about the power of of a hypothesis test, which is 1 minus the probability of a Type II Error, \\(\\beta\\). See Statistical Power in Wikipedia for a starting source if your are interested. This website provides a novel interactive visualization to help you understand power. It does require a little background on Cohenâ€™s D.",
    "sentences": ["It is also possible to make a Type II Error, which is defined as failing to reject the null hypothesis when it is actually false.", "(Failing to move to truth.) The probability of a Type II Error, \\(\\beta\\), is often unknown.", "However, practitioners often make an assumption about a detectable difference that is desired which then allows \\(\\beta\\) to be prescribed much like \\(\\alpha\\).", "In essence, the detectable difference prescribes a fixed value for \\(H_a\\).", "We can then talk about the power of of a hypothesis test, which is 1 minus the probability of a Type II Error, \\(\\beta\\).", "See Statistical Power in Wikipedia for a starting source if your are interested.", "This website provides a novel interactive visualization to help you understand power.", "It does require a little background on Cohenâ€™s D."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Type II Errors, \\(\\beta\\), and\r\nPower"
  },
  {
    "id": 116,
    "title": "ğŸ“ Sufficient Evidence",
    "url": "MakingInference.html#sufficientevidence",
    "content": "Sufficient Evidence Statistics comes in to play with hypothesis testing by defining the phrase â€œsufficient evidence.â€ When there is â€œsufficient evidenceâ€ in the data, the null hypothesis is rejected and the alternative hypothesis becomes the working hypothesis. There are many statistical approaches to this problem of measuring the significance of evidence, but in almost all cases, the final measurement of evidence is given by the \\(p\\)-value of the hypothesis test. The \\(p\\)-value of a test is defined as the probability of the evidence being as extreme or more extreme than what was observed assuming the null hypothesis is true. This is an interesting phrase that is at first difficult to understand. The â€œas extreme or more extremeâ€ part of the definition of the \\(p\\)-value comes from the idea that the null hypothesis will be rejected when the evidence in the data is extremely inconsistent with the null hypothesis. If the data is not extremely different from what we would expect under the null hypothesis, then we will continue to believe the null hypothesis. Although, it is worth emphasizing that this does not prove the null hypothesis to be true.",
    "sentences": ["Statistics comes in to play with hypothesis testing by defining the phrase â€œsufficient evidence.â€ When there is â€œsufficient evidenceâ€ in the data, the null hypothesis is rejected and the alternative hypothesis becomes the working hypothesis.", "There are many statistical approaches to this problem of measuring the significance of evidence, but in almost all cases, the final measurement of evidence is given by the \\(p\\)-value of the hypothesis test.", "The \\(p\\)-value of a test is defined as the probability of the evidence being as extreme or more extreme than what was observed assuming the null hypothesis is true.", "This is an interesting phrase that is at first difficult to understand.", "The â€œas extreme or more extremeâ€ part of the definition of the \\(p\\)-value comes from the idea that the null hypothesis will be rejected when the evidence in the data is extremely inconsistent with the null hypothesis.", "If the data is not extremely different from what we would expect under the null hypothesis, then we will continue to believe the null hypothesis.", "Although, it is worth emphasizing that this does not prove the null hypothesis to be true."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Sufficient Evidence"
  },
  {
    "id": 117,
    "title": "ğŸ“ Evidence not Proof",
    "url": "MakingInference.html#evidencenotproof",
    "content": "Evidence not Proof Hypothesis testing allows us a formal way to decide if we should â€œconclude the alternativeâ€ or â€œcontinue to accept the null.â€ It is important to remember that statistics (and science) cannot prove anything, just show evidence towards. Thus we never really prove a hypothesis is true, we simply show evidence towards or against a hypothesis.",
    "sentences": ["Hypothesis testing allows us a formal way to decide if we should â€œconclude the alternativeâ€ or â€œcontinue to accept the null.â€ It is important to remember that statistics (and science) cannot prove anything, just show evidence towards.", "Thus we never really prove a hypothesis is true, we simply show evidence towards or against a hypothesis."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Evidence not Proof"
  },
  {
    "id": 118,
    "title": "ğŸ“ Calculating the \\(p\\)-Value",
    "url": "MakingInference.html#calculatingthe\\p\\value",
    "content": "Calculating the \\(p\\)-Value Recall that the \\(p\\)-value measures how extremely the data (the evidence) differs from what is expected under the null hypothesis. Small \\(p\\)-values lead us to discard (reject) the null hypothesis. A \\(p\\)-value can be calculated whenever we have two things. A test statistic, which is a way of measuring how â€œfarâ€ the observed data is from what is expected under the null hypothesis. The sampling distribution of the test statistic, which is the theoretical distribution of the test statistic over all possible samples, assuming the null hypothesis was true. Visit the Math 221 textbook for an explanation. A distribution describes how data is spread out. When we know the shape of a distribution, we know which values are possible, but more importantly which values are most plausible (likely) and which are the least plausible (unlikely). The \\(p\\)-value uses the sampling distribution of the test statistic to measure the probability of the observed test statistic being as extreme or more extreme than the one observed. All \\(p\\)-value computation methods can be classified into two broad categories, parametric methods and nonparametric methods. Parametric Methods Parametric methods assume that, under the null hypothesis, the test statistic follows a specific theoretical parametric distribution. Parametric methods are typically more statistically powerful than nonparametric methods, but necessarily force more assumptions on the data. Parametric distributions are theoretical distributions that can be described by a mathematical function. There are many theoretical distributions. (See the List of Probability Distributions in Wikipedia for details.) Four of the most widely used parametric distributions are: The Normal Distribution Click for Details One of the most important distributions in statistics is the normal distribution. It is a theoretical distribution that approximates the distributions of many real life data distributions, like heights of people, heights of corn plants, baseball batting averages, lengths of gestational periods for many species including humans, and so on. More importantly, the sampling distribution of the sample mean \\(\\bar{x}\\) is normally distributed in two important scenarios. The parent population is normally distributed. The sample size is sufficiently large. (Often \\(n\\geq 30\\) is sufficient, but this is a general rule of thumb that is sometimes insufficient.) Mathematical Formula \\[ f(x | \\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} \\] The symbols \\(\\mu\\) and \\(\\sigma\\) are the two parameters of this distribution. The parameter \\(\\mu\\) controls the center, or mean of the distribution. The parameter \\(\\sigma\\) controls the spread, or standard deviation of the distribution. Graphical Form Comments The usefulness of the normal distribution is that we know which values of data are likely and which are unlikely by just knowing three things: that the data is normally distributed, \\(\\mu\\), the mean of the distribution, and \\(\\sigma\\), the standard deviation of the distribution. For example, as shown in the plot above, a value of \\(x=-8\\) would be very probable for the normal distribution with \\(\\mu=-5\\) and \\(\\sigma=2\\) (light blue curve). However, the value of \\(x=-8\\) would be very unlikely to occur in the normal distribution with \\(\\mu=3\\) and \\(\\sigma=3\\) (gray curve). In fact, \\(x=-8\\) would be even more unlikely an occurance for the \\(\\mu=0\\) and \\(\\sigma=1\\) distribution (dark blue curve). The Chi Squared Distribution Click for Details The chi squared distribution only allows for values that are greater than or equal to zero. While it has a few real life applications, by far its greatest use is theoretical. The test statistic of the chi squared test is distributed according to a chi squared distribution. Mathematical Formula \\[ f(x|p) = \\frac{1}{\\Gamma(p/2)2^{p/2}}x^{(p/2)-1}e^{-x/2} \\] The only parameter of the chi squared distribution is \\(p\\), which is known as the degrees of freedom. Larger values of the parameter \\(p\\) move the center of the chi squared distribution farther to the right. As \\(p\\) goes to infinity, the chi squared distribution begins to look more and more normal in shape. Note that the symbol in the denominator of the chi squared distribution, \\(\\Gamma(p/2)\\), is the Gamma function of \\(p/2\\). (See Gamma Function in Wikipedia for details.) Graphical Form Comments It is important to remember that the chi squared distribution is only defined for \\(x\\geq 0\\) and for positive values of the parameter \\(p\\). This is unlike the normal distribution which is defined for all numbers \\(x\\) from negative infinity to positive infinity as well as for all values of \\(\\mu\\) from negative infinity to positive infinity. The t Distribution Click for Details A close friend of the normal distribution is the t distribution. Although the t distribution is seldom used to model real life data, the distribution is used extensively in hypothesis testing. For example, it is the sampling distribution of the one sample t statistic. It also shows up in many other places, like in regression, in the independent samples t test, and in the paired samples t test. Mathematical Formula \\[ f(x|p) = \\frac{\\Gamma\\left(\\frac{p+1}{2}\\right)}{\\Gamma\\left(\\frac{p}{2}\\right)}\\frac{1}{\\sqrt{p\\pi}}\\frac{1}{\\left(1 + \\left(\\frac{x^2}{p}\\right)\\right)^{(p+1)/2}} \\] Notice that, similar to the chi squared distribution, the t distribution has only one parameter, the degrees of freedom \\(p\\). As the single parameter \\(p\\) is varied from \\(p=1\\), to \\(p=2\\), â€¦, \\(p=5\\), and larger and larger numbers, the resulting distribution becomes more and more normal in shape. Note that the expressions \\(\\Gamma\\left(\\frac{p+1}{2}\\right)\\) and \\(\\Gamma(p/2)\\), refer to the Gamma function. (See Gamma Function in Wikipedia for details.) Graphical Form Comments When the degrees of freedom \\(p=30\\), the resulting t distribution is almost indistinguishable visually from the normal distribution. This is one of the reasons that a sample size of 30 is often used as a rule of thumb for the sample size being â€œlarge enoughâ€ to assume the sampling distribution of the sample mean is approximately normal. The F Distribution Click for Details Another commonly used distribution for test statistics, like in ANOVA and regression, is the F distribution. Technically speaking, the F distribution is the ratio of two chi squared random variables that are each divided by their respective degrees of freedom. Mathematical Formula \\[ f(x|p_1,p_2) = \\frac{\\Gamma\\left(\\frac{p_1+p_2}{2}\\right)}{\\Gamma\\left(\\frac{p_1}{2}\\right)\\Gamma\\left(\\frac{p_2}{2}\\right)}\\frac{\\left(\\frac{p_1}{p_2}\\right)^{p_1/2}x^{(p_1-2)/2}}{\\left(1+\\left(\\frac{p_1}{p_2}\\right)x\\right)^{(p_1+p_2)/2}} \\] where \\(x\\geq 0\\) and the parameters \\(p_1\\) and \\(p_2\\) are the â€œnumeratorâ€ and â€œdenominatorâ€ degrees of freedom, respectively. Graphical Form Comments The effects of the parameters \\(p_1\\) and \\(p_2\\) on the F distribution are complicated, but generally speaking, as they both increase the distribution becomes more and more normal in shape. Nonparametric Methods Nonparametric methods place minimal assumptions on the distribution of data. They allow the data to â€œspeak for itself.â€ They are typically less powerful than the parametric alternatives, but are more broadly applicable because fewer assumptions need to be satisfied. Nonparametric methods include Rank Sum Tests and Permutation Tests.",
    "sentences": ["Recall that the \\(p\\)-value measures how extremely the data (the evidence) differs from what is expected under the null hypothesis.", "Small \\(p\\)-values lead us to discard (reject) the null hypothesis.", "A \\(p\\)-value can be calculated whenever we have two things.", "A test statistic, which is a way of measuring how â€œfarâ€ the observed data is from what is expected under the null hypothesis.", "The sampling distribution of the test statistic, which is the theoretical distribution of the test statistic over all possible samples, assuming the null hypothesis was true.", "Visit the Math 221 textbook for an explanation.", "A distribution describes how data is spread out.", "When we know the shape of a distribution, we know which values are possible, but more importantly which values are most plausible (likely) and which are the least plausible (unlikely).", "The \\(p\\)-value uses the sampling distribution of the test statistic to measure the probability of the observed test statistic being as extreme or more extreme than the one observed.", "All \\(p\\)-value computation methods can be classified into two broad categories, parametric methods and nonparametric methods."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Calculating the \\(p\\)-Value"
  },
  {
    "id": 119,
    "title": "ğŸ“ Parametric Methods",
    "url": "MakingInference.html#parametricmethods",
    "content": "Parametric Methods Parametric methods assume that, under the null hypothesis, the test statistic follows a specific theoretical parametric distribution. Parametric methods are typically more statistically powerful than nonparametric methods, but necessarily force more assumptions on the data. Parametric distributions are theoretical distributions that can be described by a mathematical function. There are many theoretical distributions. (See the List of Probability Distributions in Wikipedia for details.) Four of the most widely used parametric distributions are: The Normal Distribution Click for Details One of the most important distributions in statistics is the normal distribution. It is a theoretical distribution that approximates the distributions of many real life data distributions, like heights of people, heights of corn plants, baseball batting averages, lengths of gestational periods for many species including humans, and so on. More importantly, the sampling distribution of the sample mean \\(\\bar{x}\\) is normally distributed in two important scenarios. The parent population is normally distributed. The sample size is sufficiently large. (Often \\(n\\geq 30\\) is sufficient, but this is a general rule of thumb that is sometimes insufficient.) Mathematical Formula \\[ f(x | \\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} \\] The symbols \\(\\mu\\) and \\(\\sigma\\) are the two parameters of this distribution. The parameter \\(\\mu\\) controls the center, or mean of the distribution. The parameter \\(\\sigma\\) controls the spread, or standard deviation of the distribution. Graphical Form Comments The usefulness of the normal distribution is that we know which values of data are likely and which are unlikely by just knowing three things: that the data is normally distributed, \\(\\mu\\), the mean of the distribution, and \\(\\sigma\\), the standard deviation of the distribution. For example, as shown in the plot above, a value of \\(x=-8\\) would be very probable for the normal distribution with \\(\\mu=-5\\) and \\(\\sigma=2\\) (light blue curve). However, the value of \\(x=-8\\) would be very unlikely to occur in the normal distribution with \\(\\mu=3\\) and \\(\\sigma=3\\) (gray curve). In fact, \\(x=-8\\) would be even more unlikely an occurance for the \\(\\mu=0\\) and \\(\\sigma=1\\) distribution (dark blue curve). The Chi Squared Distribution Click for Details The chi squared distribution only allows for values that are greater than or equal to zero. While it has a few real life applications, by far its greatest use is theoretical. The test statistic of the chi squared test is distributed according to a chi squared distribution. Mathematical Formula \\[ f(x|p) = \\frac{1}{\\Gamma(p/2)2^{p/2}}x^{(p/2)-1}e^{-x/2} \\] The only parameter of the chi squared distribution is \\(p\\), which is known as the degrees of freedom. Larger values of the parameter \\(p\\) move the center of the chi squared distribution farther to the right. As \\(p\\) goes to infinity, the chi squared distribution begins to look more and more normal in shape. Note that the symbol in the denominator of the chi squared distribution, \\(\\Gamma(p/2)\\), is the Gamma function of \\(p/2\\). (See Gamma Function in Wikipedia for details.) Graphical Form Comments It is important to remember that the chi squared distribution is only defined for \\(x\\geq 0\\) and for positive values of the parameter \\(p\\). This is unlike the normal distribution which is defined for all numbers \\(x\\) from negative infinity to positive infinity as well as for all values of \\(\\mu\\) from negative infinity to positive infinity. The t Distribution Click for Details A close friend of the normal distribution is the t distribution. Although the t distribution is seldom used to model real life data, the distribution is used extensively in hypothesis testing. For example, it is the sampling distribution of the one sample t statistic. It also shows up in many other places, like in regression, in the independent samples t test, and in the paired samples t test. Mathematical Formula \\[ f(x|p) = \\frac{\\Gamma\\left(\\frac{p+1}{2}\\right)}{\\Gamma\\left(\\frac{p}{2}\\right)}\\frac{1}{\\sqrt{p\\pi}}\\frac{1}{\\left(1 + \\left(\\frac{x^2}{p}\\right)\\right)^{(p+1)/2}} \\] Notice that, similar to the chi squared distribution, the t distribution has only one parameter, the degrees of freedom \\(p\\). As the single parameter \\(p\\) is varied from \\(p=1\\), to \\(p=2\\), â€¦, \\(p=5\\), and larger and larger numbers, the resulting distribution becomes more and more normal in shape. Note that the expressions \\(\\Gamma\\left(\\frac{p+1}{2}\\right)\\) and \\(\\Gamma(p/2)\\), refer to the Gamma function. (See Gamma Function in Wikipedia for details.) Graphical Form Comments When the degrees of freedom \\(p=30\\), the resulting t distribution is almost indistinguishable visually from the normal distribution. This is one of the reasons that a sample size of 30 is often used as a rule of thumb for the sample size being â€œlarge enoughâ€ to assume the sampling distribution of the sample mean is approximately normal. The F Distribution Click for Details Another commonly used distribution for test statistics, like in ANOVA and regression, is the F distribution. Technically speaking, the F distribution is the ratio of two chi squared random variables that are each divided by their respective degrees of freedom. Mathematical Formula \\[ f(x|p_1,p_2) = \\frac{\\Gamma\\left(\\frac{p_1+p_2}{2}\\right)}{\\Gamma\\left(\\frac{p_1}{2}\\right)\\Gamma\\left(\\frac{p_2}{2}\\right)}\\frac{\\left(\\frac{p_1}{p_2}\\right)^{p_1/2}x^{(p_1-2)/2}}{\\left(1+\\left(\\frac{p_1}{p_2}\\right)x\\right)^{(p_1+p_2)/2}} \\] where \\(x\\geq 0\\) and the parameters \\(p_1\\) and \\(p_2\\) are the â€œnumeratorâ€ and â€œdenominatorâ€ degrees of freedom, respectively. Graphical Form Comments The effects of the parameters \\(p_1\\) and \\(p_2\\) on the F distribution are complicated, but generally speaking, as they both increase the distribution becomes more and more normal in shape.",
    "sentences": ["Parametric methods assume that, under the null hypothesis, the test statistic follows a specific theoretical parametric distribution.", "Parametric methods are typically more statistically powerful than nonparametric methods, but necessarily force more assumptions on the data.", "Parametric distributions are theoretical distributions that can be described by a mathematical function.", "There are many theoretical distributions.", "(See the List of Probability Distributions in Wikipedia for details.) Four of the most widely used parametric distributions are: The Normal Distribution Click for Details One of the most important distributions in statistics is the normal distribution.", "It is a theoretical distribution that approximates the distributions of many real life data distributions, like heights of people, heights of corn plants, baseball batting averages, lengths of gestational periods for many species including humans, and so on.", "More importantly, the sampling distribution of the sample mean \\(\\bar{x}\\) is normally distributed in two important scenarios.", "The parent population is normally distributed.", "The sample size is sufficiently large.", "(Often \\(n\\geq 30\\) is sufficient, but this is a general rule of thumb that is sometimes insufficient.) Mathematical Formula \\[ f(x | \\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} \\] The symbols \\(\\mu\\) and \\(\\sigma\\) are the two parameters of this distribution."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Parametric Methods"
  },
  {
    "id": 120,
    "title": "ğŸ“ The Normal Distribution",
    "url": "MakingInference.html#thenormaldistribution",
    "content": "The Normal Distribution One of the most important distributions in statistics is the normal distribution. It is a theoretical distribution that approximates the distributions of many real life data distributions, like heights of people, heights of corn plants, baseball batting averages, lengths of gestational periods for many species including humans, and so on. More importantly, the sampling distribution of the sample mean \\(\\bar{x}\\) is normally distributed in two important scenarios. The parent population is normally distributed. The sample size is sufficiently large. (Often \\(n\\geq 30\\) is sufficient, but this is a general rule of thumb that is sometimes insufficient.) Mathematical Formula \\[ f(x | \\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} \\] The symbols \\(\\mu\\) and \\(\\sigma\\) are the two parameters of this distribution. The parameter \\(\\mu\\) controls the center, or mean of the distribution. The parameter \\(\\sigma\\) controls the spread, or standard deviation of the distribution. Graphical Form Comments The usefulness of the normal distribution is that we know which values of data are likely and which are unlikely by just knowing three things: that the data is normally distributed, \\(\\mu\\), the mean of the distribution, and \\(\\sigma\\), the standard deviation of the distribution. For example, as shown in the plot above, a value of \\(x=-8\\) would be very probable for the normal distribution with \\(\\mu=-5\\) and \\(\\sigma=2\\) (light blue curve). However, the value of \\(x=-8\\) would be very unlikely to occur in the normal distribution with \\(\\mu=3\\) and \\(\\sigma=3\\) (gray curve). In fact, \\(x=-8\\) would be even more unlikely an occurance for the \\(\\mu=0\\) and \\(\\sigma=1\\) distribution (dark blue curve).",
    "sentences": ["One of the most important distributions in statistics is the normal distribution.", "It is a theoretical distribution that approximates the distributions of many real life data distributions, like heights of people, heights of corn plants, baseball batting averages, lengths of gestational periods for many species including humans, and so on.", "More importantly, the sampling distribution of the sample mean \\(\\bar{x}\\) is normally distributed in two important scenarios.", "The parent population is normally distributed.", "The sample size is sufficiently large.", "(Often \\(n\\geq 30\\) is sufficient, but this is a general rule of thumb that is sometimes insufficient.) Mathematical Formula \\[ f(x | \\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} \\] The symbols \\(\\mu\\) and \\(\\sigma\\) are the two parameters of this distribution.", "The parameter \\(\\mu\\) controls the center, or mean of the distribution.", "The parameter \\(\\sigma\\) controls the spread, or standard deviation of the distribution.", "Graphical Form Comments The usefulness of the normal distribution is that we know which values of data are likely and which are unlikely by just knowing three things: that the data is normally distributed, \\(\\mu\\), the mean of the distribution, and \\(\\sigma\\), the standard deviation of the distribution.", "For example, as shown in the plot above, a value of \\(x=-8\\) would be very probable for the normal distribution with \\(\\mu=-5\\) and \\(\\sigma=2\\) (light blue curve)."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "The Normal Distribution"
  },
  {
    "id": 121,
    "title": "ğŸ“ The Chi Squared Distribution",
    "url": "MakingInference.html#thechisquareddistribution",
    "content": "The Chi Squared Distribution The chi squared distribution only allows for values that are greater than or equal to zero. While it has a few real life applications, by far its greatest use is theoretical. The test statistic of the chi squared test is distributed according to a chi squared distribution. Mathematical Formula \\[ f(x|p) = \\frac{1}{\\Gamma(p/2)2^{p/2}}x^{(p/2)-1}e^{-x/2} \\] The only parameter of the chi squared distribution is \\(p\\), which is known as the degrees of freedom. Larger values of the parameter \\(p\\) move the center of the chi squared distribution farther to the right. As \\(p\\) goes to infinity, the chi squared distribution begins to look more and more normal in shape. Note that the symbol in the denominator of the chi squared distribution, \\(\\Gamma(p/2)\\), is the Gamma function of \\(p/2\\). (See Gamma Function in Wikipedia for details.) Graphical Form Comments It is important to remember that the chi squared distribution is only defined for \\(x\\geq 0\\) and for positive values of the parameter \\(p\\). This is unlike the normal distribution which is defined for all numbers \\(x\\) from negative infinity to positive infinity as well as for all values of \\(\\mu\\) from negative infinity to positive infinity.",
    "sentences": ["The chi squared distribution only allows for values that are greater than or equal to zero.", "While it has a few real life applications, by far its greatest use is theoretical.", "The test statistic of the chi squared test is distributed according to a chi squared distribution.", "Mathematical Formula \\[ f(x|p) = \\frac{1}{\\Gamma(p/2)2^{p/2}}x^{(p/2)-1}e^{-x/2} \\] The only parameter of the chi squared distribution is \\(p\\), which is known as the degrees of freedom.", "Larger values of the parameter \\(p\\) move the center of the chi squared distribution farther to the right.", "As \\(p\\) goes to infinity, the chi squared distribution begins to look more and more normal in shape.", "Note that the symbol in the denominator of the chi squared distribution, \\(\\Gamma(p/2)\\), is the Gamma function of \\(p/2\\).", "(See Gamma Function in Wikipedia for details.) Graphical Form Comments It is important to remember that the chi squared distribution is only defined for \\(x\\geq 0\\) and for positive values of the parameter \\(p\\).", "This is unlike the normal distribution which is defined for all numbers \\(x\\) from negative infinity to positive infinity as well as for all values of \\(\\mu\\) from negative infinity to positive infinity."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "The Chi Squared Distribution"
  },
  {
    "id": 122,
    "title": "ğŸ“ The t Distribution",
    "url": "MakingInference.html#thetdistribution",
    "content": "The t Distribution A close friend of the normal distribution is the t distribution. Although the t distribution is seldom used to model real life data, the distribution is used extensively in hypothesis testing. For example, it is the sampling distribution of the one sample t statistic. It also shows up in many other places, like in regression, in the independent samples t test, and in the paired samples t test. Mathematical Formula \\[ f(x|p) = \\frac{\\Gamma\\left(\\frac{p+1}{2}\\right)}{\\Gamma\\left(\\frac{p}{2}\\right)}\\frac{1}{\\sqrt{p\\pi}}\\frac{1}{\\left(1 + \\left(\\frac{x^2}{p}\\right)\\right)^{(p+1)/2}} \\] Notice that, similar to the chi squared distribution, the t distribution has only one parameter, the degrees of freedom \\(p\\). As the single parameter \\(p\\) is varied from \\(p=1\\), to \\(p=2\\), â€¦, \\(p=5\\), and larger and larger numbers, the resulting distribution becomes more and more normal in shape. Note that the expressions \\(\\Gamma\\left(\\frac{p+1}{2}\\right)\\) and \\(\\Gamma(p/2)\\), refer to the Gamma function. (See Gamma Function in Wikipedia for details.) Graphical Form Comments When the degrees of freedom \\(p=30\\), the resulting t distribution is almost indistinguishable visually from the normal distribution. This is one of the reasons that a sample size of 30 is often used as a rule of thumb for the sample size being â€œlarge enoughâ€ to assume the sampling distribution of the sample mean is approximately normal.",
    "sentences": ["A close friend of the normal distribution is the t distribution.", "Although the t distribution is seldom used to model real life data, the distribution is used extensively in hypothesis testing.", "For example, it is the sampling distribution of the one sample t statistic.", "It also shows up in many other places, like in regression, in the independent samples t test, and in the paired samples t test.", "Mathematical Formula \\[ f(x|p) = \\frac{\\Gamma\\left(\\frac{p+1}{2}\\right)}{\\Gamma\\left(\\frac{p}{2}\\right)}\\frac{1}{\\sqrt{p\\pi}}\\frac{1}{\\left(1 + \\left(\\frac{x^2}{p}\\right)\\right)^{(p+1)/2}} \\] Notice that, similar to the chi squared distribution, the t distribution has only one parameter, the degrees of freedom \\(p\\).", "As the single parameter \\(p\\) is varied from \\(p=1\\), to \\(p=2\\), â€¦, \\(p=5\\), and larger and larger numbers, the resulting distribution becomes more and more normal in shape.", "Note that the expressions \\(\\Gamma\\left(\\frac{p+1}{2}\\right)\\) and \\(\\Gamma(p/2)\\), refer to the Gamma function.", "(See Gamma Function in Wikipedia for details.) Graphical Form Comments When the degrees of freedom \\(p=30\\), the resulting t distribution is almost indistinguishable visually from the normal distribution.", "This is one of the reasons that a sample size of 30 is often used as a rule of thumb for the sample size being â€œlarge enoughâ€ to assume the sampling distribution of the sample mean is approximately normal."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "The t Distribution"
  },
  {
    "id": 123,
    "title": "ğŸ“ The F Distribution",
    "url": "MakingInference.html#thefdistribution",
    "content": "The F Distribution Another commonly used distribution for test statistics, like in ANOVA and regression, is the F distribution. Technically speaking, the F distribution is the ratio of two chi squared random variables that are each divided by their respective degrees of freedom. Mathematical Formula \\[ f(x|p_1,p_2) = \\frac{\\Gamma\\left(\\frac{p_1+p_2}{2}\\right)}{\\Gamma\\left(\\frac{p_1}{2}\\right)\\Gamma\\left(\\frac{p_2}{2}\\right)}\\frac{\\left(\\frac{p_1}{p_2}\\right)^{p_1/2}x^{(p_1-2)/2}}{\\left(1+\\left(\\frac{p_1}{p_2}\\right)x\\right)^{(p_1+p_2)/2}} \\] where \\(x\\geq 0\\) and the parameters \\(p_1\\) and \\(p_2\\) are the â€œnumeratorâ€ and â€œdenominatorâ€ degrees of freedom, respectively. Graphical Form Comments The effects of the parameters \\(p_1\\) and \\(p_2\\) on the F distribution are complicated, but generally speaking, as they both increase the distribution becomes more and more normal in shape.",
    "sentences": ["Another commonly used distribution for test statistics, like in ANOVA and regression, is the F distribution.", "Technically speaking, the F distribution is the ratio of two chi squared random variables that are each divided by their respective degrees of freedom.", "Mathematical Formula \\[ f(x|p_1,p_2) = \\frac{\\Gamma\\left(\\frac{p_1+p_2}{2}\\right)}{\\Gamma\\left(\\frac{p_1}{2}\\right)\\Gamma\\left(\\frac{p_2}{2}\\right)}\\frac{\\left(\\frac{p_1}{p_2}\\right)^{p_1/2}x^{(p_1-2)/2}}{\\left(1+\\left(\\frac{p_1}{p_2}\\right)x\\right)^{(p_1+p_2)/2}} \\] where \\(x\\geq 0\\) and the parameters \\(p_1\\) and \\(p_2\\) are the â€œnumeratorâ€ and â€œdenominatorâ€ degrees of freedom, respectively.", "Graphical Form Comments The effects of the parameters \\(p_1\\) and \\(p_2\\) on the F distribution are complicated, but generally speaking, as they both increase the distribution becomes more and more normal in shape."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "The F Distribution"
  },
  {
    "id": 124,
    "title": "ğŸ“ Nonparametric Methods",
    "url": "MakingInference.html#nonparametricmethods",
    "content": "Nonparametric Methods Nonparametric methods place minimal assumptions on the distribution of data. They allow the data to â€œspeak for itself.â€ They are typically less powerful than the parametric alternatives, but are more broadly applicable because fewer assumptions need to be satisfied. Nonparametric methods include Rank Sum Tests and Permutation Tests.",
    "sentences": ["Nonparametric methods place minimal assumptions on the distribution of data.", "They allow the data to â€œspeak for itself.â€ They are typically less powerful than the parametric alternatives, but are more broadly applicable because fewer assumptions need to be satisfied.", "Nonparametric methods include Rank Sum Tests and Permutation Tests."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Nonparametric Methods"
  },
  {
    "id": 125,
    "title": "Numerical Summaries",
    "url": "NumericalSummaries.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Numerical Summaries There are many ways to numerically summarize data. The fundamental idea is to describe the center, or most probable values of the data, as well as the spread, or the possible values of the data. Mean \\[ \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} \\] Measure of Center | 1 Quantitative Variable Overview The â€œbalance pointâ€ or â€œcenter of massâ€ of quantitative data. It is calculated by taking the numerical sum of the values divided by the number of values. Typically used in tandem with the standard deviation. Most appropriate for describing the most typical values for relatively normally distributed data. Influenced by outliers, so it is not appropriate for describing strongly skewed data. R Instructions To calculate a mean in R use the code: mean(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Usually this is a column from a data set. Use na.rm=TRUE if there are missing values in object so that the code reads mean(object, na.rm=TRUE). Example Code Hover your mouse over the example codes to learn more. mean â€œmeanâ€ is an R function used to calculate the mean of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the mean function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 77.88235 Note that the single number showing above is the average Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveTemp =Â  â€œAveTempâ€ is just a name we made up. It will contain the results of the mean(â€¦) function. mean( â€œmeanâ€ is an R function used to calculate the mean. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month aveTemp 5 65.55 6 79.1 7 83.9 8 83.97 9 76.9 Note that R calculated the mean Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, note that to get the â€œnicely formattedâ€ table, you would have to use library(pander) airquality %>% group_by(Month) %>% summarise(aveTemp = mean(Temp)) %>% pander() A note about missing values in dataâ€¦ mean â€œmeanâ€ is an R function used to calculate the mean of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Ozone â€œOzoneâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. ,Â  The comma allows us to specify optional commands. na.rm=TRUE Missing values are called â€œNAâ€ in R. If data contains missing values, mean(...) will give â€œNAâ€ as the result unless we â€œremoveâ€ (rm) the â€œNAâ€ (na) values. )Closing parenthsis for the mean function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 42.12931 Note that the single number showing above is the average Ozone from the airquality dataset. Because the Ozone column had missing values, we had to use the option na.rm=TRUE to get the mean to calculate. If we had left it off, we would have gotten an â€œNAâ€ result: mean(airquality$Ozone) ## [1] NA Explanation The mathematical formula used to compute the mean of data is given by the formula to the left. Although the formula looks complicated, all it states is â€œadd all the data values up and divide by the total number of values.â€ Read on to learn what all the symbols in the formula represent. Symbols in the Formula \\(\\bar{x}\\) is read â€œx-barâ€ and is the symbol typically used for the sample mean, the mean computed on a sample of data from a population. \\(\\Sigma\\), the capital Greek letter â€œsigma,â€ is the symbol used to imply â€œadd all of the data values up.â€ The \\(x_i\\)â€™s are the data values. The \\(i\\) in the \\(x_i\\) is stated to go from \\(i=1\\) all the way up to \\(n\\). In other words, data value 1 is represented by \\(x_1\\), data value 2: \\(x_2\\), \\(\\ldots\\), up through the last data value \\(x_n\\). In general, we just write \\(x_i\\). \\(n\\) represents the sample size, or number of data values. Population Mean When all of the data from a population is available, the population mean is calculated instead of the sample mean. The mathematical formula for the population mean is the same as the formula for the sample mean, but is written with slightly different notation. \\[ \\mu = \\frac{\\sum_{i=1}^N x_i}{N} \\] Notice that the symbol for the population mean is \\(\\mu\\), pronounced â€œmew,â€ another Greek letter. (Review your Greek alphabet.) The only other difference between the two formulas is that the sample mean uses a sample of data, denoted by \\(n\\), while the population mean uses all the population data, denoted by \\(N\\). Physical Interpretation The mean is sometimes described as the â€œbalance pointâ€ of the data. The following example will demonstrate. Say there are \\(n=5\\) data points with the following values. \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) The sample mean is calculated as follows. \\[ \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} = \\frac{2 + 5 + 6 + 7 + 10}{5} = 6 \\] If these values were plotted, and an â€œinfinitely thin barâ€ connected the points, then the bar would â€œbalanceâ€ at the mean (the triangle) as shown below. Middle of the Deviations The above plot demonstrates that there are equal, but opposite, â€œsums of deviationsâ€ to either side of the mean. Note that a deviation is defined as the distance from the mean to a given point. Thus, \\(x_1\\) has a deviation of -4 from the mean, \\(x_2\\) a deviation of -1, \\(x_3\\) a deviation of 0, \\(x_4\\) a deviation of 1, and \\(x_5\\) a deviation of 4. To the left there is a sum of deviations equal to -5 and on the right, a sum of deviations equal to 5. This can be verified to hold for any scenario. Effect of Outliers The mean can be strongly influenced by outliers, points that deviate abnormally from the mean. This is shown below by changing \\(x_5\\) to be 20. Note that the deviation of \\(x_5\\) is 12, and the sum of deviations to the left of the mean (\\(\\bar{x}=8\\)) is \\(-1 + -2 + -3 + -6 = -12\\). The mean of the altered data \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 20\\) is now \\(\\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} = \\frac{2 + 5 + 6 + 7 + 20}{5} = 8\\). Median \\[ \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} \\] \\(\\uparrow\\) even \\(n\\) odd \\(\\downarrow\\) \\[ x_{((n+1)/2)} \\] Measure of Center | 1 Quantitative Variable Overview The â€œmiddle data point,â€ i.e., the 50\\(^{th}\\) percentile. Half of the data is below the median and half is above the median. Typically used in tandem with the five-number summary to describe skewed data because it is not heavily influenced by outliers, i.e., it is robust. Can also be used with normally distributed data, but the mean and standard deviation are more useful measures in such cases. R Instructions To calculate a median in R use the code: median(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code median â€œmedianâ€ is an R function used to calculate the median of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the median function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 79 Note that the single number showing above is the median Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. medTemp =Â  â€œmedTempâ€ is just a name we made up. It will contain the results of the median(â€¦) function. median( â€œmedianâ€ is an R function used to calculate the median. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month medTemp 5 66 6 78 7 84 8 82 9 76 Note that R calculated the median Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(medTemp = median(Temp)) %>% pander() Explanation The mathematical formula used to compute the median of data depends on whether \\(n\\), the number of data points in the sample, is even or odd. If \\(n\\) is even, then there is no â€œmiddleâ€ data point, so the middle two values are averaged. \\[ \\text{Median} = \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} \\] If \\(n\\) is odd, then the middle data point is the median. \\[ \\text{Median} = x_{((n+1)/2)} \\] Symbols in the Formula There is no generally accepted symbol for the median. Sometimes a capital \\(M\\) or even lower-case \\(m\\) is used, but generally the word median is just written out. \\(x_{(n/2)}\\) represents the data value that is in the \\((n/2)^{th}\\) position in the ordered list of values. It only exists when \\(n\\) is even. \\(x_{(n/2+1)}\\) represents the data value that immediately follows the \\((n/2)^{th}\\) value in the ordered list of values. It only exists when \\(n\\) is even. \\(x_{((n+1)/2)}\\) represents the data value that is in the \\(((n+1)/2)^{th}\\) position in the ordered list of values. It only exists when \\(n\\) is odd. \\(n\\) represents the sample size, or number of data values in the sample. Population Median When all of the data from a population is available, the population median is calculated by the above formulas with the slight change that \\(N\\), the total number of data values in the population, instead of \\(n\\), the number of values in the sample, is used. If \\(N\\) is even, then there is no â€œmiddleâ€ data point, so the middle two values are averaged. \\[ \\text{Median} = \\frac{x_{(N/2)}+x_{(N/2+1)}}{2} \\] If \\(N\\) is odd, then the middle data point is the median. \\[ \\text{Median} = x_{((N+1)/2)} \\] Physical Interpretation The median is the \\(50^{th}\\) percentile of the data. Say there are \\(n=5\\) data points in the sample with the following values. \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) The sample median is calculated as follows. Note that \\(n=5\\) is odd. \\[ \\text{Median} = x_{((n+1)/2)} = x_{((5+1)/2)} = x_{(3)} = 6 \\] When these values are plotted it is clear that exactly 50% of the data (excluding the median) is to either side of the median. Second Example Say there was a sixth value in the data set equal to 10, so that \\(n=6\\) is even. \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) \\(x_6 = 10\\) \\[ \\text{Median} = \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} = \\frac{x_{(6/2)}+x_{(6/2+1)}}{2} = \\frac{x_{(3)}+x_{(4)}}{2} = \\frac{6+7}{2} = 6.5 \\] Effect of Outliers The median is not greatly influenced by outliers. It is said to be robust. This is shown below by changing \\(x_6\\) to be 20, which does not change the value of the median. Mode Most Frequent Value Measure of Center | 1 Quantitative or Qualitative Variable Overview The most commonly occurring value. There may be more than one mode. Seldom used, but sometimes useful. R Instructions R will not calculate a mode directly. However, to tabulate the number of times each value occurs in a dataset, use the code: table(object) object can be quantitative or qualitative, but should contain at least one repeated value or table() is not useful. Example Code Hover your mouse over the example codes to learn more. table â€œtableâ€ is an R function used to count how many times each observation occurs in a list of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Month â€œMonthâ€ is a qualitative vari",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Numerical Summaries There are many ways to numerically summarize data.", "The fundamental idea is to describe the center, or most probable values of the data, as well as the spread, or the possible values of the data.", "Mean \\[ \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} \\] Measure of Center | 1 Quantitative Variable Overview The â€œbalance pointâ€ or â€œcenter of massâ€ of quantitative data.", "It is calculated by taking the numerical sum of the values divided by the number of values.", "Typically used in tandem with the standard deviation.", "Most appropriate for describing the most typical values for relatively normally distributed data.", "Influenced by outliers, so it is not appropriate for describing strongly skewed data.", "R Instructions To calculate a mean in R use the code: mean(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Usually this is a column from a data set.", "Use na.rm=TRUE if there are missing values in object so that the code reads mean(object, na.rm=TRUE).", "Example Code Hover your mouse over the example codes to learn more.", "mean â€œmeanâ€ is an R function used to calculate the mean of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset.", "Type â€œView(airquality)â€ in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset.", ")Closing parenthsis for the mean function.", "Â Â Â Â Press Enter to run the code if you have typed it in yourself.", "You can also click here to view the output.", "Â â€¦Â  Click to View Output.", "## [1] 77.88235 Note that the single number showing above is the average Temp from the airquality dataset.", "library(tidyverse) tidyverse is an R Package that is very useful for working with data.", "airquality airquality is a dataset in R.", "Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line.", "Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column.", "Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative.", ") Functions must always end with a closing parenthesis.", "Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.", "Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data.", "aveTemp =Â  â€œAveTempâ€ is just a name we made up.", "It will contain the results of the mean(â€¦) function.", "mean( â€œmeanâ€ is an R function used to calculate the mean.", "Temp Temp is a quantitative variable (numeric vector) from the airquality dataset.", ") Functions must always end with a closing parenthesis.", ") Functions must always end with a closing parenthesis.", "Â Â Â Â Press Enter to run the code.", "Â â€¦Â  Click to View Output.", "Month aveTemp 5 65.55 6 79.1 7 83.9 8 83.97 9 76.9 Note that R calculated the mean Temp for each month in Month from the airquality dataset.", "May (5), June (6), July (7), August (8), and September (9), respectively.", "Further, note that to get the â€œnicely formattedâ€ table, you would have to use library(pander) airquality %>% group_by(Month) %>% summarise(aveTemp = mean(Temp)) %>% pander() A note about missing values in dataâ€¦ mean â€œmeanâ€ is an R function used to calculate the mean of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset.", "Type â€œView(airquality)â€ in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Ozone â€œOzoneâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset.", ",Â  The comma allows us to specify optional commands.", "na.rm=TRUE Missing values are called â€œNAâ€ in R.", "If data contains missing values, mean(...) will give â€œNAâ€ as the result unless we â€œremoveâ€ (rm) the â€œNAâ€ (na) values."],
    "type": "page",
    "page_title": "Numerical Summaries"
  },
  {
    "id": 126,
    "title": "ğŸ“ Mean",
    "url": "NumericalSummaries.html#mean",
    "content": "Mean \\[ \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} \\] Measure of Center | 1 Quantitative Variable Overview The â€œbalance pointâ€ or â€œcenter of massâ€ of quantitative data. It is calculated by taking the numerical sum of the values divided by the number of values. Typically used in tandem with the standard deviation. Most appropriate for describing the most typical values for relatively normally distributed data. Influenced by outliers, so it is not appropriate for describing strongly skewed data. R Instructions To calculate a mean in R use the code: mean(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Usually this is a column from a data set. Use na.rm=TRUE if there are missing values in object so that the code reads mean(object, na.rm=TRUE). Example Code Hover your mouse over the example codes to learn more. mean â€œmeanâ€ is an R function used to calculate the mean of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the mean function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 77.88235 Note that the single number showing above is the average Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveTemp =Â  â€œAveTempâ€ is just a name we made up. It will contain the results of the mean(â€¦) function. mean( â€œmeanâ€ is an R function used to calculate the mean. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month aveTemp 5 65.55 6 79.1 7 83.9 8 83.97 9 76.9 Note that R calculated the mean Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, note that to get the â€œnicely formattedâ€ table, you would have to use library(pander) airquality %>% group_by(Month) %>% summarise(aveTemp = mean(Temp)) %>% pander() A note about missing values in dataâ€¦ mean â€œmeanâ€ is an R function used to calculate the mean of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Ozone â€œOzoneâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. ,Â  The comma allows us to specify optional commands. na.rm=TRUE Missing values are called â€œNAâ€ in R. If data contains missing values, mean(...) will give â€œNAâ€ as the result unless we â€œremoveâ€ (rm) the â€œNAâ€ (na) values. )Closing parenthsis for the mean function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 42.12931 Note that the single number showing above is the average Ozone from the airquality dataset. Because the Ozone column had missing values, we had to use the option na.rm=TRUE to get the mean to calculate. If we had left it off, we would have gotten an â€œNAâ€ result: mean(airquality$Ozone) ## [1] NA",
    "sentences": ["\\[ \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} \\]", "Measure of Center | 1 Quantitative Variable", "Overview The â€œbalance pointâ€ or â€œcenter of massâ€ of quantitative data.", "It is calculated by taking the numerical sum of the values divided by the number of values.", "Typically used in tandem with the standard deviation.", "Most appropriate for describing the most typical values for relatively normally distributed data.", "Influenced by outliers, so it is not appropriate for describing strongly skewed data.", "R Instructions To calculate a mean in R use the code: mean(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Usually this is a column from a data set.", "Use na.rm=TRUE if there are missing values in object so that the code reads mean(object, na.rm=TRUE).", "Example Code Hover your mouse over the example codes to learn more."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Mean"
  },
  {
    "id": 127,
    "title": "ğŸ“ Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview The â€œbalance pointâ€ or â€œcenter of massâ€ of quantitative data. It is calculated by taking the numerical sum of the values divided by the number of values. Typically used in tandem with the standard deviation. Most appropriate for describing the most typical values for relatively normally distributed data. Influenced by outliers, so it is not appropriate for describing strongly skewed data.",
    "sentences": ["The â€œbalance pointâ€ or â€œcenter of massâ€ of quantitative data.", "It is calculated by taking the numerical sum of the values divided by the number of values.", "Typically used in tandem with the standard deviation.", "Most appropriate for describing the most typical values for relatively normally distributed data.", "Influenced by outliers, so it is not appropriate for describing strongly skewed data."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 128,
    "title": "ğŸ“ R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To calculate a mean in R use the code: mean(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Usually this is a column from a data set. Use na.rm=TRUE if there are missing values in object so that the code reads mean(object, na.rm=TRUE). Example Code Hover your mouse over the example codes to learn more. mean â€œmeanâ€ is an R function used to calculate the mean of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the mean function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 77.88235 Note that the single number showing above is the average Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveTemp =Â  â€œAveTempâ€ is just a name we made up. It will contain the results of the mean(â€¦) function. mean( â€œmeanâ€ is an R function used to calculate the mean. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month aveTemp 5 65.55 6 79.1 7 83.9 8 83.97 9 76.9 Note that R calculated the mean Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, note that to get the â€œnicely formattedâ€ table, you would have to use library(pander) airquality %>% group_by(Month) %>% summarise(aveTemp = mean(Temp)) %>% pander() A note about missing values in dataâ€¦ mean â€œmeanâ€ is an R function used to calculate the mean of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Ozone â€œOzoneâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. ,Â  The comma allows us to specify optional commands. na.rm=TRUE Missing values are called â€œNAâ€ in R. If data contains missing values, mean(...) will give â€œNAâ€ as the result unless we â€œremoveâ€ (rm) the â€œNAâ€ (na) values. )Closing parenthsis for the mean function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 42.12931 Note that the single number showing above is the average Ozone from the airquality dataset. Because the Ozone column had missing values, we had to use the option na.rm=TRUE to get the mean to calculate. If we had left it off, we would have gotten an â€œNAâ€ result: mean(airquality$Ozone) ## [1] NA",
    "sentences": ["To calculate a mean in R use the code: mean(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Usually this is a column from a data set.", "Use na.rm=TRUE if there are missing values in object so that the code reads mean(object, na.rm=TRUE).", "Example Code Hover your mouse over the example codes to learn more.", "mean â€œmeanâ€ is an R function used to calculate the mean of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset.", "Type â€œView(airquality)â€ in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 129,
    "title": "ğŸ“ Explanation",
    "url": "NumericalSummaries.html#explanation",
    "content": "Explanation The mathematical formula used to compute the mean of data is given by the formula to the left. Although the formula looks complicated, all it states is â€œadd all the data values up and divide by the total number of values.â€ Read on to learn what all the symbols in the formula represent. Symbols in the Formula \\(\\bar{x}\\) is read â€œx-barâ€ and is the symbol typically used for the sample mean, the mean computed on a sample of data from a population. \\(\\Sigma\\), the capital Greek letter â€œsigma,â€ is the symbol used to imply â€œadd all of the data values up.â€ The \\(x_i\\)â€™s are the data values. The \\(i\\) in the \\(x_i\\) is stated to go from \\(i=1\\) all the way up to \\(n\\). In other words, data value 1 is represented by \\(x_1\\), data value 2: \\(x_2\\), \\(\\ldots\\), up through the last data value \\(x_n\\). In general, we just write \\(x_i\\). \\(n\\) represents the sample size, or number of data values. Population Mean When all of the data from a population is available, the population mean is calculated instead of the sample mean. The mathematical formula for the population mean is the same as the formula for the sample mean, but is written with slightly different notation. \\[ \\mu = \\frac{\\sum_{i=1}^N x_i}{N} \\] Notice that the symbol for the population mean is \\(\\mu\\), pronounced â€œmew,â€ another Greek letter. (Review your Greek alphabet.) The only other difference between the two formulas is that the sample mean uses a sample of data, denoted by \\(n\\), while the population mean uses all the population data, denoted by \\(N\\). Physical Interpretation The mean is sometimes described as the â€œbalance pointâ€ of the data. The following example will demonstrate. Say there are \\(n=5\\) data points with the following values. \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) The sample mean is calculated as follows. \\[ \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} = \\frac{2 + 5 + 6 + 7 + 10}{5} = 6 \\] If these values were plotted, and an â€œinfinitely thin barâ€ connected the points, then the bar would â€œbalanceâ€ at the mean (the triangle) as shown below. Middle of the Deviations The above plot demonstrates that there are equal, but opposite, â€œsums of deviationsâ€ to either side of the mean. Note that a deviation is defined as the distance from the mean to a given point. Thus, \\(x_1\\) has a deviation of -4 from the mean, \\(x_2\\) a deviation of -1, \\(x_3\\) a deviation of 0, \\(x_4\\) a deviation of 1, and \\(x_5\\) a deviation of 4. To the left there is a sum of deviations equal to -5 and on the right, a sum of deviations equal to 5. This can be verified to hold for any scenario. Effect of Outliers The mean can be strongly influenced by outliers, points that deviate abnormally from the mean. This is shown below by changing \\(x_5\\) to be 20. Note that the deviation of \\(x_5\\) is 12, and the sum of deviations to the left of the mean (\\(\\bar{x}=8\\)) is \\(-1 + -2 + -3 + -6 = -12\\). The mean of the altered data \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 20\\) is now \\(\\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} = \\frac{2 + 5 + 6 + 7 + 20}{5} = 8\\).",
    "sentences": ["The mathematical formula used to compute the mean of data is given by the formula to the left.", "Although the formula looks complicated, all it states is â€œadd all the data values up and divide by the total number of values.â€ Read on to learn what all the symbols in the formula represent.", "Symbols in the Formula \\(\\bar{x}\\) is read â€œx-barâ€ and is the symbol typically used for the sample mean, the mean computed on a sample of data from a population.", "\\(\\Sigma\\), the capital Greek letter â€œsigma,â€ is the symbol used to imply â€œadd all of the data values up.â€ The \\(x_i\\)â€™s are the data values.", "The \\(i\\) in the \\(x_i\\) is stated to go from \\(i=1\\) all the way up to \\(n\\).", "In other words, data value 1 is represented by \\(x_1\\), data value 2: \\(x_2\\), \\(\\ldots\\), up through the last data value \\(x_n\\).", "In general, we just write \\(x_i\\).", "\\(n\\) represents the sample size, or number of data values.", "Population Mean When all of the data from a population is available, the population mean is calculated instead of the sample mean.", "The mathematical formula for the population mean is the same as the formula for the sample mean, but is written with slightly different notation."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Explanation"
  },
  {
    "id": 130,
    "title": "ğŸ“ Median",
    "url": "NumericalSummaries.html#median",
    "content": "Median \\[ \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} \\] \\(\\uparrow\\) even \\(n\\) odd \\(\\downarrow\\) \\[ x_{((n+1)/2)} \\] Measure of Center | 1 Quantitative Variable Overview The â€œmiddle data point,â€ i.e., the 50\\(^{th}\\) percentile. Half of the data is below the median and half is above the median. Typically used in tandem with the five-number summary to describe skewed data because it is not heavily influenced by outliers, i.e., it is robust. Can also be used with normally distributed data, but the mean and standard deviation are more useful measures in such cases. R Instructions To calculate a median in R use the code: median(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code median â€œmedianâ€ is an R function used to calculate the median of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the median function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 79 Note that the single number showing above is the median Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. medTemp =Â  â€œmedTempâ€ is just a name we made up. It will contain the results of the median(â€¦) function. median( â€œmedianâ€ is an R function used to calculate the median. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month medTemp 5 66 6 78 7 84 8 82 9 76 Note that R calculated the median Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(medTemp = median(Temp)) %>% pander()",
    "sentences": ["\\[ \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} \\] \\(\\uparrow\\) even \\(n\\) odd \\(\\downarrow\\) \\[ x_{((n+1)/2)} \\]", "Measure of Center | 1 Quantitative Variable", "Overview The â€œmiddle data point,â€ i.e., the 50\\(^{th}\\) percentile.", "Half of the data is below the median and half is above the median.", "Typically used in tandem with the five-number summary to describe skewed data because it is not heavily influenced by outliers, i.e., it is robust.", "Can also be used with normally distributed data, but the mean and standard deviation are more useful measures in such cases.", "R Instructions To calculate a median in R use the code: median(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code median â€œmedianâ€ is an R function used to calculate the median of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Median"
  },
  {
    "id": 131,
    "title": "ğŸ“ Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview The â€œmiddle data point,â€ i.e., the 50\\(^{th}\\) percentile. Half of the data is below the median and half is above the median. Typically used in tandem with the five-number summary to describe skewed data because it is not heavily influenced by outliers, i.e., it is robust. Can also be used with normally distributed data, but the mean and standard deviation are more useful measures in such cases.",
    "sentences": ["The â€œmiddle data point,â€ i.e., the 50\\(^{th}\\) percentile.", "Half of the data is below the median and half is above the median.", "Typically used in tandem with the five-number summary to describe skewed data because it is not heavily influenced by outliers, i.e., it is robust.", "Can also be used with normally distributed data, but the mean and standard deviation are more useful measures in such cases."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 132,
    "title": "ğŸ“ R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To calculate a median in R use the code: median(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code median â€œmedianâ€ is an R function used to calculate the median of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the median function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 79 Note that the single number showing above is the median Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. medTemp =Â  â€œmedTempâ€ is just a name we made up. It will contain the results of the median(â€¦) function. median( â€œmedianâ€ is an R function used to calculate the median. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month medTemp 5 66 6 78 7 84 8 82 9 76 Note that R calculated the median Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(medTemp = median(Temp)) %>% pander()",
    "sentences": ["To calculate a median in R use the code: median(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code median â€œmedianâ€ is an R function used to calculate the median of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset.", "Type â€œView(airquality)â€ in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset.", ")Closing parenthsis for the median function.", "Â Â Â Â Press Enter to run the code if you have typed it in yourself.", "You can also click here to view the output."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 133,
    "title": "ğŸ“ Explanation",
    "url": "NumericalSummaries.html#explanation",
    "content": "Explanation The mathematical formula used to compute the median of data depends on whether \\(n\\), the number of data points in the sample, is even or odd. If \\(n\\) is even, then there is no â€œmiddleâ€ data point, so the middle two values are averaged. \\[ \\text{Median} = \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} \\] If \\(n\\) is odd, then the middle data point is the median. \\[ \\text{Median} = x_{((n+1)/2)} \\] Symbols in the Formula There is no generally accepted symbol for the median. Sometimes a capital \\(M\\) or even lower-case \\(m\\) is used, but generally the word median is just written out. \\(x_{(n/2)}\\) represents the data value that is in the \\((n/2)^{th}\\) position in the ordered list of values. It only exists when \\(n\\) is even. \\(x_{(n/2+1)}\\) represents the data value that immediately follows the \\((n/2)^{th}\\) value in the ordered list of values. It only exists when \\(n\\) is even. \\(x_{((n+1)/2)}\\) represents the data value that is in the \\(((n+1)/2)^{th}\\) position in the ordered list of values. It only exists when \\(n\\) is odd. \\(n\\) represents the sample size, or number of data values in the sample. Population Median When all of the data from a population is available, the population median is calculated by the above formulas with the slight change that \\(N\\), the total number of data values in the population, instead of \\(n\\), the number of values in the sample, is used. If \\(N\\) is even, then there is no â€œmiddleâ€ data point, so the middle two values are averaged. \\[ \\text{Median} = \\frac{x_{(N/2)}+x_{(N/2+1)}}{2} \\] If \\(N\\) is odd, then the middle data point is the median. \\[ \\text{Median} = x_{((N+1)/2)} \\] Physical Interpretation The median is the \\(50^{th}\\) percentile of the data. Say there are \\(n=5\\) data points in the sample with the following values. \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) The sample median is calculated as follows. Note that \\(n=5\\) is odd. \\[ \\text{Median} = x_{((n+1)/2)} = x_{((5+1)/2)} = x_{(3)} = 6 \\] When these values are plotted it is clear that exactly 50% of the data (excluding the median) is to either side of the median. Second Example Say there was a sixth value in the data set equal to 10, so that \\(n=6\\) is even. \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) \\(x_6 = 10\\) \\[ \\text{Median} = \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} = \\frac{x_{(6/2)}+x_{(6/2+1)}}{2} = \\frac{x_{(3)}+x_{(4)}}{2} = \\frac{6+7}{2} = 6.5 \\] Effect of Outliers The median is not greatly influenced by outliers. It is said to be robust. This is shown below by changing \\(x_6\\) to be 20, which does not change the value of the median.",
    "sentences": ["The mathematical formula used to compute the median of data depends on whether \\(n\\), the number of data points in the sample, is even or odd.", "If \\(n\\) is even, then there is no â€œmiddleâ€ data point, so the middle two values are averaged.", "\\[ \\text{Median} = \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} \\] If \\(n\\) is odd, then the middle data point is the median.", "\\[ \\text{Median} = x_{((n+1)/2)} \\] Symbols in the Formula There is no generally accepted symbol for the median.", "Sometimes a capital \\(M\\) or even lower-case \\(m\\) is used, but generally the word median is just written out.", "\\(x_{(n/2)}\\) represents the data value that is in the \\((n/2)^{th}\\) position in the ordered list of values.", "It only exists when \\(n\\) is even.", "\\(x_{(n/2+1)}\\) represents the data value that immediately follows the \\((n/2)^{th}\\) value in the ordered list of values.", "It only exists when \\(n\\) is even.", "\\(x_{((n+1)/2)}\\) represents the data value that is in the \\(((n+1)/2)^{th}\\) position in the ordered list of values."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Explanation"
  },
  {
    "id": 134,
    "title": "ğŸ“ Mode",
    "url": "NumericalSummaries.html#mode",
    "content": "Mode Most Frequent Value Measure of Center | 1 Quantitative or Qualitative Variable Overview The most commonly occurring value. There may be more than one mode. Seldom used, but sometimes useful. R Instructions R will not calculate a mode directly. However, to tabulate the number of times each value occurs in a dataset, use the code: table(object) object can be quantitative or qualitative, but should contain at least one repeated value or table() is not useful. Example Code Hover your mouse over the example codes to learn more. table â€œtableâ€ is an R function used to count how many times each observation occurs in a list of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Month â€œMonthâ€ is a qualitative variable (technically a numeric vector) from the â€œairqualityâ€ dataset that contains repeated values. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## ## 5 6 7 8 9 ## 31 30 31 31 30 Note that the modes would be 5, 7, and 8 because these months all have the most (31) days in them. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveTemp = mean(Temp),Â  Computes the mean of the Temp column. medTemp = median(Temp),Â  Computes the median of the Temp column. sampleSize = n( ) Counts how many times each Month (the group_by statement) occurs in the dataset. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month aveTemp medTemp sampeSize 5 65.55 66 31 6 79.1 78 30 7 83.9 84 31 8 83.97 82 31 9 76.9 76 30 Note that R calculated the median Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(aveTemp = mean(Temp), medTemp = median(Temp), sampeSize = n()) %>% pander()",
    "sentences": ["Most Frequent Value", "Measure of Center | 1 Quantitative or Qualitative Variable", "Overview The most commonly occurring value.", "There may be more than one mode.", "Seldom used, but sometimes useful.", "R Instructions R will not calculate a mode directly.", "However, to tabulate the number of times each value occurs in a dataset, use the code: table(object) object can be quantitative or qualitative, but should contain at least one repeated value or table() is not useful.", "Example Code Hover your mouse over the example codes to learn more.", "table â€œtableâ€ is an R function used to count how many times each observation occurs in a list of data.", "( Parenthesis to begin the function."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Mode"
  },
  {
    "id": 135,
    "title": "ğŸ“ Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview The most commonly occurring value. There may be more than one mode. Seldom used, but sometimes useful.",
    "sentences": ["The most commonly occurring value.", "There may be more than one mode.", "Seldom used, but sometimes useful."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 136,
    "title": "ğŸ“ R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions R will not calculate a mode directly. However, to tabulate the number of times each value occurs in a dataset, use the code: table(object) object can be quantitative or qualitative, but should contain at least one repeated value or table() is not useful. Example Code Hover your mouse over the example codes to learn more. table â€œtableâ€ is an R function used to count how many times each observation occurs in a list of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Month â€œMonthâ€ is a qualitative variable (technically a numeric vector) from the â€œairqualityâ€ dataset that contains repeated values. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## ## 5 6 7 8 9 ## 31 30 31 31 30 Note that the modes would be 5, 7, and 8 because these months all have the most (31) days in them. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveTemp = mean(Temp),Â  Computes the mean of the Temp column. medTemp = median(Temp),Â  Computes the median of the Temp column. sampleSize = n( ) Counts how many times each Month (the group_by statement) occurs in the dataset. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month aveTemp medTemp sampeSize 5 65.55 66 31 6 79.1 78 30 7 83.9 84 31 8 83.97 82 31 9 76.9 76 30 Note that R calculated the median Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(aveTemp = mean(Temp), medTemp = median(Temp), sampeSize = n()) %>% pander()",
    "sentences": ["R will not calculate a mode directly.", "However, to tabulate the number of times each value occurs in a dataset, use the code: table(object) object can be quantitative or qualitative, but should contain at least one repeated value or table() is not useful.", "Example Code Hover your mouse over the example codes to learn more.", "table â€œtableâ€ is an R function used to count how many times each observation occurs in a list of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset.", "Type â€œView(airquality)â€ in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Month â€œMonthâ€ is a qualitative variable (technically a numeric vector) from the â€œairqualityâ€ dataset that contains repeated values."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 137,
    "title": "ğŸ“ Minimum",
    "url": "NumericalSummaries.html#minimum",
    "content": "Minimum \\[ x_{(1)} \\] Measure of Spread | 1 Quantitative Variable Overview The smallest occurring data value. One of the numerical summaries in the five-number summary. Typically not useful on its own. Gives a good feel for the spread in the left tail of the distribution when used with the five-number summary. R Instructions To calculate a minimum in R use the code: min(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more. min â€œminâ€ is an R function used to calculate the minimum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 56 Note that the single number showing above is the minimum Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. minTemp =Â  â€œminTempâ€ is just a name we made up. It will contain the results of the median(â€¦) function. min( â€œminâ€ is an R function used to calculate the minimum. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month minTemp 5 56 6 65 7 73 8 72 9 63 Note that R calculated the minimum Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(minTemp = min(Temp)) %>% pander()",
    "sentences": ["\\[ x_{(1)} \\]", "Measure of Spread | 1 Quantitative Variable", "Overview The smallest occurring data value.", "One of the numerical summaries in the five-number summary.", "Typically not useful on its own.", "Gives a good feel for the spread in the left tail of the distribution when used with the five-number summary.", "R Instructions To calculate a minimum in R use the code: min(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more.", "min â€œminâ€ is an R function used to calculate the minimum of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Minimum"
  },
  {
    "id": 138,
    "title": "ğŸ“ Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview The smallest occurring data value. One of the numerical summaries in the five-number summary. Typically not useful on its own. Gives a good feel for the spread in the left tail of the distribution when used with the five-number summary.",
    "sentences": ["The smallest occurring data value.", "One of the numerical summaries in the five-number summary.", "Typically not useful on its own.", "Gives a good feel for the spread in the left tail of the distribution when used with the five-number summary."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 139,
    "title": "ğŸ“ R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To calculate a minimum in R use the code: min(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more. min â€œminâ€ is an R function used to calculate the minimum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 56 Note that the single number showing above is the minimum Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. minTemp =Â  â€œminTempâ€ is just a name we made up. It will contain the results of the median(â€¦) function. min( â€œminâ€ is an R function used to calculate the minimum. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month minTemp 5 56 6 65 7 73 8 72 9 63 Note that R calculated the minimum Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(minTemp = min(Temp)) %>% pander()",
    "sentences": ["To calculate a minimum in R use the code: min(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more.", "min â€œminâ€ is an R function used to calculate the minimum of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset.", "Type â€œView(airquality)â€ in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset.", ")Closing parenthsis for the function.", "Â Â Â Â Press Enter to run the code if you have typed it in yourself."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 140,
    "title": "ğŸ“ Maximum",
    "url": "NumericalSummaries.html#maximum",
    "content": "Maximum \\[ x_{(n)} \\] Measure of Spread | 1 Quantitative Variable Overview The largest occurring data value. One of the numerical summaries in the five-number summary. Typically not useful on its own. Gives a good feel for the spread in the right tail of the distribution when used in the five-number summary. R Instructions To calculate a maximum in R use the code: max(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more. max â€œmaxâ€ is an R function used to calculate the maximum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 97 Note that the single number showing above is the maximum Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. maxTemp =Â  â€œmaxTempâ€ is just a name we made up. It will contain the results of the median(â€¦) function. max( â€œmaxâ€ is an R function used to calculate the maximum. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month maxTemp 5 81 6 93 7 92 8 97 9 93 Note that R calculated the maximum Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(maxTemp = max(Temp)) %>% pander()",
    "sentences": ["\\[ x_{(n)} \\]", "Measure of Spread | 1 Quantitative Variable", "Overview The largest occurring data value.", "One of the numerical summaries in the five-number summary.", "Typically not useful on its own.", "Gives a good feel for the spread in the right tail of the distribution when used in the five-number summary.", "R Instructions To calculate a maximum in R use the code: max(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more.", "max â€œmaxâ€ is an R function used to calculate the maximum of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Maximum"
  },
  {
    "id": 141,
    "title": "ğŸ“ Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview The largest occurring data value. One of the numerical summaries in the five-number summary. Typically not useful on its own. Gives a good feel for the spread in the right tail of the distribution when used in the five-number summary.",
    "sentences": ["The largest occurring data value.", "One of the numerical summaries in the five-number summary.", "Typically not useful on its own.", "Gives a good feel for the spread in the right tail of the distribution when used in the five-number summary."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 142,
    "title": "ğŸ“ R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To calculate a maximum in R use the code: max(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more. max â€œmaxâ€ is an R function used to calculate the maximum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 97 Note that the single number showing above is the maximum Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. maxTemp =Â  â€œmaxTempâ€ is just a name we made up. It will contain the results of the median(â€¦) function. max( â€œmaxâ€ is an R function used to calculate the maximum. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month maxTemp 5 81 6 93 7 92 8 97 9 93 Note that R calculated the maximum Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(maxTemp = max(Temp)) %>% pander()",
    "sentences": ["To calculate a maximum in R use the code: max(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more.", "max â€œmaxâ€ is an R function used to calculate the maximum of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset.", "Type â€œView(airquality)â€ in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset.", ")Closing parenthsis for the function.", "Â Â Â Â Press Enter to run the code if you have typed it in yourself."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 143,
    "title": "ğŸ“ Quartiles (five-number\r\nsummary)",
    "url": "NumericalSummaries.html#quartilesfivenumbersummary",
    "content": "Quartiles (five-number\r\nsummary) 25\\(^{th}\\), 50\\(^{th}\\), 75\\(^{th}\\) and 100\\(^{th}\\) Percentiles Measure of Center & Spread | 1 Quantitative Variable Overview Good for describing the spread of data, typically for skewed distributions. There are four quartiles. They make up the five-number summary when combined with the minimum. The second quartile is the median (50\\(^{th}\\) percentile) and the fourth quartile is the maximum (100\\(^{th}\\) percentile). The first quartile (\\(Q_1\\) or lower quartile) and third quartile (\\(Q_3\\) or upper quartile) show the spread of the â€œmiddle 50%â€ of the data, which is often called the interquartile range. Comparing the interquartile range to the minimum and maximum shows how the possible values spread out around the more probable values. R Instructions To calculate a five-number summary (and mean) in R use the code: quantile(object, percentile) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ percentile must be a value between 0 and 1. For the first quartile, it would be 0.25. For the third, it would be 0.75. Example Code Hover your mouse over the example codes to learn more. summary â€œsummaryâ€ is an R function used to calculate the five-number summary (and mean) of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Median Mean 3rd Qu. 56 72 79 77.88 85 97 Showing above are the five-number summary and mean of Temp from the airquality dataset. Note, you must use pander(summary(airquality$Temp)) to get the nicely formatted output. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. min = min(Temp),Â  Computes the min of the Temp column. Q1 = quantile(Temp, 0.25),Â  Computes the first quartile of the Temp column. med = median(Temp),Â  Computes the second quartile of the Temp column, known as the median. Q3 = quantile(Temp, 0.75),Â  Computes the third quartile of the Temp column. max = max(Temp) Computes the max of the Temp column. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month min Q1 med Q3 max 5 56 60 66 69 81 6 65 76 78 82.75 93 7 73 81.5 84 86 92 8 72 79 82 88.5 97 9 63 71 76 81 93 Note that R calculated the five-number summary for Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(min = min(Temp), Q1 = quantile(Temp, c(.25)), med = median(Temp), Q3 = quantile(Temp, c(.75)), max = max(Temp)) %>% pander()",
    "sentences": ["25\\(^{th}\\), 50\\(^{th}\\), 75\\(^{th}\\) and 100\\(^{th}\\) Percentiles", "Measure of Center & Spread | 1 Quantitative Variable", "Overview Good for describing the spread of data, typically for skewed distributions.", "There are four quartiles.", "They make up the five-number summary when combined with the minimum.", "The second quartile is the median (50\\(^{th}\\) percentile) and the fourth quartile is the maximum (100\\(^{th}\\) percentile).", "The first quartile (\\(Q_1\\) or lower quartile) and third quartile (\\(Q_3\\) or upper quartile) show the spread of the â€œmiddle 50%â€ of the data, which is often called the interquartile range.", "Comparing the interquartile range to the minimum and maximum shows how the possible values spread out around the more probable values.", "R Instructions To calculate a five-number summary (and mean) in R use the code: quantile(object, percentile) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ percentile must be a value between 0 and 1.", "For the first quartile, it would be 0.25."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Quartiles (five-number\r\nsummary)"
  },
  {
    "id": 144,
    "title": "ğŸ“ Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview Good for describing the spread of data, typically for skewed distributions. There are four quartiles. They make up the five-number summary when combined with the minimum. The second quartile is the median (50\\(^{th}\\) percentile) and the fourth quartile is the maximum (100\\(^{th}\\) percentile). The first quartile (\\(Q_1\\) or lower quartile) and third quartile (\\(Q_3\\) or upper quartile) show the spread of the â€œmiddle 50%â€ of the data, which is often called the interquartile range. Comparing the interquartile range to the minimum and maximum shows how the possible values spread out around the more probable values.",
    "sentences": ["Good for describing the spread of data, typically for skewed distributions.", "There are four quartiles.", "They make up the five-number summary when combined with the minimum.", "The second quartile is the median (50\\(^{th}\\) percentile) and the fourth quartile is the maximum (100\\(^{th}\\) percentile).", "The first quartile (\\(Q_1\\) or lower quartile) and third quartile (\\(Q_3\\) or upper quartile) show the spread of the â€œmiddle 50%â€ of the data, which is often called the interquartile range.", "Comparing the interquartile range to the minimum and maximum shows how the possible values spread out around the more probable values."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 145,
    "title": "ğŸ“ R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To calculate a five-number summary (and mean) in R use the code: quantile(object, percentile) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ percentile must be a value between 0 and 1. For the first quartile, it would be 0.25. For the third, it would be 0.75. Example Code Hover your mouse over the example codes to learn more. summary â€œsummaryâ€ is an R function used to calculate the five-number summary (and mean) of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Median Mean 3rd Qu. 56 72 79 77.88 85 97 Showing above are the five-number summary and mean of Temp from the airquality dataset. Note, you must use pander(summary(airquality$Temp)) to get the nicely formatted output. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. min = min(Temp),Â  Computes the min of the Temp column. Q1 = quantile(Temp, 0.25),Â  Computes the first quartile of the Temp column. med = median(Temp),Â  Computes the second quartile of the Temp column, known as the median. Q3 = quantile(Temp, 0.75),Â  Computes the third quartile of the Temp column. max = max(Temp) Computes the max of the Temp column. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month min Q1 med Q3 max 5 56 60 66 69 81 6 65 76 78 82.75 93 7 73 81.5 84 86 92 8 72 79 82 88.5 97 9 63 71 76 81 93 Note that R calculated the five-number summary for Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(min = min(Temp), Q1 = quantile(Temp, c(.25)), med = median(Temp), Q3 = quantile(Temp, c(.75)), max = max(Temp)) %>% pander()",
    "sentences": ["To calculate a five-number summary (and mean) in R use the code: quantile(object, percentile) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ percentile must be a value between 0 and 1.", "For the first quartile, it would be 0.25.", "For the third, it would be 0.75.", "Example Code Hover your mouse over the example codes to learn more.", "summary â€œsummaryâ€ is an R function used to calculate the five-number summary (and mean) of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset.", "Type â€œView(airquality)â€ in R to see it.", "$ The $ allows us to access any variable from the airquality dataset."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 146,
    "title": "ğŸ“ Standard Deviation",
    "url": "NumericalSummaries.html#standarddeviation",
    "content": "Standard Deviation \\(s = \\sqrt{\\frac{\\sum_{i=1}^n(x_i-\\bar{x})^2}{n-1}}\\) Measure of Spread | 1 Quantitative Variable Overview Measures how spread out the data are from the mean. It is never negative and typically not zero. Larger values mean the data is highly variable. Smaller values mean the data is consistent and not as variable. It is typically used with the mean to describe the spread of relatively normally distributed data. The order of operations in the formula is important and for this reason it is sometimes called the â€œroot mean squared error,â€ though the calculations are performed in reverse of that. (Study the formula on the left to understand.) The denominator \\(n-1\\) is called the degrees of freedom. R Instructions To calculate the standard deviation in R use the code: sd(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more. sd â€œsdâ€ is an R function used to calculate the standard deviation of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 9.46527 Note that the single number showing above is the standard deviation of Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. sdTemp =Â  â€œsdTempâ€ is just a name we made up. It will contain the results of the sd(â€¦) function. sd( â€œsdâ€ is an R function used to calculate the standard deviation Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month sdTemp 5 6.855 6 6.599 7 4.316 8 6.585 9 8.356 Note that R calculated the standard deviation of Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively.",
    "sentences": ["\\(s = \\sqrt{\\frac{\\sum_{i=1}^n(x_i-\\bar{x})^2}{n-1}}\\)", "Measure of Spread | 1 Quantitative Variable", "Overview Measures how spread out the data are from the mean.", "It is never negative and typically not zero.", "Larger values mean the data is highly variable.", "Smaller values mean the data is consistent and not as variable.", "It is typically used with the mean to describe the spread of relatively normally distributed data.", "The order of operations in the formula is important and for this reason it is sometimes called the â€œroot mean squared error,â€ though the calculations are performed in reverse of that.", "(Study the formula on the left to understand.) The denominator \\(n-1\\) is called the degrees of freedom.", "R Instructions To calculate the standard deviation in R use the code: sd(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Standard Deviation"
  },
  {
    "id": 147,
    "title": "ğŸ“ Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview Measures how spread out the data are from the mean. It is never negative and typically not zero. Larger values mean the data is highly variable. Smaller values mean the data is consistent and not as variable. It is typically used with the mean to describe the spread of relatively normally distributed data. The order of operations in the formula is important and for this reason it is sometimes called the â€œroot mean squared error,â€ though the calculations are performed in reverse of that. (Study the formula on the left to understand.) The denominator \\(n-1\\) is called the degrees of freedom.",
    "sentences": ["Measures how spread out the data are from the mean.", "It is never negative and typically not zero.", "Larger values mean the data is highly variable.", "Smaller values mean the data is consistent and not as variable.", "It is typically used with the mean to describe the spread of relatively normally distributed data.", "The order of operations in the formula is important and for this reason it is sometimes called the â€œroot mean squared error,â€ though the calculations are performed in reverse of that.", "(Study the formula on the left to understand.) The denominator \\(n-1\\) is called the degrees of freedom."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 148,
    "title": "ğŸ“ R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To calculate the standard deviation in R use the code: sd(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more. sd â€œsdâ€ is an R function used to calculate the standard deviation of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 9.46527 Note that the single number showing above is the standard deviation of Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. sdTemp =Â  â€œsdTempâ€ is just a name we made up. It will contain the results of the sd(â€¦) function. sd( â€œsdâ€ is an R function used to calculate the standard deviation Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month sdTemp 5 6.855 6 6.599 7 4.316 8 6.585 9 8.356 Note that R calculated the standard deviation of Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively.",
    "sentences": ["To calculate the standard deviation in R use the code: sd(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more.", "sd â€œsdâ€ is an R function used to calculate the standard deviation of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset.", "Type â€œView(airquality)â€ in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset.", ")Closing parenthsis for the function.", "Â Â Â Â Press Enter to run the code if you have typed it in yourself."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 149,
    "title": "ğŸ“ Explanation",
    "url": "NumericalSummaries.html#explanation",
    "content": "Explanation Data often varies. The values are not all the same. To capture, or measure how much data varies with a single number is difficult. There are a few different ideas on how to do it, but by far the most used measurement of the variability in data is the standard deviation. The first idea in measuring the variability in data is that there must be a reference point. Something from which everything varies. The most widely accepted reference point is the mean. A deviation is defined as the distance an observation lies from the reference point, the mean. This distance is obtained by subtraction in the order \\(x_i - \\bar{x}\\), where \\(x_i\\) is the data point value and \\(\\bar{x}\\) is the mean of the data. There are thus \\(n\\) deviations because there are \\(n\\) data points. Unfortunately, because of the order of subtraction in obtaining deviations, the average deviation will always work out to be zero. This is because the mean by nature splits the deviations evenly. Click here for details. One solution would be to take the absolute value of the deviations and obtain what is known as the â€œabsolute mean deviation.â€ This is sometimes done, but a far more attractive choice (to mathematicians and statisticians) is to square each deviation. Youâ€™ll have to trust us that this is the better choice. Squaring a deviation results in the expression \\((x_i-\\bar{x})^2\\). SQUARE Summing up all of the squared deviations results in the expression \\(\\sum_{i=1}^n (x_i-\\bar{x})^2\\). Dividing the sum of the squared deviations by \\(n\\) would seem like an appropriate thing to do. Experience (and some fantastic statistical theory!) demonstrated that this is wrong. Dividing by \\(n-1\\), the degrees of freedom is right. MEAN To undo the squaring of the deviations, the final results are square rooted. ROOT The end result is the beautiful formula for \\(s\\), the standard deviation! (At least the symbol for standard deviation is a simple \\(s\\).) It is also know as the ROOT-MEAN-SQUARED ERROR. Error is another word for deviation. \\[ s = \\sqrt{\\frac{\\sum_{i=1}^n(x_i-\\bar{x})^2}{n-1}} \\] The standard deviation is thus the representative deviation of all deviations in a given data set. It is never negative and only zero if all values are the same in a data set. Larger values of \\(s\\) imply the data is highly variable, very spread out or very inconsistent. Smaller values mean the data is consistent and not as variable. Population Standard Deviation When all of the data from a population is available, the population standard deviation \\(\\sigma\\) (the lower-case Greek letter â€œsigmaâ€) is calculated by the following formula. \\[ \\sigma = \\sqrt{\\frac{\\sum_{i=1}^N(x_i-\\mu)^2}{N}} \\] Note that \\(N\\) is the number of data points in the full population. In this formula the denominator is actually \\(N\\) and the deviations are calculated as the distance each data point is from the population mean \\(\\mu\\). An Example Say there are five data points given by \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) The mean of these values is \\(\\bar{x}=6\\) as shown here. The five deviations are \\((x_1 - \\bar{x}) = (2 - 6) = -4\\) \\((x_2 - \\bar{x}) = (5 - 6) = -1\\) \\((x_3 - \\bar{x}) = (6 - 6) = 0\\) \\((x_4 - \\bar{x}) = (7 - 6) = 1\\) \\((x_5 - \\bar{x}) = (10 - 6) = 4\\) The squared deviations are \\((x_1 - \\bar{x})^2 = (2 - 6)^2 = (-4)^2 = 16\\) \\((x_2 - \\bar{x})^2 = (5 - 6)^2 = (-1)^2 = 1\\) \\((x_3 - \\bar{x})^2 = (6 - 6)^2 = (0)^2 = 0\\) \\((x_4 - \\bar{x})^2 = (7 - 6)^2 = (1)^2 = 1\\) \\((x_5 - \\bar{x})^2 = (10 - 6)^2 = (4)^2 = 16\\) The sum of the squared deviations is \\[ \\sum_{i=1}^n (x_i-\\bar{x})^2 = 16 + 1 + 0 + 1 + 16 = 34 \\] Dividing this by the degrees of freedom, \\(n-1\\), gives \\[ \\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1} = \\frac{34}{5-1} = \\frac{34}{4} = 8.5 \\] Finally, \\(s\\) is obtained by taking the square root \\[ s = \\sqrt{\\frac{\\sum_{i=1}^n(x_i-\\bar{x})^2}{n-1}} = \\sqrt{8.5} \\approx 2.915 \\] The red lines below show how the standard deviation represents all deviations in this data set. Recall that the magnitudes of the individual deviations were \\(4, 1, 0, 1\\), and \\(4\\). The representative deviation is \\(2.915\\). Effect of Outliers Like the mean, the standard deviation is influenced by outliers. This is shown below by changing \\(x_5\\) to be 20. Note that the deviation of \\(x_5\\) is now 12 (instead of 4 like it was previously) and that the mean is now \\(8\\) (as shown here). The standard deviation of the altered data \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 20\\) is now \\(s \\approx 6.964\\). Not very â€œrepresentativeâ€ of all the deviations. It is biased towards the largest deviation. It is important to be aware of outliers when reporting the standard deviation \\(s\\).",
    "sentences": ["Data often varies.", "The values are not all the same.", "To capture, or measure how much data varies with a single number is difficult.", "There are a few different ideas on how to do it, but by far the most used measurement of the variability in data is the standard deviation.", "The first idea in measuring the variability in data is that there must be a reference point.", "Something from which everything varies.", "The most widely accepted reference point is the mean.", "A deviation is defined as the distance an observation lies from the reference point, the mean.", "This distance is obtained by subtraction in the order \\(x_i - \\bar{x}\\), where \\(x_i\\) is the data point value and \\(\\bar{x}\\) is the mean of the data.", "There are thus \\(n\\) deviations because there are \\(n\\) data points."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Explanation"
  },
  {
    "id": 150,
    "title": "ğŸ“ Variance",
    "url": "NumericalSummaries.html#variance",
    "content": "Variance \\[s^2 = \\frac{\\sum_{i=1}^n(x_i-\\bar{x})^2}{n-1}\\] Measure of Spread | 1 Quantitative Variable Overview Great theoretical properties, but seldom used when describing data. Difficult to interpret in context of data because it is in squared units. The standard deviation is typically used instead because it is in the original units and is thus easier to interpret. R Instructions To calculate the variance in R use the code: var(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more. var â€œvarâ€ is an R function used to calculate the variance of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 89.59133 Note that the single number showing above is the variance of Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. varTemp =Â  â€œvarTempâ€ is just a name we made up. It will contain the results of the var(â€¦) function. var( â€œvarâ€ is an R function used to calculate the variance Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month varTemp 5 46.99 6 43.54 7 18.62 8 43.37 9 69.82 Note that R calculated the variance of Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively.",
    "sentences": ["\\[s^2 = \\frac{\\sum_{i=1}^n(x_i-\\bar{x})^2}{n-1}\\]", "Measure of Spread | 1 Quantitative Variable", "Overview Great theoretical properties, but seldom used when describing data.", "Difficult to interpret in context of data because it is in squared units.", "The standard deviation is typically used instead because it is in the original units and is thus easier to interpret.", "R Instructions To calculate the variance in R use the code: var(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more.", "var â€œvarâ€ is an R function used to calculate the variance of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Variance"
  },
  {
    "id": 151,
    "title": "ğŸ“ Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview Great theoretical properties, but seldom used when describing data. Difficult to interpret in context of data because it is in squared units. The standard deviation is typically used instead because it is in the original units and is thus easier to interpret.",
    "sentences": ["Great theoretical properties, but seldom used when describing data.", "Difficult to interpret in context of data because it is in squared units.", "The standard deviation is typically used instead because it is in the original units and is thus easier to interpret."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 152,
    "title": "ğŸ“ R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To calculate the variance in R use the code: var(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more. var â€œvarâ€ is an R function used to calculate the variance of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 89.59133 Note that the single number showing above is the variance of Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. varTemp =Â  â€œvarTempâ€ is just a name we made up. It will contain the results of the var(â€¦) function. var( â€œvarâ€ is an R function used to calculate the variance Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month varTemp 5 46.99 6 43.54 7 18.62 8 43.37 9 69.82 Note that R calculated the variance of Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively.",
    "sentences": ["To calculate the variance in R use the code: var(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more.", "var â€œvarâ€ is an R function used to calculate the variance of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset.", "Type â€œView(airquality)â€ in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset.", ")Closing parenthsis for the function.", "Â Â Â Â Press Enter to run the code if you have typed it in yourself."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 153,
    "title": "ğŸ“ Range",
    "url": "NumericalSummaries.html#range",
    "content": "Range \\[x_{(n)}-x_{(1)}\\] Measure of Spread | 1 Quantitative Variable Overview The difference between the maximum and minimum values. A general rule of thumb is that the range divided by four is roughly the standard deviation. Quick to obtain, but not as good as using the standard deviation. Was used more frequently before the advent of modern calculators. R Instructions R will not automatically compute the range. It is easiest to compute the max() and min() and perform the subtraction max - min yourself. To calculate the variance in R use the code: var(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more. max â€œmaxâ€ is an R function used to calculate the maximum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â -Â  Subtraction symbol. min â€œminâ€ is an R function used to calculate the minimum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 41 Note that the single number showing above is the range of Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. rangeTemp =Â  â€œrangeTempâ€ is just a name we made up. It will contain the results of the range calculation. max( â€œmaxâ€ is an R function used to calculate the maximum Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. Â -Â  Minus sign to perform subtraction. min( â€œminâ€ is an R function used to calculate the minimum Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month rangeTemp 5 25 6 28 7 19 8 25 9 30 Note that R calculated the range of Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively.",
    "sentences": ["\\[x_{(n)}-x_{(1)}\\]", "Measure of Spread | 1 Quantitative Variable", "Overview The difference between the maximum and minimum values.", "A general rule of thumb is that the range divided by four is roughly the standard deviation.", "Quick to obtain, but not as good as using the standard deviation.", "Was used more frequently before the advent of modern calculators.", "R Instructions R will not automatically compute the range.", "It is easiest to compute the max() and min() and perform the subtraction max - min yourself.", "To calculate the variance in R use the code: var(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more.", "max â€œmaxâ€ is an R function used to calculate the maximum of data."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Range"
  },
  {
    "id": 154,
    "title": "ğŸ“ Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview The difference between the maximum and minimum values. A general rule of thumb is that the range divided by four is roughly the standard deviation. Quick to obtain, but not as good as using the standard deviation. Was used more frequently before the advent of modern calculators.",
    "sentences": ["The difference between the maximum and minimum values.", "A general rule of thumb is that the range divided by four is roughly the standard deviation.", "Quick to obtain, but not as good as using the standard deviation.", "Was used more frequently before the advent of modern calculators."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 155,
    "title": "ğŸ“ R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions R will not automatically compute the range. It is easiest to compute the max() and min() and perform the subtraction max - min yourself. To calculate the variance in R use the code: var(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more. max â€œmaxâ€ is an R function used to calculate the maximum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â -Â  Subtraction symbol. min â€œminâ€ is an R function used to calculate the minimum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] 41 Note that the single number showing above is the range of Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. rangeTemp =Â  â€œrangeTempâ€ is just a name we made up. It will contain the results of the range calculation. max( â€œmaxâ€ is an R function used to calculate the maximum Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. Â -Â  Minus sign to perform subtraction. min( â€œminâ€ is an R function used to calculate the minimum Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month rangeTemp 5 25 6 28 7 19 8 25 9 30 Note that R calculated the range of Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively.",
    "sentences": ["R will not automatically compute the range.", "It is easiest to compute the max() and min() and perform the subtraction max - min yourself.", "To calculate the variance in R use the code: var(object) object must be a quantitative variable, what R calls a â€œnumeric vector.â€ Example Code Hover your mouse over the example codes to learn more.", "max â€œmaxâ€ is an R function used to calculate the maximum of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset.", "Type â€œView(airquality)â€ in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 156,
    "title": "ğŸ“ Percentile (Quantile)",
    "url": "NumericalSummaries.html#percentilequantile",
    "content": "Percentile (Quantile) \\(\\leftarrow\\)To the Left Measure of Location | 1 Quantitative Variable Overview The percent of data that is equal to or less than a given data point. Useful for describing the relative position of a data point within a data set. If the percentile is close to 100, then the observation is one of the largest. If it is close to zero, then the observation is one of the smallest. R Instructions To compute the quantile in R for a given percentile: quantile(object, percentile) object must be a quantitative variable, or as R terms it, a â€œnumeric vectorâ€. percentile must be a value between 0 and 1 or a â€œnumeric vectorâ€ of such values. quantile( â€œquantileâ€ is an R function used to calculate the data value corresponding to a given percentile. airquality$Temp Temp is a quantitative variable (numeric vector) being accessed from the airquality dataset with the $ sign. ,Â  Comma separating the two commands of the quantile function. 0.8 The value of 0.8 specifies the 80th percentile. Any other value from 0 to 1 inclusive could be used. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. ## 80% ## 86 The 80th percentile for Temp is shown in the output above.",
    "sentences": ["\\(\\leftarrow\\)To the Left", "Measure of Location | 1 Quantitative Variable", "Overview The percent of data that is equal to or less than a given data point.", "Useful for describing the relative position of a data point within a data set.", "If the percentile is close to 100, then the observation is one of the largest.", "If it is close to zero, then the observation is one of the smallest.", "R Instructions To compute the quantile in R for a given percentile: quantile(object, percentile) object must be a quantitative variable, or as R terms it, a â€œnumeric vectorâ€.", "percentile must be a value between 0 and 1 or a â€œnumeric vectorâ€ of such values.", "quantile( â€œquantileâ€ is an R function used to calculate the data value corresponding to a given percentile.", "airquality$Temp Temp is a quantitative variable (numeric vector) being accessed from the airquality dataset with the $ sign."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Percentile (Quantile)"
  },
  {
    "id": 157,
    "title": "ğŸ“ Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview The percent of data that is equal to or less than a given data point. Useful for describing the relative position of a data point within a data set. If the percentile is close to 100, then the observation is one of the largest. If it is close to zero, then the observation is one of the smallest.",
    "sentences": ["The percent of data that is equal to or less than a given data point.", "Useful for describing the relative position of a data point within a data set.", "If the percentile is close to 100, then the observation is one of the largest.", "If it is close to zero, then the observation is one of the smallest."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 158,
    "title": "ğŸ“ R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To compute the quantile in R for a given percentile: quantile(object, percentile) object must be a quantitative variable, or as R terms it, a â€œnumeric vectorâ€. percentile must be a value between 0 and 1 or a â€œnumeric vectorâ€ of such values. quantile( â€œquantileâ€ is an R function used to calculate the data value corresponding to a given percentile. airquality$Temp Temp is a quantitative variable (numeric vector) being accessed from the airquality dataset with the $ sign. ,Â  Comma separating the two commands of the quantile function. 0.8 The value of 0.8 specifies the 80th percentile. Any other value from 0 to 1 inclusive could be used. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. ## 80% ## 86 The 80th percentile for Temp is shown in the output above.",
    "sentences": ["To compute the quantile in R for a given percentile: quantile(object, percentile) object must be a quantitative variable, or as R terms it, a â€œnumeric vectorâ€.", "percentile must be a value between 0 and 1 or a â€œnumeric vectorâ€ of such values.", "quantile( â€œquantileâ€ is an R function used to calculate the data value corresponding to a given percentile.", "airquality$Temp Temp is a quantitative variable (numeric vector) being accessed from the airquality dataset with the $ sign.", ",Â  Comma separating the two commands of the quantile function.", "0.8 The value of 0.8 specifies the 80th percentile.", "Any other value from 0 to 1 inclusive could be used.", ") Functions must always end with a closing parenthesis.", "Â Â Â Â Press Enter to run the code.", "Â â€¦Â  Click to View Output."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 159,
    "title": "ğŸ“ Proportion",
    "url": "NumericalSummaries.html#proportion",
    "content": "Proportion \\[\\hat{p}=\\frac{x}{n}\\] Measure of Center | 1 Qualitative Variable Overview The percent of observations in the data that satisfy some requirement. Obtained by dividing the number of successes \\(x\\) by the number of total observations \\(n\\). Often referred to as a percentage.",
    "sentences": ["\\[\\hat{p}=\\frac{x}{n}\\]", "Measure of Center | 1 Qualitative Variable", "Overview The percent of observations in the data that satisfy some requirement.", "Obtained by dividing the number of successes \\(x\\) by the number of total observations \\(n\\).", "Often referred to as a percentage."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Proportion"
  },
  {
    "id": 160,
    "title": "ğŸ“ Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview The percent of observations in the data that satisfy some requirement. Obtained by dividing the number of successes \\(x\\) by the number of total observations \\(n\\). Often referred to as a percentage.",
    "sentences": ["The percent of observations in the data that satisfy some requirement.", "Obtained by dividing the number of successes \\(x\\) by the number of total observations \\(n\\).", "Often referred to as a percentage."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 161,
    "title": "ğŸ“ Correlation",
    "url": "NumericalSummaries.html#correlation",
    "content": "Correlation \\(r = \\frac{\\textstyle\\sum\\left(\\frac{x-\\bar{x}}{s_x}\\right)\\left(\\frac{y-\\bar{y}}{s_y}\\right)}{n-1}\\) Measure of Association | 2 Quantitative Variables Overview Describes the strength and direction of the association between two quantitative variables. Restricted to values between -1 and 1. A value of zero denotes no association between the two variables. A value of 1 or -1 implies a perfect positive or perfect negative association, respectively. R Instructions To calculate the correlation in R use the code: cor(object1,object2) object1 and object2 must both be a quantitative variables, what R calls â€œnumeric vectors.â€ Example Code Hover your mouse over the example codes to learn more. cor â€œcorâ€ is an R function used to calculate the standard deviation of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. ,Â  The comma is needed to separate the two quantitative variables of the cor() function. The space after the comma is not required. It just looks nice. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Wind â€œWindâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] -0.4579879 Note that the single number showing next to the [1] above is the correlation of Temp and Wind from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. corTempWind =Â  â€œcorTempWindâ€ is just a name we made up. It will contain the results of the cor(â€¦) function. cor( â€œcorâ€ is an R function used to calculate the mean. Temp,Â  Temp is a quantitative variable (numeric vector) from the airquality dataset. Wind Wind is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month corTempWind 5 -0.3733 6 -0.121 7 -0.3052 8 -0.5076 9 -0.5705 Note that R calculated the correlation of Temp and Wind for each Month from the airquality dataset.",
    "sentences": ["\\(r = \\frac{\\textstyle\\sum\\left(\\frac{x-\\bar{x}}{s_x}\\right)\\left(\\frac{y-\\bar{y}}{s_y}\\right)}{n-1}\\)", "Measure of Association | 2 Quantitative Variables", "Overview Describes the strength and direction of the association between two quantitative variables.", "Restricted to values between -1 and 1.", "A value of zero denotes no association between the two variables.", "A value of 1 or -1 implies a perfect positive or perfect negative association, respectively.", "R Instructions To calculate the correlation in R use the code: cor(object1,object2) object1 and object2 must both be a quantitative variables, what R calls â€œnumeric vectors.â€ Example Code Hover your mouse over the example codes to learn more.", "cor â€œcorâ€ is an R function used to calculate the standard deviation of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Correlation"
  },
  {
    "id": 162,
    "title": "ğŸ“ Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview Describes the strength and direction of the association between two quantitative variables. Restricted to values between -1 and 1. A value of zero denotes no association between the two variables. A value of 1 or -1 implies a perfect positive or perfect negative association, respectively.",
    "sentences": ["Describes the strength and direction of the association between two quantitative variables.", "Restricted to values between -1 and 1.", "A value of zero denotes no association between the two variables.", "A value of 1 or -1 implies a perfect positive or perfect negative association, respectively."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 163,
    "title": "ğŸ“ R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To calculate the correlation in R use the code: cor(object1,object2) object1 and object2 must both be a quantitative variables, what R calls â€œnumeric vectors.â€ Example Code Hover your mouse over the example codes to learn more. cor â€œcorâ€ is an R function used to calculate the standard deviation of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. ,Â  The comma is needed to separate the two quantitative variables of the cor() function. The space after the comma is not required. It just looks nice. airquality â€œairqualityâ€ is a dataset. Type â€œView(airquality)â€ in R to see it. $ The $ allows us to access any variable from the airquality dataset. Wind â€œWindâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset. )Closing parenthsis for the function. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. ## [1] -0.4579879 Note that the single number showing next to the [1] above is the correlation of Temp and Wind from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R. Â %>%Â  The pipe operator that will send the airquality dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the airquality dataset into â€œlittleâ€ datasets, one dataset for each value in the â€œMonthâ€ column. Month â€œMonthâ€ is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis. Â %>%Â  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. corTempWind =Â  â€œcorTempWindâ€ is just a name we made up. It will contain the results of the cor(â€¦) function. cor( â€œcorâ€ is an R function used to calculate the mean. Temp,Â  Temp is a quantitative variable (numeric vector) from the airquality dataset. Wind Wind is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Month corTempWind 5 -0.3733 6 -0.121 7 -0.3052 8 -0.5076 9 -0.5705 Note that R calculated the correlation of Temp and Wind for each Month from the airquality dataset.",
    "sentences": ["To calculate the correlation in R use the code: cor(object1,object2) object1 and object2 must both be a quantitative variables, what R calls â€œnumeric vectors.â€ Example Code Hover your mouse over the example codes to learn more.", "cor â€œcorâ€ is an R function used to calculate the standard deviation of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality â€œairqualityâ€ is a dataset.", "Type â€œView(airquality)â€ in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp â€œTempâ€ is a quantitative variable (numeric vector) from the â€œairqualityâ€ dataset.", ",Â  The comma is needed to separate the two quantitative variables of the cor() function.", "The space after the comma is not required."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 164,
    "title": "Intermediate Statistics Notes",
    "url": "Paige-sNotes.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Intermediate Statistics Notes (MATH 325) Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts Donâ€™t forget to always load your libraries and to Knit often!! library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) This is the templet for the Hover Box Thingy Hover over words box! [insert description here] Â Â Â Â  Â â€¦Â  Click to View Output. There are three parts to the help box: The starter code < a href=â€œjavascript:showhide(â€˜[Insert classifying name]â€™)â€> < div class=â€œhoverchunkâ€ > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions! < span class=â€œtooltiprâ€ > [What shows up in the box] < span class=â€œtooltiprtextâ€ > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box! & nbsp;& nbsp;& nbsp;& nbsp; < span class=â€œtooltiprtextâ€ >Press Enter to run the code. < /span> & nbsp;â€¦& nbsp; < span class=â€œtooltiprtextâ€ >Click to View Output.< /span > < /span > < /div > < /a > < div id=â€œ[The classifying name that you put in the starter code!]â€ style=â€œdisplay:none;â€ > *When using, make sure you fix all the spaces when using and use â€œ< /div >â€ to show where the drop down is meant to end! Personal Notes OoOoooOOOoo letâ€™s go assignments!! Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester. As well as some personal notes on those assignments! Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys! Skill Quiz - Introduction to R Look at data sets, computing means, and creating graphs. Mean of Distance from â€œcarsâ€ data set View(cars) mean(cars$dist) ## [1] 42.98 Scatter plot of â€œcarsâ€ data set plot(dist~speed, data=cars, col=\"skyblue\",pch=16) Creating new data set with â€œ<- The Assignment Operatorâ€ create an airquality2 data set that is a copy of the airquality data set add a new column to the airquality2 data set called â€œCelciusâ€ that converts Temp from degrees Fahrenheit (F) to Ceclcius (C) using the equation: Celcius = (Temp âˆ’ 32) * 5/9. Donâ€™t forget to use airquailty$ appropriately in that equation! Hint: Typing â€œ?airqualityâ€ will allow a help file for the airquality data set to appear! write.csv(airquality,\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/airquality.cvs\") airquality2 <- airquality airquality2$Celcius <- round((airquality2$Temp-32)*5/9,1) View(airquality2) datatable(airquality2, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") PRACTICE : Scatter plot in Base R plot(Ozone~ Temp, data=airquality, xlab=\"Daily Maximum Temperature\", ylab=\"Mean Ozone in Parts per Billion (from 1300 to 1500 hours\", main=\"Exponential Growth in Ozone with Increasing Temp\", col=\"firebrick\",pch=16 ) pch Options! Week 2 | Describing Data in RStudio Talking about how to give good feedback and more coolio graphics in this week! Skill Quiz - Describing Data with R Numerical Summaries that MEASURE CENTER: Proportion Mean Mode Median Mean - Gives a good feel for the most typical values of quantitative data as long as the data is at least approximately normally distributed. Median - Gives a good feel for the most typical values of any style of distribution of quantitative data, but is especially useful for skewed data. Standard Deviation - Typically used to describe the spread of relatively normally distribued data. Five-Number Summaryt - Gives a good feel for how the possible values of quanititative data spread out around the more probable values. Practice Find the mean and standard deviation of the â€œWindâ€ speed variable in the airquality dataset. pander(summary(airquality$Wind)) Min. Median Mean 3rd Qu. 1.7 7.4 9.7 9.958 11.5 20.7 Create a Scatter plot of Temp VS. Wind plot(Wind~Temp, data=airquality, xlab= \"Daily Average Temperature\", ylab = \"Daily Average Wind Speed\", main= \"La Guardia Airport Warmer Weather Shows Less Wind\", col= \"gray\", pch= 19) Create a Histogram and Box plot of Solar.R hist(airquality$Solar.R, xlab= \"Daily Mean Radiation in Langleys (from 0800 to 1200 hours)\", main= \"Central Park, NYC Daily Average Radiation\", col= \"orange\") boxplot(Solar.R~Month, data=airquality, xlab= \"Month of the Year\", ylab= \"Radiation in Langleys (Averaged from 0800 to 1200 hours)\", main= \"Daily Mean Radiation High in July\",col=c(\"gray\",\"gray\",\"orangered\",\"gray\",\"gray\")) Graph Types Histogram - Show the distribution of heights for a sample of n=400 first grade boys. Box Plot - Compare the distribution of body weights of American adults for different ethnicities where there is a sample size of n=100 for each ethnicity. Dot Plots - Compare the distribution of bird beak lengths for different species of bird where there is a sample size of n=3 for each species. Class Activity - Principles of Good Graphics PRACTICE Histogram, Box plot, and Scatter plot with airquality data set hist(airquality$Wind, col=\"steelblue\", xlab=\"Daily Average Wind Speeds (mph)\", main=\"La Guardia Airport from May to September, 1973\") boxplot(Wind~Month, data=airquality, names=c(\"5\",\"6\",\"7\",\"8\",\"9\"), col=c(\"steelblue1\",\"steelblue2\",\"steelblue3\",\"steelblue3\",\"steelblue2\")) plot(Ozone~Temp, data=airquality, col= \"steelblue\", pch=19) Week 3 | Intro to Data Wrangling & Visulization Looking at the different categories of data that is found in the Index Page and how to discern quantitative from categorical data! Skills Quiz - Data Wrangling & Visualization Quick reminder! How do you view a dataset? View([insert the dataset here]) How do you open the help file ?([insert the dataset here]) Quanitative vs.Â Qualitative Quantitative Examples: length & width Qualitatve Examples: name, birthmonth, birthyear, sex, biggerfoot, & domhand PRACTICE : Use KidsFeet dataset Make a table displaying gender table(KidsFeet$sex)%>% pander() B G 20 19 Make a table displaying which foot is bigger Which foot is commonly bigger? : Left Foot table(KidsFeet$biggerfoot)%>% pander() L R 22 17 Make a table displaying which birthmonth is the most common Which birth month is most common among these sampled children? : March table(KidsFeet$birthmonth)%>% pander() 1 2 3 4 5 6 7 8 9 10 11 12 2 3 8 3 2 4 3 2 5 2 2 3 Make a table that shows â€œare girls or boys more likely to be left handed?â€ table(KidsFeet$domhand, KidsFeet$sex)%>% pander() Â  B G L 5 3 R 15 16 Make a table showing the summaries of childrenâ€™s foot lengths according to their gender KidsFeet %>% group_by(sex) %>% summarise(Min=min(length), Q1 = quantile(length, 0.25), Median = (median(length)), Q3 = quantile(length,0.75), Max = max(length), Mean = mean(length), SD = sd(length))%>% pander() sex Min Q1 Median Q3 Max Mean SD B 22.9 24.35 24.95 25.8 27.5 25.11 1.217 G 21.6 23.65 24.2 25.1 26.7 24.32 1.33 For this particular sample of data, which gender has the longest feet on average? Boys Which gender shows the most consistency in length of feet among children in this sample? Boys PRACTICE : Use airquality dataset run an appropriate command to obtain the mean daily temperature at LaGuardia Airport for each month, separately. airquality %>% group_by(Month) %>% summarise(`Mean Temperature`= mean(Temp)) %>% pander() Month Mean Temperature 5 65.55 6 79.1 7 83.9 8 83.97 9 76.9 Observations: Which month experiences the coolest average temperature? May By how many degrees do the average temperatures of July and August differ? 83.97-83.9 = 0.07 Between which two consecutive months is there the largest difference in average temperature? May and June What are the BEST graphics for this data set? These 3 would be useful in depicting the above information, because they nicely display the month and their averages (1) Box plot boxplot(Temp ~ Month, data=airquality, xlab=\"Month\", ylab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\") (2) Scatter plot plot(Temp ~ Month, data=airquality, xlab=\"Month\", ylab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\") (3) Dot Plot stripchart(Temp ~ Month, data=airquality, ylab=\"Month\", xlab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\", method=\"stack\") What are the WORST graphics for depicting this information? these two are the worst because they donâ€™t clearly show the months in order to allow us to compare the data of each month. (1) Scatter plot but the x axis is Day, not very clear variable plot(Temp ~ Day, data=airquality, xlab=\"Day of the Month\", ylab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\") (2) Histogram gives you the average of the whole time frame but not the specific times hist(airquality$Temp, xlab=\"Daily Temperature\", main=\"LaGuardia Airport (May to September 1973)\", col=\"slategray\") PRACTICE : Use Orange dataset View(Orange) Orange2 <- Orange %>% group_by(age)%>% summarise(MedianCir = median(circumference)) datatable(Orange2, options=list(lengthMenu =c(3,10,30)), extensions=â€œResponsiveâ€) What are the BEST graphics for this data set? Scatter plot with median growth line visualizes the relationship/ captures the overall trends between two continuous variables plot(circumference ~ age, data=Orange, ylab=\"Trunk Circumference (mm)\", xlab=\"Age of Trees (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\", pch=15) Orange.m <- median(circumference ~ age, data=Orange) lines(names(Orange.m), Orange.m, col=\"ivory3\") legend(\"topleft\", legend=\"Median Growth\", lty=1, col='ivory3', bty='n') Box plot shows how the trunk circumference varies across different ages boxplot(circumference ~ age, data=Orange, ylab=\"Trunk Circumference (mm)\", xlab=\"Age of Trees (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\") Dot plot shows the distribution of data and helps visualize all the individual data points stripchart(circumference ~ age, data=Orange, ylab=\"Trunk Circumference (mm)\", xlab=\"Age of Trees (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\", pch=15, method=\"stack\", vertical=TRUE) Question: During which age interval did the most rapid overall median growth occur (in the circumference of the orange trees that were sampled)? 664 to 1004 days (third interval) What are the WORST graphics for this data set? Box plot x-axis is sectioned out by multiple variables instead of just one, thus, not really showing us anything of value boxplot(Orange, xlab=\"Age of Tree (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\") PRACTICE : Use the Riders data set Consider the Riders dataset in R. (You may need to load library(mosaicData).) How many total riders were observed on each day of the week? (Hint: the sum() function works the same way as mean()â€¦) Riders2 <- Riders %>% group_by(day)%>% summarise(`Total Number of Riders Observed`= sum(riders)) datatable(Riders2) PRACTICE : Use the mtcars dataset How would you describe the dataset? First, type ?mtcars and look at the â€œDescriptionâ€ of the data set information Description: The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973â€“74 models). How many variables are in the mtcars data set? First, View()the data, then read how many columns are in the data set in the bottom left hand corner : 11 How many observations are in the mtcars data set? (Hint: try View(â€¦)) First, View() the data, then read the amount of entries in the data set in the bottom left hand corner : 32 How many vehicles are represented in the dataset for 4, 6, and 8 cylinder vehicles? (Hint: use the table(â€¦) function.) table(mtcars$cyl)%>% pander() 4 6 8 11 7 14 According to the mtcars data, on average, vehicles with 4 cylinders get the best (highest) gas mileage. What is the average mpg for automatic and manual transmission vehicles with 4 cylinders? (Round answers to the nearest tenth.) 0 = Automatic, 1 = Manual mtcars %>% filter(cyl == \"4\")%>% group_by(am)%>% summarise(`Mean Gas Mileage form 4 Cylinder mtcars Vehicles(mpg)`= round(mean(mpg),1))%>% mutate(Transmission = ifelse(am == 0, \"Automatic\",\"Manual\"))%>% pander() am Mean Gas Mileage form 4 Cylinder mtcars Vehicles(mpg) Transmission 0 22.9 Automatic 1 28.1 Manual According to the mtcars data, on average, vehicles with 8 cylinders have the best (fastest) quarter mile time. What is the mean quarter mile time (qsec) for automatic and manual transmission vehicles with 8 cylinders? (Round answers to nearest tenth.) mtcars %>% filter(cyl == \"8\")%>% group_by(am)%>% summarise(`Mean Quarter Mile Time for 8 Cylinder mtcars Vechiles (sec)`= round(mean(qsec),1))%>% mutate(Transmission = ifelse(am == 0, \"Automatic\",\"Manual\"))%>% pander() am Mean Quarter Mile Time for 8 Cylinder mtcars Vechiles (sec) Transmission 0 17.1 Automatic 1 14.6 Manual According to the mtcars data, how many thousands of pounds does the heaviest 6 cylinder car with an automatic transmission weigh? (Round to the nearest tenth.) mtcars %>% filter(cyl == \"6\", am == \"0\") %>% summarise(MaxWTAutomatic = round(max(wt),1))%>% pander() MaxWTAutomatic 3.5 How many more thousands of pounds does it weigh than the heaviest 6 cylinder car with a manual transmission? (Round to the nearest tenth.) 3.5 - 2.9 = 0.6 mtcars %>% filter(cyl == \"6\", am == \"1\") %>% summarise(MaxWTManual = round(max(wt),1))%>% pander() MaxWTManual 2.9 Assesment Quiz - Intro to Data Wrangling & Visualization Use the mtcars dataset in R to compute the mean â€œGross horsepowerâ€ of both automatic and manual transmissio",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Intermediate Statistics Notes (MATH 325) Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts Donâ€™t forget to always load your libraries and to Knit often!!", "library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) This is the templet for the Hover Box Thingy Hover over words box!", "[insert description here] Â Â Â Â  Â â€¦Â  Click to View Output.", "There are three parts to the help box: The starter code < a href=â€œjavascript:showhide(â€˜[Insert classifying name]â€™)â€> < div class=â€œhoverchunkâ€ > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions!", "< span class=â€œtooltiprâ€ > [What shows up in the box] < span class=â€œtooltiprtextâ€ > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box!", "& nbsp;& nbsp;& nbsp;& nbsp; < span class=â€œtooltiprtextâ€ >Press Enter to run the code.", "< /span> & nbsp;â€¦& nbsp; < span class=â€œtooltiprtextâ€ >Click to View Output.< /span > < /span > < /div > < /a > < div id=â€œ[The classifying name that you put in the starter code!]â€ style=â€œdisplay:none;â€ > *When using, make sure you fix all the spaces when using and use â€œ< /div >â€ to show where the drop down is meant to end!", "Personal Notes OoOoooOOOoo letâ€™s go assignments!!", "Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester.", "As well as some personal notes on those assignments!", "Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys!", "Skill Quiz - Introduction to R Look at data sets, computing means, and creating graphs.", "Mean of Distance from â€œcarsâ€ data set View(cars) mean(cars$dist) ## [1] 42.98 Scatter plot of â€œcarsâ€ data set plot(dist~speed, data=cars, col=\"skyblue\",pch=16) Creating new data set with â€œ<- The Assignment Operatorâ€ create an airquality2 data set that is a copy of the airquality data set add a new column to the airquality2 data set called â€œCelciusâ€ that converts Temp from degrees Fahrenheit (F) to Ceclcius (C) using the equation: Celcius = (Temp âˆ’ 32) * 5/9.", "Donâ€™t forget to use airquailty$ appropriately in that equation!", "Hint: Typing â€œ?airqualityâ€ will allow a help file for the airquality data set to appear!", "write.csv(airquality,\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/airquality.cvs\") airquality2 <- airquality airquality2$Celcius <- round((airquality2$Temp-32)*5/9,1) View(airquality2) datatable(airquality2, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") PRACTICE : Scatter plot in Base R plot(Ozone~ Temp, data=airquality, xlab=\"Daily Maximum Temperature\", ylab=\"Mean Ozone in Parts per Billion (from 1300 to 1500 hours\", main=\"Exponential Growth in Ozone with Increasing Temp\", col=\"firebrick\",pch=16 ) pch Options!", "Week 2 | Describing Data in RStudio Talking about how to give good feedback and more coolio graphics in this week!", "Skill Quiz - Describing Data with R Numerical Summaries that MEASURE CENTER: Proportion Mean Mode Median Mean - Gives a good feel for the most typical values of quantitative data as long as the data is at least approximately normally distributed.", "Median - Gives a good feel for the most typical values of any style of distribution of quantitative data, but is especially useful for skewed data.", "Standard Deviation - Typically used to describe the spread of relatively normally distribued data.", "Five-Number Summaryt - Gives a good feel for how the possible values of quanititative data spread out around the more probable values.", "Practice Find the mean and standard deviation of the â€œWindâ€ speed variable in the airquality dataset.", "pander(summary(airquality$Wind)) Min.", "Median Mean 3rd Qu.", "1.7 7.4 9.7 9.958 11.5 20.7 Create a Scatter plot of Temp VS.", "Wind plot(Wind~Temp, data=airquality, xlab= \"Daily Average Temperature\", ylab = \"Daily Average Wind Speed\", main= \"La Guardia Airport Warmer Weather Shows Less Wind\", col= \"gray\", pch= 19) Create a Histogram and Box plot of Solar.R hist(airquality$Solar.R, xlab= \"Daily Mean Radiation in Langleys (from 0800 to 1200 hours)\", main= \"Central Park, NYC Daily Average Radiation\", col= \"orange\") boxplot(Solar.R~Month, data=airquality, xlab= \"Month of the Year\", ylab= \"Radiation in Langleys (Averaged from 0800 to 1200 hours)\", main= \"Daily Mean Radiation High in July\",col=c(\"gray\",\"gray\",\"orangered\",\"gray\",\"gray\")) Graph Types Histogram - Show the distribution of heights for a sample of n=400 first grade boys.", "Box Plot - Compare the distribution of body weights of American adults for different ethnicities where there is a sample size of n=100 for each ethnicity.", "Dot Plots - Compare the distribution of bird beak lengths for different species of bird where there is a sample size of n=3 for each species.", "Class Activity - Principles of Good Graphics PRACTICE Histogram, Box plot, and Scatter plot with airquality data set hist(airquality$Wind, col=\"steelblue\", xlab=\"Daily Average Wind Speeds (mph)\", main=\"La Guardia Airport from May to September, 1973\") boxplot(Wind~Month, data=airquality, names=c(\"5\",\"6\",\"7\",\"8\",\"9\"), col=c(\"steelblue1\",\"steelblue2\",\"steelblue3\",\"steelblue3\",\"steelblue2\")) plot(Ozone~Temp, data=airquality, col= \"steelblue\", pch=19) Week 3 | Intro to Data Wrangling & Visulization Looking at the different categories of data that is found in the Index Page and how to discern quantitative from categorical data!", "Skills Quiz - Data Wrangling & Visualization Quick reminder!", "How do you view a dataset?", "View([insert the dataset here]) How do you open the help file ?([insert the dataset here]) Quanitative vs.Â Qualitative Quantitative Examples: length & width Qualitatve Examples: name, birthmonth, birthyear, sex, biggerfoot, & domhand PRACTICE : Use KidsFeet dataset Make a table displaying gender table(KidsFeet$sex)%>% pander() B G 20 19 Make a table displaying which foot is bigger Which foot is commonly bigger?", ": Left Foot table(KidsFeet$biggerfoot)%>% pander() L R 22 17 Make a table displaying which birthmonth is the most common Which birth month is most common among these sampled children?", ": March table(KidsFeet$birthmonth)%>% pander() 1 2 3 4 5 6 7 8 9 10 11 12 2 3 8 3 2 4 3 2 5 2 2 3 Make a table that shows â€œare girls or boys more likely to be left handed?â€ table(KidsFeet$domhand, KidsFeet$sex)%>% pander() Â  B G L 5 3 R 15 16 Make a table showing the summaries of childrenâ€™s foot lengths according to their gender KidsFeet %>% group_by(sex) %>% summarise(Min=min(length), Q1 = quantile(length, 0.25), Median = (median(length)), Q3 = quantile(length,0.75), Max = max(length), Mean = mean(length), SD = sd(length))%>% pander() sex Min Q1 Median Q3 Max Mean SD B 22.9 24.35 24.95 25.8 27.5 25.11 1.217 G 21.6 23.65 24.2 25.1 26.7 24.32 1.33 For this particular sample of data, which gender has the longest feet on average?", "Boys Which gender shows the most consistency in length of feet among children in this sample?", "Boys PRACTICE : Use airquality dataset run an appropriate command to obtain the mean daily temperature at LaGuardia Airport for each month, separately.", "airquality %>% group_by(Month) %>% summarise(`Mean Temperature`= mean(Temp)) %>% pander() Month Mean Temperature 5 65.55 6 79.1 7 83.9 8 83.97 9 76.9 Observations: Which month experiences the coolest average temperature?", "May By how many degrees do the average temperatures of July and August differ?", "83.97-83.9 = 0.07 Between which two consecutive months is there the largest difference in average temperature?", "May and June What are the BEST graphics for this data set?", "These 3 would be useful in depicting the above information, because they nicely display the month and their averages (1) Box plot boxplot(Temp ~ Month, data=airquality, xlab=\"Month\", ylab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\") (2) Scatter plot plot(Temp ~ Month, data=airquality, xlab=\"Month\", ylab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\") (3) Dot Plot stripchart(Temp ~ Month, data=airquality, ylab=\"Month\", xlab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\", method=\"stack\") What are the WORST graphics for depicting this information?", "these two are the worst because they donâ€™t clearly show the months in order to allow us to compare the data of each month.", "(1) Scatter plot but the x axis is Day, not very clear variable plot(Temp ~ Day, data=airquality, xlab=\"Day of the Month\", ylab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\") (2) Histogram gives you the average of the whole time frame but not the specific times hist(airquality$Temp, xlab=\"Daily Temperature\", main=\"LaGuardia Airport (May to September 1973)\", col=\"slategray\") PRACTICE : Use Orange dataset View(Orange) Orange2 <- Orange %>% group_by(age)%>% summarise(MedianCir = median(circumference)) datatable(Orange2, options=list(lengthMenu =c(3,10,30)), extensions=â€œResponsiveâ€) What are the BEST graphics for this data set?", "Scatter plot with median growth line visualizes the relationship/ captures the overall trends between two continuous variables plot(circumference ~ age, data=Orange, ylab=\"Trunk Circumference (mm)\", xlab=\"Age of Trees (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\", pch=15) Orange.m <- median(circumference ~ age, data=Orange) lines(names(Orange.m), Orange.m, col=\"ivory3\") legend(\"topleft\", legend=\"Median Growth\", lty=1, col='ivory3', bty='n') Box plot shows how the trunk circumference varies across different ages boxplot(circumference ~ age, data=Orange, ylab=\"Trunk Circumference (mm)\", xlab=\"Age of Trees (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\") Dot plot shows the distribution of data and helps visualize all the individual data points stripchart(circumference ~ age, data=Orange, ylab=\"Trunk Circumference (mm)\", xlab=\"Age of Trees (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\", pch=15, method=\"stack\", vertical=TRUE) Question: During which age interval did the most rapid overall median growth occur (in the circumference of the orange trees that were sampled)?", "664 to 1004 days (third interval) What are the WORST graphics for this data set?", "Box plot x-axis is sectioned out by multiple variables instead of just one, thus, not really showing us anything of value boxplot(Orange, xlab=\"Age of Tree (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\") PRACTICE : Use the Riders data set Consider the Riders dataset in R.", "(You may need to load library(mosaicData).) How many total riders were observed on each day of the week?", "(Hint: the sum() function works the same way as mean()â€¦) Riders2 <- Riders %>% group_by(day)%>% summarise(`Total Number of Riders Observed`= sum(riders)) datatable(Riders2) PRACTICE : Use the mtcars dataset How would you describe the dataset?", "First, type ?mtcars and look at the â€œDescriptionâ€ of the data set information Description: The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973â€“74 models).", "How many variables are in the mtcars data set?"],
    "type": "page",
    "page_title": "Intermediate Statistics Notes"
  },
  {
    "id": 165,
    "title": "ğŸ“ Cheat Sheets",
    "url": "Paige-sNotes.html#cheatsheets",
    "content": "Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts",
    "sentences": "R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts",
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Cheat Sheets"
  },
  {
    "id": 166,
    "title": "ğŸ“ Donâ€™t forget to always load your libraries and to Knit often!!",
    "url": "Paige-sNotes.html#dontforgettoalwaysloadyourlibrariesandtoknitoften",
    "content": "Donâ€™t forget to always load your libraries and to Knit often!! library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) This is the templet for the Hover Box Thingy There are three parts to the help box: The starter code < a href=â€œjavascript:showhide(â€˜[Insert classifying name]â€™)â€> < div class=â€œhoverchunkâ€ > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions! < span class=â€œtooltiprâ€ > [What shows up in the box] < span class=â€œtooltiprtextâ€ > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box! & nbsp;& nbsp;& nbsp;& nbsp; < span class=â€œtooltiprtextâ€ >Press Enter to run the code. < /span> & nbsp;â€¦& nbsp; < span class=â€œtooltiprtextâ€ >Click to View Output.< /span > < /span > < /div > < /a > < div id=â€œ[The classifying name that you put in the starter code!]â€ style=â€œdisplay:none;â€ > *When using, make sure you fix all the spaces when using and use â€œ< /div >â€ to show where the drop down is meant to end!",
    "sentences": ["library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven)", "This is the templet for the Hover Box Thingy", "There are three parts to the help box: The starter code < a href=â€œjavascript:showhide(â€˜[Insert classifying name]â€™)â€> < div class=â€œhoverchunkâ€ > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions!", "< span class=â€œtooltiprâ€ > [What shows up in the box] < span class=â€œtooltiprtextâ€ > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box!", "& nbsp;& nbsp;& nbsp;& nbsp; < span class=â€œtooltiprtextâ€ >Press Enter to run the code.", "< /span> & nbsp;â€¦& nbsp; < span class=â€œtooltiprtextâ€ >Click to View Output.< /span > < /span > < /div > < /a > < div id=â€œ[The classifying name that you put in the starter code!]â€ style=â€œdisplay:none;â€ > *When using, make sure you fix all the spaces when using and use â€œ< /div >â€ to show where the drop down is meant to end!"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Donâ€™t forget to always load your libraries and to Knit often!!"
  },
  {
    "id": 167,
    "title": "ğŸ“ Week 1 | Welcome to the Course & Introduction to RStudio",
    "url": "Paige-sNotes.html#week1welcometothecourseintroductiontorstudio",
    "content": "Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys! Skill Quiz - Introduction to R Look at data sets, computing means, and creating graphs. Mean of Distance from â€œcarsâ€ data set View(cars) mean(cars$dist) ## [1] 42.98 Scatter plot of â€œcarsâ€ data set plot(dist~speed, data=cars, col=\"skyblue\",pch=16) Creating new data set with â€œ<- The Assignment Operatorâ€ create an airquality2 data set that is a copy of the airquality data set add a new column to the airquality2 data set called â€œCelciusâ€ that converts Temp from degrees Fahrenheit (F) to Ceclcius (C) using the equation: Celcius = (Temp âˆ’ 32) * 5/9. Donâ€™t forget to use airquailty$ appropriately in that equation! Hint: Typing â€œ?airqualityâ€ will allow a help file for the airquality data set to appear! write.csv(airquality,\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/airquality.cvs\") airquality2 <- airquality airquality2$Celcius <- round((airquality2$Temp-32)*5/9,1) View(airquality2) datatable(airquality2, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") PRACTICE : Scatter plot in Base R plot(Ozone~ Temp, data=airquality, xlab=\"Daily Maximum Temperature\", ylab=\"Mean Ozone in Parts per Billion (from 1300 to 1500 hours\", main=\"Exponential Growth in Ozone with Increasing Temp\", col=\"firebrick\",pch=16 ) pch Options!",
    "sentences": ["We were setting up RStudio and doing cool little graphic thingys!", "Skill Quiz - Introduction to R Look at data sets, computing means, and creating graphs.", "Mean of Distance from â€œcarsâ€ data set View(cars) mean(cars$dist) ## [1] 42.98 Scatter plot of â€œcarsâ€ data set plot(dist~speed, data=cars, col=\"skyblue\",pch=16) Creating new data set with â€œ<- The Assignment Operatorâ€ create an airquality2 data set that is a copy of the airquality data set add a new column to the airquality2 data set called â€œCelciusâ€ that converts Temp from degrees Fahrenheit (F) to Ceclcius (C) using the equation: Celcius = (Temp âˆ’ 32) * 5/9.", "Donâ€™t forget to use airquailty$ appropriately in that equation!", "Hint: Typing â€œ?airqualityâ€ will allow a help file for the airquality data set to appear!", "write.csv(airquality,\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/airquality.cvs\") airquality2 <- airquality airquality2$Celcius <- round((airquality2$Temp-32)*5/9,1) View(airquality2) datatable(airquality2, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") PRACTICE : Scatter plot in Base R plot(Ozone~ Temp, data=airquality, xlab=\"Daily Maximum Temperature\", ylab=\"Mean Ozone in Parts per Billion (from 1300 to 1500 hours\", main=\"Exponential Growth in Ozone with Increasing Temp\", col=\"firebrick\",pch=16 ) pch Options!"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Week 1 | Welcome to the Course & Introduction to RStudio"
  },
  {
    "id": 168,
    "title": "ğŸ“ Skill Quiz - Introduction to R",
    "url": "Paige-sNotes.html#skillquizintroductiontor",
    "content": "Skill Quiz - Introduction to R Look at data sets, computing means, and creating graphs. Mean of Distance from â€œcarsâ€ data set View(cars) mean(cars$dist) Scatter plot of â€œcarsâ€ data set plot(dist~speed, data=cars, col=\"skyblue\",pch=16) Creating new data set with â€œ<- The Assignment Operatorâ€ create an airquality2 data set that is a copy of the airquality data set add a new column to the airquality2 data set called â€œCelciusâ€ that converts Temp from degrees Fahrenheit (F) to Ceclcius (C) using the equation: Celcius = (Temp âˆ’ 32) * 5/9. Donâ€™t forget to use airquailty$ appropriately in that equation! Hint: Typing â€œ?airqualityâ€ will allow a help file for the airquality data set to appear! write.csv(airquality,\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/airquality.cvs\") airquality2 <- airquality airquality2$Celcius <- round((airquality2$Temp-32)*5/9,1) View(airquality2) datatable(airquality2, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\")",
    "sentences": ["Look at data sets, computing means, and creating graphs.", "Mean of Distance from â€œcarsâ€ data set", "View(cars) mean(cars$dist)", "Scatter plot of â€œcarsâ€ data set", "plot(dist~speed, data=cars, col=\"skyblue\",pch=16)", "Creating new data set with â€œ<- The Assignment Operatorâ€", "create an airquality2 data set that is a copy of the airquality data set add a new column to the airquality2 data set called â€œCelciusâ€ that converts Temp from degrees Fahrenheit (F) to Ceclcius (C) using the equation: Celcius = (Temp âˆ’ 32) * 5/9.", "Donâ€™t forget to use airquailty$ appropriately in that equation!", "Hint: Typing â€œ?airqualityâ€ will allow a help file for the airquality data set to appear!", "write.csv(airquality,\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/airquality.cvs\") airquality2 <- airquality airquality2$Celcius <- round((airquality2$Temp-32)*5/9,1) View(airquality2) datatable(airquality2, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\")"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skill Quiz - Introduction to R"
  },
  {
    "id": 169,
    "title": "ğŸ“ Skill Quiz - Describing Data with R",
    "url": "Paige-sNotes.html#skillquizdescribingdatawithr",
    "content": "Skill Quiz - Describing Data with R Numerical Summaries that MEASURE CENTER: Proportion Mean Mode Median Mean - Gives a good feel for the most typical values of quantitative data as long as the data is at least approximately normally distributed. Median - Gives a good feel for the most typical values of any style of distribution of quantitative data, but is especially useful for skewed data. Standard Deviation - Typically used to describe the spread of relatively normally distribued data. Five-Number Summaryt - Gives a good feel for how the possible values of quanititative data spread out around the more probable values. Find the mean and standard deviation of the â€œWindâ€ speed variable in the airquality dataset. pander(summary(airquality$Wind)) Create a Scatter plot of Temp VS. plot(Wind~Temp, data=airquality, xlab= \"Daily Average Temperature\", ylab = \"Daily Average Wind Speed\", main= \"La Guardia Airport Warmer Weather Shows Less Wind\", col= \"gray\", pch= 19)",
    "sentences": ["Numerical Summaries that MEASURE CENTER:", "Proportion Mean Mode Median", "Mean - Gives a good feel for the most typical values of quantitative data as long as the data is at least approximately normally distributed.", "Median - Gives a good feel for the most typical values of any style of distribution of quantitative data, but is especially useful for skewed data.", "Standard Deviation - Typically used to describe the spread of relatively normally distribued data.", "Five-Number Summaryt - Gives a good feel for how the possible values of quanititative data spread out around the more probable values.", "Find the mean and standard deviation of the â€œWindâ€ speed variable in the airquality dataset.", "pander(summary(airquality$Wind))", "Create a Scatter plot of Temp VS.", "plot(Wind~Temp, data=airquality, xlab= \"Daily Average Temperature\", ylab = \"Daily Average Wind Speed\", main= \"La Guardia Airport Warmer Weather Shows Less Wind\", col= \"gray\", pch= 19)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skill Quiz - Describing Data with R"
  },
  {
    "id": 170,
    "title": "ğŸ“ Class Activity - Principles of Good Graphics",
    "url": "Paige-sNotes.html#classactivityprinciplesofgoodgraphics",
    "content": "Class Activity - Principles of Good Graphics Histogram, Box plot, and Scatter plot with airquality data set hist(airquality$Wind, col=\"steelblue\", xlab=\"Daily Average Wind Speeds (mph)\", main=\"La Guardia Airport from May to September, 1973\") boxplot(Wind~Month, data=airquality, names=c(\"5\",\"6\",\"7\",\"8\",\"9\"), col=c(\"steelblue1\",\"steelblue2\",\"steelblue3\",\"steelblue3\",\"steelblue2\")) plot(Ozone~Temp, data=airquality, col= \"steelblue\", pch=19)",
    "sentences": ["Histogram, Box plot, and Scatter plot with airquality data set", "hist(airquality$Wind, col=\"steelblue\", xlab=\"Daily Average Wind Speeds (mph)\", main=\"La Guardia Airport from May to September, 1973\")", "boxplot(Wind~Month, data=airquality, names=c(\"5\",\"6\",\"7\",\"8\",\"9\"), col=c(\"steelblue1\",\"steelblue2\",\"steelblue3\",\"steelblue3\",\"steelblue2\"))", "plot(Ozone~Temp, data=airquality, col= \"steelblue\", pch=19)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Principles of Good Graphics"
  },
  {
    "id": 171,
    "title": "ğŸ“ Skills Quiz - Data Wrangling & Visualization",
    "url": "Paige-sNotes.html#skillsquizdatawranglingvisualization",
    "content": "Skills Quiz - Data Wrangling & Visualization How do you view a dataset? View([insert the dataset here]) How do you open the help file ?([insert the dataset here]) Quanitative vs.Â Qualitative Quantitative Examples: length & width Qualitatve Examples: name, birthmonth, birthyear, sex, biggerfoot, & domhand PRACTICE : Use KidsFeet dataset Make a table displaying gender table(KidsFeet$sex)%>% pander() Make a table displaying which foot is bigger Which foot is commonly bigger? : Left Foot",
    "sentences": ["How do you view a dataset?", "View([insert the dataset here])", "How do you open the help file", "?([insert the dataset here])", "Quanitative vs.Â Qualitative", "Quantitative Examples: length & width Qualitatve Examples: name, birthmonth, birthyear, sex, biggerfoot, & domhand", "PRACTICE : Use KidsFeet dataset", "Make a table displaying gender", "table(KidsFeet$sex)%>% pander()", "Make a table displaying which foot is bigger Which foot is commonly bigger?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skills Quiz - Data Wrangling & Visualization"
  },
  {
    "id": 172,
    "title": "ğŸ“ Assesment Quiz - Intro to Data Wrangling & Visualization",
    "url": "Paige-sNotes.html#assesmentquizintrotodatawranglingvisualization",
    "content": "Assesment Quiz - Intro to Data Wrangling & Visualization Use the mtcars dataset in R to compute the mean â€œGross horsepowerâ€ of both automatic and manual transmission 1974 Motor Trend vehicles. mtcars %>% group_by(am)%>% summarise(mean(hp))%>% pander() Use the mtcars dataset in R to make a graph that allows you to see how the quarter mile time (qsec) of 1974 Motor Trend vehicles is effected by the number of carburetors (carb) in the vehicle. Since both qsec and carb are quantitiative, a scatterplot is the best graphic! This helps show that the average qsec time (remember, average is the middle of the dots) drops (or gets faster) as the number of carburetors increases plot(qsec ~ carb, data=mtcars) On average, the more carburetors a vehicle has, the faster its quarter mile time. Run the following codes in R. Then select the statement that most appropriately interprets the resulting graph. The graph produced by the code given shows gas mileage on the y-axis and quarter mile times on the x-axis. Also, as indicated by the legend, the color of the points is determined by whether the vehicle is automatic or manual transmission. Since both transmission types show positive moderate correlations, we can conclude that higher quarter mile times (which means slower vehicles) correlate with higher gas mileages.",
    "sentences": ["Use the mtcars dataset in R to compute the mean â€œGross horsepowerâ€ of both automatic and manual transmission 1974 Motor Trend vehicles.", "mtcars %>% group_by(am)%>% summarise(mean(hp))%>% pander()", "Use the mtcars dataset in R to make a graph that allows you to see how the quarter mile time (qsec) of 1974 Motor Trend vehicles is effected by the number of carburetors (carb) in the vehicle.", "Since both qsec and carb are quantitiative, a scatterplot is the best graphic!", "This helps show that the average qsec time (remember, average is the middle of the dots) drops (or gets faster) as the number of carburetors increases", "plot(qsec ~ carb, data=mtcars)", "On average, the more carburetors a vehicle has, the faster its quarter mile time.", "Run the following codes in R.", "Then select the statement that most appropriately interprets the resulting graph.", "The graph produced by the code given shows gas mileage on the y-axis and quarter mile times on the x-axis."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assesment Quiz - Intro to Data Wrangling & Visualization"
  },
  {
    "id": 173,
    "title": "ğŸ“ Class Activity - Types of Data, R Commands, and Graphics",
    "url": "Paige-sNotes.html#classactivitytypesofdatarcommandsandgraphics",
    "content": "Class Activity - Types of Data, R Commands, and Graphics Y Variable : the data we are after/ interested in Example with a Histogram hist(KidsFeet$birthmonth, breaks=seq(0.5,12.5,1)) Quantitative Data : data that is of units of measurement Examples: length, width KidsFeet %>% group_by(sex) %>% summarise(mean(length),max(length)) %>% pander() Categorical Data : a trait that cannot be measured Examples: sex, birth year, birth month table(KidsFeet$sex) %>% pander()",
    "sentences": ["Y Variable : the data we are after/ interested in Example with a Histogram", "hist(KidsFeet$birthmonth, breaks=seq(0.5,12.5,1))", "Quantitative Data : data that is of units of measurement", "Examples: length, width", "KidsFeet %>% group_by(sex) %>% summarise(mean(length),max(length)) %>% pander()", "Categorical Data : a trait that cannot be measured", "Examples: sex, birth year, birth month", "table(KidsFeet$sex) %>% pander()"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Types of Data, R Commands, and Graphics"
  },
  {
    "id": 174,
    "title": "ğŸ“ Class Activity - Reviewing It All",
    "url": "Paige-sNotes.html#classactivityreviewingitall",
    "content": "Class Activity - Reviewing It All Load and view the starwars dataset datatable(starwars, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") PRACTICE : Use starwars data set Use the starwars data set to create a meaningful histogram and supporting numerical summaries. hist(starwars$height) summary(starwars$height)%>% pander() you could also use favstats ( ), it gives you the standard deviation! Use the starwars data set to create a meaningful boxplot (preferably side-by-side boxplots) and supporting numerical summaries. boxplot(height~eye_color, data=starwars) this table was using favstats!!",
    "sentences": ["Load and view the starwars dataset", "datatable(starwars, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\")", "PRACTICE : Use starwars data set", "Use the starwars data set to create a meaningful histogram and supporting numerical summaries.", "hist(starwars$height)", "summary(starwars$height)%>% pander()", "you could also use favstats ( ), it gives you the standard deviation!", "Use the starwars data set to create a meaningful boxplot (preferably side-by-side boxplots) and supporting numerical summaries.", "boxplot(height~eye_color, data=starwars)", "this table was using favstats!!"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Reviewing It All"
  },
  {
    "id": 175,
    "title": "ğŸ“ Week 4 | Making Inference with t Tests",
    "url": "Paige-sNotes.html#week4makinginferencewithttests",
    "content": "Week 4 | Making Inference with t Tests Studying from the [Making Inference](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/MakingInference.html and t Test pages and work on learning the t Tests! There is are the: One Sample t Test Paired Samples t Test Independent Samples t Test",
    "sentences": ["Studying from the [Making Inference](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/MakingInference.html and t Test pages and work on learning the t Tests!", "There is are the:", "One Sample t Test Paired Samples t Test Independent Samples t Test"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Week 4 | Making Inference with t Tests"
  },
  {
    "id": 176,
    "title": "ğŸ“ Skills Quiz - t Tests",
    "url": "Paige-sNotes.html#skillsquizttests",
    "content": "Skills Quiz - t Tests Inference : the process of deciding whether patterns and trends in a sample of data from a population can be assumed to be true for the full population. does the group define the crowd? A pattern can appear one way in a sample of data, but be completely different for the full population. That is why we use statistical inference to decide when it is safe (and when it is not) to conclude that a pattern in a sample holds in the full population. P-value : the most commonly used method of deciding when to reject a null hypothesis. The things needed to compute a p-value are: A test statistic A probabiltiy distribution of the test statistics that would be possible if the null hypothesis is true If a researcher was using a signficance level of 0.05 and obtained a p-value of 0.419, they can safely conclude that the null hypothesis is true and that the alternative hypothesis is the false. If P is high, keep the guy! Decision Errors : they are two types of errors, Type I or II, that indicate to us whether or not we made a mistake in: Rejecting the null when it was true (Type I) Accepting the null when it was false (Type II) For example: In a typical U.S. Court of Law, the null hypothesis is that the person on trial is innocent. - Thus, convicting an innocent man of a crime would be an example of a Type I Error - And letting a guilty man go free would be an example of a Type II Error",
    "sentences": ["Inference : the process of deciding whether patterns and trends in a sample of data from a population can be assumed to be true for the full population.", "does the group define the crowd?", "A pattern can appear one way in a sample of data, but be completely different for the full population.", "That is why we use statistical inference to decide when it is safe (and when it is not) to conclude that a pattern in a sample holds in the full population.", "P-value : the most commonly used method of deciding when to reject a null hypothesis.", "The things needed to compute a p-value are: A test statistic A probabiltiy distribution of the test statistics that would be possible if the null hypothesis is true If a researcher was using a signficance level of 0.05 and obtained a p-value of 0.419, they can safely conclude that the null hypothesis is true and that the alternative hypothesis is the false.", "If P is high, keep the guy!", "Decision Errors : they are two types of errors, Type I or II, that indicate to us whether or not we made a mistake in: Rejecting the null when it was true (Type I) Accepting the null when it was false (Type II)", "For example: In a typical U.S.", "Court of Law, the null hypothesis is that the person on trial is innocent."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skills Quiz - t Tests"
  },
  {
    "id": 177,
    "title": "ğŸ“ Assessment Quiz - t Tests",
    "url": "Paige-sNotes.html#assessmentquizttests",
    "content": "Assessment Quiz - t Tests In a typical year, there are 52 weeks. However, 52 x 7 = 364, and as most of us know, there are 365 days in a year. This means that every year, at least one day gets to happen more than 52 times. Use appropriate R commands and the Births78 dataset to determine which day of the week in 1978 occurred 53 times. Births78 %>% group_by(wday) %>% summarise(n()) %>% pander() Use the Births78 dataset in RStudio to test the following hypotheses. H0:Î¼Wednesdayâˆ’Î¼Thursday=0 Ha:Î¼Wednesdayâˆ’Î¼Thursdayâ‰ 0 Find the p-value of the test. Birf <-filter(Births78,wday %in% c(\"Wed\",\"Thu\")) t.test(births ~ wday, data=Birf, mu=0, alternative=\"two.sided\", conf.level=0.95) Answer : P-value = 0.8855",
    "sentences": ["In a typical year, there are 52 weeks.", "However, 52 x 7 = 364, and as most of us know, there are 365 days in a year.", "This means that every year, at least one day gets to happen more than 52 times.", "Use appropriate R commands and the Births78 dataset to determine which day of the week in 1978 occurred 53 times.", "Births78 %>% group_by(wday) %>% summarise(n()) %>% pander()", "Use the Births78 dataset in RStudio to test the following hypotheses.", "H0:Î¼Wednesdayâˆ’Î¼Thursday=0 Ha:Î¼Wednesdayâˆ’Î¼Thursdayâ‰ 0", "Find the p-value of the test.", "Birf <-filter(Births78,wday %in% c(\"Wed\",\"Thu\")) t.test(births ~ wday, data=Birf, mu=0, alternative=\"two.sided\", conf.level=0.95)", "Answer : P-value = 0.8855"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assessment Quiz - t Tests"
  },
  {
    "id": 178,
    "title": "ğŸ“ Class Activity - Making Inference",
    "url": "Paige-sNotes.html#classactivitymakinginference",
    "content": "Class Activity - Making Inference Studying from the Making Inference page! Managing Decision Errors hypothesis -> beliefs null (initial) BELIEF, alternative BELIEF Goal: to discover truth Alternatively, you can think about it like: Type I Error -> Action (Throwing away truth) effected by alpha; small a, small error vice versa -Type II Error -> (Failing to move to truth) effected by beta; small b, small error vice versa Alpha and Beta work inversely of eachother! - high a -> low b - low a -> high b The P-value measures how much the evidence differs from what we expected under the null hypothesis(belief) P = â€œprobabilityâ€ P is low, Ho must go! P is high, keep the guy! null gives us a frame of reference from where the data COULD fall",
    "sentences": ["Studying from the Making Inference page!", "Managing Decision Errors", "hypothesis -> beliefs null (initial) BELIEF, alternative BELIEF Goal: to discover truth", "Alternatively, you can think about it like:", "Type I Error -> Action (Throwing away truth) effected by alpha; small a, small error vice versa -Type II Error -> (Failing to move to truth) effected by beta; small b, small error vice versa", "Alpha and Beta work inversely of eachother!", "- high a -> low b - low a -> high b", "The P-value measures how much the evidence differs from what we expected under the null hypothesis(belief) P = â€œprobabilityâ€ P is low, Ho must go!", "P is high, keep the guy!", "null gives us a frame of reference from where the data COULD fall"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Making Inference"
  },
  {
    "id": 179,
    "title": "ğŸ“ Class Activity - t Tests",
    "url": "Paige-sNotes.html#classactivityttests",
    "content": "Class Activity - t Tests P-value measures the probability of the test statistic (what is happening in front of you) if something is impossible, then we should reject the belief(the null) since the possibility of that belief happening is close to 0 Hypothesis = â€œWE BELIEVEâ€ Finding P-values with t Tests! One-Sample t Test What is the average foot size of 4th graders? t.test(KidsFeet$length, mu=28, alternative=\"two.sided\", conf.level=0.95) %>% pander() We have sufficient evidence to believe that kids feet are not averaged aroun 28 cm. Instead, they are averaged around 24 to 25 cm. Paired Samples t Test Is the length measurement of someoneâ€™s foot the same measurement of 3 widthâ€™s of their foot? t.test(KidsFeet$length, KidsFeet$width*3, paired=TRUE, mu=0, alternative=\"two.sided\",conf.level=0.95) %>% pander()",
    "sentences": ["P-value measures the probability of the test statistic (what is happening in front of you) if something is impossible, then we should reject the belief(the null) since the possibility of that belief happening is close to 0 Hypothesis = â€œWE BELIEVEâ€", "Finding P-values with t Tests!", "One-Sample t Test", "What is the average foot size of 4th graders?", "t.test(KidsFeet$length, mu=28, alternative=\"two.sided\", conf.level=0.95) %>% pander()", "We have sufficient evidence to believe that kids feet are not averaged aroun 28 cm.", "Instead, they are averaged around 24 to 25 cm.", "Paired Samples t Test", "Is the length measurement of someoneâ€™s foot the same measurement of 3 widthâ€™s of their foot?", "t.test(KidsFeet$length, KidsFeet$width*3, paired=TRUE, mu=0, alternative=\"two.sided\",conf.level=0.95) %>% pander()"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - t Tests"
  },
  {
    "id": 180,
    "title": "ğŸ“ Week 5 | Wilcoxon Tests",
    "url": "Paige-sNotes.html#week5wilcoxontests",
    "content": "Week 5 | Wilcoxon Tests Studying from the Wilcoxon Tests page! (the first nonparameteric test we encounter!) Wilcoxon Signed-Rank Test Wilcoxon Rank Sum (Mann-Whiteny) Test",
    "sentences": ["Studying from the Wilcoxon Tests page!", "(the first nonparameteric test we encounter!)", "Wilcoxon Signed-Rank Test Wilcoxon Rank Sum (Mann-Whiteny) Test"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Week 5 | Wilcoxon Tests"
  },
  {
    "id": 181,
    "title": "ğŸ“ Skills Quiz - Wilcoxon Tests",
    "url": "Paige-sNotes.html#skillsquizwilcoxontests",
    "content": "Skills Quiz - Wilcoxon Tests are NON-PARAMETRIC tests uses sums of ranks and mathematical counting techniques (a NON-PARAMETRIC distribution) to calculate the p-value t test (that are PARAMETRIC) rely on t distributions (which are PARAMETRIC distributions) to calculate the p-value while Wilcoxon tests the distribution of the Wilcoxon Test Statistic can be usefully approximated by a normal distribution when the sample size of the data being used in the test is large However, this distribution of the test statistic can never be exactly normal because the test statistic can only ever be a whole number ignores the specific values of the data and only utilize the relative positions of the data, i.e., the ranks (hence the reason they are often called Rank-Based Tests.) 1) Wilcoxon Signed-Rank Test was originally created to test hypotheses about the value of the median, but can be used to test hypotheses about the mean when data is SYMMETRIC A useful custom graphic that shows all of the data as well as a five-number summary of the data is a box plot overlaid with a dot plot The style of tests that can be performed with this test are: One Sample: testing hypotheses about the center of a distribution (where the center is subtracted from each value). Paired Samples: testing hypotheses about the center of the distribution of differences. The CornHeights Analysis Example is shown below: corn <- c(14, 56, 60, 16, 6, 8, -48, 49, 24, 28, 29, 41, -67, 23, 75) boxplot(corn, horizontal=TRUE, col=\"cornsilk\", main=\"Differences in Corn Plant Heights\", xlab=\"Cross-Fertilized Height minus Self-Fertilized Height\") stripchart(corn, pch=16, method=\"stack\", col=\"darkgray\", add=TRUE) Here is the number summary and Wilcoxon test that goes along with it!",
    "sentences": ["are NON-PARAMETRIC tests uses sums of ranks and mathematical counting techniques (a NON-PARAMETRIC distribution) to calculate the p-value t test (that are PARAMETRIC) rely on t distributions (which are PARAMETRIC distributions) to calculate the p-value while Wilcoxon tests", "the distribution of the Wilcoxon Test Statistic can be usefully approximated by a normal distribution when the sample size of the data being used in the test is large However, this distribution of the test statistic can never be exactly normal because the test statistic can only ever be a whole number", "ignores the specific values of the data and only utilize the relative positions of the data, i.e., the ranks (hence the reason they are often called Rank-Based Tests.)", "1) Wilcoxon Signed-Rank Test", "was originally created to test hypotheses about the value of the median, but can be used to test hypotheses about the mean when data is SYMMETRIC A useful custom graphic that shows all of the data as well as a five-number summary of the data is a box plot overlaid with a dot plot The style of tests that can be performed with this test are:", "One Sample: testing hypotheses about the center of a distribution (where the center is subtracted from each value).", "Paired Samples: testing hypotheses about the center of the distribution of differences.", "The CornHeights Analysis Example is shown below:", "corn <- c(14, 56, 60, 16, 6, 8, -48, 49, 24, 28, 29, 41, -67, 23, 75) boxplot(corn, horizontal=TRUE, col=\"cornsilk\", main=\"Differences in Corn Plant Heights\", xlab=\"Cross-Fertilized Height minus Self-Fertilized Height\") stripchart(corn, pch=16, method=\"stack\", col=\"darkgray\", add=TRUE)", "Here is the number summary and Wilcoxon test that goes along with it!"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skills Quiz - Wilcoxon Tests"
  },
  {
    "id": 182,
    "title": "ğŸ“ Assessment Quiz - Wilcoxon Tests",
    "url": "Paige-sNotes.html#assessmentquizwilcoxontests",
    "content": "Assessment Quiz - Wilcoxon Tests Use the Salaries dataset in R to find the number of male and female assistant professors in the dataset. This can be done in two different ways! Use the favstats() command SalariesAss <- filter(Salaries, rank == \"AsstProf\") pander(favstats(salary ~ sex, data=SalariesAss)) Use the group_by() and summarise() commands Salaries %>% group_by(rank, sex) %>% summarise(n()) %>% pander() Create an appropriate graphic using the Salaries dataset in R that would allow you to compare the distribution of salaries for faculty in discipline A (â€œtheoreticalâ€) and discipline B (â€œappliedâ€) departments. boxplot(salary ~ discipline, data=SalariesAss, col=\"white\", main=\"Faculty From U.S. Colleges\", xlab=\"Discipline\", ylab=\"Salaries\") The graphics that can me used for Wilcoxon Rank Sum Test are BOXPLOTS & DOT PLOTS, not SCATTER PLOTS",
    "sentences": ["Use the Salaries dataset in R to find the number of male and female assistant professors in the dataset.", "This can be done in two different ways!", "Use the favstats() command", "SalariesAss <- filter(Salaries, rank == \"AsstProf\") pander(favstats(salary ~ sex, data=SalariesAss))", "Use the group_by() and summarise() commands", "Salaries %>% group_by(rank, sex) %>% summarise(n()) %>% pander()", "Create an appropriate graphic using the Salaries dataset in R that would allow you to compare the distribution of salaries for faculty in discipline A (â€œtheoreticalâ€) and discipline B (â€œappliedâ€) departments.", "boxplot(salary ~ discipline, data=SalariesAss, col=\"white\", main=\"Faculty From U.S.", "Colleges\", xlab=\"Discipline\", ylab=\"Salaries\")", "The graphics that can me used for Wilcoxon Rank Sum Test are BOXPLOTS & DOT PLOTS, not SCATTER PLOTS"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assessment Quiz - Wilcoxon Tests"
  },
  {
    "id": 183,
    "title": "ğŸ“ Class Activity - Wilcoxon Tests",
    "url": "Paige-sNotes.html#classactivitywilcoxontests",
    "content": "Class Activity - Wilcoxon Tests What is the Wilcoxon Test? not concerned about the distance between values we care about the location shift/ difference in ranks - sample size, standard deviation does NOT matter! Steps to Wilcoxon Rank Sum (Mann-Whitney Test): Order Data Rank it Pull out ranks from each group Add the ranks What is the possibility of each sum being possible? Use Frank Wilcoxon spikey distribution to determine the extremities of the data Finite and discrete (normal distribution is infinite and continuous) When they detect any ties in ranks, they shift from a finite realm to a more continuous realm in order to compute it properly A <- c(68, 68, 59, 72, 64, 67, 70, 74) B <- c(60, 67, 61, 62, 67, 63, 56, 58) wilcox.test(A,B) What is the question(s) this test would answer? Do the two genders tend to pick their favorite numbers differently? Do the test scores of students from school X tend to be higher than the test score of students from school Y?",
    "sentences": ["What is the Wilcoxon Test?", "not concerned about the distance between values we care about the location shift/ difference in ranks - sample size, standard deviation does NOT matter!", "Steps to Wilcoxon Rank Sum (Mann-Whitney Test):", "Order Data Rank it Pull out ranks from each group Add the ranks What is the possibility of each sum being possible?", "Use Frank Wilcoxon spikey distribution to determine the extremities of the data Finite and discrete (normal distribution is infinite and continuous)", "When they detect any ties in ranks, they shift from a finite realm to a more continuous realm in order to compute it properly", "A <- c(68, 68, 59, 72, 64, 67, 70, 74) B <- c(60, 67, 61, 62, 67, 63, 56, 58) wilcox.test(A,B)", "What is the question(s) this test would answer?", "Do the two genders tend to pick their favorite numbers differently?", "Do the test scores of students from school X tend to be higher than the test score of students from school Y?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Wilcoxon Tests"
  },
  {
    "id": 184,
    "title": "ğŸ“ Class Activity - Wilcoxon Tests, Boxplots, and P-Values",
    "url": "Paige-sNotes.html#classactivitywilcoxontestsboxplotsandpvalues",
    "content": "Class Activity - Wilcoxon Tests, Boxplots, and P-Values Which function would you use to add a new column to your dataset? What is the goal of statistics? to quantify things (the chance of this happening if this is the truth!) test statistic shows us the measurement of how far we are from the null Which plots work best to display one quantitative variable that is separated into 2 or more groups? box plot & dot plot Box plots + Dot plots are great to show off this kind of data (because it shows the median!) Median line closer to the bottom of the box plot = Right Skewed Median line is closer to the top of the box plot = Left Skewed What two things are needed to calculate a p-value? a test statistic and a sampling distribution of that test statistics When would an independent samples t test be inappropriate to use but a Wilcoxon Rank Sum Test be appropriate? Independent Samples t Test : looks at average needs raw data (times, measurements, etc.) Wilcoxon Rank Sum Test : looks at median needs ranks (first place, second place, etc.) What is an appropriate set of hypotheses to use for an independent samples t test? For a Wilcoxon Rank Sum Test? Independent Samples t Test : mu Wilcoxon Rank Sum Test : Median or â€œstochasticallyâ€",
    "sentences": ["Which function would you use to add a new column to your dataset?", "What is the goal of statistics?", "to quantify things (the chance of this happening if this is the truth!) test statistic shows us the measurement of how far we are from the null", "Which plots work best to display one quantitative variable that is separated into 2 or more groups?", "box plot & dot plot Box plots + Dot plots are great to show off this kind of data (because it shows the median!) Median line closer to the bottom of the box plot = Right Skewed Median line is closer to the top of the box plot = Left Skewed", "What two things are needed to calculate a p-value?", "a test statistic and a sampling distribution of that test statistics", "When would an independent samples t test be inappropriate to use but a Wilcoxon Rank Sum Test be appropriate?", "Independent Samples t Test : looks at average needs raw data (times, measurements, etc.) Wilcoxon Rank Sum Test : looks at median needs ranks (first place, second place, etc.)", "What is an appropriate set of hypotheses to use for an independent samples t test?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Wilcoxon Tests, Boxplots, and P-Values"
  },
  {
    "id": 185,
    "title": "ğŸ“ Week 6 | Two-way ANOVA",
    "url": "Paige-sNotes.html#week6twowayanova",
    "content": "Week 6 | Two-way ANOVA Studying from the ANOVA page and its one quantitative column and the two categorical columns! One-Way ANOVA Test Two-Way ANOVA Test",
    "sentences": ["Studying from the ANOVA page and its one quantitative column and the two categorical columns!", "One-Way ANOVA Test Two-Way ANOVA Test"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Week 6 | Two-way ANOVA"
  },
  {
    "id": 186,
    "title": "ğŸ“ Skills Quiz - ANOVA",
    "url": "Paige-sNotes.html#skillsquizanova",
    "content": "Skills Quiz - ANOVA What is an example of a â€œfactorâ€? Hair color! Run the code â€œView(warpbreaks)â€ in R. Use that data set to identify each of the following as either a â€œfactorâ€ or a â€œlevelâ€ of a factor. datatable(warpbreaks,options=list(c(3,10,30))) What are the factors for this data set? Tension Wool What are the levels of factors? Medium (from the Tension factor) High (from the Tension factor) A (from the Wool factor) Low (from the Tension Factor) B (from the Wool factor) Why is the â€œBreaksâ€ column not a factor? The column contains quanitative values than the qualitative values",
    "sentences": ["What is an example of a â€œfactorâ€?", "Hair color!", "Run the code â€œView(warpbreaks)â€ in R.", "Use that data set to identify each of the following as either a â€œfactorâ€ or a â€œlevelâ€ of a factor.", "datatable(warpbreaks,options=list(c(3,10,30)))", "What are the factors for this data set?", "Tension Wool", "What are the levels of factors?", "Medium (from the Tension factor) High (from the Tension factor) A (from the Wool factor) Low (from the Tension Factor) B (from the Wool factor) Why is the â€œBreaksâ€ column not a factor?", "The column contains quanitative values than the qualitative values"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skills Quiz - ANOVA"
  },
  {
    "id": 187,
    "title": "ğŸ“ Assessment Quiz - ANOVA",
    "url": "Paige-sNotes.html#assessmentquizanova",
    "content": "Assessment Quiz - ANOVA In a certain studentâ€™s ANOVA analysis the â€œBetween groups varianceâ€ was 18.52 while the â€œWithin groups varianceâ€ was 4.9. What was the value of the test statistic of their ANOVA test? ANSWER : F = 3.78 As shown in the Explanation tab of the ANOVA page of the Math 325 Notebook, the ANOVA test statistic is an F statistic. It is calculated by taking the â€œBetween groups varianceâ€ and dividing this by the â€œWithin groups varianceâ€. Thus 18.52/4.9 = 3.779592 which rounds to 3.78. Make three Two-Way ANOVA graphs looking at the length, domhand, and sex columns of the KidsFeet dataset. Based on your p-values and these graphs, which of the following is a correct conclusion to reach? xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1)))",
    "sentences": ["In a certain studentâ€™s ANOVA analysis the â€œBetween groups varianceâ€ was 18.52 while the â€œWithin groups varianceâ€ was 4.9.", "What was the value of the test statistic of their ANOVA test?", "ANSWER : F = 3.78", "As shown in the Explanation tab of the ANOVA page of the Math 325 Notebook, the ANOVA test statistic is an F statistic.", "It is calculated by taking the â€œBetween groups varianceâ€ and dividing this by the â€œWithin groups varianceâ€.", "Thus 18.52/4.9 = 3.779592 which rounds to 3.78.", "Make three Two-Way ANOVA graphs looking at the length, domhand, and sex columns of the KidsFeet dataset.", "Based on your p-values and these graphs, which of the following is a correct conclusion to reach?", "xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\")", "xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1)))"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assessment Quiz - ANOVA"
  },
  {
    "id": 188,
    "title": "ğŸ“ Class Activity - One- Way ANOVA",
    "url": "Paige-sNotes.html#classactivityonewayanova",
    "content": "Class Activity - One- Way ANOVA What does ANOVA stand for? Analysis Of Variance! tests several means simultaneously Rejecting a null -> I know that there is a different Fail to reject the null -> I donâ€™t know if they differ! If you conduct a ton of tests on one data set, the probability of a Type 1 Error increases! we would expect that one of the tests ex. conduct two tests, nearly doubles the Type 1 Error Thankfully, ANOVA protects against those potential errors Hypothesis of ANOVA Null hypothesis = all means are equal Alternative hypothesis = at least one mean is different problem(s): doesnâ€™t tell us which one is different or how many are different! Explain the words â€œfactorâ€ and â€œlevelâ€",
    "sentences": ["What does ANOVA stand for?", "Analysis Of Variance!", "tests several means simultaneously", "Rejecting a null -> I know that there is a different Fail to reject the null -> I donâ€™t know if they differ!", "If you conduct a ton of tests on one data set, the probability of a Type 1 Error increases!", "we would expect that one of the tests ex.", "conduct two tests, nearly doubles the Type 1 Error Thankfully, ANOVA protects against those potential errors", "Hypothesis of ANOVA", "Null hypothesis = all means are equal Alternative hypothesis = at least one mean is different problem(s): doesnâ€™t tell us which one is different or how many are different!", "Explain the words â€œfactorâ€ and â€œlevelâ€"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - One- Way ANOVA"
  },
  {
    "id": 189,
    "title": "ğŸ“ Class Activity - Two- Way ANOVA",
    "url": "Paige-sNotes.html#classactivitytwowayanova",
    "content": "Class Activity - Two- Way ANOVA Looking at the DayCare.RmD file! How many Day Care Centers were included in the study? 10 How many Centers were included in the treatment group? The Control Group? 6 and 4 During which weeks did the researchers simply observe the Day Care Centers? first four During which weeks was the fine applied to the treatment group? w 5-16 During which weeks was the fine removed from the treatment group? w 17 What was the main response variable of interest that the researchers recorded each week on the Day Care Centers? The number of late children (does the fine change behavior?) How to change from wide to long data?",
    "sentences": ["Looking at the DayCare.RmD file!", "How many Day Care Centers were included in the study?", "10 How many Centers were included in the treatment group?", "The Control Group?", "6 and 4 During which weeks did the researchers simply observe the Day Care Centers?", "first four During which weeks was the fine applied to the treatment group?", "w 5-16 During which weeks was the fine removed from the treatment group?", "w 17 What was the main response variable of interest that the researchers recorded each week on the Day Care Centers?", "The number of late children (does the fine change behavior?)", "How to change from wide to long data?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Two- Way ANOVA"
  },
  {
    "id": 190,
    "title": "ğŸ“ Week 7 | The Kruskall-Wallis Test",
    "url": "Paige-sNotes.html#week7thekruskallwallistest",
    "content": "Week 7 | The Kruskall-Wallis Test Studying from the Kruskal-Wallis Test page and its the second nonparametric test that we see thats quantitative to multiple categorical!",
    "sentences": "Studying from the Kruskal-Wallis Test page and its the second nonparametric test that we see thats quantitative to multiple categorical!",
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Week 7 | The Kruskall-Wallis Test"
  },
  {
    "id": 191,
    "title": "ğŸ“ Skills Quiz - Kruskal-Wallis Test",
    "url": "Paige-sNotes.html#skillsquizkruskalwallistest",
    "content": "Skills Quiz - Kruskal-Wallis Test Use the SaratogaHouses data set for the following Kruskal-Wallis Test! table(SaratogaHouses$fuel) Say a homeowner in Saratoga County, New York is curious about whether upgrading their home from an oil heating fuel system to either a gas or electric fuel system would increase the resale value of their home. Use the SaratogaHouses dataset in R to answer the question, â€œwhich heating system results in the highest distribution of home resale values (price)? Null Hypothesis : The price of homes with either gas, oil, or electric heating fuel systems all come from the same distribution Alternative Hypothesis : At least one type of fuel systems results in a different distribution of prices of homes kruskal.test(price ~ fuel, data=SaratogaHouses) The Kruskal-Wallis Rank Sum Test results in a p-value of 2.2e-16, meaning there is sufficent evidence to conclude the hypothesis the distribution of prices of homes is different for at least one of the fuel system types. The graphic that best depicts the results of the Kruskall-Wallis Testâ€¦ A Boxplot!! boxplot(price ~ fuel, data=SaratogaHouses) Questions regarding the previous graph:",
    "sentences": ["Use the SaratogaHouses data set for the following Kruskal-Wallis Test!", "table(SaratogaHouses$fuel)", "Say a homeowner in Saratoga County, New York is curious about whether upgrading their home from an oil heating fuel system to either a gas or electric fuel system would increase the resale value of their home.", "Use the SaratogaHouses dataset in R to answer the question, â€œwhich heating system results in the highest distribution of home resale values (price)?", "Null Hypothesis : The price of homes with either gas, oil, or electric heating fuel systems all come from the same distribution Alternative Hypothesis : At least one type of fuel systems results in a different distribution of prices of homes", "kruskal.test(price ~ fuel, data=SaratogaHouses)", "The Kruskal-Wallis Rank Sum Test results in a p-value of 2.2e-16, meaning there is sufficent evidence to conclude the hypothesis the distribution of prices of homes is different for at least one of the fuel system types.", "The graphic that best depicts the results of the Kruskall-Wallis Testâ€¦ A Boxplot!!", "boxplot(price ~ fuel, data=SaratogaHouses)", "Questions regarding the previous graph:"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skills Quiz - Kruskal-Wallis Test"
  },
  {
    "id": 192,
    "title": "ğŸ“ Assessement Quiz - Kruskal-Wallis",
    "url": "Paige-sNotes.html#assessementquizkruskalwallis",
    "content": "Assessement Quiz - Kruskal-Wallis Use the Salaries dataset in R, library(car), to test the hypotheses and report the test statisic and the conlusion of your test \\[ H_0 : \\text{The distribution of salary is the same for Associate, Assistant, and Full Professors.} \\] \\[ H_a : \\text{The distribution of salary is different for at least one type of Professor.} \\] kruskal.test(salary ~ rank, data=Salaries) boxplot(salary ~ rank, data=Salaries) Salaries %>% group_by(rank) %>% summarise(`Median Salary`=median(salary), .groups=\"drop\") %>% pander(caption=\"Median Salary by Rank\") A waiter at a local restaurant collects data on how much they earn in tips (per person) when groups of people each pay separately for their meal versus when one member of the group pays for everyone. Their very small sample of data is as follows. Group ID Number of People in Group Type of Payment Total Tip Amount Per Person Amount 1 5 Each Paid $16 $3.20 2 3 One Paid for All $10 $3.33 3 4 One Paid for All $9 $2.25 4 4 Each Paid $15 $3.75 5 6 Each Paid $21 $3.50 Which hypothesis test would be most appropriate for deciding if the average per person tip amount is higher when people that eat in groups each pay individually for their meal? What is the correct way to describe the test statistic, H, of the Kruskal-Wallis Test? It is a combined measurement of how much the average ranks differ between each groupp-0. ANOVA uses the F = (Between groups variance)/(Within groups variance) test statistic. The One Sample t Test measures how far the sample mean is from mu for its test statistic. The Wilcoxon Rank Sum test sums the ranks from one of the groups for its test statistic.",
    "sentences": ["Use the Salaries dataset in R, library(car), to test the hypotheses and report the test statisic and the conlusion of your test", "\\[ H_0 : \\text{The distribution of salary is the same for Associate, Assistant, and Full Professors.} \\] \\[ H_a : \\text{The distribution of salary is different for at least one type of Professor.} \\]", "kruskal.test(salary ~ rank, data=Salaries)", "boxplot(salary ~ rank, data=Salaries)", "Salaries %>% group_by(rank) %>% summarise(`Median Salary`=median(salary), .groups=\"drop\") %>% pander(caption=\"Median Salary by Rank\")", "A waiter at a local restaurant collects data on how much they earn in tips (per person) when groups of people each pay separately for their meal versus when one member of the group pays for everyone.", "Their very small sample of data is as follows.", "Group ID Number of People in Group Type of Payment Total Tip Amount Per Person Amount 1 5 Each Paid $16 $3.20 2 3 One Paid for All $10 $3.33 3 4 One Paid for All $9 $2.25 4 4 Each Paid $15 $3.75 5 6 Each Paid $21 $3.50", "Which hypothesis test would be most appropriate for deciding if the average per person tip amount is higher when people that eat in groups each pay individually for their meal?", "What is the correct way to describe the test statistic, H, of the Kruskal-Wallis Test?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assessement Quiz - Kruskal-Wallis"
  },
  {
    "id": 193,
    "title": "ğŸ“ Class Activity - Idea Approval for the Week 11 â€œConsulting\r\nOpportunity or Research Projectâ€",
    "url": "Paige-sNotes.html#classactivityideaapprovalfortheweek11consultingopp",
    "content": "Class Activity - Idea Approval for the Week 11 â€œConsulting\r\nOpportunity or Research Projectâ€ Find a data analysis opportunity for a client* where you use data the client provides to answer research questions they have. *A client could be a small business owner (maybe your parents, an aunt, uncle, or someone else you personally know), a teacher you TA for, your boss you currently work for or that you worked for in the past, an old high school teacher, or even a roommate. The client could potentially even be yourself if say you were trying to buy a new car or some other important item. However, this analysis has the greatest chance of contributing to your resume if it is performed for someone else rather than just yourself. But if you did something for yourself that was say â€œblog worthyâ€ and that other people would also find interesting, then that could be meaningful as well. In the space below, state which option you are choosing, and provide details on how you are going to make this work. Over the next couple of weeks your teacher will follow up with you to see how you are doing with implementing your idea. If you have any concerns about whether your idea is a good idea or not, discuss that in person with your teacher right away. Make sure you only submit an idea that you are fairly confident will work well in the space below. Make sure it powerful and correct!! START WITH THE QUESTION OR GRAPH hide technical details use two way thinkin",
    "sentences": ["Find a data analysis opportunity for a client* where you use data the client provides to answer research questions they have.", "*A client could be a small business owner (maybe your parents, an aunt, uncle, or someone else you personally know), a teacher you TA for, your boss you currently work for or that you worked for in the past, an old high school teacher, or even a roommate.", "The client could potentially even be yourself if say you were trying to buy a new car or some other important item.", "However, this analysis has the greatest chance of contributing to your resume if it is performed for someone else rather than just yourself.", "But if you did something for yourself that was say â€œblog worthyâ€ and that other people would also find interesting, then that could be meaningful as well.", "In the space below, state which option you are choosing, and provide details on how you are going to make this work.", "Over the next couple of weeks your teacher will follow up with you to see how you are doing with implementing your idea.", "If you have any concerns about whether your idea is a good idea or not, discuss that in person with your teacher right away.", "Make sure you only submit an idea that you are fairly confident will work well in the space below.", "Make sure it powerful and correct!!"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Idea Approval for the Week 11 â€œConsulting\r\nOpportunity or Research Projectâ€"
  },
  {
    "id": 194,
    "title": "ğŸ“ Class Activity - Kruskal-Wallis Test",
    "url": "Paige-sNotes.html#classactivitykruskalwallistest",
    "content": "Class Activity - Kruskal-Wallis Test nonparametric equivalent of a One-Way ANOVA -> rank based test (not caring about data values! ) will have to use as.numeric() function to make columns into mumeric values for groups (C) use mutate(new column name = interaction(group 1, group 2)) Any similarities this test has to the Wilcoxon Rank Sum test? Discuss any differences? Similarities both are nonparametric tests! donâ€™t get a lot of powerful insights Differences Testing Groups Wilcoxon = 2 groups Kruskal-Wallis = 3 groups! Can dump all the groups at once Test Statisitic Wilcoxon (F) Kruskal-Wallis (H) C -> number of samples/groups (need 3 or more!) N -> Total of all sample sizes n_i -> each individual sample (n_i = n_1 + n_2 â€¦) R bar _i -> the mean rank for each sample What can you conclude when the p-value of the Kruskal-Wallis Test is significant? Significant : bet your money on that choice! Not Significant: using p-value to say â€œmaybeâ€ / suggesting that this is more likely to be the Tell them where to go from what the data does or doesnâ€™t give (whatâ€™s the next step?)",
    "sentences": ["nonparametric equivalent of a One-Way ANOVA -> rank based test (not caring about data values!", ") will have to use as.numeric() function to make columns into mumeric values for groups (C) use mutate(new column name = interaction(group 1, group 2))", "Any similarities this test has to the Wilcoxon Rank Sum test?", "Discuss any differences?", "Similarities both are nonparametric tests!", "donâ€™t get a lot of powerful insights Differences Testing Groups Wilcoxon = 2 groups Kruskal-Wallis = 3 groups!", "Can dump all the groups at once Test Statisitic Wilcoxon (F) Kruskal-Wallis (H) C -> number of samples/groups (need 3 or more!) N -> Total of all sample sizes n_i -> each individual sample (n_i = n_1 + n_2 â€¦) R bar _i -> the mean rank for each sample", "What can you conclude when the p-value of the Kruskal-Wallis Test is significant?", "Significant : bet your money on that choice!", "Not Significant: using p-value to say â€œmaybeâ€ / suggesting that this is more likely to be the Tell them where to go from what the data does or doesnâ€™t give (whatâ€™s the next step?)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Kruskal-Wallis Test"
  },
  {
    "id": 195,
    "title": "ğŸ“ Skill Quiz - Simple Linear Regression",
    "url": "Paige-sNotes.html#skillquizsimplelinearregression",
    "content": "Skill Quiz - Simple Linear Regression There are five assupmtions of the simple linear regression model: Constant Variance Independent Errors Normal Errors Fixed X Values Linear Relation Questions about the assumptions: Which regression assumption(s) does the residuals versus fitted values plot diagnose? Linear Relation Constance Variance Which regression assumption(s) does the Q-Q Plot of the residuals diagnose? Which regression assumption(s) does the Residuals versus Order plot diagnose? (Remember, this plot can only be created when the data was collected in a specific order.) Independent Errors Perform the following simple linear regression in R: plot(Height ~ Volume, data = trees) trees.lm <- lm(Height ~ Volume, data = trees) abline(trees.lm)",
    "sentences": ["There are five assupmtions of the simple linear regression model:", "Constant Variance Independent Errors Normal Errors Fixed X Values Linear Relation", "Questions about the assumptions:", "Which regression assumption(s) does the residuals versus fitted values plot diagnose?", "Linear Relation Constance Variance", "Which regression assumption(s) does the Q-Q Plot of the residuals diagnose?", "Which regression assumption(s) does the Residuals versus Order plot diagnose?", "(Remember, this plot can only be created when the data was collected in a specific order.)", "Independent Errors Perform the following simple linear regression in R:", "plot(Height ~ Volume, data = trees) trees.lm <- lm(Height ~ Volume, data = trees) abline(trees.lm)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skill Quiz - Simple Linear Regression"
  },
  {
    "id": 196,
    "title": "ğŸ“ Assessment Quiz - Simple Linear Regression",
    "url": "Paige-sNotes.html#assessmentquizsimplelinearregression",
    "content": "Assessment Quiz - Simple Linear Regression Use the KidsFeet dataset from library(mosaic) to perform a regression of the length of a childâ€™s foot (Y) on the width of a childâ€™s foot (X). What is the estimated slope and intercept of the least squares regression line for this data? chillin <- lm(length ~ width, data = KidsFeet) pander(summary(chillin)) Intercept: 9.82 Slope: 1.66 Continue using the KidsFeet regression that you performed in the last problem. In other words, the regression of the length of a childâ€™s foot (Y) on the width of the childâ€™s foot (X). Plot the regression in R. plot(length ~ width, data= KidsFeet) abline(chillin) The residual plot from the regression of the length of a childâ€™s foot on the width of the childâ€™s foot (using the KidsFeet dataset from library(mosaic)) shows the following residual plots. Write a statement that most correctly interprets these plots.",
    "sentences": ["Use the KidsFeet dataset from library(mosaic) to perform a regression of the length of a childâ€™s foot (Y) on the width of a childâ€™s foot (X).", "What is the estimated slope and intercept of the least squares regression line for this data?", "chillin <- lm(length ~ width, data = KidsFeet) pander(summary(chillin))", "Intercept: 9.82 Slope: 1.66", "Continue using the KidsFeet regression that you performed in the last problem.", "In other words, the regression of the length of a childâ€™s foot (Y) on the width of the childâ€™s foot (X).", "Plot the regression in R.", "plot(length ~ width, data= KidsFeet) abline(chillin)", "The residual plot from the regression of the length of a childâ€™s foot on the width of the childâ€™s foot (using the KidsFeet dataset from library(mosaic)) shows the following residual plots.", "Write a statement that most correctly interprets these plots."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assessment Quiz - Simple Linear Regression"
  },
  {
    "id": 197,
    "title": "ğŸ“ Class Activity - Simple Linear Regression",
    "url": "Paige-sNotes.html#classactivitysimplelinearregression",
    "content": "Class Activity - Simple Linear Regression Here is the Regression Applet! https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_en.html Dots are unique and beautiful! They represent individuals and help to show us a trend The Mathematical Model of Regression (including the equation of the best-fit line). \\[ \\underbrace{Y_i}_\\text{Some Label about the Y-axis} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label about the X-axis} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] - True Regression Relation = the law (the trend) Error Term = the individual in accordance to the law Error Term Normally Distributed = the difference between the predicted values from the actual values (the errors) follow a normal distribution THE LINE IS THE AVERAGE!!!!!!! (best fit line) Y hat is the line (the prediction) ex. what the weather might be tommorrow Y is the individiual point ex. what the weather is today",
    "sentences": ["Here is the Regression Applet!", "https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_en.html Dots are unique and beautiful!", "They represent individuals and help to show us a trend", "The Mathematical Model of Regression (including the equation of the best-fit line).", "\\[ \\underbrace{Y_i}_\\text{Some Label about the Y-axis} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label about the X-axis} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] - True Regression Relation = the law (the trend)", "Error Term = the individual in accordance to the law Error Term Normally Distributed = the difference between the predicted values from the actual values (the errors) follow a normal distribution", "THE LINE IS THE AVERAGE!!!!!!!", "(best fit line) Y hat is the line (the prediction) ex.", "what the weather might be tommorrow Y is the individiual point ex.", "what the weather is today"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Simple Linear Regression"
  },
  {
    "id": 198,
    "title": "ğŸ“ Class Activity - Simple Linear Regression (Part 2)",
    "url": "Paige-sNotes.html#classactivitysimplelinearregressionpart2",
    "content": "Class Activity - Simple Linear Regression (Part 2) Review the following: Slope of a line, including the statistics symbol for it. Y-intercept of a line, including the statistics symbol for it. \\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i \\text{ where } \\epsilon_i ~ N(0,\\sigma^2)\\] \\(\\beta_0\\) : True y-intercept \\(\\beta_1\\) : True Slope \\(H_0 : \\beta_1 = 0\\) \\(H_a : \\beta_1 \\neq 0\\) \\[\\hat{Y}_i = b_0 + b_1X_i\\] \\(\\hat{Y}_i\\) : Estimated line \\(b_0\\) : Sample y-intercept \\(b_1\\) : Sample Slope \\[E\\{Y_i\\} = \\beta_0 + \\beta_1X_i\\] \\(E\\{Y_i\\}\\) : True Line The â€œerrorâ€ term.",
    "sentences": ["Review the following:", "Slope of a line, including the statistics symbol for it.", "Y-intercept of a line, including the statistics symbol for it.", "\\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i \\text{ where } \\epsilon_i ~ N(0,\\sigma^2)\\]", "\\(\\beta_0\\) : True y-intercept \\(\\beta_1\\) : True Slope \\(H_0 : \\beta_1 = 0\\) \\(H_a : \\beta_1 \\neq 0\\)", "\\[\\hat{Y}_i = b_0 + b_1X_i\\]", "\\(\\hat{Y}_i\\) : Estimated line \\(b_0\\) : Sample y-intercept \\(b_1\\) : Sample Slope", "\\[E\\{Y_i\\} = \\beta_0 + \\beta_1X_i\\]", "\\(E\\{Y_i\\}\\) : True Line", "The â€œerrorâ€ term."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Simple Linear Regression (Part 2)"
  },
  {
    "id": 199,
    "title": "ğŸ“ Skills Quiz - Mulitple Linear Regression",
    "url": "Paige-sNotes.html#skillsquizmulitplelinearregression",
    "content": "Skills Quiz - Mulitple Linear Regression As explained in the help file (?SaratogaHouses), the SaratogaHouses data set contains data about many houses from Saratoga County, New York in the year 2006. Suppose a family is in search of a home that was: newly constructed has three bedrooms trying to decide how big of a livingArea they can afford whether or not the price of the home is significantly impacted by adding a fireplace Use the SH2 data set (the filtered down version of the SaratogaHouses data set) and a â€œtwo-linesâ€ multiple regression model to describe the price of a house according to the size of the livingArea of the house and whether or not the house has a fireplace (fireplaces is only 0 or 1 for the SH2 data). SH2 <- filter(SaratogaHouses, bedrooms == 3, newConstruction ==\"Yes\") The two-lines regression model for this situation would be most appropriately labeled as: \\[\\underbrace{Y_i}_\\text{Price} = \\beta_0 + \\beta_1 \\underbrace{X_{2i}}_\\text{livingArea} + \\beta_2 \\underbrace{X_{2i}}_\\text{fireplaces} + \\beta_3X_{1i}X_{2i}+\\epsilon_i\\] \\[X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Fireplace} \\\\ 0, & \\text{No Fireplace} \\end{array}\\right.\\] \\(\\beta_0\\) = The average price of a home with no fireplace and a living area of zero square feet. Since this is unrealistic, this parameter doesnâ€™t actually carry any meaning for this particular regression model. \\(\\beta_1\\) = The change in the average price of a home without a fireplace as the living area increases by 1 additional square foot. \\(\\beta_2\\) = The difference in the average price of a home with a fireplace as compared to a home without a fireplace for homes with zero square feet of living area. \\(\\beta_3\\) = The change in the effect of 1 additional square foot in the living area on the average price of homes with a fireplace as compared to homes without a fireplace.",
    "sentences": ["As explained in the help file (?SaratogaHouses), the SaratogaHouses data set contains data about many houses from Saratoga County, New York in the year 2006.", "Suppose a family is in search of a home that was:", "newly constructed has three bedrooms trying to decide how big of a livingArea they can afford whether or not the price of the home is significantly impacted by adding a fireplace", "Use the SH2 data set (the filtered down version of the SaratogaHouses data set) and a â€œtwo-linesâ€ multiple regression model to describe the price of a house according to the size of the livingArea of the house and whether or not the house has a fireplace (fireplaces is only 0 or 1 for the SH2 data).", "SH2 <- filter(SaratogaHouses, bedrooms == 3, newConstruction ==\"Yes\")", "The two-lines regression model for this situation would be most appropriately labeled as:", "\\[\\underbrace{Y_i}_\\text{Price} = \\beta_0 + \\beta_1 \\underbrace{X_{2i}}_\\text{livingArea} + \\beta_2 \\underbrace{X_{2i}}_\\text{fireplaces} + \\beta_3X_{1i}X_{2i}+\\epsilon_i\\]", "\\[X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Fireplace} \\\\ 0, & \\text{No Fireplace} \\end{array}\\right.\\]", "\\(\\beta_0\\) = The average price of a home with no fireplace and a living area of zero square feet.", "Since this is unrealistic, this parameter doesnâ€™t actually carry any meaning for this particular regression model."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skills Quiz - Mulitple Linear Regression"
  },
  {
    "id": 200,
    "title": "ğŸ“ Assessment Quiz - Multiple Linear Regression",
    "url": "Paige-sNotes.html#assessmentquizmultiplelinearregression",
    "content": "Assessment Quiz - Multiple Linear Regression What is the estimate of \\(\\beta_1\\) in the regression model below? \\[\\underbrace{Y_i}_\\text{length of foot} = \\beta_0 + \\beta_1 \\underbrace{X_{1i}}_\\text{width fo foot} + \\beta_2 \\underbrace{X_{2i}}_\\text{0 if boy, 1 if girl} + \\beta_3X_{1i}X_{2i} + \\epsilon_i\\] KidsFeet.lm <- lm(length ~ width + sex + width:sex, data=KidsFeet) pander(summary(KidsFeet.lm)) Answer: 1.5423, the model above is based of the two lines model. Then, lookign at the width row, that represents \\(\\beta_1\\)/ slope, it will give us 1.5423. State the p-value of the hypothesis test in the regression model (both shown below). \\[\\underbrace{Y_i}_\\text{stopping distance} = \\beta_0 + \\beta_1 \\underbrace{X_{1i}}_\\text{speed of car} + \\beta_2 \\underbrace{X_{1i}^2}_\\text{(speed)^2} + \\epsilon_i \\text{ where } \\epsilon_i ~ N(0,\\sigma^2)\\] \\[H_0: \\beta_2 = 0\\] \\[H_a: \\beta_2 \\neq 0\\] lm.car <- lm(dist ~ speed + I(speed^2), data= cars) pander(summary(lm.car))",
    "sentences": ["What is the estimate of \\(\\beta_1\\) in the regression model below?", "\\[\\underbrace{Y_i}_\\text{length of foot} = \\beta_0 + \\beta_1 \\underbrace{X_{1i}}_\\text{width fo foot} + \\beta_2 \\underbrace{X_{2i}}_\\text{0 if boy, 1 if girl} + \\beta_3X_{1i}X_{2i} + \\epsilon_i\\]", "KidsFeet.lm <- lm(length ~ width + sex + width:sex, data=KidsFeet) pander(summary(KidsFeet.lm))", "Answer: 1.5423, the model above is based of the two lines model.", "Then, lookign at the width row, that represents \\(\\beta_1\\)/ slope, it will give us 1.5423.", "State the p-value of the hypothesis test in the regression model (both shown below).", "\\[\\underbrace{Y_i}_\\text{stopping distance} = \\beta_0 + \\beta_1 \\underbrace{X_{1i}}_\\text{speed of car} + \\beta_2 \\underbrace{X_{1i}^2}_\\text{(speed)^2} + \\epsilon_i \\text{ where } \\epsilon_i ~ N(0,\\sigma^2)\\]", "\\[H_0: \\beta_2 = 0\\]", "\\[H_a: \\beta_2 \\neq 0\\]", "lm.car <- lm(dist ~ speed + I(speed^2), data= cars) pander(summary(lm.car))"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assessment Quiz - Multiple Linear Regression"
  },
  {
    "id": 201,
    "title": "ğŸ“ Class Activity - Mulitple Linear Regression (Part 1)",
    "url": "Paige-sNotes.html#classactivitymulitplelinearregressionpart1",
    "content": "Class Activity - Mulitple Linear Regression (Part 1) Very wild west type of test! A space we canâ€™t even depict on a graph Two lines model -> only Brother Saunders class will know whatcha mean \\[\\overbrace{Y_i}^\\text{Dependent Variable} = \\underbrace{\\beta_0}_\\text{Intercept} + \\underbrace{\\beta_1}_\\text{Slope} \\overbrace{X_i}^\\text{Independent Variable}+\\underbrace{\\beta_2}_\\text{Change in Intercept}X_{2i} + \\underbrace{\\beta_3}_\\text{Change in Slope}X_iX_{2i}+ \\underbrace{\\epsilon_i}_\\text{Errors Quantified (variation left over)}\\] intercept + slope -> visible (estimates) \\(\\beta_2\\) and beta 3 -> not visible, depicts change beta 3 = 0 -> parallel beta 2 = 0 -> same y intercept Important Elements of a Multiple Linear Regression Analysis: Use the mtcars dataset in R to reproduce the graphic shown below, or at least a graphic that shows the same information. Then perform a multiple linear regression that would give the equations for the two lines shown on the graphic. Check your work with your peers, and check off the items below that you are able to complete. Recreate the scatter plot of the mtcars dataset mylmcarz <-lm(mpg ~ qsec + am + qsec:am, data=mtcars) palette(c(\"dodgerblue\",\"red\")) plot(mpg ~ qsec, data=mtcars, col=as.factor(am), pch = 16) c <- coef(mylmcarz) curve(c[1] + c[2]*x, add=TRUE, col= \"dodgerblue\") curve(c[1]+c[3] + (c[2]+c[4])*x, add=TRUE, col=\"red\")",
    "sentences": ["Very wild west type of test!", "A space we canâ€™t even depict on a graph Two lines model -> only Brother Saunders class will know whatcha mean", "\\[\\overbrace{Y_i}^\\text{Dependent Variable} = \\underbrace{\\beta_0}_\\text{Intercept} + \\underbrace{\\beta_1}_\\text{Slope} \\overbrace{X_i}^\\text{Independent Variable}+\\underbrace{\\beta_2}_\\text{Change in Intercept}X_{2i} + \\underbrace{\\beta_3}_\\text{Change in Slope}X_iX_{2i}+ \\underbrace{\\epsilon_i}_\\text{Errors Quantified (variation left over)}\\]", "intercept + slope -> visible (estimates) \\(\\beta_2\\) and beta 3 -> not visible, depicts change beta 3 = 0 -> parallel beta 2 = 0 -> same y intercept", "Important Elements of a Multiple Linear Regression Analysis:", "Use the mtcars dataset in R to reproduce the graphic shown below, or at least a graphic that shows the same information.", "Then perform a multiple linear regression that would give the equations for the two lines shown on the graphic.", "Check your work with your peers, and check off the items below that you are able to complete.", "Recreate the scatter plot of the mtcars dataset", "mylmcarz <-lm(mpg ~ qsec + am + qsec:am, data=mtcars) palette(c(\"dodgerblue\",\"red\")) plot(mpg ~ qsec, data=mtcars, col=as.factor(am), pch = 16) c <- coef(mylmcarz) curve(c[1] + c[2]*x, add=TRUE, col= \"dodgerblue\") curve(c[1]+c[3] + (c[2]+c[4])*x, add=TRUE, col=\"red\")"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Mulitple Linear Regression (Part 1)"
  },
  {
    "id": 202,
    "title": "ğŸ“ Class Activity - Mulitple Linear Regression (Part 2)",
    "url": "Paige-sNotes.html#classactivitymulitplelinearregressionpart2",
    "content": "Class Activity - Mulitple Linear Regression (Part 2) Consider the multiple linear regression model given by: \\[Y_i = \\beta_0X_{1i} + \\beta_2X_{2i} + \\beta_3X_{1i}X_{2i}+ \\epsilon_i \\] - \\(X_{2i}\\) = 1 : Group B \\(X_{2i}\\) = 0 : Group A The â€œtwo-linesâ€ model is a simple way to compare two groups in statistics. It uses two main parts: A regular number (\\(X_{1i}\\)) that can be any value. A special number (\\(X_{2i}\\)) thatâ€™s either 0 or 1. This special number (also called a â€œdummy variableâ€ or â€œindicator variableâ€) helps turn information about groups (like â€œGroup Aâ€ or â€œGroup Bâ€) into numbers we can use in math. By using these two parts, we can create two separate lines on a graph: One line for Group A One line for Group B",
    "sentences": ["Consider the multiple linear regression model given by:", "\\[Y_i = \\beta_0X_{1i} + \\beta_2X_{2i} + \\beta_3X_{1i}X_{2i}+ \\epsilon_i \\] - \\(X_{2i}\\) = 1 : Group B", "\\(X_{2i}\\) = 0 : Group A", "The â€œtwo-linesâ€ model is a simple way to compare two groups in statistics.", "It uses two main parts:", "A regular number (\\(X_{1i}\\)) that can be any value.", "A special number (\\(X_{2i}\\)) thatâ€™s either 0 or 1.", "This special number (also called a â€œdummy variableâ€ or â€œindicator variableâ€) helps turn information about groups (like â€œGroup Aâ€ or â€œGroup Bâ€) into numbers we can use in math.", "By using these two parts, we can create two separate lines on a graph:", "One line for Group A One line for Group B"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Mulitple Linear Regression (Part 2)"
  },
  {
    "id": 203,
    "title": "ğŸ“ Skill Quiz - Logisitic Regression",
    "url": "Paige-sNotes.html#skillquizlogisiticregression",
    "content": "Skill Quiz - Logisitic Regression What does \\(P(Y_i = 1|X_i)\\) represent in logistic regression? The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s). The symbol used to represent the notation in logistic regression is \\(\\pi_i\\) The mathematical model for simple logistic regression is given by: \\[P(Y_i = 1) = \\frac{e^{stuff}}{1+e^{stuff}}= \\pi_i\\] The content for â€œstuffâ€ being a simple linear regression of the form \\(\\beta_0 + \\beta_1x_i\\) With some algebra, the simple logistic regression model can be reorganized as: \\[\\frac{\\pi_i}{1-\\pi_i}= e^{\\beta_0}e^{\\beta_1x_i}\\] We can assume that: \\(\\frac{\\pi_i}{1-\\pi_i}\\) is the odds of success, i.e.Â the odds that \\(Y_i = 1\\) The odds of success equal \\(e^{\\beta_0}\\) when \\(x_i = 0\\) The odds of success increase by the factor \\(e^{\\beta_1}\\) for every one unit increase in \\(x_i\\)",
    "sentences": ["What does \\(P(Y_i = 1|X_i)\\) represent in logistic regression?", "The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s).", "The symbol used to represent the notation in logistic regression is \\(\\pi_i\\)", "The mathematical model for simple logistic regression is given by:", "\\[P(Y_i = 1) = \\frac{e^{stuff}}{1+e^{stuff}}= \\pi_i\\]", "The content for â€œstuffâ€ being a simple linear regression of the form \\(\\beta_0 + \\beta_1x_i\\)", "With some algebra, the simple logistic regression model can be reorganized as:", "\\[\\frac{\\pi_i}{1-\\pi_i}= e^{\\beta_0}e^{\\beta_1x_i}\\]", "We can assume that:", "\\(\\frac{\\pi_i}{1-\\pi_i}\\) is the odds of success, i.e.Â the odds that \\(Y_i = 1\\) The odds of success equal \\(e^{\\beta_0}\\) when \\(x_i = 0\\) The odds of success increase by the factor \\(e^{\\beta_1}\\) for every one unit increase in \\(x_i\\)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skill Quiz - Logisitic Regression"
  },
  {
    "id": 204,
    "title": "ğŸ“ Assessment Quiz - Logistic Regression",
    "url": "Paige-sNotes.html#assessmentquizlogisticregression",
    "content": "Assessment Quiz - Logistic Regression Use an appropriate test to determine if the birth weight in ounces of a baby (wt) can be used to predict the probability that the mother smoked at all (smoke>0, or, if your Gestation data set has words in the â€œsmokeâ€ column, use smoke!=â€œneverâ€) during or prior to the pregnancy. The graphic of the correct analysis is shown below. What are the values of b0 and b1 for the curve shown in the graphic below? Gestation.glm <- glm((smoke == \"never\")~wt, data=Gestation, family=binomial) pander(summary(Gestation.glm)) (Dispersion parameter for binomial family taken to be 1 ) Here is the output of a logistic regression performed in R. What is the predicted value for \\(P(Y_i = 1 | x_i = 25)\\)? There are TWO different ways you can get this answer: it would look like this: exp(-6.6035 + 0.307025) / (1 + exp(-6.6035 + 0.307025)) = 0.7448821",
    "sentences": ["Use an appropriate test to determine if the birth weight in ounces of a baby (wt) can be used to predict the probability that the mother smoked at all (smoke>0, or, if your Gestation data set has words in the â€œsmokeâ€ column, use smoke!=â€œneverâ€) during or prior to the pregnancy.", "The graphic of the correct analysis is shown below.", "What are the values of b0 and b1 for the curve shown in the graphic below?", "Gestation.glm <- glm((smoke == \"never\")~wt, data=Gestation, family=binomial) pander(summary(Gestation.glm))", "(Dispersion parameter for binomial family taken to be 1 )", "Here is the output of a logistic regression performed in R.", "What is the predicted value for \\(P(Y_i = 1 | x_i = 25)\\)?", "There are TWO different ways you can get this answer:", "it would look like this:", "exp(-6.6035 + 0.307025) / (1 + exp(-6.6035 + 0.307025)) = 0.7448821"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assessment Quiz - Logistic Regression"
  },
  {
    "id": 205,
    "title": "ğŸ“ Class Activity -Simple Logistic Regression (Part 1)",
    "url": "Paige-sNotes.html#classactivitysimplelogisticregressionpart1",
    "content": "Class Activity -Simple Logistic Regression (Part 1) Main question template: Whats the probability that this thing happens, given **the things that are happening*?** Whatâ€™s the probability that the length of the foot we have belongs to a girl or a boy? WILL THIS HAPPEN OR NOT BASED ON THIS KNOWN INFORMATION?? a yes or no question! Explain each symbol of the simple logistic regression model, and what it represents \\[P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i\\] \\(\\pi_i\\) = P (pi is a letter ) e = a number! \\(e^{\\beta_0 +\\beta_1X_i}\\) = serves as a bounds to go from 0 (impossibility) to 1 (certainty) e to stuff / 1 + e to stuff Explain how the simple logistic regression model is being used in the Challenger analysis. (What is the explanatory variable of the analysis, and what is it allowing us to predict?)",
    "sentences": ["Main question template:", "Whats the probability that this thing happens, given **the things that are happening*?**", "Whatâ€™s the probability that the length of the foot we have belongs to a girl or a boy?", "WILL THIS HAPPEN OR NOT BASED ON THIS KNOWN INFORMATION??", "a yes or no question!", "Explain each symbol of the simple logistic regression model, and what it represents", "\\[P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i\\]", "\\(\\pi_i\\) = P (pi is a letter ) e = a number!", "\\(e^{\\beta_0 +\\beta_1X_i}\\) = serves as a bounds to go from 0 (impossibility) to 1 (certainty) e to stuff / 1 + e to stuff", "Explain how the simple logistic regression model is being used in the Challenger analysis."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity -Simple Logistic Regression (Part 1)"
  },
  {
    "id": 206,
    "title": "ğŸ“ Class Activity - Simple Logistic Regression (Part 2)",
    "url": "Paige-sNotes.html#classactivitysimplelogisticregressionpart2",
    "content": "Class Activity - Simple Logistic Regression (Part 2) \\[$log(odds) = \\beta_0 + \\beta_iX_i\\] the log of the odds are linear Probability = \\(\\frac{Successes}{Total}\\) doesnâ€™t change in a consistent way What we graph! depicts to a â€œmoment/time of decisionâ€ Odds = $ {Failure}$ â€œthe times as likely to succeedâ€ What we interpret! nothing to infinite! (never really graph the odds) multiplication likeliness, slope effects the odds! \\[\\frac{\\pi_i}{1 - \\pi_i} = e^{\\beta_0 + \\beta_1X_i}= \\underbrace{e^{\\beta_0}}_\\text{baseline odds}\\underbrace{(e^{\\beta_1})^{X_i}}}_\\text{multiplicative change to the odds}\\] When graphing, the BIG CHANGE IN THE CURVE LINE shows that you found something crazy and pretty cool! Recreate the graphic shown below in R using the KidsFeet dataset. Then answer the following questions!",
    "sentences": ["\\[$log(odds) = \\beta_0 + \\beta_iX_i\\]", "the log of the odds are linear", "Probability = \\(\\frac{Successes}{Total}\\) doesnâ€™t change in a consistent way What we graph!", "depicts to a â€œmoment/time of decisionâ€ Odds = $ {Failure}$ â€œthe times as likely to succeedâ€ What we interpret!", "nothing to infinite!", "(never really graph the odds) multiplication likeliness, slope effects the odds!", "\\[\\frac{\\pi_i}{1 - \\pi_i} = e^{\\beta_0 + \\beta_1X_i}= \\underbrace{e^{\\beta_0}}_\\text{baseline odds}\\underbrace{(e^{\\beta_1})^{X_i}}}_\\text{multiplicative change to the odds}\\]", "When graphing, the BIG CHANGE IN THE CURVE LINE shows that you found something crazy and pretty cool!", "Recreate the graphic shown below in R using the KidsFeet dataset.", "Then answer the following questions!"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Simple Logistic Regression (Part 2)"
  },
  {
    "id": 207,
    "title": "ğŸ“ Week 11 | Consulting Opportunity or Research Project",
    "url": "Paige-sNotes.html#week11consultingopportunityorresearchproject",
    "content": "Week 11 | Consulting Opportunity or Research Project Skill Quiz - Practice Final Exam Use an appropriate test and the starwars dataset in R to determine if, on average, the species of Wookiees, Gungans, or Kaminoans are taller. Class Activity - Presenting your Findings What are the five rubric categories of an analysis in this class? Hypothesis & Questions Analysis Graphical Summary Interpretation Presentation Why is each of these elements important in the analysis? How does a p-value help inform your conclusions for a particular analysis? Significant: Non significant: We can gamble on this, but thereâ€™s a sense that something is hiding here. There is a suspicious that that there is more here, but to find whats missing we can look deeper into this! What do I do when my data doesnâ€™t pass the normality tests??? Transform it! There are several ways to transforming your y value by doing: log(yvalue) (y-value)^2 The way to check transformation corrected your distribution would be to do the code: hist(datasetname$yvalue) From there, you have to work with the transformed data in the testing space and then write your interpretation based off the non-transformed data. Suck it up! If you transformed it and it doesnâ€™t create a normal distribution, no matter how much you change and check it, you just have to explain the following We have to interpret the results with CAUTION We CANNOT promise that our results are definitive",
    "sentences": ["Skill Quiz - Practice Final Exam Use an appropriate test and the starwars dataset in R to determine if, on average, the species of Wookiees, Gungans, or Kaminoans are taller.", "Class Activity - Presenting your Findings What are the five rubric categories of an analysis in this class?", "Hypothesis & Questions Analysis Graphical Summary Interpretation Presentation Why is each of these elements important in the analysis?", "How does a p-value help inform your conclusions for a particular analysis?", "Significant: Non significant: We can gamble on this, but thereâ€™s a sense that something is hiding here.", "There is a suspicious that that there is more here, but to find whats missing we can look deeper into this!", "What do I do when my data doesnâ€™t pass the normality tests???", "Transform it!", "There are several ways to transforming your y value by doing: log(yvalue) (y-value)^2 The way to check transformation corrected your distribution would be to do the code: hist(datasetname$yvalue) From there, you have to work with the transformed data in the testing space and then write your interpretation based off the non-transformed data.", "Suck it up!"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Week 11 | Consulting Opportunity or Research Project"
  },
  {
    "id": 208,
    "title": "ğŸ“ Skill Quiz - Practice Final Exam",
    "url": "Paige-sNotes.html#skillquizpracticefinalexam",
    "content": "Skill Quiz - Practice Final Exam Use an appropriate test and the starwars dataset in R to determine if, on average, the species of Wookiees, Gungans, or Kaminoans are taller.",
    "sentences": "Use an appropriate test and the starwars dataset in R to determine if, on average, the species of Wookiees, Gungans, or Kaminoans are taller.",
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skill Quiz - Practice Final Exam"
  },
  {
    "id": 209,
    "title": "ğŸ“ Class Activity - Presenting your Findings",
    "url": "Paige-sNotes.html#classactivitypresentingyourfindings",
    "content": "Class Activity - Presenting your Findings What are the five rubric categories of an analysis in this class? Hypothesis & Questions Analysis Graphical Summary Interpretation Presentation Why is each of these elements important in the analysis? How does a p-value help inform your conclusions for a particular analysis? Significant: Non significant: We can gamble on this, but thereâ€™s a sense that something is hiding here. There is a suspicious that that there is more here, but to find whats missing we can look deeper into this! What do I do when my data doesnâ€™t pass the normality tests??? There are several ways to transforming your y value by doing: log(yvalue) (y-value)^2 The way to check transformation corrected your distribution would be to do the code: hist(datasetname$yvalue) From there, you have to work with the transformed data in the testing space and then write your interpretation based off the non-transformed data. If you transformed it and it doesnâ€™t create a normal distribution, no matter how much you change and check it, you just have to explain the following We have to interpret the results with CAUTION We CANNOT promise that our results are definitive",
    "sentences": ["What are the five rubric categories of an analysis in this class?", "Hypothesis & Questions Analysis Graphical Summary Interpretation Presentation", "Why is each of these elements important in the analysis?", "How does a p-value help inform your conclusions for a particular analysis?", "Significant: Non significant: We can gamble on this, but thereâ€™s a sense that something is hiding here.", "There is a suspicious that that there is more here, but to find whats missing we can look deeper into this!", "What do I do when my data doesnâ€™t pass the normality tests???", "There are several ways to transforming your y value by doing: log(yvalue) (y-value)^2 The way to check transformation corrected your distribution would be to do the code: hist(datasetname$yvalue) From there, you have to work with the transformed data in the testing space and then write your interpretation based off the non-transformed data.", "If you transformed it and it doesnâ€™t create a normal distribution, no matter how much you change and check it, you just have to explain the following We have to interpret the results with CAUTION We CANNOT promise that our results are definitive"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Presenting your Findings"
  },
  {
    "id": 210,
    "title": "ğŸ“ Week 12 | Chi Squared Test",
    "url": "Paige-sNotes.html#week12chisquaredtest",
    "content": "Week 12 | Chi Squared Test Skills Quiz - Chi Squared Test Using the Titanic data set, answer the following: Refers to the (Explanation file of the Chi Squared Tests)[file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/ChiSquaredTests.html] in the Statistics Notebook! What do the rows and columns factors called in the following table? Rows : Class Columns : Survival How would you state the null and alternative hypotheses for a chi squared test of independence? \\[H_0 : \\text{The row variable and column variable are independent.}\\] \\[H_a : \\text{The row variable and column variable are associated.}\\] The two things need to obtain a p-value for the chi-squared test of independence are: The Test Statistic \\[\\chi^2 = \\sum_{i=1}^m \\frac{(O_i - E_i)^2}{E_i}\\] The distribution of the test statistic that is caluclated under the assumption that the null hypothesis is true The \\(E_i\\) in the \\(X^2\\) test statistic formula are called the expected counts. They are obtained by: \\[E_i = \\frac{\\text{(row total)*(column total)}}{\\text{(total total)}}\\] These values show us what values we would expect to observe if the null hypothesis was true. In other words, they provide the counts we would expect if the row variable column variable were independent. The \\(X^2\\) test statistic can be assumed to follow a: chi-squared distribution with degrees of freedom : \\(p = (r-1)*(c-1)\\) The chi-squared distribution is a parametric distribution because it has a single parameter known as the degrees of freedom, p. Person Residuals are used for interpreting the results of a chi-squared test when the alternative hypothesis can be concluded to be the truth! They show a relative measurement of how much the observed counts differ from the expected counts. Use the HairEyeColor data set to answer the following questions: glasses <- cbind( Males = c(Glasses = 5, Contacts = 12, None = 18), Females = c(Glasses = 4, Contacts = 14, None = 22)) pander(glasses) Â  Males Females Glasses 5 4 Contacts 12 14 None 18 22 What can we interpret from these bar plots from the data? The gender of an individual does not seem to be associated with whether a person wears glasses, contacts, or no eye correction because the pattern of the bars is essentially the same for male and females. The data shows that the most common results for either males or females is to not wear glasses or contacts. It also shows that the least common results for both genders is to wear glasses. In other words, the pattern for both genders is the same. barplot(glasses, beside=TRUE, legend.text=TRUE, args.legend=list(x = \"topright\", cex=0.6, bty=\"n\"), xlim= c(0, 10)) What are the null and alternative hypotheses for this data? \\[H_0 : \\text{Corrective eye wearing and gender are independent.}\\] \\[H_a : \\text{Corrective eye wearing and gender are associated.}\\] Determine if a chi-squared test of independence is appropriate for the glasses data. Yes, the requirements are met because the average of the expected counts is greater than 5 and all expected counts are greater than 1, even though some expected counts are less than 5. chis.glasses <- chisq.test(glasses) ## Warning in chisq.test(glasses): Chi-squared approximation may be incorrect pander(chis.glasses$expected) Â  Males Females Glasses 4.2 4.8 Contacts 12.13 13.87 None 18.67 21.33 View the results of our chi-squared test of the data. pander(chis.glasses) Pearsonâ€™s Chi-squared test: glasses Test statistic df P value 0.3331 2 0.8466 What are the interpretations we can get from these results? This confirms our original suspicion that we saw in the graphic! There is insufficient evidence to conclude the corrective eye wearing gender are associated. We will continue to assume the null hypothesis that whether someone wears glasses, contacts, or no corrective eye wear is independent of their gender. What about the Pearson Residuals? Since we failed to reject the null there is no interpretation to make for these data so we are not interested in the Pearson Residuals. However, so that you get the opportunity to see what the Pearson Residuals look like when we fail to reject, run the following code in R and notice that none of the residuals stand out as being exceptionally large in magnitude. pander(chis.glasses$residuals) Â  Males Females Glasses 0.3904 -0.3651 Contacts -0.03828 0.03581 None -0.1543 0.1443 This is always the case when we fail to reject the null hypothesis in a chi-squared test of independence! Asia has become a major competitor with the U.S and Western Europe in education. The following table presents the counts of university degrees awarded to students in engineering and science (natural and social sciences) for the three regions. The data (observed values) are depicted below: education <- cbind( `United States` = c(Engineering = 61941, `Natural Science` = 111158, `Social Science` = 182166), `Western Europe` = c(Engineering = 158931, `Natural Science` = 140126, `Social Science` = 116353), Asia = c(280772, 242879, 236018)) pander(education) Â  United States Western Europe Asia Engineering 61941 158931 280772 Natural Science 111158 140126 242879 Social Science 182166 116353 236018 Questions to ask: Are there any differences in the numbers of degrees awarded to each field for the different regions? In other words, are field and region associated? Or can we assume that all three countries are similar in their patterns of degrees being awarded? These questions can be stated in a hypothesis: \\[H_0 : \\text{Field and Region are independent/ not associated.}\\] \\[H_a : \\text{Field and Region are not independent/ associated}\\] Conducting the Chi-squared test chi.ed <- chisq.test(education) chi.ed ## ## Pearson's Chi-squared test ## ## data: education ## X-squared = 69890, df = 4, p-value < 2.2e-16 This shows sufficient evidence to conclude that field and region are associated! Checking requirements: edexpect <- chi.ed$expected pander(edexpect) Â  United States Western Europe Asia Engineering 116455 136171 249018 Natural Science 114719 134140 245305 Social Science 124091 145099 265346 Create an appropriate graphic in R for this analysis: use ylim= and xlim to change the stretching use cex= in args.legend= function to shrink the size of the legend barplot(education, beside=TRUE, legend.text=TRUE, args.legend= list(x=\"topleft\", bty=\"n\",cex=0.59), main=\"College Degrees Awarded by Region\", ylim=c(0, 300000)) Obtain the residuals edresid <- chi.ed$residuals pander(edresid) Â  United States Western Europe Asia Engineering -159.7 61.68 63.63 Natural Science -10.51 16.34 -4.897 Social Science 164.9 -75.47 -56.94 Questions that are answered by residuals: Which region differs the most from expected when is comes to the number of Engineering degrees awarded? United States Which region differs the most from expected when it comes to the number of Social Science degrees awarded? United States Review time!! Consider the InsectSprays dataset in R. Test the hypothesis that the mean number of bugs killed by insecticides A, B, C, D, E, and F are all the same against the alternative that at least one mean is different. datatable(InsectSprays, options(list=c(3,10,30))) What hypothesis test should we use? bug.ova <- aov(count ~ spray, data = InsectSprays) summary(bug.ova) ## Df Sum Sq Mean Sq F value Pr(>F) ## spray 5 2669 533.8 34.7 <2e-16 *** ## Residuals 66 1015 15.4 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 The p-value of the test is very small because the test statistic is 34.7 showing that there is sufficient evidence to conclude that at least one bug spray results in a different average number of bugs per agricultural experimental unit than the other sprays. What is the appropriateness of this test? par(mfrow=c(1,2)) plot(bug.ova, which=1:2, pch=16) - The appropriateness of the test is questionable because the residual plot shows that the constant (equal) variance is questionable (the first graph) and the Q-Q Plot suggests the errors are likely not normally distributed. Thus, we should not make any conclusion about these insect sprays until next week when we can use the non-parametric version of ANOVA called the â€œKruskal-Wallis Test.â€ In other words, this test is inconclusive due to difficulties with the requirements not being satisfied.",
    "sentences": ["Skills Quiz - Chi Squared Test Using the Titanic data set, answer the following: Refers to the (Explanation file of the Chi Squared Tests)[file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/ChiSquaredTests.html] in the Statistics Notebook!", "What do the rows and columns factors called in the following table?", "Rows : Class Columns : Survival How would you state the null and alternative hypotheses for a chi squared test of independence?", "\\[H_0 : \\text{The row variable and column variable are independent.}\\] \\[H_a : \\text{The row variable and column variable are associated.}\\] The two things need to obtain a p-value for the chi-squared test of independence are: The Test Statistic \\[\\chi^2 = \\sum_{i=1}^m \\frac{(O_i - E_i)^2}{E_i}\\] The distribution of the test statistic that is caluclated under the assumption that the null hypothesis is true The \\(E_i\\) in the \\(X^2\\) test statistic formula are called the expected counts.", "They are obtained by: \\[E_i = \\frac{\\text{(row total)*(column total)}}{\\text{(total total)}}\\] These values show us what values we would expect to observe if the null hypothesis was true.", "In other words, they provide the counts we would expect if the row variable column variable were independent.", "The \\(X^2\\) test statistic can be assumed to follow a: chi-squared distribution with degrees of freedom : \\(p = (r-1)*(c-1)\\) The chi-squared distribution is a parametric distribution because it has a single parameter known as the degrees of freedom, p.", "Person Residuals are used for interpreting the results of a chi-squared test when the alternative hypothesis can be concluded to be the truth!", "They show a relative measurement of how much the observed counts differ from the expected counts.", "Use the HairEyeColor data set to answer the following questions: glasses <- cbind( Males = c(Glasses = 5, Contacts = 12, None = 18), Females = c(Glasses = 4, Contacts = 14, None = 22)) pander(glasses) Â  Males Females Glasses 5 4 Contacts 12 14 None 18 22 What can we interpret from these bar plots from the data?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Week 12 | Chi Squared Test"
  },
  {
    "id": 211,
    "title": "ğŸ“ Skills Quiz - Chi Squared Test",
    "url": "Paige-sNotes.html#skillsquizchisquaredtest",
    "content": "Skills Quiz - Chi Squared Test Using the Titanic data set, answer the following: Refers to the (Explanation file of the Chi Squared Tests)[file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/ChiSquaredTests.html] in the Statistics Notebook! What do the rows and columns factors called in the following table? Rows : Class Columns : Survival How would you state the null and alternative hypotheses for a chi squared test of independence? \\[H_0 : \\text{The row variable and column variable are independent.}\\] \\[H_a : \\text{The row variable and column variable are associated.}\\] The two things need to obtain a p-value for the chi-squared test of independence are: The Test Statistic \\[\\chi^2 = \\sum_{i=1}^m \\frac{(O_i - E_i)^2}{E_i}\\] The distribution of the test statistic that is caluclated under the assumption that the null hypothesis is true",
    "sentences": ["Using the Titanic data set, answer the following:", "Refers to the (Explanation file of the Chi Squared Tests)[file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/ChiSquaredTests.html] in the Statistics Notebook!", "What do the rows and columns factors called in the following table?", "Rows : Class Columns : Survival", "How would you state the null and alternative hypotheses for a chi squared test of independence?", "\\[H_0 : \\text{The row variable and column variable are independent.}\\]", "\\[H_a : \\text{The row variable and column variable are associated.}\\]", "The two things need to obtain a p-value for the chi-squared test of independence are: The Test Statistic", "\\[\\chi^2 = \\sum_{i=1}^m \\frac{(O_i - E_i)^2}{E_i}\\]", "The distribution of the test statistic that is caluclated under the assumption that the null hypothesis is true"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skills Quiz - Chi Squared Test"
  },
  {
    "id": 212,
    "title": "ğŸ“ Assessement Quiz - Chi Squared",
    "url": "Paige-sNotes.html#assessementquizchisquared",
    "content": "Assessement Quiz - Chi Squared Perform an appropriate test in R to determine if the dominant hand of children is related to the season of the year they are born in. Select the answer showing the P-value of your test. (Note that this test is not quite appropriate for these data. Due to the small sample sizes, the average expected count is only 4.875, but all expected counts are above 1.) First part: Turn the data into a table! Kids2 <- KidsFeet %>% mutate( season = case_when( birthmonth %in% c(12,1,2) ~ \"Winter\", birthmonth %in% c(3,4,5) ~ \"Spring\", birthmonth %in% c(6,7,8) ~ \"Summer\", birthmonth %in% c(9,10,11) ~ \"Fall\" ) ) kids.tab <- table(Kids2$domhand, Kids2$season) pander(kids.tab) Part 2: Use the chi squared test! chisq.test(kids.tab) Answer: P-value = 0.6943 When performing a chi-squared test, there are always two ways to display the barplot of the table of observed counts. Which of the plots listed below depicts the same set of observed counts that are depicted in this plot?",
    "sentences": ["Perform an appropriate test in R to determine if the dominant hand of children is related to the season of the year they are born in.", "Select the answer showing the P-value of your test.", "(Note that this test is not quite appropriate for these data.", "Due to the small sample sizes, the average expected count is only 4.875, but all expected counts are above 1.)", "First part: Turn the data into a table!", "Kids2 <- KidsFeet %>% mutate( season = case_when( birthmonth %in% c(12,1,2) ~ \"Winter\", birthmonth %in% c(3,4,5) ~ \"Spring\", birthmonth %in% c(6,7,8) ~ \"Summer\", birthmonth %in% c(9,10,11) ~ \"Fall\" ) ) kids.tab <- table(Kids2$domhand, Kids2$season) pander(kids.tab)", "Part 2: Use the chi squared test!", "chisq.test(kids.tab)", "Answer: P-value = 0.6943", "When performing a chi-squared test, there are always two ways to display the barplot of the table of observed counts."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assessement Quiz - Chi Squared"
  },
  {
    "id": 213,
    "title": "ğŸ“ Class Activity - Chi Squared Test",
    "url": "Paige-sNotes.html#classactivitychisquaredtest",
    "content": "Class Activity - Chi Squared Test Study the example codes provided for the combine function c( â€¦ ) Answer the following: Explain to your neighbor what the following code does using the â€œrow-bindâ€ function: rbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) ) rbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) ) creates whats called a â€œvectorâ€ or a list/ set of values (has to be the same length) names the numbers as Good, Bruised, or Rotten, while assigning those assignments as vectors Apples and Oranges How do the results change if you use the â€œcolumn-bindâ€ function instead? cbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) ) cbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) ) Presents it in a different way! you can also put t([data set name]) to transpose the data set",
    "sentences": ["Study the example codes provided for the combine function c( â€¦ )", "Answer the following:", "Explain to your neighbor what the following code does using the â€œrow-bindâ€ function:", "rbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) )", "rbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) )", "creates whats called a â€œvectorâ€ or a list/ set of values (has to be the same length) names the numbers as Good, Bruised, or Rotten, while assigning those assignments as vectors Apples and Oranges", "How do the results change if you use the â€œcolumn-bindâ€ function instead?", "cbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) )", "cbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) )", "Presents it in a different way!"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Chi Squared Test"
  },
  {
    "id": 214,
    "title": "ğŸ“ Class Activity - Chi Squared Survey Results",
    "url": "Paige-sNotes.html#classactivitychisquaredsurveyresults",
    "content": "Class Activity - Chi Squared Survey Results Step 1: Fetching the data To use SAS data, use this code and change the pathing: gss2021 <- read_sas(â€œData/gss2021.sas7bdatâ€, NULL) Pick the columns that you are interested in! Use GSS column title meaning Columns I thought were interesting: SPANKING : Favor of spanking to discipline child 1 - 4 1 : Strongly Agree 4: Strongly Disagree POLMURDR : Citizen questioned as a murder suspect 1 (Yes) or 2 (No) Step 2: Create a table Run the following codes: gss2021 <- read_sas(\"Data/gss2021.sas7bdat\", NULL) table(gss2021$SPANKING) table(gss2021$POLMURDR) table(gss2021$SPANKING,gss2021$POLMURDR)",
    "sentences": ["Step 1: Fetching the data", "To use SAS data, use this code and change the pathing:", "gss2021 <- read_sas(â€œData/gss2021.sas7bdatâ€, NULL)", "Pick the columns that you are interested in!", "Use GSS column title meaning Columns I thought were interesting: SPANKING : Favor of spanking to discipline child 1 - 4 1 : Strongly Agree 4: Strongly Disagree POLMURDR : Citizen questioned as a murder suspect 1 (Yes) or 2 (No)", "Step 2: Create a table", "Run the following codes:", "gss2021 <- read_sas(\"Data/gss2021.sas7bdat\", NULL) table(gss2021$SPANKING)", "table(gss2021$POLMURDR)", "table(gss2021$SPANKING,gss2021$POLMURDR)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Chi Squared Survey Results"
  },
  {
    "id": 215,
    "title": "ğŸ“ Week 13 | Randomizing Testing",
    "url": "Paige-sNotes.html#week13randomizingtesting",
    "content": "Week 13 | Randomizing Testing Skills Quiz - Randomization Testing The idea behind permutation testing is that the null hypothesis can be reworded to state that â€œany pattern that has been witnessed in the sampled data is simple due to random chance .â€ Permutation tests can be applied to any hypothesis testing scenario in order to compute the p-value of the test in away that does not require any assumption s of the data! The most difficult part of any permutation test : figuring out how to permute the data! - which is performed differently for each hypothesis test Thus, being able to identify the hypothesis test from the stated hypotheses is an important skill you have hopefully started to develop. To see how you are doing with this skill, match the following null hypotheses with their appropriate test. Chi-Squared Test \\(H_0 : \\text{the two variables are not associated}\\) Wilcoxon Signed-Rank Test \\(H_0 : \\text{median of differences = 0}\\) Paired Samples t Test \\(H_0 : \\mu_d = 0\\) Wilcoxon Rank Sum Test \\(H_0 : difference in medians = 0\\) Independent Samples t Test \\(H_0 : \\mu_1 = \\mu_2\\) ANOVA \\(H_0 : \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4\\) set.seed(1140411) #allows us each to get the same set of random values sample1 <- rnorm(30, 69, 2.5) #get a random sample of n=30 normally distributed values with mu=69 and sigma=2.5 sample2 <- rnorm(30, 69, 2.5) #get another random sample of n=30 normally distributed values with mu=69 and sigma=2.5 theData <- data.frame(values = c(sample1,sample2), group = rep(c(1,2), each=30)) #load the random samples into a data set datatable(theData) boxplot(values ~ group, data = theData) Both sample 1 and sample 2 in the above code are samples of size n = 30 from a normal distribution with mean 69 and standard deviation 2.5. Thus they are each sample from the same distribution! The hypothesis would be : \\(H_0 : \\mu_1 = \\mu_2\\) for both of these data Suppose we just had theData dataset without knowledge of the population this data came from. The permutation test of the stated null hypothesis would then be coded in R as: myTest <- t.test(values ~ group, data = theData, mu =0) observedTestStat <- myTest$statistic observedTestStat N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N ) { permutedTest <- t.test(sample(values) ~ group, data = theData, mu = 0 permutedTestStats[i] <- permutedTest$statisitc } hist(permutedTestStats) abline(v=observedTestStat) sum(permutedTestStats >= observedTestStat)/N sum(permutedTestStats <= observedTestStat)/N myTest4 <- t.test(values ~ group, data = theData, mu =0) observedTestStat4 <- myTest4$statistic print(observedTestStat4) ## t ## -0.7304735 N <- 2000 permutedTestStats4 <- rep(NA, N) for(i in 1:N ){ permutedTest4 <- t.test(sample(values) ~ group, data = theData, mu = 0) permutedTestStats4[i] <- permutedTest4$statistic } hist(permutedTestStats4) abline(v=observedTestStat4) greatvalue <- sum(permutedTestStats4 >= observedTestStat4)/N print(greatvalue) ## [1] 0.765 lessvalue <- sum(permutedTestStats4 <= observedTestStat4)/N print(lessvalue) ## [1] 0.235 twovalue <- 2*sum(permutedTestStats4 <= observedTestStat4)/N print(twovalue) ## [1] 0.47 Assessment Quiz - Randomization Testing Class Activity - Permutation Testing (Part 1) How does the picture of Legos in your Permutation Tesing Overview page demonstrate the idea of permutation testing? This image represents: - clear structure/pattern - blocks were certainly organized and not sorted by random chance - suggests real pattern that is not random - if there is structure in the data, then â€œmixing up the data and dumping it out againâ€ will show very different patterns from the original This image represents: - a pile of toy blocks in a random pattern - as if the toy blocks were put in a bag, shaken up, and dumped out - This is the idea of the permutation test - if the data was just random to begin with, then we should see a very similar pattern by â€œmixing up the data and dumping it out again.â€ How is the distribution of the test statistic created using permutation testing? The distribution is created by: Randomly shuffling (permuting) the data labels many times Calculating the test statistic for each permutation Building a distribution from all these permuted test statistics How does this distribution differ from a parametric distribution? Unlike parametric distributions (like normal or t-distributions) which follow theoretical mathematical formulas, permutation distributions: Are created directly from the actual data Donâ€™t assume any particular shape or underlying distribution Better reflect the true sampling distribution for the specific dataset How is the p-value calculated from a permutation test? The p-value is calculated by: Finding how many permuted test statistics are as extreme or more extreme than the observed test statistic Dividing this count by the total number of permutations What is a for loop? What does it allow you to do? A for loop is a programming construct that: Allows you to repeat a set of instructions multiple times In permutation testing, itâ€™s used to automate the process of repeatedly shuffling data and calculating test statistics Here are some R codes to implement in the premutation testing concepts described: Function to perform permutation test permutation_test <- function(data, labels, n_permutations = 1000) { observed_stat <- calculate_test_statistic(data, labels) Store permuted statistics perm_stats <- numeric(n_permutations) For loop to perform permutations for(i in 1:n_permutations) { ***Randomly shuffle labels*** shuffled_labels <- sample(labels) ***Calculate and store test statistic*** perm_stats[i] <- calculate_test_statistic(data, shuffled_labels) } Calculate p-value p_value <- mean(perm_stats >= observed_stat) Create distribution plot hist(perm_stats, main = â€œPermutation Distributionâ€, xlab = â€œTest Statisticâ€) abline(v = observed_stat, col = â€œredâ€) return(p_value) } This code implements the key concepts we see in the selection: Uses a for loop to repeat the permutation process Randomly shuffles the data labels Calculates test statistics for each permutation Creates a distribution from the permuted statistics Calculates the p-value by comparing observed vs permuted statistics",
    "sentences": ["Skills Quiz - Randomization Testing The idea behind permutation testing is that the null hypothesis can be reworded to state that â€œany pattern that has been witnessed in the sampled data is simple due to random chance .â€ Permutation tests can be applied to any hypothesis testing scenario in order to compute the p-value of the test in away that does not require any assumption s of the data!", "The most difficult part of any permutation test : figuring out how to permute the data!", "- which is performed differently for each hypothesis test Thus, being able to identify the hypothesis test from the stated hypotheses is an important skill you have hopefully started to develop.", "To see how you are doing with this skill, match the following null hypotheses with their appropriate test.", "Chi-Squared Test \\(H_0 : \\text{the two variables are not associated}\\) Wilcoxon Signed-Rank Test \\(H_0 : \\text{median of differences = 0}\\) Paired Samples t Test \\(H_0 : \\mu_d = 0\\) Wilcoxon Rank Sum Test \\(H_0 : difference in medians = 0\\) Independent Samples t Test \\(H_0 : \\mu_1 = \\mu_2\\) ANOVA \\(H_0 : \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4\\) set.seed(1140411) #allows us each to get the same set of random values sample1 <- rnorm(30, 69, 2.5) #get a random sample of n=30 normally distributed values with mu=69 and sigma=2.5 sample2 <- rnorm(30, 69, 2.5) #get another random sample of n=30 normally distributed values with mu=69 and sigma=2.5 theData <- data.frame(values = c(sample1,sample2), group = rep(c(1,2), each=30)) #load the random samples into a data set datatable(theData) boxplot(values ~ group, data = theData) Both sample 1 and sample 2 in the above code are samples of size n = 30 from a normal distribution with mean 69 and standard deviation 2.5.", "Thus they are each sample from the same distribution!", "The hypothesis would be : \\(H_0 : \\mu_1 = \\mu_2\\) for both of these data Suppose we just had theData dataset without knowledge of the population this data came from.", "The permutation test of the stated null hypothesis would then be coded in R as: myTest <- t.test(values ~ group, data = theData, mu =0) observedTestStat <- myTest$statistic observedTestStat N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N ) { permutedTest <- t.test(sample(values) ~ group, data = theData, mu = 0 permutedTestStats[i] <- permutedTest$statisitc } hist(permutedTestStats) abline(v=observedTestStat) sum(permutedTestStats >= observedTestStat)/N sum(permutedTestStats <= observedTestStat)/N myTest4 <- t.test(values ~ group, data = theData, mu =0) observedTestStat4 <- myTest4$statistic print(observedTestStat4) ## t ## -0.7304735 N <- 2000 permutedTestStats4 <- rep(NA, N) for(i in 1:N ){ permutedTest4 <- t.test(sample(values) ~ group, data = theData, mu = 0) permutedTestStats4[i] <- permutedTest4$statistic } hist(permutedTestStats4) abline(v=observedTestStat4) greatvalue <- sum(permutedTestStats4 >= observedTestStat4)/N print(greatvalue) ## [1] 0.765 lessvalue <- sum(permutedTestStats4 <= observedTestStat4)/N print(lessvalue) ## [1] 0.235 twovalue <- 2*sum(permutedTestStats4 <= observedTestStat4)/N print(twovalue) ## [1] 0.47", "Assessment Quiz - Randomization Testing", "Class Activity - Permutation Testing (Part 1) How does the picture of Legos in your Permutation Tesing Overview page demonstrate the idea of permutation testing?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Week 13 | Randomizing Testing"
  },
  {
    "id": 216,
    "title": "ğŸ“ Skills Quiz - Randomization Testing",
    "url": "Paige-sNotes.html#skillsquizrandomizationtesting",
    "content": "Skills Quiz - Randomization Testing The idea behind permutation testing is that the null hypothesis can be reworded to state that â€œany pattern that has been witnessed in the sampled data is simple due to random chance .â€ Permutation tests can be applied to any hypothesis testing scenario in order to compute the p-value of the test in away that does not require any assumption s of the data! The most difficult part of any permutation test : figuring out how to permute the data! - which is performed differently for each hypothesis test Thus, being able to identify the hypothesis test from the stated hypotheses is an important skill you have hopefully started to develop. To see how you are doing with this skill, match the following null hypotheses with their appropriate test. Chi-Squared Test \\(H_0 : \\text{the two variables are not associated}\\) Wilcoxon Signed-Rank Test \\(H_0 : \\text{median of differences = 0}\\)",
    "sentences": ["The idea behind permutation testing is that the null hypothesis can be reworded to state that â€œany pattern that has been witnessed in the sampled data is simple due to random chance .â€", "Permutation tests can be applied to any hypothesis testing scenario in order to compute the p-value of the test in away that does not require any assumption s of the data!", "The most difficult part of any permutation test : figuring out how to permute the data!", "- which is performed differently for each hypothesis test", "Thus, being able to identify the hypothesis test from the stated hypotheses is an important skill you have hopefully started to develop.", "To see how you are doing with this skill, match the following null hypotheses with their appropriate test.", "Chi-Squared Test", "\\(H_0 : \\text{the two variables are not associated}\\)", "Wilcoxon Signed-Rank Test", "\\(H_0 : \\text{median of differences = 0}\\)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skills Quiz - Randomization Testing"
  },
  {
    "id": 217,
    "title": "ğŸ“ Class Activity - Permutation Testing (Part 1)",
    "url": "Paige-sNotes.html#classactivitypermutationtestingpart1",
    "content": "Class Activity - Permutation Testing (Part 1) How does the picture of Legos in your Permutation Tesing Overview page demonstrate the idea of permutation testing? This image represents: - clear structure/pattern - blocks were certainly organized and not sorted by random chance - suggests real pattern that is not random - if there is structure in the data, then â€œmixing up the data and dumping it out againâ€ will show very different patterns from the original This image represents: - a pile of toy blocks in a random pattern - as if the toy blocks were put in a bag, shaken up, and dumped out - This is the idea of the permutation test - if the data was just random to begin with, then we should see a very similar pattern by â€œmixing up the data and dumping it out again.â€ How is the distribution of the test statistic created using permutation testing? The distribution is created by: Randomly shuffling (permuting) the data labels many times Calculating the test statistic for each permutation Building a distribution from all these permuted test statistics How does this distribution differ from a parametric distribution? Unlike parametric distributions (like normal or t-distributions) which follow theoretical mathematical formulas, permutation distributions: Are created directly from the actual data Donâ€™t assume any particular shape or underlying distribution Better reflect the true sampling distribution for the specific dataset How is the p-value calculated from a permutation test? The p-value is calculated by:",
    "sentences": ["How does the picture of Legos in your Permutation Tesing Overview page demonstrate the idea of permutation testing?", "This image represents: - clear structure/pattern - blocks were certainly organized and not sorted by random chance - suggests real pattern that is not random - if there is structure in the data, then â€œmixing up the data and dumping it out againâ€ will show very different patterns from the original This image represents: - a pile of toy blocks in a random pattern - as if the toy blocks were put in a bag, shaken up, and dumped out - This is the idea of the permutation test - if the data was just random to begin with, then we should see a very similar pattern by â€œmixing up the data and dumping it out again.â€", "How is the distribution of the test statistic created using permutation testing?", "The distribution is created by:", "Randomly shuffling (permuting) the data labels many times Calculating the test statistic for each permutation Building a distribution from all these permuted test statistics", "How does this distribution differ from a parametric distribution?", "Unlike parametric distributions (like normal or t-distributions) which follow theoretical mathematical formulas, permutation distributions:", "Are created directly from the actual data Donâ€™t assume any particular shape or underlying distribution Better reflect the true sampling distribution for the specific dataset", "How is the p-value calculated from a permutation test?", "The p-value is calculated by:"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Permutation Testing (Part 1)"
  },
  {
    "id": 218,
    "title": "ğŸ“ Class Activity - Premutation Testing (Part 2)",
    "url": "Paige-sNotes.html#classactivitypremutationtestingpart2",
    "content": "Class Activity - Premutation Testing (Part 2) Perform an independent two-sample t test using a permutation test. Use the mtcars dataset and test whether the average weight of the four cylinder cars differs from the average weight of eight cylinder cars. Use the following psuedo code as a starting spot. Step 1 myTest <- â€¦perform the initial testâ€¦ observedTestStat <- â€¦get the test statisticâ€¦ Step 2 N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- â€¦perform test with permutedDataâ€¦ permutedTestStats[i] <- â€¦get test statisticâ€¦ } hist(permutedTestStats) abline(v=observedTestStat) Step 3 sum(permutedTestStats >= observedTestStat)/N sum(permutedTestStats <= observedTestStat)/N vroom <- mtcars %>% dplyr::select(cyl, wt) %>% dplyr::filter(cyl %in% c(4, 8)) #Step 1 myTest <- t.test(wt ~ cyl, data = vroom, mu = 0) observedTestStat <- myTest$statistic #Step 2 N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- t.test(sample(wt) ~ cyl, data = vroom, mu=0) permutedTestStats[i] <- permutedTest$statistic } hist(permutedTestStats, main= \"Permutation Test Distribution\", xlab = \"Test Statistic\") abline(v=observedTestStat, col= \"red\", lwd =2) #Step 3 grea <- sum(permutedTestStats >= observedTestStat)/N sum(permutedTestStats <= observedTestStat)/N print(observedTestStat) The observed test statistic (observedTestStat) for the previous test is [answer]. (round to two decimal places) : -6.44",
    "sentences": ["Perform an independent two-sample t test using a permutation test.", "Use the mtcars dataset and test whether the average weight of the four cylinder cars differs from the average weight of eight cylinder cars.", "Use the following psuedo code as a starting spot.", "Step 1 myTest <- â€¦perform the initial testâ€¦ observedTestStat <- â€¦get the test statisticâ€¦", "Step 2 N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- â€¦perform test with permutedDataâ€¦ permutedTestStats[i] <- â€¦get test statisticâ€¦ } hist(permutedTestStats) abline(v=observedTestStat)", "Step 3 sum(permutedTestStats >= observedTestStat)/N sum(permutedTestStats <= observedTestStat)/N", "vroom <- mtcars %>% dplyr::select(cyl, wt) %>% dplyr::filter(cyl %in% c(4, 8)) #Step 1 myTest <- t.test(wt ~ cyl, data = vroom, mu = 0) observedTestStat <- myTest$statistic #Step 2 N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- t.test(sample(wt) ~ cyl, data = vroom, mu=0) permutedTestStats[i] <- permutedTest$statistic } hist(permutedTestStats, main= \"Permutation Test Distribution\", xlab = \"Test Statistic\") abline(v=observedTestStat, col= \"red\", lwd =2)", "#Step 3 grea <- sum(permutedTestStats >= observedTestStat)/N sum(permutedTestStats <= observedTestStat)/N", "print(observedTestStat)", "The observed test statistic (observedTestStat) for the previous test is [answer]."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Premutation Testing (Part 2)"
  },
  {
    "id": 219,
    "title": "ğŸ“ Favorite Visuals/ Tips",
    "url": "Paige-sNotes.html#favoritevisualstips",
    "content": "Favorite Visuals/ Tips Analysis Tips Visual making questions Ask yourself the following questions when making a visual: Does the title present new information or does it restate what the axis state? Is there any information that should or could be explained? Do the colors/ style add to the presentation of the information? Is it accessible? (do you need to clarify how to use the graphics?) Most importantlyâ€¦ did you spell everything right?[^1] # Using ```{r, eval=FALSE} turns off the chunk, but still shows it. # Useful when you want to remember code, but not run it in this file. Background Tips Donâ€™t try to write up a ton of stuff, try to get to the point! More words \\(\\neq\\) â€œthe betterâ€ Less is more, have them explore! use â€œtabsetâ€ commands Cool Graphics Histograms Great for showing the distribution of data for a single quantitative variable when the sample size is large. Dotplots are a good alternative for smaller sample sizes. Gives a good feel for the mean and standard deviation of the data. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. cars â€œcarsâ€ is a dataset. Type â€œView(cars)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=dist â€œx=â€ declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_histogram( The â€œgeom_histogram()â€ function causes the ggplot to become a histogram. There are many other â€œgeom_â€ functions that could be used. binwidth=5,Â  The â€œbinwidthâ€ command controls the width of the bars in the histogram. fill=â€œfirebrick1â€, The â€œfillâ€ command controls the color of the insides of each bar. color=â€œyellowâ€ The â€œcolorâ€ command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  ggtitle( The â€œggtitle(â€ function is used to add a title to the plot. â€œRaces of Lightning McQueenâ€,Â  This is the cool sick title of the graph, always make it interesting and not re-explaining the axis labels. Â +Â  The addition symbol + is used to add further elements to the ggplot. xlab(â€œSpeed (mph)â€)Â  The â€œxlab(â€ command allows you name the x axis. Â +Â  The addition symbol + is used to add further elements to the ggplot. ylab(â€œDistanceâ€) The â€œylab(â€ command allows you to label the y axis. Â +Â  The addition symbol + is used to add further elements to the ggplot. theme( Â  The â€œtheme(â€ function allows us to manipulate the title, axis labels, and tick size. plot.title= Â  The â€œplot.title=â€ allows us to manipulate the title in a various amount of ways. size=20,color=â€œdodgerblueâ€,face=â€œboldâ€,family=â€œserifâ€),Â  These commands allow us to manipulate size, color, bold/italic, or font of the title. axis.title.x = Â  The â€œaxis.title.x =â€ allows us to modify the font size of the axis title (change the x to a y and it changes it for the y axis). element_text(size=16),Â  The â€œelement_text(size=â€ allows us to modify the font size. axis.text.x =Â  The â€œaxis.text.x =â€ allows us to modify the size of the values/labels along the x axis. element_text(size=12),Â  The â€œelement_text(size=â€ allows us to modify the font size. axis.title.y = Â  The â€œaxis.title.y =â€ allows us to modify the font size of the axis title. element_text(size=16),Â  The â€œelement_text(size=â€ allows us to modify the font size. axis.text.y =Â  The â€œaxis.text.y =â€ allows us to modify the size of the values/labels along the y axis. element_text(size=12)Â  The â€œelement_text(size=â€ allows us to modify the font size. )Closing parenthsis for the labs function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output. Scatter Plots Depicts the actual values of the data points, which are \\((x,y)\\) pairs. Works well for small or large sample sizes. Visualizes well the correlation between the two variables. Should be used in linear regression contexts whenever possible. Rent <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/Rent.csv\") Rent_filtered <- Rent %>% mutate(`Walking Minutes to the MC`= round((CrowFlyMetersToMC/1609)*16.67,0))%>% dplyr::select(Gender,Name,Address,`Walking Minutes to the MC`,Residents,AvgFloorPlanCost, Latitude, Longitude)%>% mutate(`Monthly Floor Plan Cost`=round(AvgFloorPlanCost/3.5,2))%>% filter(Gender == \"F\")%>% filter(`Monthly Floor Plan Cost` < 300)%>% rename(`Semester Floor Plan Cost` = AvgFloorPlanCost)%>% rename(`Apartment Complex`= Name)%>% arrange(`Monthly Floor Plan Cost`) plot_ly( the â€œplot_lyâ€ function creates an interactive scatter plot Rent_filtered, the filtered â€œRentâ€ data set x=~Monthly Floor Plan Cost, the x variable y= ~Residents, the y variable type = â€˜scatterâ€™, specifies the type of plot mode = â€˜markersâ€™, defines the drawing mode for the scatter plot markers = list(size = 15), customizes the appearance of the markers color = ~ Monthly Floor Plan Cost, Determines the color of each marker based on a variable colors = â€˜Bluesâ€™, specifies the color palette to use for the markers text = ~ paste ( defines the test that appears when you hover over the marker Apartment Complex, shows the name of the apartment complex â€ Â nâ€, adds a line break/indent (there is a backward slash in front of the n) â€œ$â€, displays the dollar sign Monthly Floor Plan Cost)) shows the monthly floor plan cost of each apartment complex when you hover over the marker %>% the pipe operator layout( modifies the layout and appearance of the plot adding titles, annotaitons, and other layout options! title = â€œPopular BYU-Idaho Female Student Housingâ€, sets the main title of the plot annotations = list( adds annotations to the plot list(x=0.7,y=1.03, plots where the annotation shows up text = â€œ(Light Blue - Cheap, Dark Blue - Expensive)â€, The annotation itself! showarrow = FALSE, xref= â€˜paperâ€™, xanchor = â€˜centerâ€™, yanchor = â€˜autoâ€™, I donâ€™t know tbh font = list( allows the customization of the x and y axis titles size = 10, color = â€œgrayâ€))), changes the size and color of the x and y axis titles xaxis = list( allows you to state the desired title of the x axis title = â€œRent Cost (USD)â€), the title of the x axis yaxis = list( allows you to state the desired title of the y axis title = â€œResidentsâ€)) the title of the y axis Â Â Â Â  Press Enter to run the code. Â â€¦Â  Click to View Output. plot_ly(Rent_filtered, x= ~`Monthly Floor Plan Cost`, y= ~Residents, type = 'scatter', mode = 'markers', markers = list(size = 15), color = ~`Monthly Floor Plan Cost`, colors = 'Blues', text = ~paste(`Apartment Complex`,\"\\n\",\"$\",`Monthly Floor Plan Cost`))%>% layout (title = \"Popular BYU-Idaho Female Student Housing\",annotations =list(list(x=0.7, y= 1.03, text = \"(Light Blue - Cheap, Dark Blue - Expensive)\", showarrow = FALSE, xref= 'paper', yref = 'paper', xanchor = 'center', yanchor = 'auto', font = list(size = 10, color = \"gray\"))), xaxis = list(title = \"Rent Cost (USD)\"), yaxis = list(title = \"Residents\")) palette(c(\"skyblue\",\"firebrick\")) plot(mpg ~ qsec, data=mtcars, col=as.factor(am), pch=16, xlab=\"Quarter Mile Time (seconds)\", ylab=\"Miles per Gallon\", main=\"1974 Motor Trend Vehicles\") legend(\"topright\", pch=16, legend=c(\"automatic\",\"manual\"), title=\"Transmission\", bty='n', col=palette()) ggplot to plot_ly example[^2] ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams) Bar Charts Depicts the number of occurrances for each category, or level, of the qualitative variable. Similar to a histogram, but there is no natural way to order the bars. Thus the white-space between each bar. It is called a Pareto chart if the bars are ordered from tallest to shortest. Clustered and stacked bar charts are often used to display information for two qualitative variables simultaneously. myplot <- ggplot(starwars, aes(x=hair_color, fill=gender)) + geom_bar() ggplotly(myplot) Boxplots Graphical depiction of the five-number summary. Great for comparing the distributions of data across several groups or categories. Provides a quick visual understanding of the location of the median as well as the range of the data. Can be useful in showing outliers. Sample size should be larger than at least five, or computing the five-number summary is not very meaningful. Side-by-side dotplots are a good alternative for smaller sample sizes. plot_ly(KidsFeet, y=~length, x=~as.factor(sex),type=\"box\",color=~sex, colors=c(\"lightblue\",\"pink\")) %>% layout( title=\"4th Grader's Average Foot Size\", xaxis=list(title=\"Gender\"),yaxis=list(title=\"Length (cm)\")) food <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv\") ## New names: ## Rows: 125 Columns: 59 ## â”€â”€ Column specification ## â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Delimiter: \",\" chr ## (19): GPA, weight, gender, breakfast, comfort_food, comfort_food_reasons, diet_current, drink, eat... dbl ## (40): calories_chicken, calories_day, comfort_food_reasons_coded...9, cook, comfort_food_reasons_c... ## â„¹ Use `spec()` to retrieve the full column specification for this data. â„¹ Specify the column types or set ## `show_col_types = FALSE` to quiet this message. ## â€¢ `comfort_food_reasons_coded` -> `comfort_food_reasons_coded...9` ## â€¢ `comfort_food_reasons_coded` -> `comfort_food_reasons_coded...11` Foood <- food %>% dplyr::select(weight, breakfast, fries, drink, soup) %>% dplyr::filter(!is.na(suppressWarnings(as.numeric(weight))) + is.na(drink)) %>% mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) %>% mutate(weight = as.numeric(weight)) boxplot(weight ~ `Food Perception Score`, data=Foood, col=c(\"darkseagreen1\",\"palegreen\",\"palegreen3\",\"palegreen4\",\"mediumseagreen\"), xlab=\"Food Perception Score\", main=\"Weight of College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=Foood, pch=16,vertical=TRUE, add=TRUE, col=\"gray28\") Dot Plots Depicts the actual values of each data point. Best for small sample sizes or for datasets where there are lots of repeated values. Histograms or boxplots are better alternatives for large sample sizes when there are few repeated values. Great for comparing the distribution of data across several groups or categories. ggplot(KidsFeet, aes(x=factor(sex),y=length, fill=sex)) + coord_flip() + geom_dotplot(binaxis=\"y\", stackdir=\"up\", position=\"dodge\", dotsize=0.45, binwidth=0.5) + scale_fill_manual(values=c(\"B\"=\"lightblue\",\"G\"=\"pink\")) + labs(title= \"Kennedy 4th Grader's Foot Size\", x=\"Length (cm)\", y=\"Gender\") Tables Always remember to pipe in a pander when you can! datatable(Rent_filtered, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") Â  \\(H_0\\) True \\(H_0\\) False Reject \\(H_0\\) Type I Error Correct Decision Accept \\(H_0\\) Correct Decision Type II Error favstats(height~eye_color, data=starwars)%>% pander() eye_color min Q1 median Q3 max mean sd n missing black 122 173 188 206 229 185 31.78 9 1 blue 150 170 180 186 234 182.2 20.32 19 0 blue-gray 182 182 182 182 182 182 NA 1 0 brown 66 164 183 185 193 167.3 34.11 19 2 dark NA NA NA NA NA NA NA 0 1 gold 191 191 191 191 191 191 NA 1 0 green, yellow 216 216 216 216 216 216 NA 1 0 hazel 170 172 174 176 178 174 5.657 2 1 orange 112 172 184 198.5 224 180.5 33.53 8 0 pink 180 180 180 180 180 180 NA 1 0 red 96 97 190 191 200 154.8 53.36 5 0 red, blue 96 96 96 96 96 96 NA 1 0 unknown 79 107.5 136 164.5 193 136 80.61 2 1 white 178 178 178 178 178 178 NA 1 0 yellow 94 167.5 175 198 264 177.8 42.22 11 0 HSS <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/HighSchoolSeniors.csv\") HSS.GJ <- dplyr::select(HSS, c(Reaction_time,Outdoor_Activities_Hours,Video_Games_Hours)) %>% filter_all(all_vars(!is.na(.))) %>% filter(Outdoor_Activities_Hours != Video_Games_Hours) %>% filter(abs(Outdoor_Activities_Hours - Video_Games_Hours) >= 10) %>% filter(Reaction_time > 0.1 & Reaction_time < 0.4) %>% mutate(More_Activity = if_else(Video_Games_Hours > Outdoor_Activities_Hours, \"Gamer\", \"Jock\")) %>% rename(`Reaction Time (Seconds)` = Reaction_time) %>% rename(`Time Spent Outside (Hours)` = Outdoor_Activities_Hours) %>% rename(`Time Spent Playing Video Games (Hours)` = Video_Games_Hours) %>% rename(`Gamer or Jock?` = More_Activity) HSS.G <- dplyr::select(HSS.GJ, c(`Reaction Time (Seconds)`,`Gamer or Jock?`)) %>% filter(`Gamer or Jock?` == \"Gamer\") HSS.J <- dplyr::select(HSS.GJ, c(`Reaction Time (Seconds)`,`Gamer or Jock?`)) %>% filter(`Gamer or Jock?` == \"Jock\") datatable(HSS.GJ, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") We added a classification column in this one! Itâ€™s pretty cool!! datatable(Foood, options=list(pageLength = 10)) %>% formatStyle('Food Perception Score', backgroundColor = 'lightgreen') kable function is pretty cool tooooooo!! questions <- c(\"Which one of these pictures do you associate the word 'breakfast'?\", \"Which picture do you associate with the word 'drink'?\", \"Which of these pictures you associate with word 'fries'?\", \"Which of the two pictures you associate with the word 'soup'?\") foodoptions <- c(\"Cereal / Donut\", \"1- Orange Juice / 2- Soda\", \"1- McDonald's Fries / 2- Home Fries\", \"1- Veggie Soup / 2- Creamy Soup\") collegestusurvey <- data.frame( Question = questions, Options = foodoptions, stringsAsFactors = FALSE) kable(collegestusurvey, col.names = c(\"Questions\", \"Food Options\"), caption =\"Food Survey\") Food Survey Questions Food Options Which one of these pictures do you associate the word â€˜breakfastâ€™? Cereal / Donut Which picture do you associate with the word â€˜drinkâ€™? 1- Orange Juice / 2- Soda Which of these pictures you associate with word â€˜friesâ€™? 1- McDonaldâ€™s Fries / 2- Home Fries Which of the two pictures you associate with the word â€˜soupâ€™? 1- Veggie Soup / 2- Creamy Soup Foodie <- favstats(weight ~ `Food Perception Score`, data = Foood)[,-10] Foodie2 <- as.data.frame(Foodie) kable(Foodie2) Food Perception Score min Q1 median Q3 max mean sd n 0 129 137.50 140.0 175.0 187 154.4286 24.31637 7 1 110 125.00 145.0 169.0 210 146.5946 25.90952 37 2 100 138.00 160.0 175.0 265 160.1887 28.97881 53 3 105 137.50 175.0 198.5 264 176.3333 48.17330 15 4 130 151.25 167.5 187.5 200 167.5000 26.78619 6 Maps Helps to show where things are in reference to something! mc_icon <- makeAwesomeIcon(icon= \"university\", iconColor = \"white\", markerColor = \"lightblue\", library = \"fa\") leaflet(data = Rent_filtered)%>% addTiles() %>% setView(lng=-111.7833595717528, lat=43.82217013118815,zoom =14.511)%>% addAwesomeMarkers(lng=-111.7827756599601, lat=43.81821783971177, icon= mc_icon, popup= \"Manwarding Center(MC)\")%>% # addMarkers(lng=-111.7826513, lat= 43.82271564, popup= \"HILL'S COLLEGE AVE APTS<br>5 Min. Walk\")%>% # addMarkers(lng=-111.7876455, lat= 43.82246794, popup= \"PINES, WOMEN<br>6 Min. Walk\")%>% # addMarkers(lng=-111.7882195, lat=43.81930189, popup= \"DAVENPORT APARTMENTS<br>5 Min. Walk\")%>% # addMarkers(lng= -111.7754656, lat=43.8174285, popup= \"BUENA VISTA<br>6 Min. Walk\")%>% # addMarkers(lng=-111.7805055, lat=43.82015341, popup= \"RIVIERA APARTMENTS<br>3 Min. Walk\")%>% # addMarkers(lng=-111.7798457, lat=43.82065271, popup= \"BAYSIDE MANOR<br>3 Min. Walk\")%>% # addMarkers(lng=-111.7890135, lat=43.81866711, popup= \"BROOKLYN APARTMENTS<br>5 Min. Walk\")%>% # addMarkers(lng=-111.7871091, lat=43.82469723, popup= \"COTTONWOOD - WOMEN<br>8 Min. Walk\")%>% # addMarkers(lng=-111.7806664, lat=43.82124102, popup= \"CRESTWOOD COTTAGE<br>4 Min. Walk\")%>% # addMarkers(lng=-111.7877153, lat=43.81913933, popup= \"ROYAL CREST<br>4 Min. Walk\")%>% # addMarkers(lng=-111.7792019, lat=43.82347422, popup= \"BLUE DOOR, WOMEN<br>6 Min. Walk\")%>% # addMarkers(lng=-111.7883, lat=43.8200102, popup= \"SUNSET HALL<br>5 Min. Walk\")%>% addMarkers(lng=~Longitude, lat=~Latitude, popup=~paste(`Apartment Complex`,\"<br>\",\"Walk Mins.\",`Walking Minutes to the MC`)) %>% addLegend(position=\"topright\", colors= c(\"lightblue\",\"cornflowerblue\"), labels = c(\"Manwarding Center (MC)\",\"Apartment Complexes\"), title= \"Building Locations\")%>% addControl(\"<strong>Click the marker to see additional information<\/strong>\",position = \"topright\", className = \"map-caption\")",
    "sentences": ["Analysis Tips Visual making questions Ask yourself the following questions when making a visual: Does the title present new information or does it restate what the axis state?", "Is there any information that should or could be explained?", "Do the colors/ style add to the presentation of the information?", "Is it accessible?", "(do you need to clarify how to use the graphics?) Most importantlyâ€¦ did you spell everything right?[^1] # Using ```{r, eval=FALSE} turns off the chunk, but still shows it.", "# Useful when you want to remember code, but not run it in this file.", "Background Tips Donâ€™t try to write up a ton of stuff, try to get to the point!", "More words \\(\\neq\\) â€œthe betterâ€ Less is more, have them explore!", "use â€œtabsetâ€ commands", "Cool Graphics Histograms Great for showing the distribution of data for a single quantitative variable when the sample size is large."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Favorite Visuals/ Tips"
  },
  {
    "id": 220,
    "title": "ğŸ“ Analysis Tips",
    "url": "Paige-sNotes.html#analysistips",
    "content": "Analysis Tips Visual making questions Ask yourself the following questions when making a visual: Does the title present new information or does it restate what the axis state? Is there any information that should or could be explained? Do the colors/ style add to the presentation of the information? Is it accessible? (do you need to clarify how to use the graphics?) Most importantlyâ€¦ did you spell everything right?[^1] # Using ```{r, eval=FALSE} turns off the chunk, but still shows it. # Useful when you want to remember code, but not run it in this file. Background Tips Donâ€™t try to write up a ton of stuff, try to get to the point! More words \\(\\neq\\) â€œthe betterâ€ Less is more, have them explore! use â€œtabsetâ€ commands",
    "sentences": ["Visual making questions Ask yourself the following questions when making a visual: Does the title present new information or does it restate what the axis state?", "Is there any information that should or could be explained?", "Do the colors/ style add to the presentation of the information?", "Is it accessible?", "(do you need to clarify how to use the graphics?) Most importantlyâ€¦ did you spell everything right?[^1] # Using ```{r, eval=FALSE} turns off the chunk, but still shows it.", "# Useful when you want to remember code, but not run it in this file.", "Background Tips Donâ€™t try to write up a ton of stuff, try to get to the point!", "More words \\(\\neq\\) â€œthe betterâ€ Less is more, have them explore!", "use â€œtabsetâ€ commands"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Analysis Tips"
  },
  {
    "id": 221,
    "title": "ğŸ“ Visual making questions",
    "url": "Paige-sNotes.html#visualmakingquestions",
    "content": "Visual making questions Ask yourself the following questions when making a visual: Does the title present new information or does it restate what the axis state? Is there any information that should or could be explained? Do the colors/ style add to the presentation of the information? Is it accessible? (do you need to clarify how to use the graphics?) Most importantlyâ€¦ did you spell everything right?[^1] # Using ```{r, eval=FALSE} turns off the chunk, but still shows it. # Useful when you want to remember code, but not run it in this file.",
    "sentences": ["Ask yourself the following questions when making a visual:", "Does the title present new information or does it restate what the axis state?", "Is there any information that should or could be explained?", "Do the colors/ style add to the presentation of the information?", "Is it accessible?", "(do you need to clarify how to use the graphics?) Most importantlyâ€¦ did you spell everything right?[^1]", "# Using ```{r, eval=FALSE} turns off the chunk, but still shows it.", "# Useful when you want to remember code, but not run it in this file."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Visual making questions"
  },
  {
    "id": 222,
    "title": "ğŸ“ Background Tips",
    "url": "Paige-sNotes.html#backgroundtips",
    "content": "Background Tips Donâ€™t try to write up a ton of stuff, try to get to the point! More words \\(\\neq\\) â€œthe betterâ€ Less is more, have them explore! use â€œtabsetâ€ commands",
    "sentences": ["Donâ€™t try to write up a ton of stuff, try to get to the point!", "More words \\(\\neq\\) â€œthe betterâ€ Less is more, have them explore!", "use â€œtabsetâ€ commands"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Background Tips"
  },
  {
    "id": 223,
    "title": "ğŸ“ Cool Graphics",
    "url": "Paige-sNotes.html#coolgraphics",
    "content": "Cool Graphics Histograms Great for showing the distribution of data for a single quantitative variable when the sample size is large. Dotplots are a good alternative for smaller sample sizes. Gives a good feel for the mean and standard deviation of the data. ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. cars â€œcarsâ€ is a dataset. Type â€œView(cars)â€ in R to see it. ,Â  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or â€œaestheticsâ€ function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=dist â€œx=â€ declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  geom_histogram( The â€œgeom_histogram()â€ function causes the ggplot to become a histogram. There are many other â€œgeom_â€ functions that could be used. binwidth=5,Â  The â€œbinwidthâ€ command controls the width of the bars in the histogram. fill=â€œfirebrick1â€, The â€œfillâ€ command controls the color of the insides of each bar. color=â€œyellowâ€ The â€œcolorâ€ command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function. Â +Â  The addition symbol + is used to add further elements to the ggplot. Â Â  ggtitle( The â€œggtitle(â€ function is used to add a title to the plot. â€œRaces of Lightning McQueenâ€,Â  This is the cool sick title of the graph, always make it interesting and not re-explaining the axis labels. Â +Â  The addition symbol + is used to add further elements to the ggplot. xlab(â€œSpeed (mph)â€)Â  The â€œxlab(â€ command allows you name the x axis. Â +Â  The addition symbol + is used to add further elements to the ggplot. ylab(â€œDistanceâ€) The â€œylab(â€ command allows you to label the y axis. Â +Â  The addition symbol + is used to add further elements to the ggplot. theme( Â  The â€œtheme(â€ function allows us to manipulate the title, axis labels, and tick size. plot.title= Â  The â€œplot.title=â€ allows us to manipulate the title in a various amount of ways. size=20,color=â€œdodgerblueâ€,face=â€œboldâ€,family=â€œserifâ€),Â  These commands allow us to manipulate size, color, bold/italic, or font of the title. axis.title.x = Â  The â€œaxis.title.x =â€ allows us to modify the font size of the axis title (change the x to a y and it changes it for the y axis). element_text(size=16),Â  The â€œelement_text(size=â€ allows us to modify the font size. axis.text.x =Â  The â€œaxis.text.x =â€ allows us to modify the size of the values/labels along the x axis. element_text(size=12),Â  The â€œelement_text(size=â€ allows us to modify the font size. axis.title.y = Â  The â€œaxis.title.y =â€ allows us to modify the font size of the axis title. element_text(size=16),Â  The â€œelement_text(size=â€ allows us to modify the font size. axis.text.y =Â  The â€œaxis.text.y =â€ allows us to modify the size of the values/labels along the y axis. element_text(size=12)Â  The â€œelement_text(size=â€ allows us to modify the font size. )Closing parenthsis for the labs function. Â Â Â Â Press Enter to run the code. Â â€¦Â  Click to View Output.",
    "sentences": ["Histograms Great for showing the distribution of data for a single quantitative variable when the sample size is large.", "Dotplots are a good alternative for smaller sample sizes.", "Gives a good feel for the mean and standard deviation of the data.", "ggplot An R function â€œggplotâ€ used to create a framework for a graphic that will have elements added to it with the + sign.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "cars â€œcarsâ€ is a dataset.", "Type â€œView(cars)â€ in R to see it.", ",Â  The comma allows us to specify optional commands to the function.", "The space after the comma is not required."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Cool Graphics"
  },
  {
    "id": 224,
    "title": "ğŸ“ Histograms",
    "url": "Paige-sNotes.html#histograms",
    "content": "Histograms Great for showing the distribution of data for a single quantitative variable when the sample size is large. Dotplots are a good alternative for smaller sample sizes. Gives a good feel for the mean and standard deviation of the data.",
    "sentences": ["Great for showing the distribution of data for a single quantitative variable when the sample size is large.", "Dotplots are a good alternative for smaller sample sizes.", "Gives a good feel for the mean and standard deviation of the data."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Histograms"
  },
  {
    "id": 225,
    "title": "ğŸ“ Scatter Plots",
    "url": "Paige-sNotes.html#scatterplots",
    "content": "Scatter Plots Depicts the actual values of the data points, which are \\((x,y)\\) pairs. Works well for small or large sample sizes. Visualizes well the correlation between the two variables. Should be used in linear regression contexts whenever possible. Rent <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/Rent.csv\") Rent_filtered <- Rent %>% mutate(`Walking Minutes to the MC`= round((CrowFlyMetersToMC/1609)*16.67,0))%>% dplyr::select(Gender,Name,Address,`Walking Minutes to the MC`,Residents,AvgFloorPlanCost, Latitude, Longitude)%>% mutate(`Monthly Floor Plan Cost`=round(AvgFloorPlanCost/3.5,2))%>% filter(Gender == \"F\")%>% filter(`Monthly Floor Plan Cost` < 300)%>% rename(`Semester Floor Plan Cost` = AvgFloorPlanCost)%>% rename(`Apartment Complex`= Name)%>% arrange(`Monthly Floor Plan Cost`) plot_ly(Rent_filtered, x= ~`Monthly Floor Plan Cost`, y= ~Residents, type = 'scatter', mode = 'markers', markers = list(size = 15), color = ~`Monthly Floor Plan Cost`, colors = 'Blues', text = ~paste(`Apartment Complex`,\"\\n\",\"$\",`Monthly Floor Plan Cost`))%>% layout (title = \"Popular BYU-Idaho Female Student Housing\",annotations =list(list(x=0.7, y= 1.03, text = \"(Light Blue - Cheap, Dark Blue - Expensive)\", showarrow = FALSE, xref= 'paper', yref = 'paper', xanchor = 'center', yanchor = 'auto', font = list(size = 10, color = \"gray\"))), xaxis = list(title = \"Rent Cost (USD)\"), yaxis = list(title = \"Residents\")) palette(c(\"skyblue\",\"firebrick\")) plot(mpg ~ qsec, data=mtcars, col=as.factor(am), pch=16, xlab=\"Quarter Mile Time (seconds)\", ylab=\"Miles per Gallon\", main=\"1974 Motor Trend Vehicles\") legend(\"topright\", pch=16, legend=c(\"automatic\",\"manual\"), title=\"Transmission\", bty='n', col=palette()) ggplot to plot_ly example[^2] ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams)",
    "sentences": ["Depicts the actual values of the data points, which are \\((x,y)\\) pairs.", "Works well for small or large sample sizes.", "Visualizes well the correlation between the two variables.", "Should be used in linear regression contexts whenever possible.", "Rent <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/Rent.csv\") Rent_filtered <- Rent %>% mutate(`Walking Minutes to the MC`= round((CrowFlyMetersToMC/1609)*16.67,0))%>% dplyr::select(Gender,Name,Address,`Walking Minutes to the MC`,Residents,AvgFloorPlanCost, Latitude, Longitude)%>% mutate(`Monthly Floor Plan Cost`=round(AvgFloorPlanCost/3.5,2))%>% filter(Gender == \"F\")%>% filter(`Monthly Floor Plan Cost` < 300)%>% rename(`Semester Floor Plan Cost` = AvgFloorPlanCost)%>% rename(`Apartment Complex`= Name)%>% arrange(`Monthly Floor Plan Cost`)", "plot_ly(Rent_filtered, x= ~`Monthly Floor Plan Cost`, y= ~Residents, type = 'scatter', mode = 'markers', markers = list(size = 15), color = ~`Monthly Floor Plan Cost`, colors = 'Blues', text = ~paste(`Apartment Complex`,\"\\n\",\"$\",`Monthly Floor Plan Cost`))%>% layout (title = \"Popular BYU-Idaho Female Student Housing\",annotations =list(list(x=0.7, y= 1.03, text = \"(Light Blue - Cheap, Dark Blue - Expensive)\", showarrow = FALSE, xref= 'paper', yref = 'paper', xanchor = 'center', yanchor = 'auto', font = list(size = 10, color = \"gray\"))), xaxis = list(title = \"Rent Cost (USD)\"), yaxis = list(title = \"Residents\"))", "palette(c(\"skyblue\",\"firebrick\")) plot(mpg ~ qsec, data=mtcars, col=as.factor(am), pch=16, xlab=\"Quarter Mile Time (seconds)\", ylab=\"Miles per Gallon\", main=\"1974 Motor Trend Vehicles\") legend(\"topright\", pch=16, legend=c(\"automatic\",\"manual\"), title=\"Transmission\", bty='n', col=palette())", "ggplot to plot_ly example[^2]", "ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Scatter Plots"
  },
  {
    "id": 226,
    "title": "ğŸ“ Bar Charts",
    "url": "Paige-sNotes.html#barcharts",
    "content": "Bar Charts Depicts the number of occurrances for each category, or level, of the qualitative variable. Similar to a histogram, but there is no natural way to order the bars. Thus the white-space between each bar. It is called a Pareto chart if the bars are ordered from tallest to shortest. Clustered and stacked bar charts are often used to display information for two qualitative variables simultaneously. myplot <- ggplot(starwars, aes(x=hair_color, fill=gender)) + geom_bar() ggplotly(myplot)",
    "sentences": ["Depicts the number of occurrances for each category, or level, of the qualitative variable.", "Similar to a histogram, but there is no natural way to order the bars.", "Thus the white-space between each bar.", "It is called a Pareto chart if the bars are ordered from tallest to shortest.", "Clustered and stacked bar charts are often used to display information for two qualitative variables simultaneously.", "myplot <- ggplot(starwars, aes(x=hair_color, fill=gender)) + geom_bar() ggplotly(myplot)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Bar Charts"
  },
  {
    "id": 227,
    "title": "ğŸ“ Boxplots",
    "url": "Paige-sNotes.html#boxplots",
    "content": "Boxplots Graphical depiction of the five-number summary. Great for comparing the distributions of data across several groups or categories. Provides a quick visual understanding of the location of the median as well as the range of the data. Can be useful in showing outliers. Sample size should be larger than at least five, or computing the five-number summary is not very meaningful. Side-by-side dotplots are a good alternative for smaller sample sizes. plot_ly(KidsFeet, y=~length, x=~as.factor(sex),type=\"box\",color=~sex, colors=c(\"lightblue\",\"pink\")) %>% layout( title=\"4th Grader's Average Foot Size\", xaxis=list(title=\"Gender\"),yaxis=list(title=\"Length (cm)\")) food <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv\") Foood <- food %>% dplyr::select(weight, breakfast, fries, drink, soup) %>% dplyr::filter(!is.na(suppressWarnings(as.numeric(weight))) + is.na(drink)) %>% mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) %>% mutate(weight = as.numeric(weight)) boxplot(weight ~ `Food Perception Score`, data=Foood, col=c(\"darkseagreen1\",\"palegreen\",\"palegreen3\",\"palegreen4\",\"mediumseagreen\"), xlab=\"Food Perception Score\", main=\"Weight of College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=Foood, pch=16,vertical=TRUE, add=TRUE, col=\"gray28\")",
    "sentences": ["Graphical depiction of the five-number summary.", "Great for comparing the distributions of data across several groups or categories.", "Provides a quick visual understanding of the location of the median as well as the range of the data.", "Can be useful in showing outliers.", "Sample size should be larger than at least five, or computing the five-number summary is not very meaningful.", "Side-by-side dotplots are a good alternative for smaller sample sizes.", "plot_ly(KidsFeet, y=~length, x=~as.factor(sex),type=\"box\",color=~sex, colors=c(\"lightblue\",\"pink\")) %>% layout( title=\"4th Grader's Average Foot Size\", xaxis=list(title=\"Gender\"),yaxis=list(title=\"Length (cm)\"))", "food <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv\")", "Foood <- food %>% dplyr::select(weight, breakfast, fries, drink, soup) %>% dplyr::filter(!is.na(suppressWarnings(as.numeric(weight))) + is.na(drink)) %>% mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) %>% mutate(weight = as.numeric(weight)) boxplot(weight ~ `Food Perception Score`, data=Foood, col=c(\"darkseagreen1\",\"palegreen\",\"palegreen3\",\"palegreen4\",\"mediumseagreen\"), xlab=\"Food Perception Score\", main=\"Weight of College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=Foood, pch=16,vertical=TRUE, add=TRUE, col=\"gray28\")"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Boxplots"
  },
  {
    "id": 228,
    "title": "ğŸ“ Dot Plots",
    "url": "Paige-sNotes.html#dotplots",
    "content": "Dot Plots Depicts the actual values of each data point. Best for small sample sizes or for datasets where there are lots of repeated values. Histograms or boxplots are better alternatives for large sample sizes when there are few repeated values. Great for comparing the distribution of data across several groups or categories. ggplot(KidsFeet, aes(x=factor(sex),y=length, fill=sex)) + coord_flip() + geom_dotplot(binaxis=\"y\", stackdir=\"up\", position=\"dodge\", dotsize=0.45, binwidth=0.5) + scale_fill_manual(values=c(\"B\"=\"lightblue\",\"G\"=\"pink\")) + labs(title= \"Kennedy 4th Grader's Foot Size\", x=\"Length (cm)\", y=\"Gender\")",
    "sentences": ["Depicts the actual values of each data point.", "Best for small sample sizes or for datasets where there are lots of repeated values.", "Histograms or boxplots are better alternatives for large sample sizes when there are few repeated values.", "Great for comparing the distribution of data across several groups or categories.", "ggplot(KidsFeet, aes(x=factor(sex),y=length, fill=sex)) + coord_flip() + geom_dotplot(binaxis=\"y\", stackdir=\"up\", position=\"dodge\", dotsize=0.45, binwidth=0.5) + scale_fill_manual(values=c(\"B\"=\"lightblue\",\"G\"=\"pink\")) + labs(title= \"Kennedy 4th Grader's Foot Size\", x=\"Length (cm)\", y=\"Gender\")"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Dot Plots"
  },
  {
    "id": 229,
    "title": "ğŸ“ Tables",
    "url": "Paige-sNotes.html#tables",
    "content": "Tables Always remember to pipe in a pander when you can! datatable(Rent_filtered, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") favstats(height~eye_color, data=starwars)%>% pander() HSS <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/HighSchoolSeniors.csv\") HSS.GJ <- dplyr::select(HSS, c(Reaction_time,Outdoor_Activities_Hours,Video_Games_Hours)) %>% filter_all(all_vars(!is.na(.))) %>% filter(Outdoor_Activities_Hours != Video_Games_Hours) %>% filter(abs(Outdoor_Activities_Hours - Video_Games_Hours) >= 10) %>% filter(Reaction_time > 0.1 & Reaction_time < 0.4) %>% mutate(More_Activity = if_else(Video_Games_Hours > Outdoor_Activities_Hours, \"Gamer\", \"Jock\")) %>% rename(`Reaction Time (Seconds)` = Reaction_time) %>% rename(`Time Spent Outside (Hours)` = Outdoor_Activities_Hours) %>% rename(`Time Spent Playing Video Games (Hours)` = Video_Games_Hours) %>% rename(`Gamer or Jock?` = More_Activity) HSS.G <- dplyr::select(HSS.GJ, c(`Reaction Time (Seconds)`,`Gamer or Jock?`)) %>% filter(`Gamer or Jock?` == \"Gamer\") HSS.J <- dplyr::select(HSS.GJ, c(`Reaction Time (Seconds)`,`Gamer or Jock?`)) %>% filter(`Gamer or Jock?` == \"Jock\") datatable(HSS.GJ, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") We added a classification column in this one! Itâ€™s pretty cool!! datatable(Foood, options=list(pageLength = 10)) %>% formatStyle('Food Perception Score', backgroundColor = 'lightgreen') kable function is pretty cool tooooooo!! questions <- c(\"Which one of these pictures do you associate the word 'breakfast'?\", \"Which picture do you associate with the word 'drink'?\", \"Which of these pictures you associate with word 'fries'?\", \"Which of the two pictures you associate with the word 'soup'?\") foodoptions <- c(\"Cereal / Donut\", \"1- Orange Juice / 2- Soda\", \"1- McDonald's Fries / 2- Home Fries\", \"1- Veggie Soup / 2- Creamy Soup\") collegestusurvey <- data.frame( Question = questions, Options = foodoptions, stringsAsFactors = FALSE) kable(collegestusurvey, col.names = c(\"Questions\", \"Food Options\"), caption =\"Food Survey\") Foodie <- favstats(weight ~ `Food Perception Score`, data = Foood)[,-10] Foodie2 <- as.data.frame(Foodie) kable(Foodie2)",
    "sentences": ["Always remember to pipe in a pander when you can!", "datatable(Rent_filtered, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\")", "favstats(height~eye_color, data=starwars)%>% pander()", "HSS <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/HighSchoolSeniors.csv\") HSS.GJ <- dplyr::select(HSS, c(Reaction_time,Outdoor_Activities_Hours,Video_Games_Hours)) %>% filter_all(all_vars(!is.na(.))) %>% filter(Outdoor_Activities_Hours != Video_Games_Hours) %>% filter(abs(Outdoor_Activities_Hours - Video_Games_Hours) >= 10) %>% filter(Reaction_time > 0.1 & Reaction_time < 0.4) %>% mutate(More_Activity = if_else(Video_Games_Hours > Outdoor_Activities_Hours, \"Gamer\", \"Jock\")) %>% rename(`Reaction Time (Seconds)` = Reaction_time) %>% rename(`Time Spent Outside (Hours)` = Outdoor_Activities_Hours) %>% rename(`Time Spent Playing Video Games (Hours)` = Video_Games_Hours) %>% rename(`Gamer or Jock?` = More_Activity) HSS.G <- dplyr::select(HSS.GJ, c(`Reaction Time (Seconds)`,`Gamer or Jock?`)) %>% filter(`Gamer or Jock?` == \"Gamer\") HSS.J <- dplyr::select(HSS.GJ, c(`Reaction Time (Seconds)`,`Gamer or Jock?`)) %>% filter(`Gamer or Jock?` == \"Jock\") datatable(HSS.GJ, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\")", "We added a classification column in this one!", "Itâ€™s pretty cool!!", "datatable(Foood, options=list(pageLength = 10)) %>% formatStyle('Food Perception Score', backgroundColor = 'lightgreen')", "kable function is pretty cool tooooooo!!", "questions <- c(\"Which one of these pictures do you associate the word 'breakfast'?\", \"Which picture do you associate with the word 'drink'?\", \"Which of these pictures you associate with word 'fries'?\", \"Which of the two pictures you associate with the word 'soup'?\") foodoptions <- c(\"Cereal / Donut\", \"1- Orange Juice / 2- Soda\", \"1- McDonald's Fries / 2- Home Fries\", \"1- Veggie Soup / 2- Creamy Soup\") collegestusurvey <- data.frame( Question = questions, Options = foodoptions, stringsAsFactors = FALSE) kable(collegestusurvey, col.names = c(\"Questions\", \"Food Options\"), caption =\"Food Survey\")", "Foodie <- favstats(weight ~ `Food Perception Score`, data = Foood)[,-10] Foodie2 <- as.data.frame(Foodie) kable(Foodie2)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Tables"
  },
  {
    "id": 230,
    "title": "ğŸ“ Maps",
    "url": "Paige-sNotes.html#maps",
    "content": "Maps Helps to show where things are in reference to something! mc_icon <- makeAwesomeIcon(icon= \"university\", iconColor = \"white\", markerColor = \"lightblue\", library = \"fa\") leaflet(data = Rent_filtered)%>% addTiles() %>% setView(lng=-111.7833595717528, lat=43.82217013118815,zoom =14.511)%>% addAwesomeMarkers(lng=-111.7827756599601, lat=43.81821783971177, icon= mc_icon, popup= \"Manwarding Center(MC)\")%>% # addMarkers(lng=-111.7826513, lat= 43.82271564, popup= \"HILL'S COLLEGE AVE APTS<br>5 Min. Walk\")%>% # addMarkers(lng=-111.7876455, lat= 43.82246794, popup= \"PINES, WOMEN<br>6 Min. Walk\")%>% # addMarkers(lng=-111.7882195, lat=43.81930189, popup= \"DAVENPORT APARTMENTS<br>5 Min. Walk\")%>% # addMarkers(lng= -111.7754656, lat=43.8174285, popup= \"BUENA VISTA<br>6 Min. Walk\")%>% # addMarkers(lng=-111.7805055, lat=43.82015341, popup= \"RIVIERA APARTMENTS<br>3 Min. Walk\")%>% # addMarkers(lng=-111.7798457, lat=43.82065271, popup= \"BAYSIDE MANOR<br>3 Min. Walk\")%>% # addMarkers(lng=-111.7890135, lat=43.81866711, popup= \"BROOKLYN APARTMENTS<br>5 Min. Walk\")%>% # addMarkers(lng=-111.7871091, lat=43.82469723, popup= \"COTTONWOOD - WOMEN<br>8 Min. Walk\")%>% # addMarkers(lng=-111.7806664, lat=43.82124102, popup= \"CRESTWOOD COTTAGE<br>4 Min. Walk\")%>% # addMarkers(lng=-111.7877153, lat=43.81913933, popup= \"ROYAL CREST<br>4 Min. Walk\")%>% # addMarkers(lng=-111.7792019, lat=43.82347422, popup= \"BLUE DOOR, WOMEN<br>6 Min. Walk\")%>% # addMarkers(lng=-111.7883, lat=43.8200102, popup= \"SUNSET HALL<br>5 Min. Walk\")%>% addMarkers(lng=~Longitude, lat=~Latitude, popup=~paste(`Apartment Complex`,\"<br>\",\"Walk Mins.\",`Walking Minutes to the MC`)) %>% addLegend(position=\"topright\", colors= c(\"lightblue\",\"cornflowerblue\"), labels = c(\"Manwarding Center (MC)\",\"Apartment Complexes\"), title= \"Building Locations\")%>% addControl(\"<strong>Click the marker to see additional information<\/strong>\",position = \"topright\", className = \"map-caption\")",
    "sentences": ["Helps to show where things are in reference to something!", "mc_icon <- makeAwesomeIcon(icon= \"university\", iconColor = \"white\", markerColor = \"lightblue\", library = \"fa\") leaflet(data = Rent_filtered)%>% addTiles() %>% setView(lng=-111.7833595717528, lat=43.82217013118815,zoom =14.511)%>% addAwesomeMarkers(lng=-111.7827756599601, lat=43.81821783971177, icon= mc_icon, popup= \"Manwarding Center(MC)\")%>% # addMarkers(lng=-111.7826513, lat= 43.82271564, popup= \"HILL'S COLLEGE AVE APTS<br>5 Min.", "Walk\")%>% # addMarkers(lng=-111.7876455, lat= 43.82246794, popup= \"PINES, WOMEN<br>6 Min.", "Walk\")%>% # addMarkers(lng=-111.7882195, lat=43.81930189, popup= \"DAVENPORT APARTMENTS<br>5 Min.", "Walk\")%>% # addMarkers(lng= -111.7754656, lat=43.8174285, popup= \"BUENA VISTA<br>6 Min.", "Walk\")%>% # addMarkers(lng=-111.7805055, lat=43.82015341, popup= \"RIVIERA APARTMENTS<br>3 Min.", "Walk\")%>% # addMarkers(lng=-111.7798457, lat=43.82065271, popup= \"BAYSIDE MANOR<br>3 Min.", "Walk\")%>% # addMarkers(lng=-111.7890135, lat=43.81866711, popup= \"BROOKLYN APARTMENTS<br>5 Min.", "Walk\")%>% # addMarkers(lng=-111.7871091, lat=43.82469723, popup= \"COTTONWOOD - WOMEN<br>8 Min.", "Walk\")%>% # addMarkers(lng=-111.7806664, lat=43.82124102, popup= \"CRESTWOOD COTTAGE<br>4 Min."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Maps"
  },
  {
    "id": 231,
    "title": "ğŸ“ Styling and Formats",
    "url": "Paige-sNotes.html#stylingandformats",
    "content": "Styling and Formats I typed some words. Some blue words, some highlighted words, some big words. Make a link to a meaningful page of the Statistics-Notebook: Customization Help <- Read through this page! Or to a page on the internet: R Colors Word Stylez Colors < span style=â€œcolor:[INSERT COLOR];â€>[INSERT WORDS] < /span > Highlighting < span style=â€œbackground-color:[INSERT COLOR];â€>[INSERT WORDS] < /span > Font Size < span style=â€œfont-size:[INSERT SIZE]em;â€>[INSERT WORDS] < /span > Underlining < u>[INSERT WORDS] < /u > Bolding, Italicize, or both! One * -> Italicize Two ** -> Bold Three *** -> Combo Shrinking font-size < div style = â€œfont-size:.8em;â€ > Changing Words to Gray Text < div style = â€œcolor:gray;â€ > Formatting Section making { .tabset .tabset-pills OR fade } Making a Caption < div style =â€œfont-size:.8em;color:#888888;â€ > [INSERT THE WORDS YOU WANT TO SAY HERE!] < / div > Inserting a Picture ! [ _ ] ( [COPY AND PASTE PICTURE INTO THE PARANTHESES] ) Practice header is 3 stars and in all caps like this: PRACTICE 3 dashes makes line in between texts (make sure they arenâ€™t touching any text and that they are spaced away from things) < br > < br > (makes bigger gap inbetween text, the more you add the wider the gap!) one dash makes a bullet point pressing space 4 times make a sub bullet point! will this make another one? Pathing Data Set Code Need to path to data set? Use this code: [Data set name] <- read_csv(â€œC:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/[Data set name]â€) or do read_csv(../../Data/[Data Set Name].csv) Also, to create a csv. file in R, use this code! write.csv([data set name],â€œC:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/[data set name].csvâ€, row.names = FALSE) Themes Want a different theme? Here are some options! â€œdefaultâ€ â€œbootstrapâ€ -> dark blue buttons â€œceruleanâ€ -> light blue buttons â€œcosmoâ€ -> light blue square button â€œdarklyâ€ -> dark blue button + black background â€œflatlyâ€ -> DARK blue button with green text! â€œjournalâ€ -> pink buttons â€œlumenâ€ -> greyish blue buttons â€œpaperâ€ -> blue buttons, lame text â€œreadableâ€ -> blue buttons, text is big and bubbly â€œsandstoneâ€ -> tan buttons â€œsimplexâ€ -> red buttons â€œspacelabâ€ -> darker blue gray buttons, softer shape â€œunitedâ€ -> redish orange buttons â€œyetiâ€ -> greyish more squared buttons â€œarchitectâ€ (from prettydoc package) â€œcaymanâ€ â€œhpstrâ€ â€œleonidsâ€ â€œtactileâ€ â€œhtml_cleanâ€ (from the rmdformats package) â€œhtml_doccoâ€ â€œmaterialâ€ â€œreadthedownâ€ Text Side by Side Want to make something side by side < div style=â€œdisplay: flex; justify-content: space-around;â€ > < div > [Insert words] < /div > < div > [more words right here] < /div > < / div > Hyper linking Want to make hyper link your stuff from one part to go back and forth between where you are and the source? (did that even make sense??) Put a [ ^ 1 ] next to the source you are wanting to link the sentence to. There is an example to show you in the visual making questions In the sources, put [ ^ 1 ] : [ The name for your hyper link in brackets ] ( the link of the video in parentheses ) [^1] : It would look like this! ggplot into plot_ly Assignment Operator (<-) your ggplot a name use ggplotly([Insert name you <- to]) to change the ggplot to a plot_ly! example is in the [^2]:scatter plot section!",
    "sentences": ["I typed some words.", "Some blue words, some highlighted words, some big words.", "Make a link to a meaningful page of the Statistics-Notebook: Customization Help <- Read through this page!", "Or to a page on the internet: R Colors", "Word Stylez Colors < span style=â€œcolor:[INSERT COLOR];â€>[INSERT WORDS] < /span > Highlighting < span style=â€œbackground-color:[INSERT COLOR];â€>[INSERT WORDS] < /span > Font Size < span style=â€œfont-size:[INSERT SIZE]em;â€>[INSERT WORDS] < /span > Underlining < u>[INSERT WORDS] < /u > Bolding, Italicize, or both!", "One * -> Italicize Two ** -> Bold Three *** -> Combo Shrinking font-size < div style = â€œfont-size:.8em;â€ > Changing Words to Gray Text < div style = â€œcolor:gray;â€ >", "Formatting Section making { .tabset .tabset-pills OR fade } Making a Caption < div style =â€œfont-size:.8em;color:#888888;â€ > [INSERT THE WORDS YOU WANT TO SAY HERE!] < / div > Inserting a Picture !", "[ _ ] ( [COPY AND PASTE PICTURE INTO THE PARANTHESES] ) Practice header is 3 stars and in all caps like this: PRACTICE 3 dashes makes line in between texts (make sure they arenâ€™t touching any text and that they are spaced away from things) < br > < br > (makes bigger gap inbetween text, the more you add the wider the gap!) one dash makes a bullet point pressing space 4 times make a sub bullet point!", "will this make another one?", "Pathing Data Set Code Need to path to data set?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Styling and Formats"
  },
  {
    "id": 232,
    "title": "ğŸ“ Word Stylez",
    "url": "Paige-sNotes.html#wordstylez",
    "content": "Word Stylez Colors < span style=â€œcolor:[INSERT COLOR];â€>[INSERT WORDS] < /span > Highlighting < span style=â€œbackground-color:[INSERT COLOR];â€>[INSERT WORDS] < /span > Font Size < span style=â€œfont-size:[INSERT SIZE]em;â€>[INSERT WORDS] < /span > Underlining < u>[INSERT WORDS] < /u > Bolding, Italicize, or both! One * -> Italicize Two ** -> Bold Three *** -> Combo Shrinking font-size < div style = â€œfont-size:.8em;â€ > Changing Words to Gray Text < div style = â€œcolor:gray;â€ >",
    "sentences": ["Colors < span style=â€œcolor:[INSERT COLOR];â€>[INSERT WORDS] < /span >", "Highlighting < span style=â€œbackground-color:[INSERT COLOR];â€>[INSERT WORDS] < /span >", "Font Size < span style=â€œfont-size:[INSERT SIZE]em;â€>[INSERT WORDS] < /span >", "Underlining < u>[INSERT WORDS] < /u >", "Bolding, Italicize, or both!", "One * -> Italicize Two ** -> Bold Three *** -> Combo", "Shrinking font-size < div style = â€œfont-size:.8em;â€ >", "Changing Words to Gray Text < div style = â€œcolor:gray;â€ >"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Word Stylez"
  },
  {
    "id": 233,
    "title": "ğŸ“ Formatting",
    "url": "Paige-sNotes.html#formatting",
    "content": "Formatting Section making { .tabset .tabset-pills OR fade } Making a Caption < div style =â€œfont-size:.8em;color:#888888;â€ > [INSERT THE WORDS YOU WANT TO SAY HERE!] < / div > Inserting a Picture ! [ _ ] ( [COPY AND PASTE PICTURE INTO THE PARANTHESES] ) Practice header is 3 stars and in all caps like this: PRACTICE 3 dashes makes line in between texts (make sure they arenâ€™t touching any text and that they are spaced away from things) < br > < br > (makes bigger gap inbetween text, the more you add the wider the gap!) one dash makes a bullet point pressing space 4 times make a sub bullet point! will this make another one? Pathing Data Set Code Need to path to data set? Use this code: [Data set name] <- read_csv(â€œC:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/[Data set name]â€) or do read_csv(../../Data/[Data Set Name].csv) Also, to create a csv. file in R, use this code! write.csv([data set name],â€œC:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/[data set name].csvâ€, row.names = FALSE) Themes Want a different theme? Here are some options! â€œdefaultâ€ â€œbootstrapâ€ -> dark blue buttons â€œceruleanâ€ -> light blue buttons â€œcosmoâ€ -> light blue square button â€œdarklyâ€ -> dark blue button + black background â€œflatlyâ€ -> DARK blue button with green text! â€œjournalâ€ -> pink buttons â€œlumenâ€ -> greyish blue buttons â€œpaperâ€ -> blue buttons, lame text â€œreadableâ€ -> blue buttons, text is big and bubbly â€œsandstoneâ€ -> tan buttons â€œsimplexâ€ -> red buttons â€œspacelabâ€ -> darker blue gray buttons, softer shape â€œunitedâ€ -> redish orange buttons â€œyetiâ€ -> greyish more squared buttons â€œarchitectâ€ (from prettydoc package) â€œcaymanâ€ â€œhpstrâ€ â€œleonidsâ€ â€œtactileâ€ â€œhtml_cleanâ€ (from the rmdformats package) â€œhtml_doccoâ€ â€œmaterialâ€ â€œreadthedownâ€",
    "sentences": ["Section making { .tabset .tabset-pills OR fade }", "Making a Caption < div style =â€œfont-size:.8em;color:#888888;â€ > [INSERT THE WORDS YOU WANT TO SAY HERE!] < / div >", "Inserting a Picture !", "[ _ ] ( [COPY AND PASTE PICTURE INTO THE PARANTHESES] ) Practice header is 3 stars and in all caps like this: PRACTICE 3 dashes makes line in between texts (make sure they arenâ€™t touching any text and that they are spaced away from things) < br > < br > (makes bigger gap inbetween text, the more you add the wider the gap!) one dash makes a bullet point pressing space 4 times make a sub bullet point!", "will this make another one?", "Pathing Data Set Code Need to path to data set?", "Use this code: [Data set name] <- read_csv(â€œC:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/[Data set name]â€) or do read_csv(../../Data/[Data Set Name].csv) Also, to create a csv.", "file in R, use this code!", "write.csv([data set name],â€œC:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/[data set name].csvâ€, row.names = FALSE)", "Themes Want a different theme?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Formatting"
  },
  {
    "id": 234,
    "title": "ğŸ“ Final Exam Practice",
    "url": "Paige-sNotes.html#finalexampractice",
    "content": "Final Exam Practice Perform an appropriate hypothesis test in R to decide if the medians displayed in the graph are significantly different or not. (Uses KidsFeet dataset) t.test(width ~ sex, data= KidsFeet, mu=0, alternative= \"two.sided\", conf.level = 0.95) What is the correct conclusion to make in a logistic regression when the goodness of fit test (either one) gives a p-value of 0.352? This shows the logistic regression provides significantly useful results. Continue to believe the logistic regression was appropriate to perform on the given data. Realize that there was insufficient evidence to conclude that the logistic regression was a good fit for the data. The logistic regression simply isnâ€™t a good fit for the data in that case. A permutation test for a Kruskal-Wallis Test Statistic is performed with the following result: Note that the numbers above the bars state the frequency of observations contained in that bar.",
    "sentences": ["Perform an appropriate hypothesis test in R to decide if the medians displayed in the graph are significantly different or not.", "(Uses KidsFeet dataset)", "t.test(width ~ sex, data= KidsFeet, mu=0, alternative= \"two.sided\", conf.level = 0.95)", "What is the correct conclusion to make in a logistic regression when the goodness of fit test (either one) gives a p-value of 0.352?", "This shows the logistic regression provides significantly useful results.", "Continue to believe the logistic regression was appropriate to perform on the given data.", "Realize that there was insufficient evidence to conclude that the logistic regression was a good fit for the data.", "The logistic regression simply isnâ€™t a good fit for the data in that case.", "A permutation test for a Kruskal-Wallis Test Statistic is performed with the following result:", "Note that the numbers above the bars state the frequency of observations contained in that bar."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Final Exam Practice"
  },
  {
    "id": 235,
    "title": "ğŸ“ Final Exam",
    "url": "Paige-sNotes.html#finalexam",
    "content": "Final Exam YoungAdults <- read_csv(\"https://raw.githubusercontent.com/saundersg/Statistics-Notebook/master/Data/YoungAdults.csv\") oolm <- lm(Weight ~ Height, data=YoungAdults) predict(oolm, data.frame(Height = 183)) ayogm <- glm(as.factor(YoungAdults$Lying) ~ Religiosity_score, data=YoungAdults, family=binomial) summary(ayogm) wilcox.test(Religiosity_score ~ Dominant_hand, data=YoungAdults) ggplot(data=YoungAdults, aes(x=as.factor(Dominant_hand), y=Religiosity_score)) + geom_boxplot() prunedudes <- YoungAdults %>% filter(Number_of_siblings == 0) %>% filter(Smoking == \"never smoked\")%>% filter(Lying == \"never\") t.test(Fear_score ~ Only_child, data = YoungAdults, mu = 0, alternative= \"two.sided\", conf.level = 0.95) YoungAdults.hwg <- YoungAdults %>% dplyr::select(Height, Weight, Gender) %>% na.omit() ggplot(YoungAdults.hwg, aes(x=Height, y=Weight, color=Gender))+ geom_point() + geom_smooth(method = \"lm\", se=F) YoungAdults_biggerfam <- YoungAdults %>% filter(Number_of_siblings > 3) plot(Religiosity_score ~ Fear_score, data= YoungAdults_biggerfam) relig_vs_fear.lm <- lm(Religiosity_score ~ Fear_score, data=YoungAdults_biggerfam) abline(relig_vs_fear.lm) observedTestStat <- summary(relig_vs_fear.lm)[[4]][2,3]",
    "sentences": ["YoungAdults <- read_csv(\"https://raw.githubusercontent.com/saundersg/Statistics-Notebook/master/Data/YoungAdults.csv\")", "oolm <- lm(Weight ~ Height, data=YoungAdults) predict(oolm, data.frame(Height = 183))", "ayogm <- glm(as.factor(YoungAdults$Lying) ~ Religiosity_score, data=YoungAdults, family=binomial) summary(ayogm)", "wilcox.test(Religiosity_score ~ Dominant_hand, data=YoungAdults)", "ggplot(data=YoungAdults, aes(x=as.factor(Dominant_hand), y=Religiosity_score)) + geom_boxplot()", "prunedudes <- YoungAdults %>% filter(Number_of_siblings == 0) %>% filter(Smoking == \"never smoked\")%>% filter(Lying == \"never\")", "t.test(Fear_score ~ Only_child, data = YoungAdults, mu = 0, alternative= \"two.sided\", conf.level = 0.95)", "YoungAdults.hwg <- YoungAdults %>% dplyr::select(Height, Weight, Gender) %>% na.omit() ggplot(YoungAdults.hwg, aes(x=Height, y=Weight, color=Gender))+ geom_point() + geom_smooth(method = \"lm\", se=F)", "YoungAdults_biggerfam <- YoungAdults %>% filter(Number_of_siblings > 3) plot(Religiosity_score ~ Fear_score, data= YoungAdults_biggerfam) relig_vs_fear.lm <- lm(Religiosity_score ~ Fear_score, data=YoungAdults_biggerfam) abline(relig_vs_fear.lm)", "observedTestStat <- summary(relig_vs_fear.lm)[[4]][2,3]"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Final Exam"
  },
  {
    "id": 236,
    "title": "Permutation Tests",
    "url": "PermutationTests.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Permutation Tests A nonparametric approach to computing the p-value for any test statistic in just about any scenario. Overview In almost all hypothesis testing scenarios, the null hypothesis can be interpreted as follows. \\(H_0\\): Any pattern that has been witnessed in the sampled data is simply due to random chance. Permutation Tests depend completely on this single idea. If all patterns in the data really are simply due to random chance, then the null hypothesis is true. Further, random re-samples of the data should show similar lack of patterns. However, if the pattern in the data is real, then random re-samples of the data will show very different patterns from the original. Consider the following image. In that image, the toy blocks on the left show a clear pattern or structure. They are nicely organized into colored piles. This suggests a real pattern that is not random. Someone certainly organized those blocks into that pattern. The blocks didnâ€™t land that way by random chance. On the other hand, the pile of toy blocks shown on the right is certainly a random pattern. This is a pattern that would result if the toy blocks were put into a bag, shaken up, and dumped out. This is the idea of the permutation test. If there is structure in the data, then â€œmixing up the data and dumping it out againâ€ will show very different patterns from the original. However, if the data was just random to begin with, then we would see a similar pattern by â€œmixing up the data and dumping it out again.â€ The process of a permutation test is: Compute a test statistic for the original data. Re-sample the data (â€œshake it up and dump it outâ€) thousands of times, computing a new test statistic each time, to create a sampling distribution of the test statistic. Compute the p-value of the permutation test as the percentage of test statistics that are as extreme or more extreme than the one originally observed. In review, the sampling distribution is created by permuting (randomly rearranging) the data thousands of times and calculating a test statistic on each permuted version of the data. A histogram of the test statistics then provides the sampling distribution of the test statistic needed to compute the p-value of the original test statistic. R Instructions Any permutation test can be performed in R with a for loop. #Step 1 Compute a test statistic for the original data. myTest <- â€¦perform the initial testâ€¦ This could be a t.test, wilcox.test, aov, kruskal.test, lm, glm, chisq.test, or any other R code that results in a test statistic. It could even simply be the mean or standard deviation of the data. observedTestStat <- â€¦get the test statisticâ€¦ Save the test statistic of your test into the object called observedTestStat. For tests that always result in a single test statistic like a t.test, wilcox.test, kruskal.test, and chisq.test it is myTest$statistic. For an aov, lm, or glm try printing summary(myTest)[] to see what values you are interested in using. observedTestStat Print the value of the test statistic of your test to the screen. This is the value that we now need to use to compute the P-value from by finding the probability that a randomly computed test statistic would be â€œmore extremeâ€ than this originally observed value. #Step 2 Re-sample the data (â€œshake it up and dump it outâ€) thousands of times, computing a new test statistic each time, to create a sampling distribution of the test statistic. N <- 2000Â Â Â Â Â Â  N is the number of times you will reuse the data to create the sampling distribution of the test statistic. A typical choice is 2000, but sometimes 10000, or 100000 reuses are needed before useful answers can be obtained. permutedTestStats <-Â  This is a storage container that will be used to store the test statistics from each of the thousands of reuses of the data. rep(NA, N) The rep() function repeats a given value N times. This particular statement repeats NAâ€™s or â€œmissing valuesâ€ N times. This gives us N â€œemptyâ€ storage spots inside of permutedTestStats that we can use to store the N test statistics from the N reuses of the data we will make in our for loop. for The for loop is a programming tool that lets us tell R to run a certain code over and over again for a certain number of times. Â (i in Â  In R, the for loop must be followed by a space, then an opening parenthesis, then a variable name (in this case the variable is called â€œiâ€), then the word â€œinâ€ then a list of values. 1:N The 1:N gives R the list of values 1, 2, 3, â€¦ and so on all the way up to N. These values are passed into i one at a time and the code inside the for loop is performed first for i=1, then again for i=2, then again for i=3 and so on until finally i=N. At that point, the for loop ends. ) Required closing parenthesis on the for (i in 1:N) statement. { This bracket opens the code section of the for loop. Any code placed between the opening { and closing } brackets will be performed over and over again for each value of i=1, i=2, â€¦ up through i=N. Â Â  Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized. permutedTest <- â€¦perform test with permutedDataâ€¦ The same test that was performed on the original data, should be performed again but with randomly permuted data. The easiest way to permute data is with sample(y-variable-name) inside your test. See the Explanation tab for details. Â Â  Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized. permutedTestStats This is the storage container that was built prior to the for (i in 1:N) code. Inside the for loop, this container is filled value by value using the square brackets [i]. [i] The square brackets [i] allows us to access the â€œiâ€th position of permutedTestStats. Remember, since this code is inside of the for loop, i=1 the first time through the code, then i=2 the second time through the code, i=3 the third time through, and so on up until i=N the Nth time through the code. Â <- â€¦get test statisticâ€¦ The test statistic from permutedTest is accessed here and stored into permutedTestStats[i]. } The closing } bracket ends the code that is repeated over and over again inside the for loop. hist(permutedTestStats) Creating a histogram of the sampling distribution of the test statistics obtained from the reused and permuted data allows us to visually compare the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. abline(v=observedTestStat) This adds the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. #Step 3 Compute the p-value of the permutation test as the percentage of test statistics that are as extreme or more extreme than the one originally observed. sum(permutedTestStats >= observedTestStat)/N This computes a â€œgreater thanâ€ p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the right hand side of the histogram. sum(permutedTestStats <= observedTestStat)/N This computes a â€œless thanâ€ p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the left hand side of the histogram. myTest <- ...perform the initial test... This could be a t.test, wilcox.test, aov, kruskal.test, lm, glm, chisq.test, or any other R code that results in a test statistic. It could even simply be the mean or standard deviation of the data. observedTestStat <- ...get the test statistic... Save the test statistic of your test into the object called observedTestStat. For tests that always result in a single test statistic like a t.test, wilcox.test, kruskal.test, and chisq.test it is myTest$statistic. For an aov, lm, or glm try printing summary(myTest)[] to see what values you are interested in using. N <- 2000 N is the number of times you will reuse the data to create the sampling distribution of the test statistic. A typical choice is 2000, but sometimes 10000, or 100000 reuses are needed before useful answers can be obtained. permutedTestStats <- rep(NA, N) This is a storage container that will be used to store the test statistics from each of the thousands of reuses of the data. The rep() function repeats a given value N times. This particular statement repeats NAâ€™s or â€œmissing valuesâ€ N times. This gives us N â€œemptyâ€ storage spots inside of permutedTestStats that we can use to store the N test statistics from the N reuses of the data we will make in our for loop. for (i in 1:N)\\{ The for loop is a programming tool that lets us tell R to run a certain code over and over again for a certain number of times. In R, the for loop must be followed by a space, then an opening parenthesis, then a variable name (in this case the variable is called â€œiâ€), then the word â€œinâ€ then a list of values. The 1:N gives R the list of values 1, 2, 3, â€¦ and so on all the way up to N. These values are passed into i one at a time and the code inside the for loop is performed first for i=1, then again for i=2, then again for i=3 and so on until finally i=N. At that point, the for loop ends. There is a required closing parenthesis on the for (i in 1:N) statement. Any code placed between the opening { and closing } brackets will be performed over and over again for each value of i=1, i=2, â€¦ up through i=N. Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized. permutedData <- ...randomly permute the data... This is the most important part of the permutation test and takes some thinking. The data must be randomly reorganized in a way consistent with the null hypothesis. What that means exactly is specific to each scenario. Read the Explanation tab for further details on the logic you should use here. permutedTest <- ...perform test with permutedData... The same test that was performed on the original data, should be performed again on the randomly permuted data. permutedTestStats[i] <- ...get test statistic... This is the storage container that was built prior to the for (i in 1:N) code. Inside the for loop, this container is filled value by value using the square brackets [i]. The square brackets [i] allows us to access the â€œiâ€th position of permutedTestStats. Remember, since this code is inside of the for loop, i=1 the first time through the code, then i=2 the second time through the code, i=3 the third time through, and so on up until i=N the Nth time through the code. The test statistic from permutedTest is accessed here and stored into permutedTestStats[i]. } The closing } bracket ends the code that is repeated over and over again inside the for loop. hist(permutedTestStats) Creating a histogram of the sampling distribution of the test statistics obtained from the reused and permuted data allows us to visually compare the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. abline(v=observedTestStat) This adds the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. sum(permutedTestStats >= observedTestStat)/N This computes a â€œgreater thanâ€ p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the right hand side of the histogram. sum(permutedTestStats <= observedTestStat)/N This computes a â€œless thanâ€ p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the left hand side of the histogram. Explanation The most difficult part of a permutation test is in the random permuting of the data. How the permuting is performed depends on the type of hypothesis test being performed. It is important to remember that the permutation test only changes the way the p-value is calculated. Everything else about the original test is unchanged when switching to a permutation test. Independent Samples t Test For the independent sample t Test, we will use the data from the independent sleep analysis. In that analysis, we were using the sleep data to test the hypotheses: \\[ H_0: \\mu_\\text{Extra Hours of Sleep with Drug 1} - \\mu_\\text{Extra Hours of Sleep with Drug 2} = 0 \\] \\[ H_a: \\mu_\\text{Extra Hours of Sleep with Drug 1} - \\mu_\\text{Extra Hours of Sleep with Drug 2} \\neq 0 \\] We used a significance level of \\(\\alpha = 0.05\\) and obtained a P-value of \\(0.07939\\). Letâ€™s demonstrate how a permutation test could be used to obtain this same p-value. (Technically you only need to use a permutation test when the requirements of the original test were not satisfied. However, it is also reasonable to perform a permutation test anytime you want. No requirements need to be checked when performing a permutation test.) # First run the initial test and gain the test statistic: myTest <- t.test(extra ~ group, data = sleep, mu = 0) observedTestStat <- myTest$statistic # Now we run the permutations to create a distribution of test statistics N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- t.test(sample(extra) ~ group, data = sleep, mu = 0) permutedTestStats[i] <- permutedTest$statistic } # Now we show a histogram of that distribution hist(permutedTestStats, col = \"skyblue\") abline(v = observedTestStat, col = \"red\", lwd = 3) #Greater-Than p-value: Not the correct one in this case sum(permutedTestStats >= observedTestStat)/N # Less-Than p-value: Not the correct one for this data sum(permutedTestStats <= observedTestStat)/N # Two-Sided p-value: This is the one we want based on our alternative hypothesis. 2*sum(permutedT",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Permutation Tests A nonparametric approach to computing the p-value for any test statistic in just about any scenario.", "Overview In almost all hypothesis testing scenarios, the null hypothesis can be interpreted as follows.", "\\(H_0\\): Any pattern that has been witnessed in the sampled data is simply due to random chance.", "Permutation Tests depend completely on this single idea.", "If all patterns in the data really are simply due to random chance, then the null hypothesis is true.", "Further, random re-samples of the data should show similar lack of patterns.", "However, if the pattern in the data is real, then random re-samples of the data will show very different patterns from the original.", "Consider the following image.", "In that image, the toy blocks on the left show a clear pattern or structure.", "They are nicely organized into colored piles.", "This suggests a real pattern that is not random.", "Someone certainly organized those blocks into that pattern.", "The blocks didnâ€™t land that way by random chance.", "On the other hand, the pile of toy blocks shown on the right is certainly a random pattern.", "This is a pattern that would result if the toy blocks were put into a bag, shaken up, and dumped out.", "This is the idea of the permutation test.", "If there is structure in the data, then â€œmixing up the data and dumping it out againâ€ will show very different patterns from the original.", "However, if the data was just random to begin with, then we would see a similar pattern by â€œmixing up the data and dumping it out again.â€ The process of a permutation test is: Compute a test statistic for the original data.", "Re-sample the data (â€œshake it up and dump it outâ€) thousands of times, computing a new test statistic each time, to create a sampling distribution of the test statistic.", "Compute the p-value of the permutation test as the percentage of test statistics that are as extreme or more extreme than the one originally observed.", "In review, the sampling distribution is created by permuting (randomly rearranging) the data thousands of times and calculating a test statistic on each permuted version of the data.", "A histogram of the test statistics then provides the sampling distribution of the test statistic needed to compute the p-value of the original test statistic.", "R Instructions Any permutation test can be performed in R with a for loop.", "#Step 1 Compute a test statistic for the original data.", "myTest <- â€¦perform the initial testâ€¦ This could be a t.test, wilcox.test, aov, kruskal.test, lm, glm, chisq.test, or any other R code that results in a test statistic.", "It could even simply be the mean or standard deviation of the data.", "observedTestStat <- â€¦get the test statisticâ€¦ Save the test statistic of your test into the object called observedTestStat.", "For tests that always result in a single test statistic like a t.test, wilcox.test, kruskal.test, and chisq.test it is myTest$statistic.", "For an aov, lm, or glm try printing summary(myTest)[] to see what values you are interested in using.", "observedTestStat Print the value of the test statistic of your test to the screen.", "This is the value that we now need to use to compute the P-value from by finding the probability that a randomly computed test statistic would be â€œmore extremeâ€ than this originally observed value.", "#Step 2 Re-sample the data (â€œshake it up and dump it outâ€) thousands of times, computing a new test statistic each time, to create a sampling distribution of the test statistic.", "N <- 2000Â Â Â Â Â Â  N is the number of times you will reuse the data to create the sampling distribution of the test statistic.", "A typical choice is 2000, but sometimes 10000, or 100000 reuses are needed before useful answers can be obtained.", "permutedTestStats <-Â  This is a storage container that will be used to store the test statistics from each of the thousands of reuses of the data.", "rep(NA, N) The rep() function repeats a given value N times.", "This particular statement repeats NAâ€™s or â€œmissing valuesâ€ N times.", "This gives us N â€œemptyâ€ storage spots inside of permutedTestStats that we can use to store the N test statistics from the N reuses of the data we will make in our for loop.", "for The for loop is a programming tool that lets us tell R to run a certain code over and over again for a certain number of times.", "Â (i in Â  In R, the for loop must be followed by a space, then an opening parenthesis, then a variable name (in this case the variable is called â€œiâ€), then the word â€œinâ€ then a list of values.", "1:N The 1:N gives R the list of values 1, 2, 3, â€¦ and so on all the way up to N.", "These values are passed into i one at a time and the code inside the for loop is performed first for i=1, then again for i=2, then again for i=3 and so on until finally i=N.", "At that point, the for loop ends.", ") Required closing parenthesis on the for (i in 1:N) statement.", "{ This bracket opens the code section of the for loop.", "Any code placed between the opening { and closing } brackets will be performed over and over again for each value of i=1, i=2, â€¦ up through i=N.", "Â Â  Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized.", "permutedTest <- â€¦perform test with permutedDataâ€¦ The same test that was performed on the original data, should be performed again but with randomly permuted data.", "The easiest way to permute data is with sample(y-variable-name) inside your test.", "See the Explanation tab for details."],
    "type": "page",
    "page_title": "Permutation Tests"
  },
  {
    "id": 237,
    "title": "ğŸ“ Overview",
    "url": "PermutationTests.html#overview",
    "content": "Overview In almost all hypothesis testing scenarios, the null hypothesis can be interpreted as follows. \\(H_0\\): Any pattern that has been witnessed in the sampled data is simply due to random chance. Permutation Tests depend completely on this single idea. If all patterns in the data really are simply due to random chance, then the null hypothesis is true. Further, random re-samples of the data should show similar lack of patterns. However, if the pattern in the data is real, then random re-samples of the data will show very different patterns from the original. Consider the following image. In that image, the toy blocks on the left show a clear pattern or structure. They are nicely organized into colored piles. This suggests a real pattern that is not random. Someone certainly organized those blocks into that pattern. The blocks didnâ€™t land that way by random chance. On the other hand, the pile of toy blocks shown on the right is certainly a random pattern. This is a pattern that would result if the toy blocks were put into a bag, shaken up, and dumped out. This is the idea of the permutation test. If there is structure in the data, then â€œmixing up the data and dumping it out againâ€ will show very different patterns from the original. However, if the data was just random to begin with, then we would see a similar pattern by â€œmixing up the data and dumping it out again.â€ The process of a permutation test is: Compute a test statistic for the original data. Re-sample the data (â€œshake it up and dump it outâ€) thousands of times, computing a new test statistic each time, to create a sampling distribution of the test statistic. Compute the p-value of the permutation test as the percentage of test statistics that are as extreme or more extreme than the one originally observed. In review, the sampling distribution is created by permuting (randomly rearranging) the data thousands of times and calculating a test statistic on each permuted version of the data. A histogram of the test statistics then provides the sampling distribution of the test statistic needed to compute the p-value of the original test statistic.",
    "sentences": ["In almost all hypothesis testing scenarios, the null hypothesis can be interpreted as follows.", "\\(H_0\\): Any pattern that has been witnessed in the sampled data is simply due to random chance.", "Permutation Tests depend completely on this single idea.", "If all patterns in the data really are simply due to random chance, then the null hypothesis is true.", "Further, random re-samples of the data should show similar lack of patterns.", "However, if the pattern in the data is real, then random re-samples of the data will show very different patterns from the original.", "Consider the following image.", "In that image, the toy blocks on the left show a clear pattern or structure.", "They are nicely organized into colored piles.", "This suggests a real pattern that is not random."],
    "type": "section",
    "page_title": "Permutation Tests",
    "section_title": "Overview"
  },
  {
    "id": 238,
    "title": "ğŸ“ R Instructions",
    "url": "PermutationTests.html#rinstructions",
    "content": "R Instructions Any permutation test can be performed in R with a for loop. #Step 1 Compute a test statistic for the original data. myTest <- â€¦perform the initial testâ€¦ This could be a t.test, wilcox.test, aov, kruskal.test, lm, glm, chisq.test, or any other R code that results in a test statistic. It could even simply be the mean or standard deviation of the data. observedTestStat <- â€¦get the test statisticâ€¦ Save the test statistic of your test into the object called observedTestStat. For tests that always result in a single test statistic like a t.test, wilcox.test, kruskal.test, and chisq.test it is myTest$statistic. For an aov, lm, or glm try printing summary(myTest)[] to see what values you are interested in using. observedTestStat Print the value of the test statistic of your test to the screen. This is the value that we now need to use to compute the P-value from by finding the probability that a randomly computed test statistic would be â€œmore extremeâ€ than this originally observed value. #Step 2 Re-sample the data (â€œshake it up and dump it outâ€) thousands of times, computing a new test statistic each time, to create a sampling distribution of the test statistic. N <- 2000Â Â Â Â Â Â  N is the number of times you will reuse the data to create the sampling distribution of the test statistic. A typical choice is 2000, but sometimes 10000, or 100000 reuses are needed before useful answers can be obtained. permutedTestStats <-Â  This is a storage container that will be used to store the test statistics from each of the thousands of reuses of the data. rep(NA, N) The rep() function repeats a given value N times. This particular statement repeats NAâ€™s or â€œmissing valuesâ€ N times. This gives us N â€œemptyâ€ storage spots inside of permutedTestStats that we can use to store the N test statistics from the N reuses of the data we will make in our for loop. for The for loop is a programming tool that lets us tell R to run a certain code over and over again for a certain number of times. Â (i in Â  In R, the for loop must be followed by a space, then an opening parenthesis, then a variable name (in this case the variable is called â€œiâ€), then the word â€œinâ€ then a list of values. 1:N The 1:N gives R the list of values 1, 2, 3, â€¦ and so on all the way up to N. These values are passed into i one at a time and the code inside the for loop is performed first for i=1, then again for i=2, then again for i=3 and so on until finally i=N. At that point, the for loop ends. ) Required closing parenthesis on the for (i in 1:N) statement. { This bracket opens the code section of the for loop. Any code placed between the opening { and closing } brackets will be performed over and over again for each value of i=1, i=2, â€¦ up through i=N. Â Â  Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized. permutedTest <- â€¦perform test with permutedDataâ€¦ The same test that was performed on the original data, should be performed again but with randomly permuted data. The easiest way to permute data is with sample(y-variable-name) inside your test. See the Explanation tab for details. Â Â  Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized. permutedTestStats This is the storage container that was built prior to the for (i in 1:N) code. Inside the for loop, this container is filled value by value using the square brackets [i]. [i] The square brackets [i] allows us to access the â€œiâ€th position of permutedTestStats. Remember, since this code is inside of the for loop, i=1 the first time through the code, then i=2 the second time through the code, i=3 the third time through, and so on up until i=N the Nth time through the code. Â <- â€¦get test statisticâ€¦ The test statistic from permutedTest is accessed here and stored into permutedTestStats[i]. } The closing } bracket ends the code that is repeated over and over again inside the for loop. hist(permutedTestStats) Creating a histogram of the sampling distribution of the test statistics obtained from the reused and permuted data allows us to visually compare the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. abline(v=observedTestStat) This adds the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. #Step 3 Compute the p-value of the permutation test as the percentage of test statistics that are as extreme or more extreme than the one originally observed. sum(permutedTestStats >= observedTestStat)/N This computes a â€œgreater thanâ€ p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the right hand side of the histogram. sum(permutedTestStats <= observedTestStat)/N This computes a â€œless thanâ€ p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the left hand side of the histogram. myTest <- ...perform the initial test... This could be a t.test, wilcox.test, aov, kruskal.test, lm, glm, chisq.test, or any other R code that results in a test statistic. It could even simply be the mean or standard deviation of the data. observedTestStat <- ...get the test statistic... Save the test statistic of your test into the object called observedTestStat. For tests that always result in a single test statistic like a t.test, wilcox.test, kruskal.test, and chisq.test it is myTest$statistic. For an aov, lm, or glm try printing summary(myTest)[] to see what values you are interested in using. N <- 2000 N is the number of times you will reuse the data to create the sampling distribution of the test statistic. A typical choice is 2000, but sometimes 10000, or 100000 reuses are needed before useful answers can be obtained. permutedTestStats <- rep(NA, N) This is a storage container that will be used to store the test statistics from each of the thousands of reuses of the data. The rep() function repeats a given value N times. This particular statement repeats NAâ€™s or â€œmissing valuesâ€ N times. This gives us N â€œemptyâ€ storage spots inside of permutedTestStats that we can use to store the N test statistics from the N reuses of the data we will make in our for loop. for (i in 1:N)\\{ The for loop is a programming tool that lets us tell R to run a certain code over and over again for a certain number of times. In R, the for loop must be followed by a space, then an opening parenthesis, then a variable name (in this case the variable is called â€œiâ€), then the word â€œinâ€ then a list of values. The 1:N gives R the list of values 1, 2, 3, â€¦ and so on all the way up to N. These values are passed into i one at a time and the code inside the for loop is performed first for i=1, then again for i=2, then again for i=3 and so on until finally i=N. At that point, the for loop ends. There is a required closing parenthesis on the for (i in 1:N) statement. Any code placed between the opening { and closing } brackets will be performed over and over again for each value of i=1, i=2, â€¦ up through i=N. Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized. permutedData <- ...randomly permute the data... This is the most important part of the permutation test and takes some thinking. The data must be randomly reorganized in a way consistent with the null hypothesis. What that means exactly is specific to each scenario. Read the Explanation tab for further details on the logic you should use here. permutedTest <- ...perform test with permutedData... The same test that was performed on the original data, should be performed again on the randomly permuted data. permutedTestStats[i] <- ...get test statistic... This is the storage container that was built prior to the for (i in 1:N) code. Inside the for loop, this container is filled value by value using the square brackets [i]. The square brackets [i] allows us to access the â€œiâ€th position of permutedTestStats. Remember, since this code is inside of the for loop, i=1 the first time through the code, then i=2 the second time through the code, i=3 the third time through, and so on up until i=N the Nth time through the code. The test statistic from permutedTest is accessed here and stored into permutedTestStats[i]. } The closing } bracket ends the code that is repeated over and over again inside the for loop. hist(permutedTestStats) Creating a histogram of the sampling distribution of the test statistics obtained from the reused and permuted data allows us to visually compare the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. abline(v=observedTestStat) This adds the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. sum(permutedTestStats >= observedTestStat)/N This computes a â€œgreater thanâ€ p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the right hand side of the histogram. sum(permutedTestStats <= observedTestStat)/N This computes a â€œless thanâ€ p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the left hand side of the histogram.",
    "sentences": ["Any permutation test can be performed in R with a for loop.", "#Step 1 Compute a test statistic for the original data.", "myTest <- â€¦perform the initial testâ€¦ This could be a t.test, wilcox.test, aov, kruskal.test, lm, glm, chisq.test, or any other R code that results in a test statistic.", "It could even simply be the mean or standard deviation of the data.", "observedTestStat <- â€¦get the test statisticâ€¦ Save the test statistic of your test into the object called observedTestStat.", "For tests that always result in a single test statistic like a t.test, wilcox.test, kruskal.test, and chisq.test it is myTest$statistic.", "For an aov, lm, or glm try printing summary(myTest)[] to see what values you are interested in using.", "observedTestStat Print the value of the test statistic of your test to the screen.", "This is the value that we now need to use to compute the P-value from by finding the probability that a randomly computed test statistic would be â€œmore extremeâ€ than this originally observed value.", "#Step 2 Re-sample the data (â€œshake it up and dump it outâ€) thousands of times, computing a new test statistic each time, to create a sampling distribution of the test statistic."],
    "type": "section",
    "page_title": "Permutation Tests",
    "section_title": "R Instructions"
  },
  {
    "id": 239,
    "title": "ğŸ“ Explanation",
    "url": "PermutationTests.html#explanation",
    "content": "Explanation The most difficult part of a permutation test is in the random permuting of the data. How the permuting is performed depends on the type of hypothesis test being performed. It is important to remember that the permutation test only changes the way the p-value is calculated. Everything else about the original test is unchanged when switching to a permutation test. Independent Samples t Test For the independent sample t Test, we will use the data from the independent sleep analysis. In that analysis, we were using the sleep data to test the hypotheses: \\[ H_0: \\mu_\\text{Extra Hours of Sleep with Drug 1} - \\mu_\\text{Extra Hours of Sleep with Drug 2} = 0 \\] \\[ H_a: \\mu_\\text{Extra Hours of Sleep with Drug 1} - \\mu_\\text{Extra Hours of Sleep with Drug 2} \\neq 0 \\] We used a significance level of \\(\\alpha = 0.05\\) and obtained a P-value of \\(0.07939\\). Letâ€™s demonstrate how a permutation test could be used to obtain this same p-value. (Technically you only need to use a permutation test when the requirements of the original test were not satisfied. However, it is also reasonable to perform a permutation test anytime you want. No requirements need to be checked when performing a permutation test.) # First run the initial test and gain the test statistic: myTest <- t.test(extra ~ group, data = sleep, mu = 0) observedTestStat <- myTest$statistic # Now we run the permutations to create a distribution of test statistics N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- t.test(sample(extra) ~ group, data = sleep, mu = 0) permutedTestStats[i] <- permutedTest$statistic } # Now we show a histogram of that distribution hist(permutedTestStats, col = \"skyblue\") abline(v = observedTestStat, col = \"red\", lwd = 3) #Greater-Than p-value: Not the correct one in this case sum(permutedTestStats >= observedTestStat)/N # Less-Than p-value: Not the correct one for this data sum(permutedTestStats <= observedTestStat)/N # Two-Sided p-value: This is the one we want based on our alternative hypothesis. 2*sum(permutedTestStats <= observedTestStat)/N Note The Wilcoxon Rank Sum test is run using the same code except with myTest <- wilcox.test(y ~ x, data=...) instead of t.test(...) in both Stepâ€™s 1 and 2. Other Examples Paired Samples t Test (and Wilcoxon Signed-Rank) (click to show/hide) Paired Data Example See the Sleep Paired t Test example for the background and context of the study. Here is how to perform the test as a permutation test instead of a t test. The question that this sleep data can answer concerns which drug is more effective at increasing the amount of extra sleep an individual receives. The associated hypotheses would be \\[ H_0: \\mu_d = 0 \\] \\[ H_a: \\mu_d \\neq 0 \\] where \\(\\mu_d\\) denotes the true mean of the differences between the observations for each drug obtained from each individual. Differences would be obtained by \\(d_i = \\text{extra}_{1i} - \\text{extra}_{2i}\\). To perform a permutation test of the hypothesis that the drugs are equally effective, we use the following code. # Perform the initial test: myTest <- with(sleep, t.test(extra[group==1], extra[group==2], paired = TRUE, mu = 0)) # Get the test statistic from the test: observedTestStat <- myTest$statistic # Obtain the permutation sampling distribution N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permuteValues <- sample(c(-1,1), size=10, replace=TRUE) permutedTest <- with(sleep, t.test(permuteValues*(extra[group==1] - extra[group==2]), mu = 0)) #Note, t.test(group1 - group2) is the same as t.test(group1, group2, paired=TRUE). permutedTestStats[i] <- permutedTest$statistic } hist(permutedTestStats) abline(v=observedTestStat, col='skyblue', lwd=3) # Greater than p-value: (not what we want here) sum(permutedTestStats >= observedTestStat)/N ## [1] 1 # Less than p-value: sum(permutedTestStats <= observedTestStat)/N ## [1] 0.002 # Correct two sided p-value for this study: 2*sum(permutedTestStats <= observedTestStat)/N ## [1] 0.004 Note: ANOVA (click to show/hide) One-Way ANOVA For this example, we will use the data from the chick weights analysis. # Again, we run the initial test and find the test statistic myTest <- aov(weight ~ feed, data = chickwts) observedTestStat <- summary(myTest)[[1]]$`F value`[1] # For this permutation, we need to shake up the groups similar to the Independent Sample example N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- aov(sample(weight) ~ feed, data = chickwts) permutedTestStats[i] <- summary(permutedTest)[[1]]$`F value`[1] } # The histogram of this distribution gives an interesting insight into the results hist(permutedTestStats, col = \"skyblue\", xlim = c(0,16)) abline(v = observedTestStat, col = \"red\", lwd = 3) # Here is the greater-than p-value (since the F-distribution is right skewed # this is the only p-value of interest.) sum(permutedTestStats >= observedTestStat)/N Two-Way ANOVA For the two-way ANOVA, I will use the data from the warpbreaks analysis. # The initial test is done in the same way as one-way ANOVA but there is a little more to find the test statistic myTest <- aov(breaks ~ wool + tension + wool:tension, data=warpbreaks) # This first test statistic is the comparison between the two types of wool observedTestStatW <- summary(myTest)[[1]]$`F value`[1] # This second test statistic is the comparison between the three types of tension observedTestStatT <- summary(myTest)[[1]]$`F value`[2] # The third test statistic is the comparison of the interaction of wool types and tension observedTestStatWT <- summary(myTest)[[1]]$`F value`[3] # Now comes three different permutations for the test. First is for wool, second is for tension, and third is the interaction N <- 2000 permutedTestStatsW <- rep(NA, N) permutedTestStatsT <- rep(NA, N) permutedTestStatsWT <- rep(NA, N) for (i in 1:N){ permutedTest <- aov(sample(breaks) ~ wool + tension + wool:tension, data=warpbreaks) permutedTestStatsW[i] <- summary(permutedTest)[[1]]$`F value`[1] permutedTestStatsT[i] <- summary(permutedTest)[[1]]$`F value`[2] permutedTestStatsWT[i] <- summary(permutedTest)[[1]]$`F value`[3] } # We likewise need three differenct plots to show the distribution. First is wool, second is tension, and third is the interaction hist(permutedTestStatsW, col = \"skyblue\", xlim = c(3,14)) abline(v = observedTestStatW, col = \"red\", lwd = 3) hist(permutedTestStatsT, col = \"skyblue\") abline(v = observedTestStatT, col = \"red\", lwd = 3) hist(permutedTestStatsWT, col = \"skyblue\") abline(v = observedTestStatWT, col = \"red\", lwd = 3) # Greater-than p-value: the three situations are in order sum(permutedTestStatsW >= observedTestStatW)/N sum(permutedTestStatsT >= observedTestStatT)/N sum(permutedTestStatsWT >= observedTestStatWT)/N # Less-than p-value: again, they are in order sum(permutedTestStatsW <= observedTestStatW)/N sum(permutedTestStatsT <= observedTestStatT)/N sum(permutedTestStatsWT <= observedTestStatWT)/N # Two-sided p-values: 2*sum(permutedTestStatsW >= observedTestStatW)/N 2*sum(permutedTestStatsT >= observedTestStatT)/N 2*sum(permutedTestStatsWT >= observedTestStatWT)/N Simple Linear Regression (click to show/hide) For this example, I will use the trees dataset to compare the Girth and Height of black cherry trees. # The test and then the test statistic is found in a similar way to that of an ANOVA (this is the t statistic) myTest <- lm(Height ~ Girth, data = trees) observedTestStat <- summary(myTest)[[4]][2,3] # The permutation part is set up in this way N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- lm(sample(Height) ~ Girth, data = trees) permutedTestStats[i] <- summary(permutedTest)[[4]][2,3] } # Here, as before, is the histogram of the distribution of the test statistics hist(permutedTestStats, col = \"skyblue\") abline(v = observedTestStat, col = \"red\", lwd = 3) # Less-than p-value: sum(permutedTestStats <= observedTestStat)/N # Greater-than p-value: sum(permutedTestStats >= observedTestStat)/N # Two-Sided p-value: 2*sum(permutedTestStats >= observedTestStat)/N",
    "sentences": ["The most difficult part of a permutation test is in the random permuting of the data.", "How the permuting is performed depends on the type of hypothesis test being performed.", "It is important to remember that the permutation test only changes the way the p-value is calculated.", "Everything else about the original test is unchanged when switching to a permutation test.", "Independent Samples t Test For the independent sample t Test, we will use the data from the independent sleep analysis.", "In that analysis, we were using the sleep data to test the hypotheses: \\[ H_0: \\mu_\\text{Extra Hours of Sleep with Drug 1} - \\mu_\\text{Extra Hours of Sleep with Drug 2} = 0 \\] \\[ H_a: \\mu_\\text{Extra Hours of Sleep with Drug 1} - \\mu_\\text{Extra Hours of Sleep with Drug 2} \\neq 0 \\] We used a significance level of \\(\\alpha = 0.05\\) and obtained a P-value of \\(0.07939\\).", "Letâ€™s demonstrate how a permutation test could be used to obtain this same p-value.", "(Technically you only need to use a permutation test when the requirements of the original test were not satisfied.", "However, it is also reasonable to perform a permutation test anytime you want.", "No requirements need to be checked when performing a permutation test.) # First run the initial test and gain the test statistic: myTest <- t.test(extra ~ group, data = sleep, mu = 0) observedTestStat <- myTest$statistic # Now we run the permutations to create a distribution of test statistics N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- t.test(sample(extra) ~ group, data = sleep, mu = 0) permutedTestStats[i] <- permutedTest$statistic } # Now we show a histogram of that distribution hist(permutedTestStats, col = \"skyblue\") abline(v = observedTestStat, col = \"red\", lwd = 3) #Greater-Than p-value: Not the correct one in this case sum(permutedTestStats >= observedTestStat)/N # Less-Than p-value: Not the correct one for this data sum(permutedTestStats <= observedTestStat)/N # Two-Sided p-value: This is the one we want based on our alternative hypothesis."],
    "type": "section",
    "page_title": "Permutation Tests",
    "section_title": "Explanation"
  },
  {
    "id": 240,
    "title": "R Commands",
    "url": "RCommands.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus R Commands â€œFor the things we have to learn before we can do them, we learn by doing them.â€ â€• Aristotle, The Nicomachean Ethics Getting Started Hover your mouse here to begin. This book requires that you interact with it to learn. Hovering is the first step. Now click right here on these words to get started. The Help Command Getting help in R is easy. Usage ?something This command pulls up the help file for whatever you write in the place of something. Examples Click to view. Hover to learn. The quick way to access the help function in R. cars The name of a dataset can be typed to open the help file for that dataset. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. The quick way to access the help function in R. data The name of an R function, like data can also be used to open the help file for that function. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. The quick way to access the help function in R. mean The mean function computes the mean of a column of quantitative data. Typing the name of an R function, like mean can also be used to open the help file for that function. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. $ The Selection Operator Once you have a dataset, you need to be able to access columns from it. Usage DataSetName$ColumnName The $ operator allows you to access the individual columns of a dataset. Tip: think of the data set as a â€œstoreâ€ from which you â€œpurchaseâ€ a column using â€œmoneyâ€: $. Example Code airquality The airqaulity dataset. This could be the name of any dataset instead of airquality. $ Grabs the column, or variable, from the dataset to be used. This is typically used when computing say the mean (or other statistic) of a single column of the data. Wind The name of any column of the dataset can be entered after the dollar sign. In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality). Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. This allows you to compute things about that column, like the mean or standard deviation. mean( The mean function computes the mean of a column of quantitative data. airquality The airquality dataset. This could be the name of any dataset instead of airquality. $ Grabs the column, or variable, from the dataset to be used. This is typically used when computing say the mean (or other statistic) of a single column of the data. Wind The name of any column of the dataset can be entered after the dollar sign. In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality). ) Closing parenthesis to the mean() function. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. 9.958 sd( The sd function computes the standard deviation of a column of quantitative data. airquality The airqaulity dataset. This could be the name of any dataset instead of airquality. $ Grabs the column, or variable, from the dataset to be used. This is typically used when computing say the mean (or other statistic) of a single column of the data. Wind The name of any column of the dataset can be entered after the dollar sign. In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality). ) Closing parenthesis to the sd() function. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. 3.523 See Numerical Summaries for more stats functions like mean() and sd(). <- The Assignment Operator Being able to save your work is important! Usage Â Â  Keyboard Shortcut: Alt - NameYouCreate <- some R commands <- (Less than symbol < with a hyphen -) is called the assignment operator and lets you store the results of the some R commands into an object called NameYouCreate. NameYouCreate is any name that begins with a letter, but can use numbers, periods, and underscores thereafter. To use spaces in the name, you must use `your Name` encased in back-ticks, but this is not recommended. Example Code cars2 First we name the object we are creating. In this case, we are making a copy of the cars dataset, so it is logical to call it cars2, but it could be bob, c2 or any name you wanted to use. Just be careful to not use names that are already in use! Â  <- Â  The <- assignment operator will take whatever is on the right hand side and save it into the name written on the left hand side. cars In this case the cars dataset is being copied to cars2 so that we can change cars2 without changing the original cars dataset. Â Â Â Â Press Enter to run the code. cars2 The new copy of the cars dataset that we just created $ftpersec The $ selection operator can be used to create a new column in a dataset when used with the <- assignment operator. Â <-Â  The <- assignment operator will take the results of the right-hand-side and save them into the name on the left-hand-side. cars2$speed * 5280 / 3600 This calculation converts the miles per hour of the cars2 speed column into feet per seconds because there are 5280 feet in a mile and 60 minutes in an hour and 60 seconds in a minute. View(cars2) The cars2 dataset now contains a 3rd column called feetpersec. Compare this to the original cars dataset to see how it changed. Â Click to Show OutputÂ  Click to View Output. c( ) The Combine Function Think of this function as the â€œback-packâ€ function, just like putting different books into one back-pack. Usage c(value 1, value 2, value 3, ... ) The c( ) function combines values into a single object called a â€œvectorâ€. values 1, 2, 3, ... can be numbers or characters, i.e., words, but must be all of one type or the other. Example Code Classlist <- Classlist is a new object being created using the assignment operator <- that will contain the four names listed above. Â c( The combine function c( ) is being used in this case to group character values representing names of students into a single object named â€œClasslistâ€. â€œJacksonâ€, â€œJaredâ€, â€œJillâ€, â€œJaneâ€) These are the values we are grouping into the object named Classlist. Â Â Â Â Press Enter to run the code. Ages <- Â  The assignment operator <- is being used to create the object called Ages that will contain the ages of each student on the Classlist. c( The R function â€œc()â€ allows us to group together values in order to save them into an object. 8, 9, 7, 8 The values, separated by commaâ€™s, that are being grouped together. In this case, numbers are being grouped together. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Colors <- Â  The assignment operator <- is being used to create the object called Colors that will have one color for each student on the Classlist. c( The R function â€œc()â€ allows us to group together values in order to save them into an object. â€œredâ€, â€œblueâ€, â€œgreenâ€, â€œyellowâ€ The values, separated by commaâ€™s, that are being grouped together. In this case, characters are being grouped together. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. table( ) This is a way to quickly count how many times each value occurs in a column or columns. Usage table(NameOfDataset$columnName) table(NameOfDataset$columnName1, NameOfDataset$columnName2) The table( ) function counts how many times each value in a column of data occurs. NameOfDataset is the ane of a data set, like cars or airquality or KidsFeet. columnName is the name of a column from the data set. columnName1 and columnName2 are two different names of columns from the data set. Example Code speedCounts <-speedCounts is a new object being created using the assignment operator <- that will contain the counts of how many times each â€œspeedâ€ occurs in the cars data set speed column. Â table( The table function table( ) is being used in this case to count how many times each speed occurs in the cars data set speed column. cars This is the name of the data set. $ The $ is used to access a given column from the data set. speed This is the name of the column we are interested in from the cars data set. ) Always close off your functions in R with a closing parathesis. speedCounts Typing the name of an object will print the results to the screen. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. ## ## 4 7 8 9 10 11 12 13 14 15 16 17 18 19 20 22 23 24 25 ## 2 2 1 1 3 2 4 4 4 3 2 3 4 3 5 1 1 4 1 Notice how the speed of â€œ4â€ occurs 2 times, same for the speed of 7, but the speed of 8 only occurs 1 time and so on with the other speeds. The first row of the output is the value from the speed column. The number on the second line shows how many times that value occurred in the speed column. library(mosaic) library(mosaic) is needed to access the KidsFeet data set that is used in this example. If you donâ€™t have the mosaic library, you will need to run install.packages(\"mosaic\") to install it first. From then on, you can open mosaic to use it with the command library(mosaic). You need only install packages once. You must library them each time you wish to use them. birthdays <-birthdays is a new object being created using the assignment operator <- that will contain the counts of how many birthdays occur in each month for each gender in the KidsFeet dataset. Â table( The table function table( ) is being used in this case to count how many birthdays occur in each month for children of each gender. KidsFeet This is the name of the data set. $ The $ is used to access a given column from the data set. sex This is the name of the column we are interested in becoming the rows of our final table. ,Â  Comma separating the two columns of the data set you want to table. KidsFeet This is the name of the data set. $ The $ is used to access a given column from the data set. birthmonth This is the name of the column we are interested in becoming the columns of our final table. ) Always close off your functions in R with a closing parathesis. birthdays Typing the name of an object will print the results to the screen. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. ## ## 1 2 3 4 5 6 7 8 9 10 11 12 ## B 1 2 3 2 1 1 2 2 2 1 1 2 ## G 1 1 5 1 1 3 1 0 3 1 1 1 The left column contains the â€œsexâ€ values of â€œBâ€ and â€œGâ€ (Boy and Girl). The top row contains the birthmonths (1 through 12). The numbers within the row of the table next to the â€œBâ€ show how many Boys had birthdays in each month of the year. The numbers within the row of the table next to the â€œGâ€ show how many Girls had birthdays in each month of the year. filter( ) Used to reduce a dataset to a smaller set of rows than the original dataset contained. Usage filter(NameOfDataset, columnName filteringRules) filter() is the function that filters out certain rows of the dataset. NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet. columnName is the name of one of the columns from the dataset. You can use colnames(NameOfDataset) or View(NameOfDataset) to see the names. filteringRules consists of some Logical Expression (see table below) that selects only the rows from the original dataset that meet the criterion. Filtering Rule Logical Expression Equals one â€œthingâ€ columnName == something Equals Any Of Several â€œthingsâ€ columnName %in% c(something1,something2,...) Not Equal (one thing) columnName != something Not Equals Any of (several things) !columnName %in% c(something1,something2,...) Less Than columnName < value Less Then or Equal to columnName <= value Greater Than columnName > value Greater Than or Equal to columnName >= value AND expression1 & expression2 OR expression1 | expression2 Equals NA is.na(columnName) Not NA !is.na(columnName) Example Code library(tidyverse) The tidyverse library is needed to access the filter function used in the following example codes. library(mosaic) The mosaic library is needed to access the KidsFeet data set used in the following example codes. Equals one â€œthingâ€â€¦ Kids87 <-Â  Kids87 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet, Â  â€œfilterâ€ is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. birthyear A quantitative column of the KidsFeet dataset that we want to use to reduce the dataset. Â == 87 This â€œfiltering ruleâ€ filters the data down to just those children who had a birthyear equal to 87. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. KidsBoys <-Â  KidsBoys is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet, Â  â€œfilterâ€ is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. sex A categorical column of the KidsFeet dataset that we want to use to reduce the dataset. Â == â€œBâ€ This â€œfiltering ruleâ€ filters the data down to just those children who are boys. Words must be quoted â€œBâ€ but values are just typed directly. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. Equals Any of Several â€œthingsâ€â€¦ KidsSummer <-Â  KidsSummer is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet, Â  â€œfilterâ€ is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. birthmonth The co",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus R Commands â€œFor the things we have to learn before we can do them, we learn by doing them.â€ â€• Aristotle, The Nicomachean Ethics Getting Started Hover your mouse here to begin.", "This book requires that you interact with it to learn.", "Hovering is the first step.", "Now click right here on these words to get started.", "The Help Command Getting help in R is easy.", "Usage ?something This command pulls up the help file for whatever you write in the place of something.", "Examples Click to view.", "Hover to learn.", "The quick way to access the help function in R.", "cars The name of a dataset can be typed to open the help file for that dataset.", "Â Â Â Â Press Enter to run the code.", "Â Click to Show OutputÂ  Click to View Output.", "The quick way to access the help function in R.", "data The name of an R function, like data can also be used to open the help file for that function.", "Â Â Â Â Press Enter to run the code.", "Â Click to Show OutputÂ  Click to View Output.", "The quick way to access the help function in R.", "mean The mean function computes the mean of a column of quantitative data.", "Typing the name of an R function, like mean can also be used to open the help file for that function.", "Â Â Â Â Press Enter to run the code.", "Â Click to Show OutputÂ  Click to View Output.", "$ The Selection Operator Once you have a dataset, you need to be able to access columns from it.", "Usage DataSetName$ColumnName The $ operator allows you to access the individual columns of a dataset.", "Tip: think of the data set as a â€œstoreâ€ from which you â€œpurchaseâ€ a column using â€œmoneyâ€: $.", "Example Code airquality The airqaulity dataset.", "This could be the name of any dataset instead of airquality.", "$ Grabs the column, or variable, from the dataset to be used.", "This is typically used when computing say the mean (or other statistic) of a single column of the data.", "Wind The name of any column of the dataset can be entered after the dollar sign.", "In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality).", "Â Â Â Â Press Enter to run the code.", "Â Click to Show OutputÂ  Click to View Output.", "This allows you to compute things about that column, like the mean or standard deviation.", "mean( The mean function computes the mean of a column of quantitative data.", "airquality The airquality dataset.", "This could be the name of any dataset instead of airquality.", "$ Grabs the column, or variable, from the dataset to be used.", "This is typically used when computing say the mean (or other statistic) of a single column of the data.", "Wind The name of any column of the dataset can be entered after the dollar sign.", "In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality).", ") Closing parenthesis to the mean() function.", "Â Â Â Â Press Enter to run the code.", "Â Click to Show OutputÂ  Click to View Output.", "9.958 sd( The sd function computes the standard deviation of a column of quantitative data.", "airquality The airqaulity dataset.", "This could be the name of any dataset instead of airquality.", "$ Grabs the column, or variable, from the dataset to be used.", "This is typically used when computing say the mean (or other statistic) of a single column of the data.", "Wind The name of any column of the dataset can be entered after the dollar sign.", "In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality)."],
    "type": "page",
    "page_title": "R Commands"
  },
  {
    "id": 241,
    "title": "ğŸ“ Getting Started",
    "url": "RCommands.html#gettingstarted",
    "content": "Getting Started Hover your mouse here to begin. This book requires that you interact with it to learn. Hovering is the first step. Now click right here on these words to get started.",
    "sentences": ["Hover your mouse here to begin.", "This book requires that you interact with it to learn.", "Hovering is the first step.", "Now click right here on these words to get started."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "Getting Started"
  },
  {
    "id": 242,
    "title": "ğŸ“ ? The Help Command",
    "url": "RCommands.html#thehelpcommand",
    "content": "? The Help Command Getting help in R is easy. Usage ?something This command pulls up the help file for whatever you write in the place of something. Examples Click to view. Hover to learn. The quick way to access the help function in R. cars The name of a dataset can be typed to open the help file for that dataset. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. The quick way to access the help function in R. data The name of an R function, like data can also be used to open the help file for that function. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. The quick way to access the help function in R. mean The mean function computes the mean of a column of quantitative data. Typing the name of an R function, like mean can also be used to open the help file for that function. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output.",
    "sentences": ["Getting help in R is easy.", "Usage ?something This command pulls up the help file for whatever you write in the place of something.", "Examples Click to view.", "Hover to learn.", "The quick way to access the help function in R.", "cars The name of a dataset can be typed to open the help file for that dataset.", "Â Â Â Â Press Enter to run the code.", "Â Click to Show OutputÂ  Click to View Output.", "The quick way to access the help function in R.", "data The name of an R function, like data can also be used to open the help file for that function."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "? The Help Command"
  },
  {
    "id": 243,
    "title": "ğŸ“ $ The Selection Operator",
    "url": "RCommands.html#theselectionoperator",
    "content": "$ The Selection Operator Once you have a dataset, you need to be able to access columns from it. Usage DataSetName$ColumnName The $ operator allows you to access the individual columns of a dataset. Tip: think of the data set as a â€œstoreâ€ from which you â€œpurchaseâ€ a column using â€œmoneyâ€: $. Example Code airquality The airqaulity dataset. This could be the name of any dataset instead of airquality. $ Grabs the column, or variable, from the dataset to be used. This is typically used when computing say the mean (or other statistic) of a single column of the data. Wind The name of any column of the dataset can be entered after the dollar sign. In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality). Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. This allows you to compute things about that column, like the mean or standard deviation. mean( The mean function computes the mean of a column of quantitative data. airquality The airquality dataset. This could be the name of any dataset instead of airquality. $ Grabs the column, or variable, from the dataset to be used. This is typically used when computing say the mean (or other statistic) of a single column of the data. Wind The name of any column of the dataset can be entered after the dollar sign. In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality). ) Closing parenthesis to the mean() function. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. 9.958 sd( The sd function computes the standard deviation of a column of quantitative data. airquality The airqaulity dataset. This could be the name of any dataset instead of airquality. $ Grabs the column, or variable, from the dataset to be used. This is typically used when computing say the mean (or other statistic) of a single column of the data. Wind The name of any column of the dataset can be entered after the dollar sign. In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality). ) Closing parenthesis to the sd() function. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. 3.523 See Numerical Summaries for more stats functions like mean() and sd().",
    "sentences": ["Once you have a dataset, you need to be able to access columns from it.", "Usage DataSetName$ColumnName The $ operator allows you to access the individual columns of a dataset.", "Tip: think of the data set as a â€œstoreâ€ from which you â€œpurchaseâ€ a column using â€œmoneyâ€: $.", "Example Code airquality The airqaulity dataset.", "This could be the name of any dataset instead of airquality.", "$ Grabs the column, or variable, from the dataset to be used.", "This is typically used when computing say the mean (or other statistic) of a single column of the data.", "Wind The name of any column of the dataset can be entered after the dollar sign.", "In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality).", "Â Â Â Â Press Enter to run the code."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "$ The Selection Operator"
  },
  {
    "id": 244,
    "title": "ğŸ“ <- The Assignment Operator",
    "url": "RCommands.html#theassignmentoperator",
    "content": "<- The Assignment Operator Being able to save your work is important! Usage Â Â  Keyboard Shortcut: Alt - NameYouCreate <- some R commands <- (Less than symbol < with a hyphen -) is called the assignment operator and lets you store the results of the some R commands into an object called NameYouCreate. NameYouCreate is any name that begins with a letter, but can use numbers, periods, and underscores thereafter. To use spaces in the name, you must use `your Name` encased in back-ticks, but this is not recommended. Example Code cars2 First we name the object we are creating. In this case, we are making a copy of the cars dataset, so it is logical to call it cars2, but it could be bob, c2 or any name you wanted to use. Just be careful to not use names that are already in use! Â  <- Â  The <- assignment operator will take whatever is on the right hand side and save it into the name written on the left hand side. cars In this case the cars dataset is being copied to cars2 so that we can change cars2 without changing the original cars dataset. Â Â Â Â Press Enter to run the code. cars2 The new copy of the cars dataset that we just created $ftpersec The $ selection operator can be used to create a new column in a dataset when used with the <- assignment operator. Â <-Â  The <- assignment operator will take the results of the right-hand-side and save them into the name on the left-hand-side. cars2$speed * 5280 / 3600 This calculation converts the miles per hour of the cars2 speed column into feet per seconds because there are 5280 feet in a mile and 60 minutes in an hour and 60 seconds in a minute. View(cars2) The cars2 dataset now contains a 3rd column called feetpersec. Compare this to the original cars dataset to see how it changed. Â Click to Show OutputÂ  Click to View Output.",
    "sentences": ["Being able to save your work is important!", "Usage Â Â  Keyboard Shortcut: Alt - NameYouCreate <- some R commands <- (Less than symbol < with a hyphen -) is called the assignment operator and lets you store the results of the some R commands into an object called NameYouCreate.", "NameYouCreate is any name that begins with a letter, but can use numbers, periods, and underscores thereafter.", "To use spaces in the name, you must use `your Name` encased in back-ticks, but this is not recommended.", "Example Code cars2 First we name the object we are creating.", "In this case, we are making a copy of the cars dataset, so it is logical to call it cars2, but it could be bob, c2 or any name you wanted to use.", "Just be careful to not use names that are already in use!", "Â  <- Â  The <- assignment operator will take whatever is on the right hand side and save it into the name written on the left hand side.", "cars In this case the cars dataset is being copied to cars2 so that we can change cars2 without changing the original cars dataset.", "Â Â Â Â Press Enter to run the code."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "<- The Assignment Operator"
  },
  {
    "id": 245,
    "title": "ğŸ“ c( ) The Combine Function",
    "url": "RCommands.html#cthecombinefunction",
    "content": "c( ) The Combine Function Think of this function as the â€œback-packâ€ function, just like putting different books into one back-pack. Usage c(value 1, value 2, value 3, ... ) The c( ) function combines values into a single object called a â€œvectorâ€. values 1, 2, 3, ... can be numbers or characters, i.e., words, but must be all of one type or the other. Example Code Classlist <- Classlist is a new object being created using the assignment operator <- that will contain the four names listed above. Â c( The combine function c( ) is being used in this case to group character values representing names of students into a single object named â€œClasslistâ€. â€œJacksonâ€, â€œJaredâ€, â€œJillâ€, â€œJaneâ€) These are the values we are grouping into the object named Classlist. Â Â Â Â Press Enter to run the code. Ages <- Â  The assignment operator <- is being used to create the object called Ages that will contain the ages of each student on the Classlist. c( The R function â€œc()â€ allows us to group together values in order to save them into an object. 8, 9, 7, 8 The values, separated by commaâ€™s, that are being grouped together. In this case, numbers are being grouped together. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Colors <- Â  The assignment operator <- is being used to create the object called Colors that will have one color for each student on the Classlist. c( The R function â€œc()â€ allows us to group together values in order to save them into an object. â€œredâ€, â€œblueâ€, â€œgreenâ€, â€œyellowâ€ The values, separated by commaâ€™s, that are being grouped together. In this case, characters are being grouped together. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output.",
    "sentences": ["Think of this function as the â€œback-packâ€ function, just like putting different books into one back-pack.", "Usage c(value 1, value 2, value 3, ...", ") The c( ) function combines values into a single object called a â€œvectorâ€.", "values 1, 2, 3, ...", "can be numbers or characters, i.e., words, but must be all of one type or the other.", "Example Code Classlist <- Classlist is a new object being created using the assignment operator <- that will contain the four names listed above.", "Â c( The combine function c( ) is being used in this case to group character values representing names of students into a single object named â€œClasslistâ€.", "â€œJacksonâ€, â€œJaredâ€, â€œJillâ€, â€œJaneâ€) These are the values we are grouping into the object named Classlist.", "Â Â Â Â Press Enter to run the code.", "Ages <- Â  The assignment operator <- is being used to create the object called Ages that will contain the ages of each student on the Classlist."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "c( ) The Combine Function"
  },
  {
    "id": 246,
    "title": "ğŸ“ table( )",
    "url": "RCommands.html#table",
    "content": "table( ) This is a way to quickly count how many times each value occurs in a column or columns. Usage table(NameOfDataset$columnName) table(NameOfDataset$columnName1, NameOfDataset$columnName2) The table( ) function counts how many times each value in a column of data occurs. NameOfDataset is the ane of a data set, like cars or airquality or KidsFeet. columnName is the name of a column from the data set. columnName1 and columnName2 are two different names of columns from the data set. Example Code speedCounts <-speedCounts is a new object being created using the assignment operator <- that will contain the counts of how many times each â€œspeedâ€ occurs in the cars data set speed column. Â table( The table function table( ) is being used in this case to count how many times each speed occurs in the cars data set speed column. cars This is the name of the data set. $ The $ is used to access a given column from the data set. speed This is the name of the column we are interested in from the cars data set. ) Always close off your functions in R with a closing parathesis. speedCounts Typing the name of an object will print the results to the screen. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. ## ## 4 7 8 9 10 11 12 13 14 15 16 17 18 19 20 22 23 24 25 ## 2 2 1 1 3 2 4 4 4 3 2 3 4 3 5 1 1 4 1 Notice how the speed of â€œ4â€ occurs 2 times, same for the speed of 7, but the speed of 8 only occurs 1 time and so on with the other speeds. The first row of the output is the value from the speed column. The number on the second line shows how many times that value occurred in the speed column. library(mosaic) library(mosaic) is needed to access the KidsFeet data set that is used in this example. If you donâ€™t have the mosaic library, you will need to run install.packages(\"mosaic\") to install it first. From then on, you can open mosaic to use it with the command library(mosaic). You need only install packages once. You must library them each time you wish to use them. birthdays <-birthdays is a new object being created using the assignment operator <- that will contain the counts of how many birthdays occur in each month for each gender in the KidsFeet dataset. Â table( The table function table( ) is being used in this case to count how many birthdays occur in each month for children of each gender. KidsFeet This is the name of the data set. $ The $ is used to access a given column from the data set. sex This is the name of the column we are interested in becoming the rows of our final table. ,Â  Comma separating the two columns of the data set you want to table. KidsFeet This is the name of the data set. $ The $ is used to access a given column from the data set. birthmonth This is the name of the column we are interested in becoming the columns of our final table. ) Always close off your functions in R with a closing parathesis. birthdays Typing the name of an object will print the results to the screen. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. ## ## 1 2 3 4 5 6 7 8 9 10 11 12 ## B 1 2 3 2 1 1 2 2 2 1 1 2 ## G 1 1 5 1 1 3 1 0 3 1 1 1 The left column contains the â€œsexâ€ values of â€œBâ€ and â€œGâ€ (Boy and Girl). The top row contains the birthmonths (1 through 12). The numbers within the row of the table next to the â€œBâ€ show how many Boys had birthdays in each month of the year. The numbers within the row of the table next to the â€œGâ€ show how many Girls had birthdays in each month of the year.",
    "sentences": ["This is a way to quickly count how many times each value occurs in a column or columns.", "Usage table(NameOfDataset$columnName) table(NameOfDataset$columnName1, NameOfDataset$columnName2) The table( ) function counts how many times each value in a column of data occurs.", "NameOfDataset is the ane of a data set, like cars or airquality or KidsFeet.", "columnName is the name of a column from the data set.", "columnName1 and columnName2 are two different names of columns from the data set.", "Example Code speedCounts <-speedCounts is a new object being created using the assignment operator <- that will contain the counts of how many times each â€œspeedâ€ occurs in the cars data set speed column.", "Â table( The table function table( ) is being used in this case to count how many times each speed occurs in the cars data set speed column.", "cars This is the name of the data set.", "$ The $ is used to access a given column from the data set.", "speed This is the name of the column we are interested in from the cars data set."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "table( )"
  },
  {
    "id": 247,
    "title": "ğŸ“ filter( )",
    "url": "RCommands.html#filter",
    "content": "filter( ) Used to reduce a dataset to a smaller set of rows than the original dataset contained. Usage filter(NameOfDataset, columnName filteringRules) filter() is the function that filters out certain rows of the dataset. NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet. columnName is the name of one of the columns from the dataset. You can use colnames(NameOfDataset) or View(NameOfDataset) to see the names. filteringRules consists of some Logical Expression (see table below) that selects only the rows from the original dataset that meet the criterion. Filtering Rule Logical Expression Equals one â€œthingâ€ columnName == something Equals Any Of Several â€œthingsâ€ columnName %in% c(something1,something2,...) Not Equal (one thing) columnName != something Not Equals Any of (several things) !columnName %in% c(something1,something2,...) Less Than columnName < value Less Then or Equal to columnName <= value Greater Than columnName > value Greater Than or Equal to columnName >= value AND expression1 & expression2 OR expression1 | expression2 Equals NA is.na(columnName) Not NA !is.na(columnName) Example Code library(tidyverse) The tidyverse library is needed to access the filter function used in the following example codes. library(mosaic) The mosaic library is needed to access the KidsFeet data set used in the following example codes. Equals one â€œthingâ€â€¦ Kids87 <-Â  Kids87 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet, Â  â€œfilterâ€ is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. birthyear A quantitative column of the KidsFeet dataset that we want to use to reduce the dataset. Â == 87 This â€œfiltering ruleâ€ filters the data down to just those children who had a birthyear equal to 87. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. KidsBoys <-Â  KidsBoys is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet, Â  â€œfilterâ€ is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. sex A categorical column of the KidsFeet dataset that we want to use to reduce the dataset. Â == â€œBâ€ This â€œfiltering ruleâ€ filters the data down to just those children who are boys. Words must be quoted â€œBâ€ but values are just typed directly. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. Equals Any of Several â€œthingsâ€â€¦ KidsSummer <-Â  KidsSummer is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet, Â  â€œfilterâ€ is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. birthmonth The column of the KidsFeet dataset that we want to use to reduce the dataset. Â %in% c(6,7,8) This is the â€œfiltering ruleâ€. It will filter the data down to just those children who were born during the summer, i.e., birthmonth equal to either 6, 7, or 8. Notice how the c( ) function is being used to combine the values of 6, 7, and 8 together into a single list of numbers. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. Does not equal one thingâ€¦ KidsNotJosh <-Â  KidsNotJosh is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet, Â  â€œfilterâ€ is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. name The column of the KidsFeet dataset that we want to use to reduce the dataset. Â != â€œJoshâ€ This is the â€œfiltering ruleâ€. It will filter the data down to just those children who are NOT named â€œJoshâ€. In this case, it removed just two students who were named â€œJoshâ€. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. Less thanâ€¦ KidsLength24 <-Â  KidsLength24 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet, Â  â€œfilterâ€ is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. length The column of the KidsFeet dataset that we want to use to reduce the dataset. Â < 24 This is the â€œfiltering ruleâ€. It will filter the data down to just those children who have a foot length less than 24 cm. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. Less than or equal toâ€¦ KidsLessEq24 <-Â  KidsLessEq24 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet, Â  â€œfilterâ€ is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. length The column of the KidsFeet dataset that we want to use to reduce the dataset. Â <= 24 This is the â€œfiltering ruleâ€. It will filter the data down to just those children who have a foot length less than or equal to 24 cm. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. Greater thanâ€¦ KidsWider9 <-Â  KidsNotJosh is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet, Â  â€œfilterâ€ is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. width The column of the KidsFeet dataset that we want to use to reduce the dataset. Â > 9 This is the â€œfiltering ruleâ€. It will filter the data down to just those children who have a foot width greater than 9 cm. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. Greater than or equal toâ€¦ KidsWiderEq9 <-Â  KidsWiderEq9 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet, Â  â€œfilterâ€ is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. width The column of the KidsFeet dataset that we want to use to reduce the dataset. Â >= 9 This is the â€œfiltering ruleâ€. It will filter the data down to just those children who have a foot width greater than or equal to 9 cm. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. The â€œandâ€ statementâ€¦ GirlsWide9 <-Â  GirlsWide9 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet, Â  â€œfilterâ€ is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. sex The first column of the KidsFeet dataset that we want to use to reduce the dataset. Â == â€œGâ€ This is the first â€œfiltering ruleâ€. It will filter the data down to just those children who are girls. Â &Â  The & is the AND statement. It joins to filtering criteria together into a single criteria where both conditions must be met. In this case, it ensures we get only girls with foot widths greater than 9 cm. width The second column of the KidsFeet dataset that we want to use to reduce the dataset. Â > 9 This is the second â€œfiltering ruleâ€. It will filter the data down to just those children who have a foot width greater than 9 cm. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. The â€œorâ€ statementâ€¦ KidsWinter <-Â  KidsWinter is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet, Â  â€œfilterâ€ is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. birthmonth The first column of the KidsFeet dataset that we want to use to reduce the dataset. Â <= 2 This is the first â€œfiltering ruleâ€. It will filter the data down to just those children who are born in January or February. Â |Â  The | is the OR statement. It joins to filtering criteria together into a single criteria where either condition gives us what we want. In this case, it keeps any child born in January, February, November, or December. birthmonth The second column of the KidsFeet dataset that we want to use to reduce the dataset. In this case, it is the same as the first column. Â >= 11 This is the second â€œfiltering ruleâ€. It will filter the data down to just those children who are born in November or December. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output.",
    "sentences": ["Used to reduce a dataset to a smaller set of rows than the original dataset contained.", "Usage filter(NameOfDataset, columnName filteringRules) filter() is the function that filters out certain rows of the dataset.", "NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet.", "columnName is the name of one of the columns from the dataset.", "You can use colnames(NameOfDataset) or View(NameOfDataset) to see the names.", "filteringRules consists of some Logical Expression (see table below) that selects only the rows from the original dataset that meet the criterion.", "Filtering Rule Logical Expression Equals one â€œthingâ€ columnName == something Equals Any Of Several â€œthingsâ€ columnName %in% c(something1,something2,...) Not Equal (one thing) columnName != something Not Equals Any of (several things) !columnName %in% c(something1,something2,...) Less Than columnName < value Less Then or Equal to columnName <= value Greater Than columnName > value Greater Than or Equal to columnName >= value AND expression1 & expression2 OR expression1 | expression2 Equals NA is.na(columnName) Not NA !is.na(columnName) Example Code library(tidyverse) The tidyverse library is needed to access the filter function used in the following example codes.", "library(mosaic) The mosaic library is needed to access the KidsFeet data set used in the following example codes.", "Equals one â€œthingâ€â€¦ Kids87 <-Â  Kids87 is a name we made up.", "The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "filter( )"
  },
  {
    "id": 248,
    "title": "ğŸ“ select( )",
    "url": "RCommands.html#select",
    "content": "select( ) Used to select out certain columns from a dataset. Usage select(NameOfDataset, listOfColumnNames) select( ) is the function that selects out certain columns of the dataset. NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet. listOfColumnNames is a vector of names of columns from the dataset, usually supplied inside a combine c(...) statement. Example Code KidsNameBirth <-Â  KidsNameBirth is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the select(...) function into this name. select(KidsFeet, Â  â€œselectâ€ is a function from library(tidyverse) that selects out specified columns from the original dataset in the order specified. c(name, birthyear, birthmonth) The columns of the KidsFeet dataset that we want to select out of the original dataset. Notice how the concatenation function c(...) is used to list out the columns we want. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. KidsBigLength <-Â  KidsBigLength is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the select(...) function into this name. select(KidsFeet, Â  â€œselectâ€ is a function from library(tidyverse) that selects out specified columns from the original dataset in the order specified. c(biggerfoot, length) The columns of the KidsFeet dataset that we want to select out of the original dataset. The order in which columns are selected is the order in which they are placed in the new data set. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output.",
    "sentences": ["Used to select out certain columns from a dataset.", "Usage select(NameOfDataset, listOfColumnNames) select( ) is the function that selects out certain columns of the dataset.", "NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet.", "listOfColumnNames is a vector of names of columns from the dataset, usually supplied inside a combine c(...) statement.", "Example Code KidsNameBirth <-Â  KidsNameBirth is a name we made up.", "The assignment operator <- will save the reduced version of the KidsFeet dataset created by the select(...) function into this name.", "select(KidsFeet, Â  â€œselectâ€ is a function from library(tidyverse) that selects out specified columns from the original dataset in the order specified.", "c(name, birthyear, birthmonth) The columns of the KidsFeet dataset that we want to select out of the original dataset.", "Notice how the concatenation function c(...) is used to list out the columns we want.", ") Always close off your functions in R with a closing parathesis."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "select( )"
  },
  {
    "id": 249,
    "title": "ğŸ“ %>% The Pipe Operator",
    "url": "RCommands.html#thepipeoperator",
    "content": "%>% The Pipe Operator Just like the pipes in your kitchen sink, the pipe operator takes â€œwater from the sinkâ€ and â€œsends it down to somewhere else.â€ Usage Â Â  Keyboard Shortcut: Ctrl Shift M NameOfDataset %>% Â Â some R commands that follow on the next line %>%, the pipe operator, is created by typing percent symbols % on both sides of a greater than symbol >. It lets you take whatever is on the left of the symbol and â€œpipe it down intoâ€ some R commands that follow on the next line. NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet. Note: you should load library(tidyverse) before using the %>% operator. Example Code Kids2 <-Â  This provides a name for the new reduced version of the KidsFeet dataset that is going to be created by the combined use of filter(...) and select(...). KidsFeet KidsFeet is a dataset found in library(mosaic). Click on this code to View the dataset and the resulting Kids2 dataset. Â %>%Â  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line. Â Â  filter( â€œfilterâ€ is a function from library(tidyverse) that allows us to reduce the number of rows in the KidsFeet dataset by filtering according to certain criteria. birthyear Represents the column of data that we want to use to reduce the rows of the dataset. Â == 87 This is the â€œfiltering ruleâ€. It will filter the data down to just those children who had a birthyear equal to 87. ) Always close off your functions in R with a closing parathesis. Â %>%Â  The pipe operator that will send the filtered version of the KidsFeet dataset down inside of the code on the following line. Â Â  select( â€œselectâ€ is a function from library(tidyverse) that selects out specified columns from the current dataset in the order specified. c(name, birthyear, length) The columns of the filtered KidsFeet dataset that we want to select. Notice how the concatenation function c(...) is used to list out the columns we want. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output.",
    "sentences": ["Just like the pipes in your kitchen sink, the pipe operator takes â€œwater from the sinkâ€ and â€œsends it down to somewhere else.â€ Usage Â Â  Keyboard Shortcut: Ctrl Shift M NameOfDataset %>% Â Â some R commands that follow on the next line %>%, the pipe operator, is created by typing percent symbols % on both sides of a greater than symbol >.", "It lets you take whatever is on the left of the symbol and â€œpipe it down intoâ€ some R commands that follow on the next line.", "NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet.", "Note: you should load library(tidyverse) before using the %>% operator.", "Example Code Kids2 <-Â  This provides a name for the new reduced version of the KidsFeet dataset that is going to be created by the combined use of filter(...) and select(...).", "KidsFeet KidsFeet is a dataset found in library(mosaic).", "Click on this code to View the dataset and the resulting Kids2 dataset.", "Â %>%Â  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line.", "Â Â  filter( â€œfilterâ€ is a function from library(tidyverse) that allows us to reduce the number of rows in the KidsFeet dataset by filtering according to certain criteria.", "birthyear Represents the column of data that we want to use to reduce the rows of the dataset."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "%>% The Pipe Operator"
  },
  {
    "id": 250,
    "title": "ğŸ“ summarise( ) and group_by( )",
    "url": "RCommands.html#summariseandgroupby",
    "content": "summarise( ) and group_by( ) Compute numerical summaries on data or on groupings within the data. Usage NameofDataset %>% Â Â  summarise(nameYouLike = some_stats_function(columnName)) OR NameofDataset %>% Â Â  group_by(columnGroupsName) %>% Â Â  summarise(nameYouLike = some_stats_function(columnName)) NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet. %>% is the pipe operator that â€œpipes dataâ€ down into R commands on the next line. group_by(...) is an R function from library(tidyverse) that groups data according to a specified column (or columns). summarise(...) is an R function from library(tidyverse) that computes numerical summaries on data or groups of data. columnGroupsName is the name of a column that represents qualitative (categorical) data. This column is used to separate the dataset into little datasets, one â€œlittle datasetâ€ for each group or category in the columnGroupsName column. nameYouLike is just that. Some name you come up with. some_stats_function(...) is a stats function like mean(...), sd(...), n(...) or so on. columnName is the name of a column from the dataset that you want to compute numerical summaries on. Example Code KidsFeet KidsFeet is a dataset found in library(mosaic). Â %>%Â  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveLength A name we came up with that will store the results of the numerical summary. Â = mean(length) This computes the mean(...) of the length column from the KidsFeet dataset. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. aveLength 24.72 KidsFeet KidsFeet is a dataset found in library(mosaic). Â %>%Â  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveLength A name we came up with that will store the results of the numerical summary. Â = mean(length),Â  This computes the mean(...) of the length column from the KidsFeet dataset. Â Â Â Â Â Â Â Â Â Â Â Â  sdLength A name we came up with that will store the results of the numerical summary. Â = sd(length),Â  This computes the sd(...) of the length column from the KidsFeet dataset. Â Â Â Â Â Â Â Â Â Â Â Â  sampleSize A name we came up with that will store the results of the numerical summary. Â = n( ) This computes the n(...), or sample size, of the length column from the KidsFeet dataset. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. aveLength sdLength sampleSize 24.72 1.318 39 KidsFeet KidsFeet is a dataset found in library(mosaic). Â %>%Â  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the dataset up into â€œlittle groupsâ€ according to the column specified. sex â€œsexâ€ is a column from the KidsFeet dataset that records the gender of each child. ) Always close off your functions in R with a closing parathesis. Â %>%Â  The pipe operator that will send the grouped according to gender version of the KidsFeet dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveLength A name we came up with that will store the results of the numerical summary. Â = mean(length),Â  This computes the mean(...) of the length column from the KidsFeet dataset. Â Â Â Â Â Â Â Â Â Â Â Â  sdLength A name we came up with that will store the results of the numerical summary. Â = sd(length),Â  This computes the sd(...) of the length column from the KidsFeet dataset. Â Â Â Â Â Â Â Â Â Â Â Â  sampleSize A name we came up with that will store the results of the numerical summary. Â = n( ) This computes the n(...), or sample size, of the length column from the KidsFeet dataset. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. sex aveLength sdLength sampleSize B 25.11 1.217 20 G 24.32 1.33 19 For more uses of summarise(...) and group_by(...) see the Example codes on the various â€œR Instructionsâ€ of the Numerical Summaries page.",
    "sentences": ["Compute numerical summaries on data or on groupings within the data.", "Usage NameofDataset %>% Â Â  summarise(nameYouLike = some_stats_function(columnName)) OR NameofDataset %>% Â Â  group_by(columnGroupsName) %>% Â Â  summarise(nameYouLike = some_stats_function(columnName)) NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet.", "%>% is the pipe operator that â€œpipes dataâ€ down into R commands on the next line.", "group_by(...) is an R function from library(tidyverse) that groups data according to a specified column (or columns).", "summarise(...) is an R function from library(tidyverse) that computes numerical summaries on data or groups of data.", "columnGroupsName is the name of a column that represents qualitative (categorical) data.", "This column is used to separate the dataset into little datasets, one â€œlittle datasetâ€ for each group or category in the columnGroupsName column.", "nameYouLike is just that.", "Some name you come up with.", "some_stats_function(...) is a stats function like mean(...), sd(...), n(...) or so on."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "summarise( ) and group_by( )"
  },
  {
    "id": 251,
    "title": "ğŸ“ mutate( )",
    "url": "RCommands.html#mutate",
    "content": "mutate( ) Transform a column or add a new column of data to a data set. Usage NameofDataset %>% Â Â  mutate(nameYouLike = some_transformation) NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet. %>% is the pipe operator that â€œpipes dataâ€ down into R commands on the next line. nameYouLike is just that. Some name you come up with that will be the name of a new column in the dataset. some_transformation is just that. See the example codes for ideas. Example Code mtcars2 <-Â  mtcars2 is a new dataset we are creating that will contain all of mtcars data set along with a couple new columns we are creating. mtcars mtcars is a dataset found in base R. Typing View(mtcars) and ?mtcars in the console will help you learn more about the dataset. Â %>%Â  The pipe operator that will send the mtcars dataset down inside of the code on the following line. Â Â  mutate( â€œmutateâ€ is a function from library(tidyverse) that allows us to transform columns of data. Â Â Â Â Â Â Â Â Â cyl_factor = as.factor(cyl), â€œcyl_factorâ€ is a name we came up with that will store the results of the transformation of the â€œcylâ€ column. Here we are simply converting the â€œcylâ€ column from type numeric to a factor. Treating the â€œcylâ€ column as a factor could be useful in certain situations. Â Â Â Â Â Â Â Â Â weight = wt * 1000 â€œweightâ€ is a name we came up with that will store the results of the transformation of the â€œwtâ€ column. Taking a closer look with ?mtcars shows us that wt is in 1000 lbs. Here we are just multiplying each row in the column by 1000. Â Â Â ) Closing parenthesis for the mutate(â€¦) function. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. Notice the last two columns of cyl_factor and weight are what we added to the data set mtcars2. As seen by View(mtcars), the original mtcars data set did not have these two columns. We used the mutate function to create them and saved our work in a new data set called mtcars2. Â  mpg cyl disp hp drat wt qsec vs am gear carb disp2 cyl_factor weight Mazda RX4 21 6 160 110 3.9 2.62 16.46 0 1 4 4 25600 6 2620 Mazda RX4 Wag 21 6 160 110 3.9 2.875 17.02 0 1 4 4 25600 6 2875 Datsun 710 22.8 4 108 93 3.85 2.32 18.61 1 1 4 1 11664 4 2320 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 66564 6 3215 Hornet Sportabout 18.7 8 360 175 3.15 3.44 17.02 0 0 3 2 129600 8 3440 Valiant 18.1 6 225 105 2.76 3.46 20.22 1 0 3 1 50625 6 3460 Duster 360 14.3 8 360 245 3.21 3.57 15.84 0 0 3 4 129600 8 3570 Merc 240D 24.4 4 146.7 62 3.69 3.19 20 1 0 4 2 21521 4 3190 Merc 230 22.8 4 140.8 95 3.92 3.15 22.9 1 0 4 2 19825 4 3150 Merc 280 19.2 6 167.6 123 3.92 3.44 18.3 1 0 4 4 28090 6 3440 Merc 280C 17.8 6 167.6 123 3.92 3.44 18.9 1 0 4 4 28090 6 3440 Merc 450SE 16.4 8 275.8 180 3.07 4.07 17.4 0 0 3 3 76066 8 4070 Merc 450SL 17.3 8 275.8 180 3.07 3.73 17.6 0 0 3 3 76066 8 3730 Merc 450SLC 15.2 8 275.8 180 3.07 3.78 18 0 0 3 3 76066 8 3780 Cadillac Fleetwood 10.4 8 472 205 2.93 5.25 17.98 0 0 3 4 222784 8 5250 Lincoln Continental 10.4 8 460 215 3 5.424 17.82 0 0 3 4 211600 8 5424 Chrysler Imperial 14.7 8 440 230 3.23 5.345 17.42 0 0 3 4 193600 8 5345 Fiat 128 32.4 4 78.7 66 4.08 2.2 19.47 1 1 4 1 6194 4 2200 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 5730 4 1615 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.9 1 1 4 1 5055 4 1835 Toyota Corona 21.5 4 120.1 97 3.7 2.465 20.01 1 0 3 1 14424 4 2465 Dodge Challenger 15.5 8 318 150 2.76 3.52 16.87 0 0 3 2 101124 8 3520 AMC Javelin 15.2 8 304 150 3.15 3.435 17.3 0 0 3 2 92416 8 3435 Camaro Z28 13.3 8 350 245 3.73 3.84 15.41 0 0 3 4 122500 8 3840 Pontiac Firebird 19.2 8 400 175 3.08 3.845 17.05 0 0 3 2 160000 8 3845 Fiat X1-9 27.3 4 79 66 4.08 1.935 18.9 1 1 4 1 6241 4 1935 Porsche 914-2 26 4 120.3 91 4.43 2.14 16.7 0 1 5 2 14472 4 2140 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.9 1 1 5 2 9044 4 1513 Ford Pantera L 15.8 8 351 264 4.22 3.17 14.5 0 1 5 4 123201 8 3170 Ferrari Dino 19.7 6 145 175 3.62 2.77 15.5 0 1 5 6 21025 6 2770 Maserati Bora 15 8 301 335 3.54 3.57 14.6 0 1 5 8 90601 8 3570 Volvo 142E 21.4 4 121 109 4.11 2.78 18.6 1 1 4 2 14641 4 2780 Kids3 <-Â  Kids3 is a new dataset we are creating that will contain all of KidsFeet data set along with a couple new columns we are creating. KidsFeet KidsFeet is a dataset found in library(mosaic). Â %>%Â  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line. Â Â  mutate( â€œmutateâ€ is a function from library(tidyverse) that allows us to transform columns of data. Â Â Â Â Â Â Â Â Â season = case_when( â€œseasonâ€ is a name we came up with that will store the results of the transformation of the â€œbirthmonthâ€ column. The case_when(â€¦) function from library(tidyverse) allows us to perform more complicated transformations with columns. Â Â Â Â Â Â Â Â Â Â Â birthmonth %in% c(12,1,2) ~ â€œWinterâ€, The body of case_when(â€¦) is of the form logical expression ~ \"newValueName\". This statement says that we want the values in the column â€œbirthmonthâ€ that are equal to 12, 1, and 2 to be assigned to the value â€œWinterâ€ in the new â€œseasonâ€ column. Â Â Â Â Â Â Â Â Â Â Â birthmonth %in% c(3,4,5) ~ â€œSpringâ€, The body of case_when(â€¦) is of the form logical expression ~ \"newValueName\". This statement says that we want the values in the column â€œbirthmonthâ€ that are equal to 3, 4, and 5 to be assigned to the value â€œSpringâ€ in the new â€œseasonâ€ column. Â Â Â Â Â Â Â Â Â Â Â birthmonth %in% c(6,7,8) ~ â€œSummerâ€, The body of case_when(â€¦) is of the form logical expression ~ \"newValueName\". This statement says that we want the values in the column â€œbirthmonthâ€ that are equal to 6, 7, and 8 to be assigned to the value â€œSummerâ€ in the new â€œseasonâ€ column. Â Â Â Â Â Â Â Â Â Â Â birthmonth %in% c(9,10,11) ~ â€œFallâ€ The body of case_when(â€¦) is of the form logical expression ~ \"newValueName\". This statement says that we want the values in the column â€œbirthmonthâ€ that are equal to 9, 10, and 11 to be assigned to the value â€œFallâ€ in the new â€œseasonâ€ column. Â Â Â Â Â Â Â Â Â Â ) Closing parenthesis of the case_when(â€¦) function. Â Â Â Â Â Â Â Â ) Closing parenthesis for the mutate(â€¦) function. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. Here is what the Kids3 data set looks like. It would be good if you compared this to the original KidsFeet data set by running View(KidsFeet) to see how things have changed. Notice that on the far right we have added the season column. name birthmonth birthyear length width sex biggerfoot domhand season David 5 88 24.4 8.4 B L R Spring Lars 10 87 25.4 8.8 B L L Fall Zach 12 87 24.5 9.7 B R R Winter Josh 1 88 25.2 9.8 B L R Winter Lang 2 88 25.1 8.9 B L R Winter Scotty 3 88 25.7 9.7 B R R Spring Edward 2 88 26.1 9.6 B L R Winter Caitlin 6 88 23 8.8 G L R Summer Eleanor 5 88 23.6 9.3 G R R Spring Damon 9 88 22.9 8.8 B R L Fall Mark 9 87 27.5 9.8 B R R Fall Ray 3 88 24.8 8.9 B L R Spring Cal 8 87 26.1 9.1 B L R Summer Cam 3 88 27 9.8 B L R Spring Julie 11 87 26 9.3 G L R Fall Kate 4 88 23.7 7.9 G R R Spring Caroline 12 87 24 8.7 G R L Winter Maggie 3 88 24.7 8.8 G R R Spring Lee 6 88 26.7 9 G L L Summer Heather 3 88 25.5 9.5 G R R Spring Andy 6 88 24 9.2 B R R Summer Josh 7 88 24.4 8.6 B L R Summer Laura 9 88 24 8.3 G R L Fall Erica 9 88 24.5 9 G L R Fall Peggy 10 88 24.2 8.1 G L R Fall Glen 7 88 27.1 9.4 B L R Summer Abby 2 88 26.1 9.5 G L R Winter David 12 87 25.5 9.5 B R R Winter Mike 11 88 24.2 8.9 B L R Fall Dwayne 8 88 23.9 9.3 B R L Summer Danielle 6 88 24 9.3 G L R Summer Caitlin 7 88 22.5 8.6 G R R Summer Leigh 3 88 24.5 8.6 G L R Spring Dylan 4 88 23.6 9 B R L Spring Peter 4 88 24.7 8.6 B R L Spring Hannah 3 88 22.9 8.5 G L R Spring Teshanna 3 88 26 9 G L R Spring Hayley 1 88 21.6 7.9 G R R Winter Alisha 9 88 24.6 8.8 G L R Fall Kids4 <-Â  Kids4 is a new dataset we are creating that will contain all of KidsFeet data set along with a couple new columns we are creating. KidsFeet KidsFeet is a dataset found in library(mosaic). Â %>%Â  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line. Â Â  mutate( â€œmutateâ€ is a function from library(tidyverse) that allows us to transform columns of data. Â Â Â Â Â Â Â Â Â lengthIN = length / 2.54, â€œlengthINâ€ is a name we came up with that will store the results of the transformation of the â€œlengthâ€ column. This is just converting the length data from cm to inches. Â Â Â Â Â Â Â Â Â widthIN = width / 2.54, â€œwidthINâ€ is a name we came up with that will store the results of the transformation of the â€œwidthâ€ column. This is just converting the width data from cm to inches. Â Â Â Â Â Â Â Â Â lengthSplit = ifelse(length < median(length), Â Â Â Â Â Â Â Â Â â€œUnder 50th Percentileâ€, Â Â Â Â Â Â Â Â Â â€œ50th Percentile or Greaterâ€), â€œlengthSplitâ€ is a name we came up with that will store the results of the ifelse(â€¦) function. The ifelse(â€¦) function in this case is being used to split the length column by the median of that column. The ifelse(â€¦) function is of the form ifelse( Logical Condition , valueIfConditionTrue, valueIfConditionFalse). Â Â Â Â Â Â Â Â Â gender = case_when( â€œgenderâ€ is a name we came up with that will store the results of the transformation of the â€œsexâ€ column. The case_when(â€¦) function from library(tidyverse) allows us to perform more complicated transformations with columns. Â Â Â Â Â Â Â Â Â Â Â sex == â€œBâ€ ~ â€œBoyâ€, The body of case_when(â€¦) is of the form logical expression ~ \"newValueName\". This part of the case_when(â€¦) function is being used to change the value of â€œBâ€ to â€œBoyâ€. Â Â Â Â Â Â Â Â Â Â Â sex == â€œGâ€ ~ â€œGirlâ€ The body of case_when(â€¦) is of the form logical expression ~ \"newValueName\". This part of the case_when(â€¦) function is being used to change the value of â€œGâ€ to â€œGirlâ€. Â Â Â Â Â Â Â Â Â Â ) Closing parenthesis for the case_when(â€¦) function. Â Â Â Â Â Â Â Â ) Closing parenthesis for the mutate(â€¦) function. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. Notice the addition (on the right) of three new columns: lengthIN, widthIN, and gender. name birthmonth birthyear length width sex biggerfoot domhand lengthIN widthIN lengthSplit gender David 5 88 24.4 8.4 B L R 9.606 3.307 Under 50th Percentile Boy Lars 10 87 25.4 8.8 B L L 10 3.465 50th Percentile or Greater Boy Zach 12 87 24.5 9.7 B R R 9.646 3.819 50th Percentile or Greater Boy Josh 1 88 25.2 9.8 B L R 9.921 3.858 50th Percentile or Greater Boy Lang 2 88 25.1 8.9 B L R 9.882 3.504 50th Percentile or Greater Boy Scotty 3 88 25.7 9.7 B R R 10.12 3.819 50th Percentile or Greater Boy Edward 2 88 26.1 9.6 B L R 10.28 3.78 50th Percentile or Greater Boy Caitlin 6 88 23 8.8 G L R 9.055 3.465 Under 50th Percentile Girl Eleanor 5 88 23.6 9.3 G R R 9.291 3.661 Under 50th Percentile Girl Damon 9 88 22.9 8.8 B R L 9.016 3.465 Under 50th Percentile Boy Mark 9 87 27.5 9.8 B R R 10.83 3.858 50th Percentile or Greater Boy Ray 3 88 24.8 8.9 B L R 9.764 3.504 50th Percentile or Greater Boy Cal 8 87 26.1 9.1 B L R 10.28 3.583 50th Percentile or Greater Boy Cam 3 88 27 9.8 B L R 10.63 3.858 50th Percentile or Greater Boy Julie 11 87 26 9.3 G L R 10.24 3.661 50th Percentile or Greater Girl Kate 4 88 23.7 7.9 G R R 9.331 3.11 Under 50th Percentile Girl Caroline 12 87 24 8.7 G R L 9.449 3.425 Under 50th Percentile Girl Maggie 3 88 24.7 8.8 G R R 9.724 3.465 50th Percentile or Greater Girl Lee 6 88 26.7 9 G L L 10.51 3.543 50th Percentile or Greater Girl Heather 3 88 25.5 9.5 G R R 10.04 3.74 50th Percentile or Greater Girl Andy 6 88 24 9.2 B R R 9.449 3.622 Under 50th Percentile Boy Josh 7 88 24.4 8.6 B L R 9.606 3.386 Under 50th Percentile Boy Laura 9 88 24 8.3 G R L 9.449 3.268 Under 50th Percentile Girl Erica 9 88 24.5 9 G L R 9.646 3.543 50th Percentile or Greater Girl Peggy 10 88 24.2 8.1 G L R 9.528 3.189 Under 50th Percentile Girl Glen 7 88 27.1 9.4 B L R 10.67 3.701 50th Percentile or Greater Boy Abby 2 88 26.1 9.5 G L R 10.28 3.74 50th Percentile or Greater Girl David 12 87 25.5 9.5 B R R 10.04 3.74 50th Percentile or Greater Boy Mike 11 88 24.2 8.9 B L R 9.528 3.504 Under 50th Percentile Boy Dwayne 8 88 23.9 9.3 B R L 9.409 3.661 Under 50th Percentile Boy Danielle 6 88 24 9.3 G L R 9.449 3.661 Under 50th Percentile Girl Caitlin 7 88 22.5 8.6 G R R 8.858 3.386 Under 50th Percentile Girl Leigh 3 88 24.5 8.6 G L R 9.646 3.386 50th Percentile or Greater Girl Dylan 4 88 23.6 9 B R L 9.291 3.543 Under 50th Percentile Boy Peter 4 88 24.7 8.6 B R L 9.724 3.386 50th Percentile or Greater Boy Hannah 3 88 22.9 8.5 G L R 9.016 3.346 Under 50th Percentile Girl Teshanna 3 88 26 9 G L R 10.24 3.543 50th Percentile or Greater Girl Hayley 1 88 21.6 7.9 G R R 8.504 3.11 Under 50th Percentile Girl Alisha 9 88 24.6 8.8 G L R 9.685 3.465 50th Percentile or Greater Girl airquality2 <-Â  airquality is a new dataset we are creating that will contain all of the airquality data set along with a new column we are creating. airquality airquality is a dataset found in base R. Typing View(airquality) and ?airquality in the console will help you learn more about the dataset. Â %>%Â  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line. Â Â  mutate( â€œmutateâ€ is a function from library(tidyverse) that allows us to transform columns of data. Month_Full = â€œMonth_Fullâ€ is a name we came up with that will store the results of the transformation of the â€œMonthâ€ column. Â month( month(â€¦) is from library(lubridate) and changes the â€œMonthâ€ column from type integer to type datetime. Month, â€œMonthâ€ is the â€œMonthâ€ column from airquality. Â label = TRUE, â€œlabel = TRUEâ€ tells month(â€¦) to change the month numbers to abbreviated month names. Â abbr = FALSE â€œabbr = FALSEâ€ changes the abbreviated month names to the full month names. ) Closing parenthesis for the month(â€¦) function. ) Closing parenthesis for the mutate(â€¦) function. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. Insert Image Here Other case_when( ) Uses case_when(length > 25 & width > 9 ~ \"Long and Wide\", length < 25 & width > 9 ~ \"Short and Wide\", length > 25 & width < 9 ~ \"Long and Thin\", length < 25 & width < 9 ~ \"Short and Thin\") replace_na( ) Function newDataName <- dataName %>% mutate(newColumnName = replace_na(columnName, value)) as.numeric( ) Function newDataName <- dataName %>% mutate(newColumnName = as.numeric(columnName)) as.character( ) Function newDataName <- dataName %>% mutate(newColumnName = as.character(columnName)) as.factor( ) Function newDataName <- dataName %>% mutate(newColumnName = as.factor(columnName)) Pull Out Numbers Only with parse_number(â€¦) newDataName <- dataName %>% mutate(newColumnName = parse_number(columnName)) # So stuff like c(\"500+\", \"20\", \"80 \", \"15a\") would become # just c(500, 20, 80, 15). That's nice!",
    "sentences": ["Transform a column or add a new column of data to a data set.", "Usage NameofDataset %>% Â Â  mutate(nameYouLike = some_transformation) NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet.", "%>% is the pipe operator that â€œpipes dataâ€ down into R commands on the next line.", "nameYouLike is just that.", "Some name you come up with that will be the name of a new column in the dataset.", "some_transformation is just that.", "See the example codes for ideas.", "Example Code mtcars2 <-Â  mtcars2 is a new dataset we are creating that will contain all of mtcars data set along with a couple new columns we are creating.", "mtcars mtcars is a dataset found in base R.", "Typing View(mtcars) and ?mtcars in the console will help you learn more about the dataset."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "mutate( )"
  },
  {
    "id": 252,
    "title": "ğŸ“ arrange( )",
    "url": "RCommands.html#arrange",
    "content": "arrange( ) Arrange data by a certain column, or columns, i.e.Â â€œsortâ€ the data. Usage NameofDataset %>% Â Â  arrange(columnName1) Note: arrange(columnName1, columnName2, ...) is also possible. NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet. %>% is the pipe operator that â€œpipes dataâ€ down into R commands on the next line. arrange(...) is an R function from library(tidyverse) that arranges a data set by order for the column given. columnName1 is the name of a column from the dataset that you want to compute numerical summaries on. columnName2 is the name of a column from the dataset that you want to compute numerical summaries on. implies that you can arrange by as many columns as you want. Example Code KidsFeet KidsFeet is a dataset found in library(mosaic). Â %>%Â  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line. Â Â  arrange( â€œarrangeâ€ is an R function from library(tidyverse) that arranges a data set by order for the column given. birthmonth birthmonth is the name of one of the columns of the KidsFeet data set. Specifying this name will cause the data to be sorted by birthmonth from 1 to 12. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. name birthmonth birthyear length width sex biggerfoot domhand Josh 1 88 25.2 9.8 B L R Hayley 1 88 21.6 7.9 G R R Lang 2 88 25.1 8.9 B L R Edward 2 88 26.1 9.6 B L R Abby 2 88 26.1 9.5 G L R Scotty 3 88 25.7 9.7 B R R Ray 3 88 24.8 8.9 B L R Cam 3 88 27 9.8 B L R Maggie 3 88 24.7 8.8 G R R Heather 3 88 25.5 9.5 G R R Leigh 3 88 24.5 8.6 G L R Hannah 3 88 22.9 8.5 G L R Teshanna 3 88 26 9 G L R Kate 4 88 23.7 7.9 G R R Dylan 4 88 23.6 9 B R L Peter 4 88 24.7 8.6 B R L David 5 88 24.4 8.4 B L R Eleanor 5 88 23.6 9.3 G R R Caitlin 6 88 23 8.8 G L R Lee 6 88 26.7 9 G L L Andy 6 88 24 9.2 B R R Danielle 6 88 24 9.3 G L R Josh 7 88 24.4 8.6 B L R Glen 7 88 27.1 9.4 B L R Caitlin 7 88 22.5 8.6 G R R Cal 8 87 26.1 9.1 B L R Dwayne 8 88 23.9 9.3 B R L Damon 9 88 22.9 8.8 B R L Mark 9 87 27.5 9.8 B R R Laura 9 88 24 8.3 G R L Erica 9 88 24.5 9 G L R Alisha 9 88 24.6 8.8 G L R Lars 10 87 25.4 8.8 B L L Peggy 10 88 24.2 8.1 G L R Julie 11 87 26 9.3 G L R Mike 11 88 24.2 8.9 B L R Zach 12 87 24.5 9.7 B R R Caroline 12 87 24 8.7 G R L David 12 87 25.5 9.5 B R R KidsFeet KidsFeet is a dataset found in library(mosaic). Â %>%Â  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line. Â Â  arrange( â€œarrangeâ€ is an R function from library(tidyverse) that arranges a data set by order for the column given. desc( This causes the arranging to be done in descending order (highest to lowest). birthmonth birthmonth is the name of one of the columns of the KidsFeet data set. Specifying this name will cause the data to be sorted by birthmonth from 1 to 12. ) Always close off your functions in R with a closing parathesis. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. name birthmonth birthyear length width sex biggerfoot domhand Zach 12 87 24.5 9.7 B R R Caroline 12 87 24 8.7 G R L David 12 87 25.5 9.5 B R R Julie 11 87 26 9.3 G L R Mike 11 88 24.2 8.9 B L R Lars 10 87 25.4 8.8 B L L Peggy 10 88 24.2 8.1 G L R Damon 9 88 22.9 8.8 B R L Mark 9 87 27.5 9.8 B R R Laura 9 88 24 8.3 G R L Erica 9 88 24.5 9 G L R Alisha 9 88 24.6 8.8 G L R Cal 8 87 26.1 9.1 B L R Dwayne 8 88 23.9 9.3 B R L Josh 7 88 24.4 8.6 B L R Glen 7 88 27.1 9.4 B L R Caitlin 7 88 22.5 8.6 G R R Caitlin 6 88 23 8.8 G L R Lee 6 88 26.7 9 G L L Andy 6 88 24 9.2 B R R Danielle 6 88 24 9.3 G L R David 5 88 24.4 8.4 B L R Eleanor 5 88 23.6 9.3 G R R Kate 4 88 23.7 7.9 G R R Dylan 4 88 23.6 9 B R L Peter 4 88 24.7 8.6 B R L Scotty 3 88 25.7 9.7 B R R Ray 3 88 24.8 8.9 B L R Cam 3 88 27 9.8 B L R Maggie 3 88 24.7 8.8 G R R Heather 3 88 25.5 9.5 G R R Leigh 3 88 24.5 8.6 G L R Hannah 3 88 22.9 8.5 G L R Teshanna 3 88 26 9 G L R Lang 2 88 25.1 8.9 B L R Edward 2 88 26.1 9.6 B L R Abby 2 88 26.1 9.5 G L R Josh 1 88 25.2 9.8 B L R Hayley 1 88 21.6 7.9 G R R",
    "sentences": ["Arrange data by a certain column, or columns, i.e.Â â€œsortâ€ the data.", "Usage NameofDataset %>% Â Â  arrange(columnName1) Note: arrange(columnName1, columnName2, ...) is also possible.", "NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet.", "%>% is the pipe operator that â€œpipes dataâ€ down into R commands on the next line.", "arrange(...) is an R function from library(tidyverse) that arranges a data set by order for the column given.", "columnName1 is the name of a column from the dataset that you want to compute numerical summaries on.", "columnName2 is the name of a column from the dataset that you want to compute numerical summaries on.", "implies that you can arrange by as many columns as you want.", "Example Code KidsFeet KidsFeet is a dataset found in library(mosaic).", "Â %>%Â  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "arrange( )"
  },
  {
    "id": 253,
    "title": "ğŸ“ pander( )",
    "url": "RCommands.html#pander",
    "content": "pander( ) Makes output of most commands â€œbeautifulâ€. Usage library(pander) thenâ€¦ pander(someCode) OR someCode %>% Â Â  pander( ) Note: pander(stuff, caption=\"Some useful caption\", ...) is also possible. someCode is exactly that, some coding you have done that creates output that you want displayed nicely. %>% is the pipe operator that â€œpipes dataâ€ down into R commands on the next line. pander(...) is an R function from library(pander) that makes most R output look nice. other useful commands like split.table=Inf. Example Code pander( pander is an R function that makes output look nice. table(KidsFeet$sex, KidsFeet$birthmonth), Code that makes a table of how many boys and girls were born in each month of the year. Â  caption=â€œCounts of Birthdays by Monthâ€ The caption=â€ â€ command is very useful for giving your output a small title. ) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. Counts of Birthdays by Month Â  1 2 3 4 5 6 7 8 9 10 11 12 B 1 2 3 2 1 1 2 2 2 1 1 2 G 1 1 5 1 1 3 1 0 3 1 1 1 KidsFeet KidsFeet is a dataset found in library(mosaic). Â %>%Â  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line. Â Â  group_by( â€œgroup_byâ€ is a function from library(tidyverse) that allows us to split the dataset up into â€œlittle groupsâ€ according to the column specified. sex â€œsexâ€ is a column from the KidsFeet dataset that records the gender of each child. ) Always close off your functions in R with a closing parathesis. Â %>%Â  The pipe operator that will send the grouped according to gender version of the KidsFeet dataset down inside of the code on the following line. Â Â  summarise( â€œsummariseâ€ is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveLength A name we came up with that will store the results of the numerical summary. Â = mean(length),Â  This computes the mean(...) of the length column from the KidsFeet dataset. Â Â Â Â Â Â Â Â Â Â Â Â  sdLength A name we came up with that will store the results of the numerical summary. Â = sd(length),Â  This computes the sd(...) of the length column from the KidsFeet dataset. Â Â Â Â Â Â Â Â Â Â Â Â  sampleSize A name we came up with that will store the results of the numerical summary. Â = n( ) This computes the n(...), or sample size, of the length column from the KidsFeet dataset. ) Always close off your functions in R with a closing parathesis. Â %>%Â  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line. Â Â  pander( The pander function will make the output of the above code look nice. caption=â€œDoesnâ€™t that look nice?â€) Always close off your functions in R with a closing parathesis. Â Â Â Â Press Enter to run the code. Â Click to Show OutputÂ  Click to View Output. Doesnâ€™t that look nice? sex aveLength sdLength sampleSize B 25.11 1.217 20 G 24.32 1.33 19 What it looks like if you don't pander: # A tibble: 2 Ã— 4 sex aveLength sdLength sampleSize <fct> <dbl> <dbl> <int> 1 B 25.105 1.21676 20 2 G 24.3211 1.33024 19",
    "sentences": ["Makes output of most commands â€œbeautifulâ€.", "Usage library(pander) thenâ€¦ pander(someCode) OR someCode %>% Â Â  pander( ) Note: pander(stuff, caption=\"Some useful caption\", ...) is also possible.", "someCode is exactly that, some coding you have done that creates output that you want displayed nicely.", "%>% is the pipe operator that â€œpipes dataâ€ down into R commands on the next line.", "pander(...) is an R function from library(pander) that makes most R output look nice.", "other useful commands like split.table=Inf.", "Example Code pander( pander is an R function that makes output look nice.", "table(KidsFeet$sex, KidsFeet$birthmonth), Code that makes a table of how many boys and girls were born in each month of the year.", "Â  caption=â€œCounts of Birthdays by Monthâ€ The caption=â€ â€ command is very useful for giving your output a small title.", ") Always close off your functions in R with a closing parathesis."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "pander( )"
  },
  {
    "id": 254,
    "title": "R Markdown Hints",
    "url": "RMarkdownHints.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus R Markdown Hints Think of an R Markdown File, or Rmd for short, as a command center. You write commands, then Knit the file, and an html output file is created according to your commands. Overview Click Here to Learn More Carefully read through all parts of this image to learnâ€¦ Close The above tabs (blue bottons that read â€œClick Here to Learn Moreâ€ and â€œCloseâ€) were created with the code: ## {.tabset .tabset-pills .tabset-fade} ### Click Here to Learn More Carefully read through all parts of this image to learn... ![](./Images/Rmd2html.png) ### Close ## Creating Links To make a link use the code [Name of Link](addressForLink). Linking to parts of your textbook: [Numerical Summaries](NumericalSummaries.html) becomes Numerical Summaries [Boxplots](GraphicalSummaries.html#boxplots) becomes Boxplots [R Commands](RCommands.html) becomes R Commands Linking to outside resources: [R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) becomes R Colors Creating Headers There are six available sizes of headings you can use in an Rmd file (left in image) that show up as shown below (right in image). Emphasizing Words To italisize a word use the asterisk (Shift 8) *italisize*. To bold a word use the double asterisk **bold**. The back tic can be used tohighlightwords by placing back tics on each side of a word: highlight `. Bullet Points Simple Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: * This is the first item. * This is the second. * This is the third. Numbered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: 1. This is the first item. This is the second. This is the third. Lettered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: A) This is the first item. B) This is the second. C) This is the third. Nested Lists What is \\(2+2\\)? 4 8 What is \\(3\\times5\\)? 14 15 1. What is $2+2$? **4** b. What is $3\\times5$? 14 b. **15** Math Equations Use the dollar signs $x=5$ to write \\(x=5\\) or $z=\\frac{x-\\mu}{\\sigma}$ to write \\(z=\\frac{x-\\mu}{\\sigma}\\). For a nicely centered equation use the double dollar signs $$ $$ on separate lines $$ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} $$ to get \\[ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] Or $$ H_0: \\mu_\\text{Group 1} = \\mu_\\text{Group 2} $$ $$ H_a: \\mu_\\text{Group 1} \\neq \\mu_\\text{Group 2} $$ to get \\[ H_0: \\mu_{\\text{Group 1}} = \\mu_{\\text{Group 2}} \\] \\[ H_a: \\mu_{\\text{Group 1}} \\neq \\mu_{\\text{Group 2}} \\] Symbol list: Symbol LaTeX Math Code \\(\\alpha\\) $\\alpha$ \\(\\beta\\) $\\beta$ \\(\\sigma\\) $\\sigma$ \\(\\epsilon\\) $\\epsilon$ \\(\\bar{x}\\) $\\bar{x}$ \\(\\hat{Y}\\) $\\hat{Y}$ \\(=\\) $=$ \\(\\ne\\) $\\ne$ or $\\neq$ \\(>\\) $>$ \\(<\\) $<$ \\(\\ge\\) $\\ge$ \\(\\le\\) $\\le$ \\(\\{ \\}\\) $\\{ \\}$ \\(\\text{Type just text}\\) $\\text{Type just text}$ \\(\\overbrace{Y_i}^\\text{label}\\) $\\overbrace{Y_i}^\\text{label}$ \\(\\underbrace{Y_i}_\\text{label}\\) $\\underbrace{Y_i}_\\text{label}$ Here is a list of all supported LaTeX commands. Insert a Picture To add a picture to your document, say some notes you took down on paper from class, Use the code: ![](./Images/insertPictureNotes.jpg) to getâ€¦ Tables There are many ways to make tables in R Markdown. Here is a simple way to make a â€œpipeâ€ table. | Name | Age | Gender | |---------------|---------------|--------------| | Jill | 8 | Female | | Jack | 9 | Male | Name Age Gender Jill 8 Female Jack 9 Male Themes Notice in the YAML (at the top of the RMD file) there is a line that reads: â€œtheme: ceruleanâ€ Other possible themes are â€œdefaultâ€, â€œceruleanâ€, â€œjournalâ€, â€œflatlyâ€, â€œreadableâ€, â€œspacelabâ€, â€œunitedâ€, and â€œcosmoâ€. You can also change the highlighting by adding the line â€œhighlight: tangoâ€ to the YAML as follows. --- title: \"Markdown Hints\" output: html_document: theme: cerulean highlight: tango --- Other highlighting options are â€œdefaultâ€, â€œtangoâ€, â€œpygmentsâ€, â€œkateâ€, â€œmonochromeâ€, â€œespressoâ€, â€œzenburnâ€, â€œhaddockâ€, and â€œtextmateâ€. More Information Go to the rmarkdown.rstudio.com website for more information on how to use R Markdown. Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus R Markdown Hints Think of an R Markdown File, or Rmd for short, as a command center. You write commands, then Knit the file, and an html output file is created according to your commands. Overview Click Here to Learn More Carefully read through all parts of this image to learnâ€¦ Close The above tabs (blue bottons that read â€œClick Here to Learn Moreâ€ and â€œCloseâ€) were created with the code: ## {.tabset .tabset-pills .tabset-fade} ### Click Here to Learn More Carefully read through all parts of this image to learn... ![](./Images/Rmd2html.png) ### Close ## Creating Links To make a link use the code [Name of Link](addressForLink). Linking to parts of your textbook: [Numerical Summaries](NumericalSummaries.html) becomes Numerical Summaries [Boxplots](GraphicalSummaries.html#boxplots) becomes Boxplots [R Commands](RCommands.html) becomes R Commands Linking to outside resources: [R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) becomes R Colors Creating Headers There are six available sizes of headings you can use in an Rmd file (left in image) that show up as shown below (right in image). Emphasizing Words To italisize a word use the asterisk (Shift 8) *italisize*. To bold a word use the double asterisk **bold**. The back tic can be used tohighlightwords by placing back tics on each side of a word: highlight `. Bullet Points Simple Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: * This is the first item. * This is the second. * This is the third. Numbered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: 1. This is the first item. This is the second. This is the third. Lettered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: A) This is the first item. B) This is the second. C) This is the third. Nested Lists What is \\(2+2\\)? 4 8 What is \\(3\\times5\\)? 14 15 1. What is $2+2$? **4** b. What is $3\\times5$? 14 b. **15** Math Equations Use the dollar signs $x=5$ to write \\(x=5\\) or $z=\\frac{x-\\mu}{\\sigma}$ to write \\(z=\\frac{x-\\mu}{\\sigma}\\). For a nicely centered equation use the double dollar signs $$ $$ on separate lines $$ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} $$ to get \\[ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] Or $$ H_0: \\mu_\\text{Group 1} = \\mu_\\text{Group 2} $$ $$ H_a: \\mu_\\text{Group 1} \\neq \\mu_\\text{Group 2} $$ to get \\[ H_0: \\mu_{\\text{Group 1}} = \\mu_{\\text{Group 2}} \\] \\[ H_a: \\mu_{\\text{Group 1}} \\neq \\mu_{\\text{Group 2}} \\] Symbol list: Symbol LaTeX Math Code \\(\\alpha\\) $\\alpha$ \\(\\beta\\) $\\beta$ \\(\\sigma\\) $\\sigma$ \\(\\epsilon\\) $\\epsilon$ \\(\\bar{x}\\) $\\bar{x}$ \\(\\hat{Y}\\) $\\hat{Y}$ \\(=\\) $=$ \\(\\ne\\) $\\ne$ or $\\neq$ \\(>\\) $>$ \\(<\\) $<$ \\(\\ge\\) $\\ge$ \\(\\le\\) $\\le$ \\(\\{ \\}\\) $\\{ \\}$ \\(\\text{Type just text}\\) $\\text{Type just text}$ \\(\\overbrace{Y_i}^\\text{label}\\) $\\overbrace{Y_i}^\\text{label}$ \\(\\underbrace{Y_i}_\\text{label}\\) $\\underbrace{Y_i}_\\text{label}$ Here is a list of all supported LaTeX commands. Insert a Picture To add a picture to your document, say some notes you took down on paper from class, Use the code: ![](./Images/insertPictureNotes.jpg) to getâ€¦ Tables There are many ways to make tables in R Markdown. Here is a simple way to make a â€œpipeâ€ table. | Name | Age | Gender | |---------------|---------------|--------------| | Jill | 8 | Female | | Jack | 9 | Male | Name Age Gender Jill 8 Female Jack 9 Male Themes Notice in the YAML (at the top of the RMD file) there is a line that reads: â€œtheme: ceruleanâ€ Other possible themes are â€œdefaultâ€, â€œceruleanâ€, â€œjournalâ€, â€œflatlyâ€, â€œreadableâ€, â€œspacelabâ€, â€œunitedâ€, and â€œcosmoâ€. You can also change the highlighting by adding the line â€œhighlight: tangoâ€ to the YAML as follows. --- title: \"Markdown Hints\" output: html_document: theme: cerulean highlight: tango --- Other highlighting options are â€œdefaultâ€, â€œtangoâ€, â€œpygmentsâ€, â€œkateâ€, â€œmonochromeâ€, â€œespressoâ€, â€œzenburnâ€, â€œhaddockâ€, and â€œtextmateâ€. More Information Go to the rmarkdown.rstudio.com website for more information on how to use R Markdown. Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus R Markdown Hints Think of an R Markdown File, or Rmd for short, as a command center. You write commands, then Knit the file, and an html output file is created according to your commands. Overview Click Here to Learn More Carefully read through all parts of this image to learnâ€¦ Close The above tabs (blue bottons that read â€œClick Here to Learn Moreâ€ and â€œCloseâ€) were created with the code: ## {.tabset .tabset-pills .tabset-fade} ### Click Here to Learn More Carefully read through all parts of this image to learn... ![](./Images/Rmd2html.png) ### Close ## Creating Links To make a link use the code [Name of Link](addressForLink). Linking to parts of your textbook: [Numerical Summaries](NumericalSummaries.html) becomes Numerical Summaries [Boxplots](GraphicalSummaries.html#boxplots) becomes Boxplots [R Commands](RCommands.html) becomes R Commands Linking to outside resources: [R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) becomes R Colors Creating Headers There are six available sizes of headings you can use in an Rmd file (left in image) that show up as shown below (right in image). Emphasizing Words To italisize a word use the asterisk (Shift 8) *italisize*. To bold a word use the double asterisk **bold**. The back tic can be used tohighlightwords by placing back tics on each side of a word: highlight `. Bullet Points Simple Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: * This is the first item. * This is the second. * This is the third. Numbered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: 1. This is the first item. This is the second. This is the third. Lettered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: A) This is the first item. B) This is the second. C) This is the third. Nested Lists What is \\(2+2\\)? 4 8 What is \\(3\\times5\\)? 14 15 1. What is $2+2$? **4** b. What is $3\\times5$? 14 b. **15** Math Equations Use the dollar signs $x=5$ to write \\(x=5\\) or $z=\\frac{x-\\mu}{\\sigma}$ to write \\(z=\\frac{x-\\mu}{\\sigma}\\). For a nicely centered equation use the double dollar signs $$ $$ on separate lines $$ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} $$ to get \\[ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] Or $$ H_0: \\mu_\\text{Group 1} = \\mu_\\text{Group 2} $$ $$ H_a: \\mu_\\text{Group 1} \\neq \\mu_\\text{Group 2} $$ to get \\[ H_0: \\mu_{\\text{Group 1}} = \\mu_{\\text{Group 2}} \\] \\[ H_a: \\mu_{\\text{Group 1}} \\neq \\mu_{\\text{Group 2}} \\] Symbol list: Symbol LaTeX Math Code \\(\\alpha\\) $\\alpha$ \\(\\beta\\) $\\beta$ \\(\\sigma\\) $\\sigma$ \\(\\epsilon\\) $\\epsilon$ \\(\\bar{x}\\) $\\bar{x}$ \\(\\hat{Y}\\) $\\hat{Y}$ \\(=\\) $=$ \\(\\ne\\) $\\ne$ or $\\neq$ \\(>\\) $>$ \\(<\\) $<$ \\(\\ge\\) $\\ge$ \\(\\le\\) $\\le$ \\(\\{ \\}\\) $\\{ \\}$ \\(\\text{Type just text}\\) $\\text{Type just text}$ \\(\\overbrace{Y_i}^\\text{label}\\) $\\overbrace{Y_i}^\\text{label}$ \\(\\underbrace{Y_i}_\\text{label}\\) $\\underbrace{Y_i}_\\text{label}$ Here is a list of all supported LaTeX commands. Insert a Picture To add a picture to your document, say some notes you took down on paper from class, Use the code: ![](./Images/insertPictureNotes.jpg) to getâ€¦ Tables There are many ways to make tables in R Markdown. Here is a simple way to make a â€œpipeâ€ table. | Name | Age | Gender | |---------------|---------------|--------------| | Jill | 8 | Female | | Jack | 9 | Male | Name Age Gender Jill 8 Female Jack 9 Male Themes Notice in the YAML (at the top of the RMD file) there is a line that reads: â€œtheme: ceruleanâ€ Other possible themes are â€œdefaultâ€, â€œceruleanâ€, â€œjournalâ€, â€œflatlyâ€, â€œreadableâ€, â€œspacelabâ€, â€œunit",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus R Markdown Hints Think of an R Markdown File, or Rmd for short, as a command center.", "You write commands, then Knit the file, and an html output file is created according to your commands.", "Overview Click Here to Learn More Carefully read through all parts of this image to learnâ€¦ Close The above tabs (blue bottons that read â€œClick Here to Learn Moreâ€ and â€œCloseâ€) were created with the code: ## {.tabset .tabset-pills .tabset-fade} ### Click Here to Learn More Carefully read through all parts of this image to learn...", "![](./Images/Rmd2html.png) ### Close ## Creating Links To make a link use the code [Name of Link](addressForLink).", "Linking to parts of your textbook: [Numerical Summaries](NumericalSummaries.html) becomes Numerical Summaries [Boxplots](GraphicalSummaries.html#boxplots) becomes Boxplots [R Commands](RCommands.html) becomes R Commands Linking to outside resources: [R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) becomes R Colors Creating Headers There are six available sizes of headings you can use in an Rmd file (left in image) that show up as shown below (right in image).", "Emphasizing Words To italisize a word use the asterisk (Shift 8) *italisize*.", "To bold a word use the double asterisk **bold**.", "The back tic can be used tohighlightwords by placing back tics on each side of a word: highlight `.", "Bullet Points Simple Lists To achieve the result: This is the first item.", "This is the second.", "This is the third.", "Use the code: To achieve the result: * This is the first item.", "* This is the second.", "* This is the third.", "Numbered Lists To achieve the result: This is the first item.", "This is the second.", "This is the third.", "Use the code: To achieve the result: 1.", "This is the first item.", "This is the second.", "This is the third.", "Lettered Lists To achieve the result: This is the first item.", "This is the second.", "This is the third.", "Use the code: To achieve the result: A) This is the first item.", "B) This is the second.", "C) This is the third.", "Nested Lists What is \\(2+2\\)?", "4 8 What is \\(3\\times5\\)?", "14 15 1.", "What is $2+2$?", "**4** b.", "What is $3\\times5$?", "14 b.", "**15** Math Equations Use the dollar signs $x=5$ to write \\(x=5\\) or $z=\\frac{x-\\mu}{\\sigma}$ to write \\(z=\\frac{x-\\mu}{\\sigma}\\).", "For a nicely centered equation use the double dollar signs $$ $$ on separate lines $$ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} $$ to get \\[ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] Or $$ H_0: \\mu_\\text{Group 1} = \\mu_\\text{Group 2} $$ $$ H_a: \\mu_\\text{Group 1} \\neq \\mu_\\text{Group 2} $$ to get \\[ H_0: \\mu_{\\text{Group 1}} = \\mu_{\\text{Group 2}} \\] \\[ H_a: \\mu_{\\text{Group 1}} \\neq \\mu_{\\text{Group 2}} \\] Symbol list: Symbol LaTeX Math Code \\(\\alpha\\) $\\alpha$ \\(\\beta\\) $\\beta$ \\(\\sigma\\) $\\sigma$ \\(\\epsilon\\) $\\epsilon$ \\(\\bar{x}\\) $\\bar{x}$ \\(\\hat{Y}\\) $\\hat{Y}$ \\(=\\) $=$ \\(\\ne\\) $\\ne$ or $\\neq$ \\(>\\) $>$ \\(<\\) $<$ \\(\\ge\\) $\\ge$ \\(\\le\\) $\\le$ \\(\\{ \\}\\) $\\{ \\}$ \\(\\text{Type just text}\\) $\\text{Type just text}$ \\(\\overbrace{Y_i}^\\text{label}\\) $\\overbrace{Y_i}^\\text{label}$ \\(\\underbrace{Y_i}_\\text{label}\\) $\\underbrace{Y_i}_\\text{label}$ Here is a list of all supported LaTeX commands.", "Insert a Picture To add a picture to your document, say some notes you took down on paper from class, Use the code: ![](./Images/insertPictureNotes.jpg) to getâ€¦ Tables There are many ways to make tables in R Markdown.", "Here is a simple way to make a â€œpipeâ€ table.", "| Name | Age | Gender | |---------------|---------------|--------------| | Jill | 8 | Female | | Jack | 9 | Male | Name Age Gender Jill 8 Female Jack 9 Male Themes Notice in the YAML (at the top of the RMD file) there is a line that reads: â€œtheme: ceruleanâ€ Other possible themes are â€œdefaultâ€, â€œceruleanâ€, â€œjournalâ€, â€œflatlyâ€, â€œreadableâ€, â€œspacelabâ€, â€œunitedâ€, and â€œcosmoâ€.", "You can also change the highlighting by adding the line â€œhighlight: tangoâ€ to the YAML as follows.", "--- title: \"Markdown Hints\" output: html_document: theme: cerulean highlight: tango --- Other highlighting options are â€œdefaultâ€, â€œtangoâ€, â€œpygmentsâ€, â€œkateâ€, â€œmonochromeâ€, â€œespressoâ€, â€œzenburnâ€, â€œhaddockâ€, and â€œtextmateâ€.", "More Information Go to the rmarkdown.rstudio.com website for more information on how to use R Markdown.", "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus R Markdown Hints Think of an R Markdown File, or Rmd for short, as a command center.", "You write commands, then Knit the file, and an html output file is created according to your commands.", "Overview Click Here to Learn More Carefully read through all parts of this image to learnâ€¦ Close The above tabs (blue bottons that read â€œClick Here to Learn Moreâ€ and â€œCloseâ€) were created with the code: ## {.tabset .tabset-pills .tabset-fade} ### Click Here to Learn More Carefully read through all parts of this image to learn...", "![](./Images/Rmd2html.png) ### Close ## Creating Links To make a link use the code [Name of Link](addressForLink).", "Linking to parts of your textbook: [Numerical Summaries](NumericalSummaries.html) becomes Numerical Summaries [Boxplots](GraphicalSummaries.html#boxplots) becomes Boxplots [R Commands](RCommands.html) becomes R Commands Linking to outside resources: [R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) becomes R Colors Creating Headers There are six available sizes of headings you can use in an Rmd file (left in image) that show up as shown below (right in image).", "Emphasizing Words To italisize a word use the asterisk (Shift 8) *italisize*.", "To bold a word use the double asterisk **bold**.", "The back tic can be used tohighlightwords by placing back tics on each side of a word: highlight `."],
    "type": "page",
    "page_title": "R Markdown Hints"
  },
  {
    "id": 255,
    "title": "ğŸ“ Overview",
    "url": "RMarkdownHints.html#overview",
    "content": "Overview Click Here to Learn More Carefully read through all parts of this image to learnâ€¦ Close The above tabs (blue bottons that read â€œClick Here to Learn Moreâ€ and â€œCloseâ€) were created with the code: ## {.tabset .tabset-pills .tabset-fade} ### Click Here to Learn More Carefully read through all parts of this image to learn... ![](./Images/Rmd2html.png) ### Close ##",
    "sentences": ["Click Here to Learn More Carefully read through all parts of this image to learnâ€¦ Close", "The above tabs (blue bottons that read â€œClick Here to Learn Moreâ€ and â€œCloseâ€) were created with the code: ## {.tabset .tabset-pills .tabset-fade} ### Click Here to Learn More Carefully read through all parts of this image to learn...", "![](./Images/Rmd2html.png) ### Close ##"],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Overview"
  },
  {
    "id": 256,
    "title": "ğŸ“ Click Here to Learn More",
    "url": "RMarkdownHints.html#clickheretolearnmore",
    "content": "Click Here to Learn More Carefully read through all parts of this image to learnâ€¦",
    "sentences": "Carefully read through all parts of this image to learnâ€¦",
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Click Here to Learn More"
  },
  {
    "id": 257,
    "title": "ğŸ“ Creating Links",
    "url": "RMarkdownHints.html#creatinglinks",
    "content": "Creating Links To make a link use the code [Name of Link](addressForLink). Linking to parts of your textbook: [Numerical Summaries](NumericalSummaries.html) becomes Numerical Summaries [Boxplots](GraphicalSummaries.html#boxplots) becomes Boxplots [R Commands](RCommands.html) becomes R Commands Linking to outside resources: [R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) becomes R Colors",
    "sentences": ["To make a link use the code [Name of Link](addressForLink).", "Linking to parts of your textbook:", "[Numerical Summaries](NumericalSummaries.html) becomes Numerical Summaries [Boxplots](GraphicalSummaries.html#boxplots) becomes Boxplots [R Commands](RCommands.html) becomes R Commands", "Linking to outside resources:", "[R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) becomes R Colors"],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Creating Links"
  },
  {
    "id": 258,
    "title": "ğŸ“ Creating Headers",
    "url": "RMarkdownHints.html#creatingheaders",
    "content": "Creating Headers There are six available sizes of headings you can use in an Rmd file (left in image) that show up as shown below (right in image).",
    "sentences": "There are six available sizes of headings you can use in an Rmd file (left in image) that show up as shown below (right in image).",
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Creating Headers"
  },
  {
    "id": 259,
    "title": "ğŸ“ Emphasizing Words",
    "url": "RMarkdownHints.html#emphasizingwords",
    "content": "Emphasizing Words To italisize a word use the asterisk (Shift 8) *italisize*. To bold a word use the double asterisk **bold**. The back tic can be used tohighlightwords by placing back tics on each side of a word: highlight `.",
    "sentences": ["To italisize a word use the asterisk (Shift 8) *italisize*.", "To bold a word use the double asterisk **bold**.", "The back tic can be used tohighlightwords by placing back tics on each side of a word: highlight `."],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Emphasizing Words"
  },
  {
    "id": 260,
    "title": "ğŸ“ Bullet Points",
    "url": "RMarkdownHints.html#bulletpoints",
    "content": "Bullet Points Simple Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: * This is the first item. * This is the second. * This is the third. Numbered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: 1. This is the first item. This is the second. This is the third.",
    "sentences": ["Simple Lists To achieve the result: This is the first item.", "This is the second.", "This is the third.", "Use the code: To achieve the result: * This is the first item.", "* This is the second.", "* This is the third.", "Numbered Lists To achieve the result: This is the first item.", "This is the second.", "This is the third.", "Use the code: To achieve the result: 1."],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Bullet Points"
  },
  {
    "id": 261,
    "title": "ğŸ“ Simple Lists",
    "url": "RMarkdownHints.html#simplelists",
    "content": "Simple Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: * This is the first item. * This is the second. * This is the third.",
    "sentences": ["To achieve the result: This is the first item.", "This is the second.", "This is the third.", "Use the code: To achieve the result: * This is the first item.", "* This is the second.", "* This is the third."],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Simple Lists"
  },
  {
    "id": 262,
    "title": "ğŸ“ Numbered Lists",
    "url": "RMarkdownHints.html#numberedlists",
    "content": "Numbered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: 1. This is the first item. This is the second. This is the third.",
    "sentences": ["To achieve the result: This is the first item.", "This is the second.", "This is the third.", "Use the code: To achieve the result: 1.", "This is the first item.", "This is the second.", "This is the third."],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Numbered Lists"
  },
  {
    "id": 263,
    "title": "ğŸ“ Lettered Lists",
    "url": "RMarkdownHints.html#letteredlists",
    "content": "Lettered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: A) This is the first item. B) This is the second. C) This is the third.",
    "sentences": ["To achieve the result: This is the first item.", "This is the second.", "This is the third.", "Use the code: To achieve the result: A) This is the first item.", "B) This is the second.", "C) This is the third."],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Lettered Lists"
  },
  {
    "id": 264,
    "title": "ğŸ“ Nested Lists",
    "url": "RMarkdownHints.html#nestedlists",
    "content": "Nested Lists What is \\(2+2\\)? 4 8 What is \\(3\\times5\\)? What is $2+2$? **4** b. What is $3\\times5$? 14 b.",
    "sentences": ["What is \\(2+2\\)?", "4 8 What is \\(3\\times5\\)?", "What is $2+2$?", "**4** b.", "What is $3\\times5$?", "14 b."],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Nested Lists"
  },
  {
    "id": 265,
    "title": "ğŸ“ Math Equations",
    "url": "RMarkdownHints.html#mathequations",
    "content": "Math Equations Use the dollar signs $x=5$ to write \\(x=5\\) or $z=\\frac{x-\\mu}{\\sigma}$ to write \\(z=\\frac{x-\\mu}{\\sigma}\\). For a nicely centered equation use the double dollar signs $$ $$ on separate lines to get \\[ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] to get \\[ H_0: \\mu_{\\text{Group 1}} = \\mu_{\\text{Group 2}} \\] \\[ H_a: \\mu_{\\text{Group 1}} \\neq \\mu_{\\text{Group 2}} \\] Here is a list of all supported LaTeX commands.",
    "sentences": ["Use the dollar signs $x=5$ to write \\(x=5\\) or $z=\\frac{x-\\mu}{\\sigma}$ to write \\(z=\\frac{x-\\mu}{\\sigma}\\).", "For a nicely centered equation use the double dollar signs $$ $$ on separate lines", "to get \\[ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\]", "to get \\[ H_0: \\mu_{\\text{Group 1}} = \\mu_{\\text{Group 2}} \\] \\[ H_a: \\mu_{\\text{Group 1}} \\neq \\mu_{\\text{Group 2}} \\]", "Here is a list of all supported LaTeX commands."],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Math Equations"
  },
  {
    "id": 266,
    "title": "ğŸ“ Insert a Picture",
    "url": "RMarkdownHints.html#insertapicture",
    "content": "Insert a Picture To add a picture to your document, say some notes you took down on paper from class, Use the code: ![](./Images/insertPictureNotes.jpg) to getâ€¦",
    "sentences": ["To add a picture to your document, say some notes you took down on paper from class,", "Use the code: ![](./Images/insertPictureNotes.jpg) to getâ€¦"],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Insert a Picture"
  },
  {
    "id": 267,
    "title": "ğŸ“ Tables",
    "url": "RMarkdownHints.html#tables",
    "content": "Tables There are many ways to make tables in R Markdown. Here is a simple way to make a â€œpipeâ€ table. | Name | Age | Gender | |---------------|---------------|--------------| | Jill | 8 | Female | | Jack | 9 | Male |",
    "sentences": ["There are many ways to make tables in R Markdown.", "Here is a simple way to make a â€œpipeâ€ table.", "| Name | Age | Gender | |---------------|---------------|--------------| | Jill | 8 | Female | | Jack | 9 | Male |"],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Tables"
  },
  {
    "id": 268,
    "title": "ğŸ“ Themes",
    "url": "RMarkdownHints.html#themes",
    "content": "Themes Notice in the YAML (at the top of the RMD file) there is a line that reads: â€œtheme: ceruleanâ€ Other possible themes are â€œdefaultâ€, â€œceruleanâ€, â€œjournalâ€, â€œflatlyâ€, â€œreadableâ€, â€œspacelabâ€, â€œunitedâ€, and â€œcosmoâ€. You can also change the highlighting by adding the line â€œhighlight: tangoâ€ to the YAML as follows. --- title: \"Markdown Hints\" output: html_document: theme: cerulean highlight: tango --- Other highlighting options are â€œdefaultâ€, â€œtangoâ€, â€œpygmentsâ€, â€œkateâ€, â€œmonochromeâ€, â€œespressoâ€, â€œzenburnâ€, â€œhaddockâ€, and â€œtextmateâ€.",
    "sentences": ["Notice in the YAML (at the top of the RMD file) there is a line that reads:", "â€œtheme: ceruleanâ€", "Other possible themes are", "â€œdefaultâ€, â€œceruleanâ€, â€œjournalâ€, â€œflatlyâ€, â€œreadableâ€, â€œspacelabâ€, â€œunitedâ€, and â€œcosmoâ€.", "You can also change the highlighting by adding the line â€œhighlight: tangoâ€ to the YAML as follows.", "--- title: \"Markdown Hints\" output: html_document: theme: cerulean highlight: tango ---", "Other highlighting options are", "â€œdefaultâ€, â€œtangoâ€, â€œpygmentsâ€, â€œkateâ€, â€œmonochromeâ€, â€œespressoâ€, â€œzenburnâ€, â€œhaddockâ€, and â€œtextmateâ€."],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Themes"
  },
  {
    "id": 269,
    "title": "ğŸ“ More Information",
    "url": "RMarkdownHints.html#moreinformation",
    "content": "More Information Go to the rmarkdown.rstudio.com website for more information on how to use R Markdown.",
    "sentences": "Go to the rmarkdown.rstudio.com website for more information on how to use R Markdown.",
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "More Information"
  },
  {
    "id": 270,
    "title": "t Tests",
    "url": "tTests.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus t Tests Much of statistical inference concerns the location of the population mean \\(\\mu\\) for a given parametric distribution. Some of the most common approaches to making inference about \\(\\mu\\) utilize a test statistic that follows a t distribution. One Sample t Test A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable. Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average? Is human body temperature really 98.6&deg; F on average? Do I spend less than $3 a day, on average, purchasing snacks? Requirements This test is only appropriate when both of the following are satisfied. The sample is representative of the population. (Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal. This is a safe assumption when either (a) the population data can be assumed to be normally distributed using a Q-Q Plot or (b) the size of the sample (n) that was taken from the population is large (at least n > 30, but â€œlargeâ€ really depends on how badly the data is skewed). If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population. If the requirements are not met, then that doesnâ€™t mean the results of the test are necessarily bad, but there is no guarantee that they are good. Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits R Instructions Console Help Command: ?t.test() t.test(NameOfYourData$Y, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set, like mtcars or KidsFeet. Y must be a â€œnumericâ€ vector of quantitative data. YourNull is the numeric value from your null hypothesis for \\(\\mu\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(NameOfYourData$Y) Example Code Hover your mouse over the example codes to learn more. t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. mtcars â€˜mtcarsâ€™ is a dataset. Type â€˜View(mtcars)â€™ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,Â  â€˜mpgâ€™ is Y, a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,Â  The numeric value from the null hypothesis is 20 meaning \\(\\mu=20\\). alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1âˆ’Î±. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. One Sample t-test EXPLANATION. data: mtcars$mpg EXPLANATION. t = 0.08506, EXPLANATION. Â df = 31, EXPLANATION. Â p-value = 0.9328 EXPLANATION. alternative hypothesis: true mean is not equal to 20 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â 17.91768 EXPLANATION. Â 22.26357 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION. Â 20.09062 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. mtcars â€˜mtcarsâ€™ is a dataset. Type â€˜View(mtcars)â€™ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg) â€˜mpgâ€™ is a quantitative variable (numeric vector) from the mtcars dataset. Â Click to Show OutputÂ  Click to View Output. ## [1] 20 18 Explanation In many cases where it is of interest to test a claim about a single population mean \\(\\mu\\), the one sample t test is used. This is an appropriate decision whenever the sampling distribution of the sample mean can be assumed to be normal and the data represents a simple random sample from the population. In the figure below, the null hypothesis \\(H_0: \\mu = \\mu_0\\) is represented by the normal distribution (gray) centered at \\(\\mu_0\\). Note that \\(\\mu_0\\) is just some specified number. This shows how the null hypothesis represents the assumption about the center of the distribution of the data. After a hypothesis (null) is established and an alternative hypothesis similarly declared, a simple random sample of data of size \\(n\\) is obtained from the population of interest. In the plot above, this is depicted by the points (blue dots) which are centered around their sample mean \\(\\bar{x}\\). Above the points (blue dots) is shown a second normal distribution (blue dashed line) which represents the idea that the alternative hypothesis allows for a normal distribution which is potentially more consistent with the data than the one specified under the null hypothesis. The role of the one sample t test is to measure the probability of a sample mean being as extreme or more extreme from the hypothesized value of \\(\\mu_0\\) than the one observed assuming the null hypothesis is true. This probability is of course the p-value of the test. This works because the sampling distribution of the sample mean has been assumed to be normal. In this case, the distribution of the test statistic t, \\[ t = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\] is known to follow a t distribution with \\(n-1\\) degrees of freedom. (The mathematics that provide this result are phenominal! You can consult any advanced statistical textbook for the details.) The p-value of the one sample t test represents the probability that the test statistic \\(t\\) is as extreme or more extreme than the one observed according to a t-distribution with \\(n-1\\) degrees of freedom. If the probability (the p-value) is close enough to zero (smaller than \\(\\alpha\\)) then it is determined that the most plausible hypothesis is the alternative hypothesis, and thus the null is â€œrejectedâ€ in favor of the alternative. Paired Samples t Test The paired samples t test is used when a value is hypothesized for the popluation mean of the differences, \\(\\mu_d\\), obtained from paired observations. Overview Questions The Paired Samples t Test can be used to answer questions like: From pre-test to post-test is there an improvement on average in the subjects? How much taller are husbands than their wives, on average? Do hospital patients that are carefully matched together according to reason for being in the hospital, age, gender, ethnicity, height, and weight show increased stay times in the hospital when infected with a nosocomial infection compared to those who were not infected? Requirements The test is only appropriate when both of the following are satisfied. The sample of differences is representative of the population differences. The sampling distribution of the sample mean of the differences \\(\\bar{d}\\) (\\(\\bar{x}\\) of the differences) can be assumed to be normal. (This second requirement can be assumed to be satisfied when (a) the differences themselves can be assumed to be normal from a Q-Q Plot, or (b) when the sample size \\(n\\) of the differences is large.) Hypotheses Math Code $$ H_0: \\mu_d = 0 $$ $$ H_a: \\mu_d \\neq 0 $$ \\(H_0: \\mu_d = \\text{some number, but typically 0}\\)\\(H_a: \\mu_d \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepPaired studentPaired R Instructions Console Help Command: ?t.test() Option 1: t.test(NameOfYourData$Y1, NameOfYourData$Y2, paired = TRUE, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set like sleep or mtcars or KidsFeet. Y1 must be a â€œnumericâ€ vector that represents the quantitative data from the first sample of data. Y2 must be a â€œnumericâ€ vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y1 - Y2) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the â€œgroup1â€ data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the â€œgroup2â€ data from the sleep data set t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. sleep2$extra,Â  A numeric vector that represents the hours of extra sleep that the group had with drug 2. sleep1$extra,Â  A numeric vector that represents the hours of extra sleep that the same group had with drug 1. paired=TRUE,Â  Indicates that this is a paired t-Test. This will cause the subtraction of sleep2$extra - sleep1$extra to be performed to obtain the paired differences. To cause the subtraction to occur in the other order, reverse the order sleep1$extra, sleep2$extra occur in the t.test(â€¦) function. mu = 0,Â  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\). Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. Paired t-test EXPLANATION. data: sleep2$extra and sleep1$extra EXPLANATION. t = 4.0621, EXPLANATION. Â df = 9, EXPLANATION. Â p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â 0.7001142 EXPLANATION. Â 2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of the differences EXPLANATION. Â  1.58 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. sleep2$extra The hours of extra sleep that the group had with drug 2. Â -Â  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. ) Closing parenthesis for qqPlot(â€¦) function. Â Click to Show OutputÂ  Click to View Output. ## [1] 9 5 Option 2: Compute the differences yourself instead of using paired=TRUE. differences = NameOfYourData$Y1 - NameOfYourData$Y2 t.test(differences, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a â€œnumericâ€ vector that represents the quantitative data from the first sample of data. Y2 must be a â€œnumericâ€ vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. differences are the resulting differences obtained from subtracting Y1 - Y2. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(differences) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the â€œgroup1â€ data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the â€œgroup2â€ data from the sleep data set differencesÂ <-Â  Saved the computed differences to an object called â€˜differencesâ€™. sleep2$extra The hours of extra sleep that the group had with drug 2. Â -Â  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. differences,Â  â€˜differencesâ€™ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2. mu = 0,Â  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\). Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. One Sample t-test EXPLANATION. data: differences EXPLANATION. t = 4.0621, EXPLANATION. Â df = 9, EXPLANATION. Â p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â 0.7001142 EXPLANATION. Â 2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION. Â  1.58 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. differences) â€˜differencesâ€™ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2. Â Click to Show OutputÂ  Click to View Output. ## [1] 9 5 Explanation The paired samples t test considers the single mean of all the differences from the paired values. Thus, the paired samples t test essentially becomes a one sample t test on the differences between paired observations. Hence the requirement is that the sampling distribution of the sample mean of the differences, \\(\\bar{d}\\), can be assumed to be normally distri",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus t Tests Much of statistical inference concerns the location of the population mean \\(\\mu\\) for a given parametric distribution.", "Some of the most common approaches to making inference about \\(\\mu\\) utilize a test statistic that follows a t distribution.", "One Sample t Test A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable.", "Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average?", "Is human body temperature really 98.6&deg; F on average?", "Do I spend less than $3 a day, on average, purchasing snacks?", "Requirements This test is only appropriate when both of the following are satisfied.", "The sample is representative of the population.", "(Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal.", "This is a safe assumption when either (a) the population data can be assumed to be normally distributed using a Q-Q Plot or (b) the size of the sample (n) that was taken from the population is large (at least n > 30, but â€œlargeâ€ really depends on how badly the data is skewed).", "If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population.", "If the requirements are not met, then that doesnâ€™t mean the results of the test are necessarily bad, but there is no guarantee that they are good.", "Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits R Instructions Console Help Command: ?t.test() t.test(NameOfYourData$Y, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set, like mtcars or KidsFeet.", "Y must be a â€œnumericâ€ vector of quantitative data.", "YourNull is the numeric value from your null hypothesis for \\(\\mu\\).", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Testing Assumptions library(car) qqPlot(NameOfYourData$Y) Example Code Hover your mouse over the example codes to learn more.", "t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests.", "mtcars â€˜mtcarsâ€™ is a dataset.", "Type â€˜View(mtcars)â€™ in R to view the dataset.", "$ The $ allows us to access any variable from the mtcars dataset.", "mpg,Â  â€˜mpgâ€™ is Y, a quantitative variable (numeric vector) from the mtcars dataset.", "mu = 20,Â  The numeric value from the null hypothesis is 20 meaning \\(\\mu=20\\).", "alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu\\neq20\\).", "conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1âˆ’Î±.", "Â Â Â Â Press Enter to run the code if you have typed it in yourself.", "You can also click here to view the output.", "Â Click to Show OutputÂ  Click to View Output.", "One Sample t-test EXPLANATION.", "data: mtcars$mpg EXPLANATION.", "t = 0.08506, EXPLANATION.", "Â df = 31, EXPLANATION.", "Â p-value = 0.9328 EXPLANATION.", "alternative hypothesis: true mean is not equal to 20 EXPLANATION.", "95 percent confidence interval: EXPLANATION.", "Â 17.91768 EXPLANATION.", "Â 22.26357 EXPLANATION.", "sample estimates: EXPLANATION.", "mean of x EXPLANATION.", "Â 20.09062 EXPLANATION.", "qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot.", "mtcars â€˜mtcarsâ€™ is a dataset.", "Type â€˜View(mtcars)â€™ in R to view the dataset.", "$ The $ allows us to access any variable from the mtcars dataset.", "mpg) â€˜mpgâ€™ is a quantitative variable (numeric vector) from the mtcars dataset.", "Â Click to Show OutputÂ  Click to View Output.", "## [1] 20 18 Explanation In many cases where it is of interest to test a claim about a single population mean \\(\\mu\\), the one sample t test is used.", "This is an appropriate decision whenever the sampling distribution of the sample mean can be assumed to be normal and the data represents a simple random sample from the population."],
    "type": "page",
    "page_title": "t Tests"
  },
  {
    "id": 271,
    "title": "ğŸ“ One Sample t Test",
    "url": "tTests.html#onesamplettest",
    "content": "One Sample t Test A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable. Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average? Is human body temperature really 98.6&deg; F on average? Do I spend less than $3 a day, on average, purchasing snacks? Requirements This test is only appropriate when both of the following are satisfied. The sample is representative of the population. (Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal. This is a safe assumption when either (a) the population data can be assumed to be normally distributed using a Q-Q Plot or (b) the size of the sample (n) that was taken from the population is large (at least n > 30, but â€œlargeâ€ really depends on how badly the data is skewed). If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population. If the requirements are not met, then that doesnâ€™t mean the results of the test are necessarily bad, but there is no guarantee that they are good. Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits",
    "sentences": ["A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable.", "Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average?", "Is human body temperature really 98.6&deg; F on average?", "Do I spend less than $3 a day, on average, purchasing snacks?", "Requirements This test is only appropriate when both of the following are satisfied.", "The sample is representative of the population.", "(Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal.", "This is a safe assumption when either (a) the population data can be assumed to be normally distributed using a Q-Q Plot or (b) the size of the sample (n) that was taken from the population is large (at least n > 30, but â€œlargeâ€ really depends on how badly the data is skewed).", "If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population.", "If the requirements are not met, then that doesnâ€™t mean the results of the test are necessarily bad, but there is no guarantee that they are good."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "One Sample t Test"
  },
  {
    "id": 272,
    "title": "ğŸ“ Overview",
    "url": "tTests.html#overview",
    "content": "Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average? Is human body temperature really 98.6&deg; F on average? Do I spend less than $3 a day, on average, purchasing snacks? Requirements This test is only appropriate when both of the following are satisfied. The sample is representative of the population. (Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal. This is a safe assumption when either (a) the population data can be assumed to be normally distributed using a Q-Q Plot or (b) the size of the sample (n) that was taken from the population is large (at least n > 30, but â€œlargeâ€ really depends on how badly the data is skewed). If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population. If the requirements are not met, then that doesnâ€™t mean the results of the test are necessarily bad, but there is no guarantee that they are good. Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits",
    "sentences": ["Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average?", "Is human body temperature really 98.6&deg; F on average?", "Do I spend less than $3 a day, on average, purchasing snacks?", "Requirements This test is only appropriate when both of the following are satisfied.", "The sample is representative of the population.", "(Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal.", "This is a safe assumption when either (a) the population data can be assumed to be normally distributed using a Q-Q Plot or (b) the size of the sample (n) that was taken from the population is large (at least n > 30, but â€œlargeâ€ really depends on how badly the data is skewed).", "If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population.", "If the requirements are not met, then that doesnâ€™t mean the results of the test are necessarily bad, but there is no guarantee that they are good.", "Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits"],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "Overview"
  },
  {
    "id": 273,
    "title": "ğŸ“ R Instructions",
    "url": "tTests.html#rinstructions",
    "content": "R Instructions Console Help Command: ?t.test() t.test(NameOfYourData$Y, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set, like mtcars or KidsFeet. Y must be a â€œnumericâ€ vector of quantitative data. YourNull is the numeric value from your null hypothesis for \\(\\mu\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(NameOfYourData$Y) Example Code Hover your mouse over the example codes to learn more. t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. mtcars â€˜mtcarsâ€™ is a dataset. Type â€˜View(mtcars)â€™ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,Â  â€˜mpgâ€™ is Y, a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,Â  The numeric value from the null hypothesis is 20 meaning \\(\\mu=20\\). alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1âˆ’Î±. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. One Sample t-test EXPLANATION. data: mtcars$mpg EXPLANATION. t = 0.08506, EXPLANATION. Â df = 31, EXPLANATION. Â p-value = 0.9328 EXPLANATION. alternative hypothesis: true mean is not equal to 20 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â 17.91768 EXPLANATION. Â 22.26357 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION. Â 20.09062 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. mtcars â€˜mtcarsâ€™ is a dataset. Type â€˜View(mtcars)â€™ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg) â€˜mpgâ€™ is a quantitative variable (numeric vector) from the mtcars dataset. Â Click to Show OutputÂ  Click to View Output. ## [1] 20 18",
    "sentences": ["Console Help Command: ?t.test() t.test(NameOfYourData$Y, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set, like mtcars or KidsFeet.", "Y must be a â€œnumericâ€ vector of quantitative data.", "YourNull is the numeric value from your null hypothesis for \\(\\mu\\).", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Testing Assumptions library(car) qqPlot(NameOfYourData$Y) Example Code Hover your mouse over the example codes to learn more.", "t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests.", "mtcars â€˜mtcarsâ€™ is a dataset.", "Type â€˜View(mtcars)â€™ in R to view the dataset."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "R Instructions"
  },
  {
    "id": 274,
    "title": "ğŸ“ Explanation",
    "url": "tTests.html#explanation",
    "content": "Explanation In many cases where it is of interest to test a claim about a single population mean \\(\\mu\\), the one sample t test is used. This is an appropriate decision whenever the sampling distribution of the sample mean can be assumed to be normal and the data represents a simple random sample from the population. In the figure below, the null hypothesis \\(H_0: \\mu = \\mu_0\\) is represented by the normal distribution (gray) centered at \\(\\mu_0\\). Note that \\(\\mu_0\\) is just some specified number. This shows how the null hypothesis represents the assumption about the center of the distribution of the data. After a hypothesis (null) is established and an alternative hypothesis similarly declared, a simple random sample of data of size \\(n\\) is obtained from the population of interest. In the plot above, this is depicted by the points (blue dots) which are centered around their sample mean \\(\\bar{x}\\). Above the points (blue dots) is shown a second normal distribution (blue dashed line) which represents the idea that the alternative hypothesis allows for a normal distribution which is potentially more consistent with the data than the one specified under the null hypothesis. The role of the one sample t test is to measure the probability of a sample mean being as extreme or more extreme from the hypothesized value of \\(\\mu_0\\) than the one observed assuming the null hypothesis is true. This probability is of course the p-value of the test. This works because the sampling distribution of the sample mean has been assumed to be normal. In this case, the distribution of the test statistic t, \\[ t = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\] is known to follow a t distribution with \\(n-1\\) degrees of freedom. (The mathematics that provide this result are phenominal! You can consult any advanced statistical textbook for the details.) The p-value of the one sample t test represents the probability that the test statistic \\(t\\) is as extreme or more extreme than the one observed according to a t-distribution with \\(n-1\\) degrees of freedom. If the probability (the p-value) is close enough to zero (smaller than \\(\\alpha\\)) then it is determined that the most plausible hypothesis is the alternative hypothesis, and thus the null is â€œrejectedâ€ in favor of the alternative.",
    "sentences": ["In many cases where it is of interest to test a claim about a single population mean \\(\\mu\\), the one sample t test is used.", "This is an appropriate decision whenever the sampling distribution of the sample mean can be assumed to be normal and the data represents a simple random sample from the population.", "In the figure below, the null hypothesis \\(H_0: \\mu = \\mu_0\\) is represented by the normal distribution (gray) centered at \\(\\mu_0\\).", "Note that \\(\\mu_0\\) is just some specified number.", "This shows how the null hypothesis represents the assumption about the center of the distribution of the data.", "After a hypothesis (null) is established and an alternative hypothesis similarly declared, a simple random sample of data of size \\(n\\) is obtained from the population of interest.", "In the plot above, this is depicted by the points (blue dots) which are centered around their sample mean \\(\\bar{x}\\).", "Above the points (blue dots) is shown a second normal distribution (blue dashed line) which represents the idea that the alternative hypothesis allows for a normal distribution which is potentially more consistent with the data than the one specified under the null hypothesis.", "The role of the one sample t test is to measure the probability of a sample mean being as extreme or more extreme from the hypothesized value of \\(\\mu_0\\) than the one observed assuming the null hypothesis is true.", "This probability is of course the p-value of the test."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "Explanation"
  },
  {
    "id": 275,
    "title": "ğŸ“ Paired Samples t Test",
    "url": "tTests.html#pairedsamplesttest",
    "content": "Paired Samples t Test The paired samples t test is used when a value is hypothesized for the popluation mean of the differences, \\(\\mu_d\\), obtained from paired observations. Overview Questions The Paired Samples t Test can be used to answer questions like: From pre-test to post-test is there an improvement on average in the subjects? How much taller are husbands than their wives, on average? Do hospital patients that are carefully matched together according to reason for being in the hospital, age, gender, ethnicity, height, and weight show increased stay times in the hospital when infected with a nosocomial infection compared to those who were not infected? Requirements The test is only appropriate when both of the following are satisfied. The sample of differences is representative of the population differences. The sampling distribution of the sample mean of the differences \\(\\bar{d}\\) (\\(\\bar{x}\\) of the differences) can be assumed to be normal. (This second requirement can be assumed to be satisfied when (a) the differences themselves can be assumed to be normal from a Q-Q Plot, or (b) when the sample size \\(n\\) of the differences is large.) Hypotheses Math Code $$ H_0: \\mu_d = 0 $$ $$ H_a: \\mu_d \\neq 0 $$ \\(H_0: \\mu_d = \\text{some number, but typically 0}\\)\\(H_a: \\mu_d \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepPaired studentPaired R Instructions Console Help Command: ?t.test() Option 1: t.test(NameOfYourData$Y1, NameOfYourData$Y2, paired = TRUE, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set like sleep or mtcars or KidsFeet. Y1 must be a â€œnumericâ€ vector that represents the quantitative data from the first sample of data. Y2 must be a â€œnumericâ€ vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y1 - Y2) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the â€œgroup1â€ data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the â€œgroup2â€ data from the sleep data set t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. sleep2$extra,Â  A numeric vector that represents the hours of extra sleep that the group had with drug 2. sleep1$extra,Â  A numeric vector that represents the hours of extra sleep that the same group had with drug 1. paired=TRUE,Â  Indicates that this is a paired t-Test. This will cause the subtraction of sleep2$extra - sleep1$extra to be performed to obtain the paired differences. To cause the subtraction to occur in the other order, reverse the order sleep1$extra, sleep2$extra occur in the t.test(â€¦) function. mu = 0,Â  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\). Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. Paired t-test EXPLANATION. data: sleep2$extra and sleep1$extra EXPLANATION. t = 4.0621, EXPLANATION. Â df = 9, EXPLANATION. Â p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â 0.7001142 EXPLANATION. Â 2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of the differences EXPLANATION. Â  1.58 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. sleep2$extra The hours of extra sleep that the group had with drug 2. Â -Â  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. ) Closing parenthesis for qqPlot(â€¦) function. Â Click to Show OutputÂ  Click to View Output. ## [1] 9 5 Option 2: Compute the differences yourself instead of using paired=TRUE. differences = NameOfYourData$Y1 - NameOfYourData$Y2 t.test(differences, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a â€œnumericâ€ vector that represents the quantitative data from the first sample of data. Y2 must be a â€œnumericâ€ vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. differences are the resulting differences obtained from subtracting Y1 - Y2. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(differences) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the â€œgroup1â€ data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the â€œgroup2â€ data from the sleep data set differencesÂ <-Â  Saved the computed differences to an object called â€˜differencesâ€™. sleep2$extra The hours of extra sleep that the group had with drug 2. Â -Â  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. differences,Â  â€˜differencesâ€™ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2. mu = 0,Â  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\). Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. One Sample t-test EXPLANATION. data: differences EXPLANATION. t = 4.0621, EXPLANATION. Â df = 9, EXPLANATION. Â p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â 0.7001142 EXPLANATION. Â 2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION. Â  1.58 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. differences) â€˜differencesâ€™ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2. Â Click to Show OutputÂ  Click to View Output.",
    "sentences": ["The paired samples t test is used when a value is hypothesized for the popluation mean of the differences, \\(\\mu_d\\), obtained from paired observations.", "Overview Questions The Paired Samples t Test can be used to answer questions like: From pre-test to post-test is there an improvement on average in the subjects?", "How much taller are husbands than their wives, on average?", "Do hospital patients that are carefully matched together according to reason for being in the hospital, age, gender, ethnicity, height, and weight show increased stay times in the hospital when infected with a nosocomial infection compared to those who were not infected?", "Requirements The test is only appropriate when both of the following are satisfied.", "The sample of differences is representative of the population differences.", "The sampling distribution of the sample mean of the differences \\(\\bar{d}\\) (\\(\\bar{x}\\) of the differences) can be assumed to be normal.", "(This second requirement can be assumed to be satisfied when (a) the differences themselves can be assumed to be normal from a Q-Q Plot, or (b) when the sample size \\(n\\) of the differences is large.) Hypotheses Math Code $$ H_0: \\mu_d = 0 $$ $$ H_a: \\mu_d \\neq 0 $$ \\(H_0: \\mu_d = \\text{some number, but typically 0}\\)\\(H_a: \\mu_d \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepPaired studentPaired", "R Instructions Console Help Command: ?t.test() Option 1: t.test(NameOfYourData$Y1, NameOfYourData$Y2, paired = TRUE, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set like sleep or mtcars or KidsFeet.", "Y1 must be a â€œnumericâ€ vector that represents the quantitative data from the first sample of data."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "Paired Samples t Test"
  },
  {
    "id": 276,
    "title": "ğŸ“ Overview",
    "url": "tTests.html#overview",
    "content": "Overview Questions The Paired Samples t Test can be used to answer questions like: From pre-test to post-test is there an improvement on average in the subjects? How much taller are husbands than their wives, on average? Do hospital patients that are carefully matched together according to reason for being in the hospital, age, gender, ethnicity, height, and weight show increased stay times in the hospital when infected with a nosocomial infection compared to those who were not infected? Requirements The test is only appropriate when both of the following are satisfied. The sample of differences is representative of the population differences. The sampling distribution of the sample mean of the differences \\(\\bar{d}\\) (\\(\\bar{x}\\) of the differences) can be assumed to be normal. (This second requirement can be assumed to be satisfied when (a) the differences themselves can be assumed to be normal from a Q-Q Plot, or (b) when the sample size \\(n\\) of the differences is large.) Hypotheses Math Code $$ H_0: \\mu_d = 0 $$ $$ H_a: \\mu_d \\neq 0 $$ \\(H_0: \\mu_d = \\text{some number, but typically 0}\\)\\(H_a: \\mu_d \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepPaired studentPaired",
    "sentences": ["Questions The Paired Samples t Test can be used to answer questions like: From pre-test to post-test is there an improvement on average in the subjects?", "How much taller are husbands than their wives, on average?", "Do hospital patients that are carefully matched together according to reason for being in the hospital, age, gender, ethnicity, height, and weight show increased stay times in the hospital when infected with a nosocomial infection compared to those who were not infected?", "Requirements The test is only appropriate when both of the following are satisfied.", "The sample of differences is representative of the population differences.", "The sampling distribution of the sample mean of the differences \\(\\bar{d}\\) (\\(\\bar{x}\\) of the differences) can be assumed to be normal.", "(This second requirement can be assumed to be satisfied when (a) the differences themselves can be assumed to be normal from a Q-Q Plot, or (b) when the sample size \\(n\\) of the differences is large.) Hypotheses Math Code $$ H_0: \\mu_d = 0 $$ $$ H_a: \\mu_d \\neq 0 $$ \\(H_0: \\mu_d = \\text{some number, but typically 0}\\)\\(H_a: \\mu_d \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepPaired studentPaired"],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "Overview"
  },
  {
    "id": 277,
    "title": "ğŸ“ R Instructions",
    "url": "tTests.html#rinstructions",
    "content": "R Instructions Console Help Command: ?t.test() Option 1: t.test(NameOfYourData$Y1, NameOfYourData$Y2, paired = TRUE, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set like sleep or mtcars or KidsFeet. Y1 must be a â€œnumericâ€ vector that represents the quantitative data from the first sample of data. Y2 must be a â€œnumericâ€ vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y1 - Y2) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the â€œgroup1â€ data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the â€œgroup2â€ data from the sleep data set t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. sleep2$extra,Â  A numeric vector that represents the hours of extra sleep that the group had with drug 2. sleep1$extra,Â  A numeric vector that represents the hours of extra sleep that the same group had with drug 1. paired=TRUE,Â  Indicates that this is a paired t-Test. This will cause the subtraction of sleep2$extra - sleep1$extra to be performed to obtain the paired differences. To cause the subtraction to occur in the other order, reverse the order sleep1$extra, sleep2$extra occur in the t.test(â€¦) function. mu = 0,Â  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\). Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. Paired t-test EXPLANATION. data: sleep2$extra and sleep1$extra EXPLANATION. t = 4.0621, EXPLANATION. Â df = 9, EXPLANATION. Â p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â 0.7001142 EXPLANATION. Â 2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of the differences EXPLANATION. Â  1.58 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. sleep2$extra The hours of extra sleep that the group had with drug 2. Â -Â  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. ) Closing parenthesis for qqPlot(â€¦) function. Â Click to Show OutputÂ  Click to View Output. ## [1] 9 5 Option 2: Compute the differences yourself instead of using paired=TRUE. differences = NameOfYourData$Y1 - NameOfYourData$Y2 t.test(differences, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a â€œnumericâ€ vector that represents the quantitative data from the first sample of data. Y2 must be a â€œnumericâ€ vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. differences are the resulting differences obtained from subtracting Y1 - Y2. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(differences) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the â€œgroup1â€ data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the â€œgroup2â€ data from the sleep data set differencesÂ <-Â  Saved the computed differences to an object called â€˜differencesâ€™. sleep2$extra The hours of extra sleep that the group had with drug 2. Â -Â  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. differences,Â  â€˜differencesâ€™ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2. mu = 0,Â  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\). Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. One Sample t-test EXPLANATION. data: differences EXPLANATION. t = 4.0621, EXPLANATION. Â df = 9, EXPLANATION. Â p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â 0.7001142 EXPLANATION. Â 2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION. Â  1.58 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. differences) â€˜differencesâ€™ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2. Â Click to Show OutputÂ  Click to View Output.",
    "sentences": ["Console Help Command: ?t.test() Option 1: t.test(NameOfYourData$Y1, NameOfYourData$Y2, paired = TRUE, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set like sleep or mtcars or KidsFeet.", "Y1 must be a â€œnumericâ€ vector that represents the quantitative data from the first sample of data.", "Y2 must be a â€œnumericâ€ vector that represents the quantitative data from the second sample of data.", "This vector must be in the same order as the first sample so that the pairing can take place.", "YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\).", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Testing Assumptions library(car) qqPlot(Y1 - Y2) Example Code Hover your mouse over the example codes to learn more.", "sleep1 <- filter(sleep, group==1) This splits out the â€œgroup1â€ data from the sleep data set."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "R Instructions"
  },
  {
    "id": 278,
    "title": "ğŸ“ Explanation",
    "url": "tTests.html#explanation",
    "content": "Explanation The paired samples t test considers the single mean of all the differences from the paired values. Thus, the paired samples t test essentially becomes a one sample t test on the differences between paired observations. Hence the requirement is that the sampling distribution of the sample mean of the differences, \\(\\bar{d}\\), can be assumed to be normally distributed. (It is also required that the obtained differences represent a simple random sample of the full population of possible differences.) The paired samples t test is similar to the independent samples t test scenario, except that there is extra information that allows values from one sample to be paired with a value from the other sample. This pairing of values allows for a more direct analysis of the change or difference individuals experience between the two samples. The points in the plot below demonstrate how points are paired together, and the only thing of interest are the differences between the paired points.",
    "sentences": ["The paired samples t test considers the single mean of all the differences from the paired values.", "Thus, the paired samples t test essentially becomes a one sample t test on the differences between paired observations.", "Hence the requirement is that the sampling distribution of the sample mean of the differences, \\(\\bar{d}\\), can be assumed to be normally distributed.", "(It is also required that the obtained differences represent a simple random sample of the full population of possible differences.) The paired samples t test is similar to the independent samples t test scenario, except that there is extra information that allows values from one sample to be paired with a value from the other sample.", "This pairing of values allows for a more direct analysis of the change or difference individuals experience between the two samples.", "The points in the plot below demonstrate how points are paired together, and the only thing of interest are the differences between the paired points."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "Explanation"
  },
  {
    "id": 279,
    "title": "ğŸ“ Independent Samples t\r\nTest",
    "url": "tTests.html#independentsamplesttest",
    "content": "Independent Samples t\r\nTest The independent samples t test is used when a value is hypothesized for the difference between two (possibly) different population means, \\(\\mu_1 - \\mu_2\\). Overview Questions The Independent Samples t Test can be used to answer questions like: Are boys taller than girls on average? Do students who show up to class everyday get higher scores on average than those who donâ€™t? Do you take more steps on average on weekdays or on weekends? Requirements The test is only appropriate when both of the following are satisfied. Both samples are representative of the population. (Simple random samples are the best way to do this.) The sampling distribution of the difference of the sample means \\((\\bar{x}_1 - \\bar{x}_2)\\) can be assumed to be normal. (This is a safe assumption when the sample size of each group is \\(30\\) or greater or when the population data from each group can be assumed to be normal with a Q-Q Plot.) Hypotheses Math Code $$ H_0: \\mu_\\text{label 1} - \\mu_\\text{label 2} = 13.2 $$ $$ H_a: \\mu_\\text{label 1} - \\mu_\\text{label 2} \\neq 13.2 $$ \\(H_0: \\mu_1 - \\mu_2 = \\text{some number, but typically 0}\\) \\(H_a: \\mu_1 - \\mu_2 \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepInd student1 student2 R Instructions Console Help Command: ?t.test() There are two ways to perform the test. Option 1: t.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a â€œnumericâ€ vector from YourData that represents the data for both samples. X must be a â€œfactorâ€ or â€œcharacterâ€ vector from YourData that represents the group assignment for each observation. There can only be two groups specified in this column of data. YourNull is the numeric value from your null hypothesis for \\(\\mu_1-\\mu_2\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y ~ X, data=YourData) Example Code Hover your mouse over the example codes to learn more. t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. lengthÂ  â€˜lengthâ€™ is a quantitative variable (numeric vector). ~Â  â€˜~â€™ is the tilde symbol. sex,Â  â€˜sexâ€™ is a â€˜factorâ€™ or â€˜characterâ€™ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet,Â  â€˜KidsFeetâ€™ is a dataset in library(mosaic). Type View(KidsFeet) to view it. mu = 0,Â  The numeric value from the null hypothesis for Î¼1-Î¼2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo-sidedâ€ meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\) Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. Welch Two Sample t-test EXPLANATION. data: length by sex EXPLANATION. t = 1.9174, EXPLANATION. Â df = 36.275, EXPLANATION. Â p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â -0.04502067 EXPLANATION. Â 1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean in group B mean in group G EXPLANATION. Â  25.10500 EXPLANATION. Â  24.32105 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. lengthÂ  â€˜lengthâ€™ is a quantitative variable (numeric vector). ~Â  â€˜~â€™ is the tilde symbol. sex,Â  â€˜sexâ€™ is a â€œfactorâ€ or â€œcharacterâ€ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet) â€˜KidsFeetâ€™ is a dataset in library(mosaic). Type View(KidsFeet) to view it. Â Click to Show OutputÂ  Click to View Output. Option 2: t.test(NameOfYourData$Y1, NameOfYourData$Y2, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a â€œnumericâ€ vector that represents the quantitative data from the first sample. Y2 must be a â€œnumericâ€ vector that represents the quantitative data from the second sample. YourNull is the numeric value from your null hypothesis for the difference of \\(\\mu_1-\\mu_2\\). This is typically zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) par(mfrow=c(1,2)) qqPlot(NameOfYourData$Y1) qqPlot(NameOfYourData$Y2) Example Code Hover your mouse over the example codes to learn more. t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. KidsFeet$length[KidsFeet$sexÂ ==Â â€œBâ€],Â  A numeric vector that represents the quantitative data or the foot length for the first sample of data which in this case is the boys. KidsFeet$length[KidsFeet$sexÂ ==Â â€œGâ€],Â  A numeric vector that represents the quantitative data or the foot length for the second sample of data which in this case is the girls. mu = 0,Â  The numeric value from the null hypothesis for Î¼1-Î¼2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo-sidedâ€ meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\) Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Welch Two Sample t-test EXPLANATION. data: KidsFeet$length[KidsFeet$sex == â€œBâ€] and KidsFeet$length[KidsFeet$sex == â€œGâ€] EXPLANATION. t = 1.9174, EXPLANATION. Â df = 36.275, EXPLANATION. Â p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â -0.04502067 EXPLANATION. Â 1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean of x mean of y EXPLANATION. Â  25.10500 EXPLANATION. Â  24.32105 EXPLANATION. par( â€˜parâ€™ is a R function that can be used to set or query graphical parameters. mfrow=c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. KidsFeet$length[KidsFeet$sexÂ ==Â â€œBâ€]) A numeric vector that represents the quantitative data or the foot length for the first sample of data which in this case is the boys. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. KidsFeet$length[KidsFeet$sexÂ ==Â â€œGâ€]) A numeric vector that represents the quantitative data or the foot length for the second sample of data which in this case is the girls. Â â€¦Â  Click to View Output. ## [1] 9 8 ## [1] 18 7",
    "sentences": ["The independent samples t test is used when a value is hypothesized for the difference between two (possibly) different population means, \\(\\mu_1 - \\mu_2\\).", "Overview Questions The Independent Samples t Test can be used to answer questions like: Are boys taller than girls on average?", "Do students who show up to class everyday get higher scores on average than those who donâ€™t?", "Do you take more steps on average on weekdays or on weekends?", "Requirements The test is only appropriate when both of the following are satisfied.", "Both samples are representative of the population.", "(Simple random samples are the best way to do this.) The sampling distribution of the difference of the sample means \\((\\bar{x}_1 - \\bar{x}_2)\\) can be assumed to be normal.", "(This is a safe assumption when the sample size of each group is \\(30\\) or greater or when the population data from each group can be assumed to be normal with a Q-Q Plot.) Hypotheses Math Code $$ H_0: \\mu_\\text{label 1} - \\mu_\\text{label 2} = 13.2 $$ $$ H_a: \\mu_\\text{label 1} - \\mu_\\text{label 2} \\neq 13.2 $$ \\(H_0: \\mu_1 - \\mu_2 = \\text{some number, but typically 0}\\) \\(H_a: \\mu_1 - \\mu_2 \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepInd student1 student2", "R Instructions Console Help Command: ?t.test() There are two ways to perform the test.", "Option 1: t.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a â€œnumericâ€ vector from YourData that represents the data for both samples."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "Independent Samples t\r\nTest"
  },
  {
    "id": 280,
    "title": "ğŸ“ Overview",
    "url": "tTests.html#overview",
    "content": "Overview Questions The Independent Samples t Test can be used to answer questions like: Are boys taller than girls on average? Do students who show up to class everyday get higher scores on average than those who donâ€™t? Do you take more steps on average on weekdays or on weekends? Requirements The test is only appropriate when both of the following are satisfied. Both samples are representative of the population. (Simple random samples are the best way to do this.) The sampling distribution of the difference of the sample means \\((\\bar{x}_1 - \\bar{x}_2)\\) can be assumed to be normal. (This is a safe assumption when the sample size of each group is \\(30\\) or greater or when the population data from each group can be assumed to be normal with a Q-Q Plot.) Hypotheses Math Code $$ H_0: \\mu_\\text{label 1} - \\mu_\\text{label 2} = 13.2 $$ $$ H_a: \\mu_\\text{label 1} - \\mu_\\text{label 2} \\neq 13.2 $$ \\(H_0: \\mu_1 - \\mu_2 = \\text{some number, but typically 0}\\) \\(H_a: \\mu_1 - \\mu_2 \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepInd student1 student2",
    "sentences": ["Questions The Independent Samples t Test can be used to answer questions like: Are boys taller than girls on average?", "Do students who show up to class everyday get higher scores on average than those who donâ€™t?", "Do you take more steps on average on weekdays or on weekends?", "Requirements The test is only appropriate when both of the following are satisfied.", "Both samples are representative of the population.", "(Simple random samples are the best way to do this.) The sampling distribution of the difference of the sample means \\((\\bar{x}_1 - \\bar{x}_2)\\) can be assumed to be normal.", "(This is a safe assumption when the sample size of each group is \\(30\\) or greater or when the population data from each group can be assumed to be normal with a Q-Q Plot.) Hypotheses Math Code $$ H_0: \\mu_\\text{label 1} - \\mu_\\text{label 2} = 13.2 $$ $$ H_a: \\mu_\\text{label 1} - \\mu_\\text{label 2} \\neq 13.2 $$ \\(H_0: \\mu_1 - \\mu_2 = \\text{some number, but typically 0}\\) \\(H_a: \\mu_1 - \\mu_2 \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepInd student1 student2"],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "Overview"
  },
  {
    "id": 281,
    "title": "ğŸ“ R Instructions",
    "url": "tTests.html#rinstructions",
    "content": "R Instructions Console Help Command: ?t.test() There are two ways to perform the test. Option 1: t.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a â€œnumericâ€ vector from YourData that represents the data for both samples. X must be a â€œfactorâ€ or â€œcharacterâ€ vector from YourData that represents the group assignment for each observation. There can only be two groups specified in this column of data. YourNull is the numeric value from your null hypothesis for \\(\\mu_1-\\mu_2\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y ~ X, data=YourData) Example Code Hover your mouse over the example codes to learn more. t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. lengthÂ  â€˜lengthâ€™ is a quantitative variable (numeric vector). ~Â  â€˜~â€™ is the tilde symbol. sex,Â  â€˜sexâ€™ is a â€˜factorâ€™ or â€˜characterâ€™ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet,Â  â€˜KidsFeetâ€™ is a dataset in library(mosaic). Type View(KidsFeet) to view it. mu = 0,Â  The numeric value from the null hypothesis for Î¼1-Î¼2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo-sidedâ€ meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\) Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â Click to Show OutputÂ  Click to View Output. Welch Two Sample t-test EXPLANATION. data: length by sex EXPLANATION. t = 1.9174, EXPLANATION. Â df = 36.275, EXPLANATION. Â p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â -0.04502067 EXPLANATION. Â 1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean in group B mean in group G EXPLANATION. Â  25.10500 EXPLANATION. Â  24.32105 EXPLANATION. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. lengthÂ  â€˜lengthâ€™ is a quantitative variable (numeric vector). ~Â  â€˜~â€™ is the tilde symbol. sex,Â  â€˜sexâ€™ is a â€œfactorâ€ or â€œcharacterâ€ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet) â€˜KidsFeetâ€™ is a dataset in library(mosaic). Type View(KidsFeet) to view it. Â Click to Show OutputÂ  Click to View Output. Option 2: t.test(NameOfYourData$Y1, NameOfYourData$Y2, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a â€œnumericâ€ vector that represents the quantitative data from the first sample. Y2 must be a â€œnumericâ€ vector that represents the quantitative data from the second sample. YourNull is the numeric value from your null hypothesis for the difference of \\(\\mu_1-\\mu_2\\). This is typically zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) par(mfrow=c(1,2)) qqPlot(NameOfYourData$Y1) qqPlot(NameOfYourData$Y2) Example Code Hover your mouse over the example codes to learn more. t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests. KidsFeet$length[KidsFeet$sexÂ ==Â â€œBâ€],Â  A numeric vector that represents the quantitative data or the foot length for the first sample of data which in this case is the boys. KidsFeet$length[KidsFeet$sexÂ ==Â â€œGâ€],Â  A numeric vector that represents the quantitative data or the foot length for the second sample of data which in this case is the girls. mu = 0,Â  The numeric value from the null hypothesis for Î¼1-Î¼2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo-sidedâ€ meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\) Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Welch Two Sample t-test EXPLANATION. data: KidsFeet$length[KidsFeet$sex == â€œBâ€] and KidsFeet$length[KidsFeet$sex == â€œGâ€] EXPLANATION. t = 1.9174, EXPLANATION. Â df = 36.275, EXPLANATION. Â p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION. Â -0.04502067 EXPLANATION. Â 1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean of x mean of y EXPLANATION. Â  25.10500 EXPLANATION. Â  24.32105 EXPLANATION. par( â€˜parâ€™ is a R function that can be used to set or query graphical parameters. mfrow=c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. KidsFeet$length[KidsFeet$sexÂ ==Â â€œBâ€]) A numeric vector that represents the quantitative data or the foot length for the first sample of data which in this case is the boys. qqPlot( â€˜qqPlotâ€™ is a R function from library(car) that creates a qqPlot. KidsFeet$length[KidsFeet$sexÂ ==Â â€œGâ€]) A numeric vector that represents the quantitative data or the foot length for the second sample of data which in this case is the girls. Â â€¦Â  Click to View Output. ## [1] 9 8 ## [1] 18 7",
    "sentences": ["Console Help Command: ?t.test() There are two ways to perform the test.", "Option 1: t.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a â€œnumericâ€ vector from YourData that represents the data for both samples.", "X must be a â€œfactorâ€ or â€œcharacterâ€ vector from YourData that represents the group assignment for each observation.", "There can only be two groups specified in this column of data.", "YourNull is the numeric value from your null hypothesis for \\(\\mu_1-\\mu_2\\).", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Testing Assumptions library(car) qqPlot(Y ~ X, data=YourData) Example Code Hover your mouse over the example codes to learn more.", "t.test( â€˜t.testâ€™ is an R function that performs one and two sample t-tests."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "R Instructions"
  },
  {
    "id": 282,
    "title": "ğŸ“ Explanation",
    "url": "tTests.html#explanation",
    "content": "Explanation The first figure below depicts the scenario where the difference in means of two separate normal distributions is non-zero. In other words, the two distributions have different means, \\(\\mu_1\\) and \\(\\mu_2\\), respectively. It is worth emphasizing that the values of \\(\\mu_1\\) and \\(\\mu_2\\) are unknown to the researcher. The only thing observed are two separate samples of data (blue dots) of sizes \\(n_1\\) and \\(n_2\\), respectively. For the scenario depicted, the null hypothesis that \\(H_0: \\mu_1 - \\mu_2 = 0\\) (i.e., that \\(\\mu_1=\\mu_2\\)) is rejected in favor of the alternative that \\(H_a: \\mu_1 - \\mu_2 \\neq 0\\) based on the sample data observed. This dicision would be correct as the true difference in means, \\(\\mu_1-\\mu_2\\) is non-zero in this case. When the null hypothesis is true, that \\(H_0: \\mu_1 - \\mu_2 = 0\\), then it follows that the test statistic \\(t\\) that is obtained by measuring the distance between the two sample means, \\(\\bar{x}_1-\\bar{x}_2\\), and appropriately standardizing the result follows a \\(t\\) distribution with degrees of freedom less than or equal to \\(n_1+n_2-2\\). Thus, the \\(p\\)-value of the independent samples \\(t\\) test is obtained by using this \\(t\\) distribution to calculate the probability of a test statistic \\(t\\) being as extreme or more extreme than the one observed assuming the null hypothesis is true. \\[ t = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{s_1/n_1+s_2/n_2 }} \\] The plot below demonstrates what data might look like when the null hypothesis is actually true. In other words, when both samples come from the same distribution.",
    "sentences": ["The first figure below depicts the scenario where the difference in means of two separate normal distributions is non-zero.", "In other words, the two distributions have different means, \\(\\mu_1\\) and \\(\\mu_2\\), respectively.", "It is worth emphasizing that the values of \\(\\mu_1\\) and \\(\\mu_2\\) are unknown to the researcher.", "The only thing observed are two separate samples of data (blue dots) of sizes \\(n_1\\) and \\(n_2\\), respectively.", "For the scenario depicted, the null hypothesis that \\(H_0: \\mu_1 - \\mu_2 = 0\\) (i.e., that \\(\\mu_1=\\mu_2\\)) is rejected in favor of the alternative that \\(H_a: \\mu_1 - \\mu_2 \\neq 0\\) based on the sample data observed.", "This dicision would be correct as the true difference in means, \\(\\mu_1-\\mu_2\\) is non-zero in this case.", "When the null hypothesis is true, that \\(H_0: \\mu_1 - \\mu_2 = 0\\), then it follows that the test statistic \\(t\\) that is obtained by measuring the distance between the two sample means, \\(\\bar{x}_1-\\bar{x}_2\\), and appropriately standardizing the result follows a \\(t\\) distribution with degrees of freedom less than or equal to \\(n_1+n_2-2\\).", "Thus, the \\(p\\)-value of the independent samples \\(t\\) test is obtained by using this \\(t\\) distribution to calculate the probability of a test statistic \\(t\\) being as extreme or more extreme than the one observed assuming the null hypothesis is true.", "\\[ t = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{s_1/n_1+s_2/n_2 }} \\] The plot below demonstrates what data might look like when the null hypothesis is actually true.", "In other words, when both samples come from the same distribution."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "Explanation"
  },
  {
    "id": 283,
    "title": "Wilcoxon Tests",
    "url": "WilcoxonTests.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Wilcoxon Tests Wilcoxon tests allow for the testing of hypotheses about the value of the the median without assuming the test statistic follows any parametric distribution. They are often seen as nonparametric alternatives to the various t tests. However, they can also be used on ordinal data (data that is not quite quantitative, but is ordered) unlike t tests which require quantitative data. Wilcoxon Signed-Rank Test For testing hypotheses about the value of the median of (1) one sample of quantitative data or (2) one set of differences from paired data. Overview The nonparametric equivalent of the paired-samples t test as well as the one-sample t test. Best for smaller sample sizes where the distribution of the data is not normal. The t test is more appropriate when the data is normal or when the sample size is large. While the test will work in most scenarios it suffers slightly when ties (repeated values) are present in the data. If many ties are present in the data, the test is not appropriate. If only a few ties are present, the test is still appropriate. Hypotheses Originally created to test hypotheses about the value of the median, but works as well for the mean when the distribution of the data is symmetrical. One Sample of Data Math Code $$ H_0: \\text{Median} = \\text{(Some Number)} $$ $$ H_a: \\text{Median} \\neq \\text{(Same Number)} $$ \\(H_0: \\text{Median} = \\text{(Some Number)}\\) \\(H_a: \\text{Median} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{(Some Number)}\\) Paired Samples of Data Math Code $$ H_0: \\text{median of differences} = 0 $$ $$ H_a: \\text{median of differences} \\neq 0 $$ \\(H_0: \\text{median of differences} = 0\\) \\(H_a: \\text{median of differences} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ 0\\) Examples: sleep, CornHeights R Instructions Console Help Command: ?wilcox.test() Paired Data wilcox.test(Y1, Y2, mu = YourNull, alternative = YourAlternative, paired = TRUE, conf.level = 0.95) Y1 must be a â€œnumericâ€ vector. One set of measurements from the pair. Y2 also a â€œnumericâ€ vector. Other set of measurements from the pair. YourNull is the numeric value from your null hypothesis for the median of differences from the paired data. Usually zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests. sleep$extra[sleep$group==1],Â  The hours of extra sleep that the group had with drug 2. sleep$extra[sleep$group==2],Â  The hours of extra sleep that the same group had with drug 1. mu = 0,Â  The numeric value from the null hypothesis for the median of differences from the paired data is 0 meaning the null hypothesis is \\(\\text{median of differences} = 0\\). paired=TRUE,Â  This command forces a â€œpairedâ€ samples test to be performed. alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\text{median of differences} \\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\). Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Wilcoxon signed rank test with continuity correction The phrase â€œwith continuity correctionâ€ implies that instead of using the â€œexactâ€ distribution of the test statistic a â€œnormal approximationâ€ was used instead to compute the p-value. Further, a small correction was made to allow for the change from the â€œdiscreteâ€ exact distribution to the â€œcontinuous normal distributionâ€ when calculating the p-value. data: sleep$extra[sleep$group == 1] and sleep$extra[sleep$group == 2] This statement of the output just reminds you of the code you used to perform the test. The important thing is recognizing that the first group listed is â€œGroup 1â€ and the second group listed is â€œGroup 2.â€ This is especially important when using alternative hypotheses of â€œlessâ€ or â€œgreaterâ€ as the order is always â€œGroup 1â€ is â€œlessâ€ than â€œGroup 2â€ or â€œGroup 1â€ is â€œgreaterâ€ than â€œGroup 2.â€ V = 0, This is the test statistic of the test, i.e., the sum of the ranks from the positive group minus the minimum sum of ranks possible. Â p-value = 0.009091 This is the p-value of the test. If no warning is displayed when the test is run, then this is the â€œexactâ€ p-value from the non-parametric Wilcoxon Test Statistic distribution. Sometimes a message will appear stating â€œCannot compute exact p-value with tiesâ€ or other similar messages. In those cases, the p-value is still considered valid even though it is obtained through a normal approximation to the exact distribution. alternative hypothesis: true location shift is not equal to 0 This reports that the alternative hypothesis was â€œtwo-sided.â€ If the alternative had been â€œlessâ€ or â€œgreaterâ€ the wording would change accordingly. One Sample wilcox.test(object, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object must be a â€œnumericâ€ vector. YourNull is the numeric value from your null hypothesis for the median (even though it says â€œmuâ€). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests. mtcars â€˜mtcarsâ€™ is a dataset. Type â€˜View(mtcars)â€™ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,Â  â€˜mpgâ€™ is a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,Â  The numeric value from the null hypothesis is 20 meaning \\(\\mu = 20\\). alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1âˆ’Î±. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Wilcoxon signed rank test with continuity correction This reports on the type of test performed. The phrase â€œwith continuity correctionâ€ implies the normal approximation was used when calculating the p-value of the test. data: mtcars$mpg This print-out reminds us that the mpg column of the mtcars data was used as â€œYâ€ in the test. V = 249, The test statistic of the test. Â p-value = 0.7863 The p-value of the test. alternative hypothesis: true location is not equal to 20 The words â€œnot equalâ€ tell us this was a two-sided test. Had it been a one-sided test, either the word â€œlessâ€ or the word â€œgreaterâ€ would have appeared instead of â€œnot equal.â€ Explanation In many cases it is of interest to perform a hypothesis test about the location of the center of a distribution of data. The Wilcoxon Signed Rank Test allows a nonparametric approach to doing this. The Wilcoxon Signed-Rank Test covers two important scenarios. One sample of data from a population. (Not very common.) The differences obtained from paired data. (Very common.) The Wilcoxon methods are most easily explained through examples, beginning with the paired data for which the method was originally created. Scroll down for the One Sample Example if that is what you are really interested in. However, it is still recommended that you read the paired data example first. Paired Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon. Background Height differences â€œbetween cross- and self- fertilized corn plants of the same pairâ€ were collected. The experiment hypothesized that the center of the distribution of the height differences would be zero, with the alternative being that the center was not zero. The result of the data collection was 15 height differences: Differences: 14, 56, 60, 16, 6, 8, -48, 49, 24, 28, 29, 41, -67, 23, 75 Step 1 The first step of the Wilcoxon Signed Rank Test is to order the differences from smallest magnitude to largest magnitude. Negative signs are essentially ignored at this point and only magnitudes of the numbers matter. Sorted Differences: 6, 8, 14, 16, 23, 24, 28, 29, 41, -48, 49, 56, 60, -67, 75 Step 2 The next step is to rank the ordered values. Negative signs are attached to the ranks corresponding to negative numbers. Â  Differences: 6 8 14 16 23 24 28 29 41 -48 49 56 60 -67 75 Ranks: 1 2 3 4 5 6 7 8 9 -10 11 12 13 -14 15 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=15\\). Step 3 The ranks are then put into two groups. Negative Ranks Positive Ranks -10, -14 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15 Step 4 One of the groups is summed, usually the group with the fewest observations. Only the absolute values of the ranks are summed. Sum of Negative Ranks: \\(\\left|-10\\right| + \\left|-14\\right| = 24\\) The sum of the ranks becomes the test statistic of the Wilcoxon Test. The test statistic is sometimes called \\(W\\) or \\(V\\) or \\(U\\). Step 5 The \\(p\\)-value of the test is then obtained by computing the probability of the test statistic being as extreme or more extreme than the one obtained. This is done by first computing the probability of all possible values the test statistic could have obtained using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=15\\) ranks, the possible sums of ranks range from 0 to 120 and hit every integer in between, i.e., \\(1, 2, 3, \\ldots, 120\\). (Note, if summing the negative ranks these sums would technically all be negative.) To verify that \\(120\\) is the largest sum possible for \\(n=15\\) ranks, note that: \\(1+15 = 16\\), \\(2 + 14 = 16\\), \\(3+13 = 16\\), \\(4+12=16\\), \\(5+11=16\\), \\(6+10=16\\), \\(7+9=16\\), and finally that \\(8 = \\frac{16}{2}\\). Thus, there are 7 sums of 16 and one sum of \\(\\frac{16}{2}\\). This could be said in a mathematically equivalent way by stating there are \\(\\frac{14}{2}\\) sums of 16 and one sum of \\(\\frac{16}{2}\\). By multiplication this gives \\[ \\frac{14}{2}\\cdot\\frac{16}{1} + \\frac{1}{1}\\cdot\\frac{16}{2} = \\frac{14\\cdot16 + 1\\cdot16}{2} = \\frac{15\\cdot16}{2} = \\frac{n(n+1)}{2} = 120 \\] The probability of each sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 32,768 total different groups of ranks possible when there are \\(n=15\\) ranks.) For example, a sum of 1 can happen only one way, only the rank of 1 is in the group. A sum of 2 can also only happen 1 way. The sum of 3 however, can happen two ways: we could have the ranks of 1 and 2 in the group, or just the rank of 3 in the group. A similar counting technique is then implemented for each possible sum. After all the calculations are performed, the distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(24\\) (or its opposite of \\(120-24=96\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of (the absolute value of) negative ranks as extreme or more extreme than \\(24\\) is \\(p=0.04126\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would reject the null hypothesis that the center of the distribution of differences is zero. We conclude that the center of the distribution is greater than zero because the sum of negative ranks is much smaller than we expected under the zero center hypothesis (the null). Thus, there is sufficient evidence to conclude that the centers of the distributions of â€œcross- and self-fertilized corn plantsâ€ heights are not equal. One is greater than the other. Notice how the following dot plot shows that the differences are in favor of the cross-fertilized plants (the first group in the subtraction) being taller. This is true even though two self-fertilized plants were much taller than their cross-fertilized counterpart (the two negative differences). Comment If the distribution of differences is symmetric, then the hypotheses can be written as \\[ H_0: \\mu = 0 \\] \\[ H_a: \\mu \\neq 0 \\] If the distribution is skewed, then the hypotheses technically refer to the median instead of the mean and should be written as \\[ H_0: \\text{median} = 0 \\] \\[ H_a: \\text{median} \\neq 0 \\] One Sample Example The idea behind the one sample Wilcoxon Signed Rank test is nearly identical to the paired data. The only change is that the median must be subtracted from all observed values to obtain the differences. Note that the mean is equal to the median when data is symmetric. Background Suppose we are interested in testing to see if the median hourly wage of BYU-Idaho students during their off-track employment is equal to the minimum wage in Idaho, $7.25 an hour as of January 1st, 2015. Five randomly sampled hourly wages from BYU-Idaho Math 221B students provides the following data. Wages: $6.00, $9.00, $8.10, $18.00, $10.45 The differences are then obtained by subtracting the hypothesized value for the median (or mean if the dat",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Wilcoxon Tests Wilcoxon tests allow for the testing of hypotheses about the value of the the median without assuming the test statistic follows any parametric distribution.", "They are often seen as nonparametric alternatives to the various t tests.", "However, they can also be used on ordinal data (data that is not quite quantitative, but is ordered) unlike t tests which require quantitative data.", "Wilcoxon Signed-Rank Test For testing hypotheses about the value of the median of (1) one sample of quantitative data or (2) one set of differences from paired data.", "Overview The nonparametric equivalent of the paired-samples t test as well as the one-sample t test.", "Best for smaller sample sizes where the distribution of the data is not normal.", "The t test is more appropriate when the data is normal or when the sample size is large.", "While the test will work in most scenarios it suffers slightly when ties (repeated values) are present in the data.", "If many ties are present in the data, the test is not appropriate.", "If only a few ties are present, the test is still appropriate.", "Hypotheses Originally created to test hypotheses about the value of the median, but works as well for the mean when the distribution of the data is symmetrical.", "One Sample of Data Math Code $$ H_0: \\text{Median} = \\text{(Some Number)} $$ $$ H_a: \\text{Median} \\neq \\text{(Same Number)} $$ \\(H_0: \\text{Median} = \\text{(Some Number)}\\) \\(H_a: \\text{Median} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{(Some Number)}\\) Paired Samples of Data Math Code $$ H_0: \\text{median of differences} = 0 $$ $$ H_a: \\text{median of differences} \\neq 0 $$ \\(H_0: \\text{median of differences} = 0\\) \\(H_a: \\text{median of differences} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ 0\\) Examples: sleep, CornHeights R Instructions Console Help Command: ?wilcox.test() Paired Data wilcox.test(Y1, Y2, mu = YourNull, alternative = YourAlternative, paired = TRUE, conf.level = 0.95) Y1 must be a â€œnumericâ€ vector.", "One set of measurements from the pair.", "Y2 also a â€œnumericâ€ vector.", "Other set of measurements from the pair.", "YourNull is the numeric value from your null hypothesis for the median of differences from the paired data.", "Usually zero.", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Example Code Hover your mouse over the example codes to learn more.", "wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests.", "sleep$extra[sleep$group==1],Â  The hours of extra sleep that the group had with drug 2.", "sleep$extra[sleep$group==2],Â  The hours of extra sleep that the same group had with drug 1.", "mu = 0,Â  The numeric value from the null hypothesis for the median of differences from the paired data is 0 meaning the null hypothesis is \\(\\text{median of differences} = 0\\).", "paired=TRUE,Â  This command forces a â€œpairedâ€ samples test to be performed.", "alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\text{median of differences} \\neq0\\).", "conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).", "Â Â Â Â Press Enter to run the code if you have typed it in yourself.", "You can also click here to view the output.", "Â â€¦Â  Click to View Output.", "Wilcoxon signed rank test with continuity correction The phrase â€œwith continuity correctionâ€ implies that instead of using the â€œexactâ€ distribution of the test statistic a â€œnormal approximationâ€ was used instead to compute the p-value.", "Further, a small correction was made to allow for the change from the â€œdiscreteâ€ exact distribution to the â€œcontinuous normal distributionâ€ when calculating the p-value.", "data: sleep$extra[sleep$group == 1] and sleep$extra[sleep$group == 2] This statement of the output just reminds you of the code you used to perform the test.", "The important thing is recognizing that the first group listed is â€œGroup 1â€ and the second group listed is â€œGroup 2.â€ This is especially important when using alternative hypotheses of â€œlessâ€ or â€œgreaterâ€ as the order is always â€œGroup 1â€ is â€œlessâ€ than â€œGroup 2â€ or â€œGroup 1â€ is â€œgreaterâ€ than â€œGroup 2.â€ V = 0, This is the test statistic of the test, i.e., the sum of the ranks from the positive group minus the minimum sum of ranks possible.", "Â p-value = 0.009091 This is the p-value of the test.", "If no warning is displayed when the test is run, then this is the â€œexactâ€ p-value from the non-parametric Wilcoxon Test Statistic distribution.", "Sometimes a message will appear stating â€œCannot compute exact p-value with tiesâ€ or other similar messages.", "In those cases, the p-value is still considered valid even though it is obtained through a normal approximation to the exact distribution.", "alternative hypothesis: true location shift is not equal to 0 This reports that the alternative hypothesis was â€œtwo-sided.â€ If the alternative had been â€œlessâ€ or â€œgreaterâ€ the wording would change accordingly.", "One Sample wilcox.test(object, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object must be a â€œnumericâ€ vector.", "YourNull is the numeric value from your null hypothesis for the median (even though it says â€œmuâ€).", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Example Code Hover your mouse over the example codes to learn more.", "wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests.", "mtcars â€˜mtcarsâ€™ is a dataset.", "Type â€˜View(mtcars)â€™ in R to view the dataset.", "$ The $ allows us to access any variable from the mtcars dataset."],
    "type": "page",
    "page_title": "Wilcoxon Tests"
  },
  {
    "id": 284,
    "title": "ğŸ“ Wilcoxon Signed-Rank\r\nTest",
    "url": "WilcoxonTests.html#wilcoxonsignedranktest",
    "content": "Wilcoxon Signed-Rank\r\nTest For testing hypotheses about the value of the median of (1) one sample of quantitative data or (2) one set of differences from paired data. Overview The nonparametric equivalent of the paired-samples t test as well as the one-sample t test. Best for smaller sample sizes where the distribution of the data is not normal. The t test is more appropriate when the data is normal or when the sample size is large. While the test will work in most scenarios it suffers slightly when ties (repeated values) are present in the data. If many ties are present in the data, the test is not appropriate. If only a few ties are present, the test is still appropriate. Hypotheses Originally created to test hypotheses about the value of the median, but works as well for the mean when the distribution of the data is symmetrical. One Sample of Data Math Code $$ H_0: \\text{Median} = \\text{(Some Number)} $$ $$ H_a: \\text{Median} \\neq \\text{(Same Number)} $$ \\(H_0: \\text{Median} = \\text{(Some Number)}\\) \\(H_a: \\text{Median} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{(Some Number)}\\) Paired Samples of Data Math Code $$ H_0: \\text{median of differences} = 0 $$ $$ H_a: \\text{median of differences} \\neq 0 $$ \\(H_0: \\text{median of differences} = 0\\) \\(H_a: \\text{median of differences} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ 0\\) Examples: sleep, CornHeights R Instructions Console Help Command: ?wilcox.test() Paired Data wilcox.test(Y1, Y2, mu = YourNull, alternative = YourAlternative, paired = TRUE, conf.level = 0.95) Y1 must be a â€œnumericâ€ vector. One set of measurements from the pair. Y2 also a â€œnumericâ€ vector. Other set of measurements from the pair. YourNull is the numeric value from your null hypothesis for the median of differences from the paired data. Usually zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests. sleep$extra[sleep$group==1],Â  The hours of extra sleep that the group had with drug 2. sleep$extra[sleep$group==2],Â  The hours of extra sleep that the same group had with drug 1. mu = 0,Â  The numeric value from the null hypothesis for the median of differences from the paired data is 0 meaning the null hypothesis is \\(\\text{median of differences} = 0\\). paired=TRUE,Â  This command forces a â€œpairedâ€ samples test to be performed. alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\text{median of differences} \\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\). Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Wilcoxon signed rank test with continuity correction The phrase â€œwith continuity correctionâ€ implies that instead of using the â€œexactâ€ distribution of the test statistic a â€œnormal approximationâ€ was used instead to compute the p-value. Further, a small correction was made to allow for the change from the â€œdiscreteâ€ exact distribution to the â€œcontinuous normal distributionâ€ when calculating the p-value. data: sleep$extra[sleep$group == 1] and sleep$extra[sleep$group == 2] This statement of the output just reminds you of the code you used to perform the test. The important thing is recognizing that the first group listed is â€œGroup 1â€ and the second group listed is â€œGroup 2.â€ This is especially important when using alternative hypotheses of â€œlessâ€ or â€œgreaterâ€ as the order is always â€œGroup 1â€ is â€œlessâ€ than â€œGroup 2â€ or â€œGroup 1â€ is â€œgreaterâ€ than â€œGroup 2.â€ V = 0, This is the test statistic of the test, i.e., the sum of the ranks from the positive group minus the minimum sum of ranks possible. Â p-value = 0.009091 This is the p-value of the test. If no warning is displayed when the test is run, then this is the â€œexactâ€ p-value from the non-parametric Wilcoxon Test Statistic distribution. Sometimes a message will appear stating â€œCannot compute exact p-value with tiesâ€ or other similar messages. In those cases, the p-value is still considered valid even though it is obtained through a normal approximation to the exact distribution. alternative hypothesis: true location shift is not equal to 0 This reports that the alternative hypothesis was â€œtwo-sided.â€ If the alternative had been â€œlessâ€ or â€œgreaterâ€ the wording would change accordingly. One Sample wilcox.test(object, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object must be a â€œnumericâ€ vector. YourNull is the numeric value from your null hypothesis for the median (even though it says â€œmuâ€). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests. mtcars â€˜mtcarsâ€™ is a dataset. Type â€˜View(mtcars)â€™ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,Â  â€˜mpgâ€™ is a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,Â  The numeric value from the null hypothesis is 20 meaning \\(\\mu = 20\\). alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1âˆ’Î±. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Wilcoxon signed rank test with continuity correction This reports on the type of test performed. The phrase â€œwith continuity correctionâ€ implies the normal approximation was used when calculating the p-value of the test. data: mtcars$mpg This print-out reminds us that the mpg column of the mtcars data was used as â€œYâ€ in the test. V = 249, The test statistic of the test. Â p-value = 0.7863 The p-value of the test. alternative hypothesis: true location is not equal to 20 The words â€œnot equalâ€ tell us this was a two-sided test. Had it been a one-sided test, either the word â€œlessâ€ or the word â€œgreaterâ€ would have appeared instead of â€œnot equal.â€",
    "sentences": ["For testing hypotheses about the value of the median of (1) one sample of quantitative data or (2) one set of differences from paired data.", "Overview The nonparametric equivalent of the paired-samples t test as well as the one-sample t test.", "Best for smaller sample sizes where the distribution of the data is not normal.", "The t test is more appropriate when the data is normal or when the sample size is large.", "While the test will work in most scenarios it suffers slightly when ties (repeated values) are present in the data.", "If many ties are present in the data, the test is not appropriate.", "If only a few ties are present, the test is still appropriate.", "Hypotheses Originally created to test hypotheses about the value of the median, but works as well for the mean when the distribution of the data is symmetrical.", "One Sample of Data Math Code $$ H_0: \\text{Median} = \\text{(Some Number)} $$ $$ H_a: \\text{Median} \\neq \\text{(Same Number)} $$ \\(H_0: \\text{Median} = \\text{(Some Number)}\\) \\(H_a: \\text{Median} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{(Some Number)}\\) Paired Samples of Data Math Code $$ H_0: \\text{median of differences} = 0 $$ $$ H_a: \\text{median of differences} \\neq 0 $$ \\(H_0: \\text{median of differences} = 0\\) \\(H_a: \\text{median of differences} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ 0\\) Examples: sleep, CornHeights", "R Instructions Console Help Command: ?wilcox.test() Paired Data wilcox.test(Y1, Y2, mu = YourNull, alternative = YourAlternative, paired = TRUE, conf.level = 0.95) Y1 must be a â€œnumericâ€ vector."],
    "type": "section",
    "page_title": "Wilcoxon Tests",
    "section_title": "Wilcoxon Signed-Rank\r\nTest"
  },
  {
    "id": 285,
    "title": "ğŸ“ Overview",
    "url": "WilcoxonTests.html#overview",
    "content": "Overview The nonparametric equivalent of the paired-samples t test as well as the one-sample t test. Best for smaller sample sizes where the distribution of the data is not normal. The t test is more appropriate when the data is normal or when the sample size is large. While the test will work in most scenarios it suffers slightly when ties (repeated values) are present in the data. If many ties are present in the data, the test is not appropriate. If only a few ties are present, the test is still appropriate. Hypotheses Originally created to test hypotheses about the value of the median, but works as well for the mean when the distribution of the data is symmetrical. One Sample of Data Math Code $$ H_0: \\text{Median} = \\text{(Some Number)} $$ $$ H_a: \\text{Median} \\neq \\text{(Same Number)} $$ \\(H_0: \\text{Median} = \\text{(Some Number)}\\) \\(H_a: \\text{Median} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{(Some Number)}\\) Paired Samples of Data Math Code $$ H_0: \\text{median of differences} = 0 $$ $$ H_a: \\text{median of differences} \\neq 0 $$ \\(H_0: \\text{median of differences} = 0\\) \\(H_a: \\text{median of differences} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ 0\\) Examples: sleep, CornHeights",
    "sentences": ["The nonparametric equivalent of the paired-samples t test as well as the one-sample t test.", "Best for smaller sample sizes where the distribution of the data is not normal.", "The t test is more appropriate when the data is normal or when the sample size is large.", "While the test will work in most scenarios it suffers slightly when ties (repeated values) are present in the data.", "If many ties are present in the data, the test is not appropriate.", "If only a few ties are present, the test is still appropriate.", "Hypotheses Originally created to test hypotheses about the value of the median, but works as well for the mean when the distribution of the data is symmetrical.", "One Sample of Data Math Code $$ H_0: \\text{Median} = \\text{(Some Number)} $$ $$ H_a: \\text{Median} \\neq \\text{(Same Number)} $$ \\(H_0: \\text{Median} = \\text{(Some Number)}\\) \\(H_a: \\text{Median} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{(Some Number)}\\) Paired Samples of Data Math Code $$ H_0: \\text{median of differences} = 0 $$ $$ H_a: \\text{median of differences} \\neq 0 $$ \\(H_0: \\text{median of differences} = 0\\) \\(H_a: \\text{median of differences} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ 0\\) Examples: sleep, CornHeights"],
    "type": "section",
    "page_title": "Wilcoxon Tests",
    "section_title": "Overview"
  },
  {
    "id": 286,
    "title": "ğŸ“ R Instructions",
    "url": "WilcoxonTests.html#rinstructions",
    "content": "R Instructions Console Help Command: ?wilcox.test() Paired Data wilcox.test(Y1, Y2, mu = YourNull, alternative = YourAlternative, paired = TRUE, conf.level = 0.95) Y1 must be a â€œnumericâ€ vector. One set of measurements from the pair. Y2 also a â€œnumericâ€ vector. Other set of measurements from the pair. YourNull is the numeric value from your null hypothesis for the median of differences from the paired data. Usually zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests. sleep$extra[sleep$group==1],Â  The hours of extra sleep that the group had with drug 2. sleep$extra[sleep$group==2],Â  The hours of extra sleep that the same group had with drug 1. mu = 0,Â  The numeric value from the null hypothesis for the median of differences from the paired data is 0 meaning the null hypothesis is \\(\\text{median of differences} = 0\\). paired=TRUE,Â  This command forces a â€œpairedâ€ samples test to be performed. alternative = â€œtwo.sidedâ€,Â  The alternative hypothesis is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\text{median of differences} \\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\). Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Wilcoxon signed rank test with continuity correction The phrase â€œwith continuity correctionâ€ implies that instead of using the â€œexactâ€ distribution of the test statistic a â€œnormal approximationâ€ was used instead to compute the p-value. Further, a small correction was made to allow for the change from the â€œdiscreteâ€ exact distribution to the â€œcontinuous normal distributionâ€ when calculating the p-value. data: sleep$extra[sleep$group == 1] and sleep$extra[sleep$group == 2] This statement of the output just reminds you of the code you used to perform the test. The important thing is recognizing that the first group listed is â€œGroup 1â€ and the second group listed is â€œGroup 2.â€ This is especially important when using alternative hypotheses of â€œlessâ€ or â€œgreaterâ€ as the order is always â€œGroup 1â€ is â€œlessâ€ than â€œGroup 2â€ or â€œGroup 1â€ is â€œgreaterâ€ than â€œGroup 2.â€ V = 0, This is the test statistic of the test, i.e., the sum of the ranks from the positive group minus the minimum sum of ranks possible. Â p-value = 0.009091 This is the p-value of the test. If no warning is displayed when the test is run, then this is the â€œexactâ€ p-value from the non-parametric Wilcoxon Test Statistic distribution. Sometimes a message will appear stating â€œCannot compute exact p-value with tiesâ€ or other similar messages. In those cases, the p-value is still considered valid even though it is obtained through a normal approximation to the exact distribution. alternative hypothesis: true location shift is not equal to 0 This reports that the alternative hypothesis was â€œtwo-sided.â€ If the alternative had been â€œlessâ€ or â€œgreaterâ€ the wording would change accordingly. One Sample wilcox.test(object, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object must be a â€œnumericâ€ vector. YourNull is the numeric value from your null hypothesis for the median (even though it says â€œmuâ€). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests. mtcars â€˜mtcarsâ€™ is a dataset. Type â€˜View(mtcars)â€™ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,Â  â€˜mpgâ€™ is a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,Â  The numeric value from the null hypothesis is 20 meaning \\(\\mu = 20\\). alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo.sidedâ€ meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1âˆ’Î±. Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Wilcoxon signed rank test with continuity correction This reports on the type of test performed. The phrase â€œwith continuity correctionâ€ implies the normal approximation was used when calculating the p-value of the test. data: mtcars$mpg This print-out reminds us that the mpg column of the mtcars data was used as â€œYâ€ in the test. V = 249, The test statistic of the test. Â p-value = 0.7863 The p-value of the test. alternative hypothesis: true location is not equal to 20 The words â€œnot equalâ€ tell us this was a two-sided test. Had it been a one-sided test, either the word â€œlessâ€ or the word â€œgreaterâ€ would have appeared instead of â€œnot equal.â€",
    "sentences": ["Console Help Command: ?wilcox.test() Paired Data wilcox.test(Y1, Y2, mu = YourNull, alternative = YourAlternative, paired = TRUE, conf.level = 0.95) Y1 must be a â€œnumericâ€ vector.", "One set of measurements from the pair.", "Y2 also a â€œnumericâ€ vector.", "Other set of measurements from the pair.", "YourNull is the numeric value from your null hypothesis for the median of differences from the paired data.", "Usually zero.", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Example Code Hover your mouse over the example codes to learn more."],
    "type": "section",
    "page_title": "Wilcoxon Tests",
    "section_title": "R Instructions"
  },
  {
    "id": 287,
    "title": "ğŸ“ Explanation",
    "url": "WilcoxonTests.html#explanation",
    "content": "Explanation In many cases it is of interest to perform a hypothesis test about the location of the center of a distribution of data. The Wilcoxon Signed Rank Test allows a nonparametric approach to doing this. The Wilcoxon Signed-Rank Test covers two important scenarios. One sample of data from a population. (Not very common.) The differences obtained from paired data. (Very common.) The Wilcoxon methods are most easily explained through examples, beginning with the paired data for which the method was originally created. Scroll down for the One Sample Example if that is what you are really interested in. However, it is still recommended that you read the paired data example first. Paired Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon. Background Height differences â€œbetween cross- and self- fertilized corn plants of the same pairâ€ were collected. The experiment hypothesized that the center of the distribution of the height differences would be zero, with the alternative being that the center was not zero. The result of the data collection was 15 height differences: Differences: 14, 56, 60, 16, 6, 8, -48, 49, 24, 28, 29, 41, -67, 23, 75 Step 1 The first step of the Wilcoxon Signed Rank Test is to order the differences from smallest magnitude to largest magnitude. Negative signs are essentially ignored at this point and only magnitudes of the numbers matter. Sorted Differences: 6, 8, 14, 16, 23, 24, 28, 29, 41, -48, 49, 56, 60, -67, 75 Step 2 The next step is to rank the ordered values. Negative signs are attached to the ranks corresponding to negative numbers. Â  Differences: 6 8 14 16 23 24 28 29 41 -48 49 56 60 -67 75 Ranks: 1 2 3 4 5 6 7 8 9 -10 11 12 13 -14 15 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=15\\). Step 3 The ranks are then put into two groups. Negative Ranks Positive Ranks -10, -14 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15 Step 4 One of the groups is summed, usually the group with the fewest observations. Only the absolute values of the ranks are summed. Sum of Negative Ranks: \\(\\left|-10\\right| + \\left|-14\\right| = 24\\) The sum of the ranks becomes the test statistic of the Wilcoxon Test. The test statistic is sometimes called \\(W\\) or \\(V\\) or \\(U\\). Step 5 The \\(p\\)-value of the test is then obtained by computing the probability of the test statistic being as extreme or more extreme than the one obtained. This is done by first computing the probability of all possible values the test statistic could have obtained using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=15\\) ranks, the possible sums of ranks range from 0 to 120 and hit every integer in between, i.e., \\(1, 2, 3, \\ldots, 120\\). (Note, if summing the negative ranks these sums would technically all be negative.) To verify that \\(120\\) is the largest sum possible for \\(n=15\\) ranks, note that: \\(1+15 = 16\\), \\(2 + 14 = 16\\), \\(3+13 = 16\\), \\(4+12=16\\), \\(5+11=16\\), \\(6+10=16\\), \\(7+9=16\\), and finally that \\(8 = \\frac{16}{2}\\). Thus, there are 7 sums of 16 and one sum of \\(\\frac{16}{2}\\). This could be said in a mathematically equivalent way by stating there are \\(\\frac{14}{2}\\) sums of 16 and one sum of \\(\\frac{16}{2}\\). By multiplication this gives \\[ \\frac{14}{2}\\cdot\\frac{16}{1} + \\frac{1}{1}\\cdot\\frac{16}{2} = \\frac{14\\cdot16 + 1\\cdot16}{2} = \\frac{15\\cdot16}{2} = \\frac{n(n+1)}{2} = 120 \\] The probability of each sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 32,768 total different groups of ranks possible when there are \\(n=15\\) ranks.) For example, a sum of 1 can happen only one way, only the rank of 1 is in the group. A sum of 2 can also only happen 1 way. The sum of 3 however, can happen two ways: we could have the ranks of 1 and 2 in the group, or just the rank of 3 in the group. A similar counting technique is then implemented for each possible sum. After all the calculations are performed, the distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(24\\) (or its opposite of \\(120-24=96\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of (the absolute value of) negative ranks as extreme or more extreme than \\(24\\) is \\(p=0.04126\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would reject the null hypothesis that the center of the distribution of differences is zero. We conclude that the center of the distribution is greater than zero because the sum of negative ranks is much smaller than we expected under the zero center hypothesis (the null). Thus, there is sufficient evidence to conclude that the centers of the distributions of â€œcross- and self-fertilized corn plantsâ€ heights are not equal. One is greater than the other. Notice how the following dot plot shows that the differences are in favor of the cross-fertilized plants (the first group in the subtraction) being taller. This is true even though two self-fertilized plants were much taller than their cross-fertilized counterpart (the two negative differences). Comment If the distribution of differences is symmetric, then the hypotheses can be written as \\[ H_0: \\mu = 0 \\] \\[ H_a: \\mu \\neq 0 \\] If the distribution is skewed, then the hypotheses technically refer to the median instead of the mean and should be written as \\[ H_0: \\text{median} = 0 \\] \\[ H_a: \\text{median} \\neq 0 \\] One Sample Example The idea behind the one sample Wilcoxon Signed Rank test is nearly identical to the paired data. The only change is that the median must be subtracted from all observed values to obtain the differences. Note that the mean is equal to the median when data is symmetric. Background Suppose we are interested in testing to see if the median hourly wage of BYU-Idaho students during their off-track employment is equal to the minimum wage in Idaho, $7.25 an hour as of January 1st, 2015. Five randomly sampled hourly wages from BYU-Idaho Math 221B students provides the following data. Wages: $6.00, $9.00, $8.10, $18.00, $10.45 The differences are then obtained by subtracting the hypothesized value for the median (or mean if the data is symmetric) from all observations. Differences: -1.25, 1.75, 0.85, 10.75, 3.20 Note: from this point down, the wording of this example is identical to the paired data example (above) with the numbers changed to match \\(n=5\\). It is useful to continue reading to reinforce the idea of the Wilcoxon Signed Rank Test, but no new knowledge will be presented. Step 1 The first step of the Wilcoxon Signed Rank Test is to order the differences from smallest magnitude to largest magnitude. Negative signs are essentially ignored at this point and only magnitudes of the numbers matter. Sorted Differences: 0.85, -1.25, 1.75, 3.20, 10.75 Step 2 The next step is to rank the ordered values. Negative signs are attached to the ranks corresponding to negative numbers. Ranks: 1, -2, 3, 4, 5 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=5\\). Step 3 The ranks are then put into two groups. Negative Ranks Positive Ranks -2 1, 3, 4, 5 Step 4 One of the groups is summed, usually the group with the fewest observations. Sum of Negative Ranks: \\(\\left|-2\\right| = 2\\) Step 5 The \\(p\\)-value of the test is then obtained by computing the probabilities of all possible sums using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=5\\) ranks, the possible sums of ranks range from 0 to 15 and hit every integer in between, i.e., \\(1, 2, 3, \\ldots, 15\\). (Note, if summing the negative ranks these sums would technically all be negative.) To verify that \\(15\\) is the largest sum possible for \\(n=5\\) ranks, note that: \\(1+5 = 6\\), \\(2 + 4 = 6\\), and finally \\(3 = \\frac{6}{2}\\). Thus, there are 2 sums of 6 and one sum of \\(\\frac{6}{2}\\). This could be said in a mathematically equivalent way by stating there are \\(\\frac{4}{2}\\) sums of 6 and one sum of \\(\\frac{6}{2}\\). By multiplication this gives \\[ \\frac{4}{2}\\cdot\\frac{6}{1} + \\frac{1}{1}\\cdot\\frac{6}{2} = \\frac{4\\cdot6 + 1\\cdot6}{2} = \\frac{5\\cdot6}{2} = \\frac{n(n+1)}{2} = 15 \\] The probability of each sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 32 total different groups of ranks possible when there are \\(n=5\\) ranks.) For example, a sum of 1 can happen only one way, only the rank of 1 is in the group. A sum of 2 can also only happen 1 way. The sum of 3 however, can happen two ways: we could have the ranks of 1 and 2 in the group, or just the rank of 3 in the group. A similar counting technique is then implemented for each possible sum. After all the calculations are performed, the distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(2\\) (or its opposite of \\(15-2=13\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of negative ranks as extreme or more extreme than \\(-2\\) is \\(p=0.1875\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would fail to reject the null hypothesis that the center of the distribution of differences is zero. We will continue to assume the null hypothesis was true, that the median off-track hourly wage of BYU-Idaho students is the same as the Idaho minimum wage. Final Comment Notice that when the sample size is large the distribution of the test statistic can be approximated by a normal distribution. Most software applications use this approximation when the sample size is over \\(50\\), i.e., for \\(n\\geq50\\) because computing all the possible sums of ranks becomes incredibly time consuming. Thus, for large sample sizes the results will be almost identical whether the Wilcoxon Tests or a t Test is used.",
    "sentences": ["In many cases it is of interest to perform a hypothesis test about the location of the center of a distribution of data.", "The Wilcoxon Signed Rank Test allows a nonparametric approach to doing this.", "The Wilcoxon Signed-Rank Test covers two important scenarios.", "One sample of data from a population.", "(Not very common.) The differences obtained from paired data.", "(Very common.) The Wilcoxon methods are most easily explained through examples, beginning with the paired data for which the method was originally created.", "Scroll down for the One Sample Example if that is what you are really interested in.", "However, it is still recommended that you read the paired data example first.", "Paired Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon.", "Background Height differences â€œbetween cross- and self- fertilized corn plants of the same pairâ€ were collected."],
    "type": "section",
    "page_title": "Wilcoxon Tests",
    "section_title": "Explanation"
  },
  {
    "id": 288,
    "title": "ğŸ“ Wilcoxon Rank Sum\r\n(Mann-Whitney) Test",
    "url": "WilcoxonTests.html#wilcoxonranksummannwhitneytest",
    "content": "Wilcoxon Rank Sum\r\n(Mann-Whitney) Test For testing the equality of the medians of two (possibly different) distributions of a quantitative variable. Overview The nonparametric equivalent of the Independent Samples t Test. Can also be used when data is ordered (ordinal) but does not have an exact measurement. For example, first place, second place, and so on. The Independent Samples t Test is more appropriate when the distributions are normal, or when the sample size for each sample is large. The test is negatively affected when there are ties (repeated values) present in the data, but the results are still useful if there are relatively few ties. Hypotheses Originally designed to test for the equality of medians from two identically shaped distributions. Math Code $$ H_0: \\text{difference in medians} = 0 $$ $$ H_a: \\text{difference in medians} \\neq 0 $$ \\(H_0: \\text{difference in medians} = 0\\) \\(H_a: \\text{difference in medians} \\neq 0\\) However, the test also allows for the more general hypotheses that one distribution is stochastically greater than the other. \\(H_0: \\text{the distributions are stochastically equal}\\) \\(H_a: \\text{one distribution is stochastically greater than the other}\\) If these hypotheses are used, then the distributions do not have to be identically distributed. (Note: Menâ€™s heights are stochastically greater than womenâ€™s heights because men are generally taller than women, but not all men are taller than all women.) Examples: BugSpray, MoralIntegration",
    "sentences": ["For testing the equality of the medians of two (possibly different) distributions of a quantitative variable.", "Overview The nonparametric equivalent of the Independent Samples t Test.", "Can also be used when data is ordered (ordinal) but does not have an exact measurement.", "For example, first place, second place, and so on.", "The Independent Samples t Test is more appropriate when the distributions are normal, or when the sample size for each sample is large.", "The test is negatively affected when there are ties (repeated values) present in the data, but the results are still useful if there are relatively few ties.", "Hypotheses Originally designed to test for the equality of medians from two identically shaped distributions.", "Math Code $$ H_0: \\text{difference in medians} = 0 $$ $$ H_a: \\text{difference in medians} \\neq 0 $$ \\(H_0: \\text{difference in medians} = 0\\) \\(H_a: \\text{difference in medians} \\neq 0\\) However, the test also allows for the more general hypotheses that one distribution is stochastically greater than the other.", "\\(H_0: \\text{the distributions are stochastically equal}\\) \\(H_a: \\text{one distribution is stochastically greater than the other}\\) If these hypotheses are used, then the distributions do not have to be identically distributed.", "(Note: Menâ€™s heights are stochastically greater than womenâ€™s heights because men are generally taller than women, but not all men are taller than all women.) Examples: BugSpray, MoralIntegration"],
    "type": "section",
    "page_title": "Wilcoxon Tests",
    "section_title": "Wilcoxon Rank Sum\r\n(Mann-Whitney) Test"
  },
  {
    "id": 289,
    "title": "ğŸ“ Overview",
    "url": "WilcoxonTests.html#overview",
    "content": "Overview The nonparametric equivalent of the Independent Samples t Test. Can also be used when data is ordered (ordinal) but does not have an exact measurement. For example, first place, second place, and so on. The Independent Samples t Test is more appropriate when the distributions are normal, or when the sample size for each sample is large. The test is negatively affected when there are ties (repeated values) present in the data, but the results are still useful if there are relatively few ties. Hypotheses Originally designed to test for the equality of medians from two identically shaped distributions. Math Code $$ H_0: \\text{difference in medians} = 0 $$ $$ H_a: \\text{difference in medians} \\neq 0 $$ \\(H_0: \\text{difference in medians} = 0\\) \\(H_a: \\text{difference in medians} \\neq 0\\) However, the test also allows for the more general hypotheses that one distribution is stochastically greater than the other. \\(H_0: \\text{the distributions are stochastically equal}\\) \\(H_a: \\text{one distribution is stochastically greater than the other}\\) If these hypotheses are used, then the distributions do not have to be identically distributed. (Note: Menâ€™s heights are stochastically greater than womenâ€™s heights because men are generally taller than women, but not all men are taller than all women.) Examples: BugSpray, MoralIntegration",
    "sentences": ["The nonparametric equivalent of the Independent Samples t Test.", "Can also be used when data is ordered (ordinal) but does not have an exact measurement.", "For example, first place, second place, and so on.", "The Independent Samples t Test is more appropriate when the distributions are normal, or when the sample size for each sample is large.", "The test is negatively affected when there are ties (repeated values) present in the data, but the results are still useful if there are relatively few ties.", "Hypotheses Originally designed to test for the equality of medians from two identically shaped distributions.", "Math Code $$ H_0: \\text{difference in medians} = 0 $$ $$ H_a: \\text{difference in medians} \\neq 0 $$ \\(H_0: \\text{difference in medians} = 0\\) \\(H_a: \\text{difference in medians} \\neq 0\\) However, the test also allows for the more general hypotheses that one distribution is stochastically greater than the other.", "\\(H_0: \\text{the distributions are stochastically equal}\\) \\(H_a: \\text{one distribution is stochastically greater than the other}\\) If these hypotheses are used, then the distributions do not have to be identically distributed.", "(Note: Menâ€™s heights are stochastically greater than womenâ€™s heights because men are generally taller than women, but not all men are taller than all women.) Examples: BugSpray, MoralIntegration"],
    "type": "section",
    "page_title": "Wilcoxon Tests",
    "section_title": "Overview"
  },
  {
    "id": 290,
    "title": "ğŸ“ R Instructions",
    "url": "WilcoxonTests.html#rinstructions",
    "content": "R Instructions Console Help Command: ?wilcox.test() There are two ways to perform the test. Option 1: wilcox.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a â€œnumericâ€ vector from YourData that represents the data for both samples. X must be a â€œfactorâ€ or â€œcharacterâ€ vector from YourData that represents the group assignment for each observation. There can only be two groups specified in this column of data. YourNull is the numeric value from your null hypothesis for the difference in medians from the two groups. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests. lengthÂ  â€˜lengthâ€™ is a quantitative variable (numeric vector). ~Â  â€˜~â€™ is the tilde symbol. sex,Â  â€˜sexâ€™ is a â€˜factorâ€™ or â€˜characterâ€™ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet,Â  â€˜KidsFeetâ€™ is a dataset in library(mosaic). Type View(KidsFeet) to view it. mu = 0,Â  The numeric value from the null hypothesis for the difference in medians from the two groups is 0 meaning the null hypothesis is \\(\\text{difference in medians} = 0\\) alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo-sidedâ€ meaning the alternative hypothesis is \\(\\text{difference in medians} \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\) Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Wilcoxon rank sum test with continuity correction This states the test that was performed and that a normal approximation to the test statistic was used instead of the exact distribution. data: length by sex This states that the length column was split into the two groups found in the â€œsexâ€ column. Unfortunately, it forgets to remind us that the test used the KidsFeet data set. V = 252, The test statistic of the test. In this case, the sum of the ranks from the alphabetically first group minus the minimum sum of ranks possible. Â p-value = 0.0836 The p-value of the test. alternative hypothesis: true location shift is not equal to 0 This reports that the test used an alternative hypothesis of â€œnot equalâ€ (a two-sided test). Further, the phrase â€œtrue location shiftâ€ emphasizes that the Wilcoxon Rank Sum Test is testing to see if one distribution is shifted higher or lower than the other. Option 2: wilcox.test(object1, object2, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object1 must be a â€œnumericâ€ vector that represents the first sample of data. obejct2 must be a â€œnumericâ€ vector that represents the second sample of data. YourNull is the numeric value from your null hypothesis for the difference in medians from the two groups. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests. KidsFeet$length[KidsFeet$sexÂ ==Â â€œBâ€],Â  A numeric vector of foot length for the first sample of data or for the group of boys. KidsFeet$length[KidsFeet$sexÂ ==Â â€œGâ€],Â  A numeric vector of foot length for the second sample of data or for the group of girls. mu = 0,Â  The numeric value from the null hypothesis for the difference in medians from the two groups is 0 meaning the null hypothesis is \\(\\text{difference in medians} = 0\\) alternative = â€œtwo.sidedâ€,Â  The alternative is â€œtwo-sidedâ€ meaning the alternative hypothesis is \\(\\text{difference in medians} \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\) Â Â Â Â Press Enter to run the code if you have typed it in yourself. You can also click here to view the output. Â â€¦Â  Click to View Output. Wilcoxon rank sum test with continuity correction This states the type of test performed. data: KidsFeet$length[KidsFeet$sex == â€œBâ€] and KidsFeet$length[KidsFeet$sex == â€œGâ€] Reminds you what data was used for the test and points out that the first set of data listed is â€œGroup 1â€ and the second set of data listed is â€œGroup 2.â€ V = 252, The test statistic of the test. Â p-value = 0.0836 The p-value of the test. alternative hypothesis: true location shift is not equal to 0 The alternative hypothesis of the test. The phrase â€œnot equalâ€ implies a â€œtwo-sidedâ€ alternative was used.",
    "sentences": ["Console Help Command: ?wilcox.test() There are two ways to perform the test.", "Option 1: wilcox.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a â€œnumericâ€ vector from YourData that represents the data for both samples.", "X must be a â€œfactorâ€ or â€œcharacterâ€ vector from YourData that represents the group assignment for each observation.", "There can only be two groups specified in this column of data.", "YourNull is the numeric value from your null hypothesis for the difference in medians from the two groups.", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Example Code Hover your mouse over the example codes to learn more.", "wilcox.test( â€˜wilcox.testâ€™ is a function for non-parametric one and two sample tests."],
    "type": "section",
    "page_title": "Wilcoxon Tests",
    "section_title": "R Instructions"
  },
  {
    "id": 291,
    "title": "ğŸ“ Explanation",
    "url": "WilcoxonTests.html#explanation",
    "content": "Explanation In many cases it is of interest to perform a hypothesis test concerning the equality of the centers of two (possibly different) distributions. In other words, an independent samples test. The Wilcoxon Rank Sum Test allows a nonparametric approach to doing this. It is often considered the nonparametric equivalent of the independent samples t test. The method is most easily explained through an example. The theory behind it is very similar to the theory behind the Wilcoxon Signed-Rank Test. Independent Samples Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon. Background The percent of flies (bugs) killed from two different concentrations of a certain spray were recorded from 16 different trials, 8 trials per treatment concentration. The experiment hypothesized that the center of the distributions of the percent killed by either concentration were the same. In other words, that both treatments were equally effective. The alternative hypothesis was that the treatments differed in their effectiveness. Spray Concentration Percent Killed A 68, 68, 59, 72, 64, 67, 70, 74 B 60, 67, 61, 62, 67, 63, 56, 58 Step 1 The first step of the Wilcoxon Rank Sum Test is to order all the data from smallest magnitude to largest magnitude, while keeping track of the group. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Step 2 The next step is to rank the ordered values. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=16\\). Any ranks that are tied need to have the average rank assigned to each of those that are tied. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Rank 1 2 3 4 5 6 7 8 10 10 10 12.5 12.5 14 15 16 Step 3 The ranks are then returned to their original groups. Ranks of Spray A Ranks of Spray B 3, 8, 10, 12.5, 12.5, 14, 15, 16 1, 2, 4, 5, 6, 7, 10, 10 Step 4 The ranks are summed for one of the groups. (It does not matter which group.) Sum of Ranks for Spray A: \\[ 3+8+10+12.5+12.5+14+15+16 = 91 \\] Step 5 The \\(p\\)-value of the test is then obtained by computing the probabilities of all possible sums that one group can achieve using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=16\\) ranks, with just \\(8\\) of the ranks assigned to one group, the possible sums of ranks range from \\(36\\) to \\(100\\) and include every integer in between, i.e., \\(36, 37, 38, \\ldots, 100\\). Note that the smallest sum would be obtained if the ranks 1-8 were in the group. The largest sum would be obtained if the ranks 9-16 were in the group. The probability of each possible sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 12,870 total different sets of ranks possible when there are \\(n=16\\) ranks and \\(8\\) are assigned to one group.) The distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(91\\) (or its opposite of \\(136-91=45\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of negative ranks as extreme or more extreme than \\(91\\) is \\(p=0.01476\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would reject the null hypothesis that the difference in the center of the distributions is zero. We conclude that the Spray Concentration A is more effective at killing flies (bugs). Comment The hypotheses for the Wilcoxon Rank Sum Test are difficult to write out in simple mathematical statements. The test is often referred to as a test of medians, but goes deeper than this. Technically, it allows us to determine if one distribution is stochastically larger than another. In other words, if one distribution typically gives larger values than does another distribution. If the distributions are identically shaped and have the same spread, then this implies the medians (and means) are different. Thus, the hypotheses for the test can be written mathematically as \\[ H_0: \\text{difference in medians} = 0 \\] \\[ H_a: \\text{difference in medians} \\neq 0 \\] or even as \\[ H_0: \\mu_1-\\mu_2 = 0 \\] \\[ H_a: \\mu_1-\\mu_2 \\neq 0 \\] However, it is important to remember that the estimated difference in location parameters that results from the test does not provide a measurement on either of these things. However, the \\(p\\)-value can lead us to determine whether to reject, or fail to reject the null, whichever of the above hypotheses is used. As stated in the R help file for this test ?wilcox.test() â€œthe estimator for the difference in location parameters does not estimate the difference in medians (a common misconception) but rather the median of the difference between a sample from [the first population] and a sample from [the second population].â€ These are technical details that most people ignore without encountering too much difficulty. However, it does remind us that the t test is more easily interpreted whenever it is appropriate to use that test. Final Comment Notice that when the sample size is large the distribution of the test statistic can be approximated by a normal distribution. Most software applications use this approximation when the sample size is over \\(50\\), i.e., for \\(n\\geq50\\) because computing all the possible sums of ranks becomes incredibly time consuming. Thus, for large sample sizes the results will be almost identical whether the Wilcoxon Tests or a t Test is used.",
    "sentences": ["In many cases it is of interest to perform a hypothesis test concerning the equality of the centers of two (possibly different) distributions.", "In other words, an independent samples test.", "The Wilcoxon Rank Sum Test allows a nonparametric approach to doing this.", "It is often considered the nonparametric equivalent of the independent samples t test.", "The method is most easily explained through an example.", "The theory behind it is very similar to the theory behind the Wilcoxon Signed-Rank Test.", "Independent Samples Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon.", "Background The percent of flies (bugs) killed from two different concentrations of a certain spray were recorded from 16 different trials, 8 trials per treatment concentration.", "The experiment hypothesized that the center of the distributions of the percent killed by either concentration were the same.", "In other words, that both treatments were equally effective."],
    "type": "section",
    "page_title": "Wilcoxon Tests",
    "section_title": "Explanation"
  }
]
