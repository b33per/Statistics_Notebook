[
  {
    "id": 1,
    "title": "Experimental Design Notes",
    "url": "326PaigesNotes.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Experimental Design Notes (MATH 326) Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts Don’t forget to always load your libraries and to Knit often!! library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) This is the templet for the Hover Box Thingy Hover over words box! [insert description here]       …  Click to View Output. There are three parts to the help box: The starter code < a href=“javascript:showhide(‘[Insert classifying name]’)”> < div class=“hoverchunk” > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions! < span class=“tooltipr” > [What shows up in the box] < span class=“tooltiprtext” > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box! & nbsp;& nbsp;& nbsp;& nbsp; < span class=“tooltiprtext” >Press Enter to run the code. < /span> & nbsp;…& nbsp; < span class=“tooltiprtext” >Click to View Output.< /span > < /span > < /div > < /a > < div id=“[The classifying name that you put in the starter code!]” style=“display:none;” > *When using, make sure you fix all the spaces when using and use “< /div >” to show where the drop down is meant to end! Personal Notes OoOoooOOOoo let’s go assignments!! Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester. As well as some personal notes on those assignments! Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys! Skill Quiz - Introduction to R Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Experimental Design Notes (MATH 326) Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts Don’t forget to always load your libraries and to Knit often!! library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) This is the templet for the Hover Box Thingy Hover over words box! [insert description here]       …  Click to View Output. There are three parts to the help box: The starter code < a href=“javascript:showhide(‘[Insert classifying name]’)”> < div class=“hoverchunk” > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions! < span class=“tooltipr” > [What shows up in the box] < span class=“tooltiprtext” > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box! & nbsp;& nbsp;& nbsp;& nbsp; < span class=“tooltiprtext” >Press Enter to run the code. < /span> & nbsp;…& nbsp; < span class=“tooltiprtext” >Click to View Output.< /span > < /span > < /div > < /a > < div id=“[The classifying name that you put in the starter code!]” style=“display:none;” > *When using, make sure you fix all the spaces when using and use “< /div >” to show where the drop down is meant to end! Personal Notes OoOoooOOOoo let’s go assignments!! Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester. As well as some personal notes on those assignments! Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys! Skill Quiz - Introduction to R Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Experimental Design Notes (MATH 326) Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts Don’t forget to always load your libraries and to Knit often!! library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) This is the templet for the Hover Box Thingy Hover over words box! [insert description here]       …  Click to View Output. There are three parts to the help box: The starter code < a href=“javascript:showhide(‘[Insert classifying name]’)”> < div class=“hoverchunk” > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions! < span class=“tooltipr” > [What shows up in the box] < span class=“tooltiprtext” > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box! & nbsp;& nbsp;& nbsp;& nbsp; < span class=“tooltiprtext” >Press Enter to run the code. < /span> & nbsp;…& nbsp; < span class=“tooltiprtext” >Click to View Output.< /span > < /span > < /div > < /a > < div id=“[The classifying name that you put in the starter code!]” style=“display:none;” > *When using, make sure you fix all the spaces when using and use “< /div >” to show where the drop down is meant to end! Personal Notes OoOoooOOOoo let’s go assignments!! Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester. As well as some personal notes on those assignments! Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys! Skill Quiz - Introduction to R Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) Good Example Analysis Poor Example Analysis High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Outlier Theory Assignment Code Show All Code Hide All Code Experimental Design Notes (MATH 326) Code Show All Code Hide All Code Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts RStudio Cheat Sheets RStudio Cheat Sheets R Base Commands Cheat Sheet R Base Commands Cheat Sheet Don’t forget to always load your libraries and to Knit often!! library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) This is the templet for the Hover Box Thingy Hover over words box! [insert description here]       …  Click to View Output. There are three parts to the help box: The starter code < a href=“javascript:showhide(‘[Insert classifying name]’)”> < div class=“hoverchunk” > The description code This is the pattern of the reoccurring code that you need to look o",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Experimental Design Notes (MATH 326) Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts Don’t forget to always load your libraries and to Knit often!!", "library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) This is the templet for the Hover Box Thingy Hover over words box!", "[insert description here]       …  Click to View Output.", "There are three parts to the help box: The starter code < a href=“javascript:showhide(‘[Insert classifying name]’)”> < div class=“hoverchunk” > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions!", "< span class=“tooltipr” > [What shows up in the box] < span class=“tooltiprtext” > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box!", "& nbsp;& nbsp;& nbsp;& nbsp; < span class=“tooltiprtext” >Press Enter to run the code.", "< /span> & nbsp;…& nbsp; < span class=“tooltiprtext” >Click to View Output.< /span > < /span > < /div > < /a > < div id=“[The classifying name that you put in the starter code!]” style=“display:none;” > *When using, make sure you fix all the spaces when using and use “< /div >” to show where the drop down is meant to end!", "Personal Notes OoOoooOOOoo let’s go assignments!!", "Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester.", "As well as some personal notes on those assignments!", "Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys!", "Skill Quiz - Introduction to R", "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Experimental Design Notes (MATH 326) Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts Don’t forget to always load your libraries and to Knit often!!", "library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) This is the templet for the Hover Box Thingy Hover over words box!", "[insert description here]       …  Click to View Output.", "There are three parts to the help box: The starter code < a href=“javascript:showhide(‘[Insert classifying name]’)”> < div class=“hoverchunk” > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions!", "< span class=“tooltipr” > [What shows up in the box] < span class=“tooltiprtext” > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box!", "& nbsp;& nbsp;& nbsp;& nbsp; < span class=“tooltiprtext” >Press Enter to run the code.", "< /span> & nbsp;…& nbsp; < span class=“tooltiprtext” >Click to View Output.< /span > < /span > < /div > < /a > < div id=“[The classifying name that you put in the starter code!]” style=“display:none;” > *When using, make sure you fix all the spaces when using and use “< /div >” to show where the drop down is meant to end!", "Personal Notes OoOoooOOOoo let’s go assignments!!", "Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester.", "As well as some personal notes on those assignments!", "Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys!", "Skill Quiz - Introduction to R", "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Experimental Design Notes (MATH 326) Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts Don’t forget to always load your libraries and to Knit often!!", "library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) This is the templet for the Hover Box Thingy Hover over words box!", "[insert description here]       …  Click to View Output.", "There are three parts to the help box: The starter code < a href=“javascript:showhide(‘[Insert classifying name]’)”> < div class=“hoverchunk” > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions!", "< span class=“tooltipr” > [What shows up in the box] < span class=“tooltiprtext” > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box!", "& nbsp;& nbsp;& nbsp;& nbsp; < span class=“tooltiprtext” >Press Enter to run the code.", "< /span> & nbsp;…& nbsp; < span class=“tooltiprtext” >Click to View Output.< /span > < /span > < /div > < /a > < div id=“[The classifying name that you put in the starter code!]” style=“display:none;” > *When using, make sure you fix all the spaces when using and use “< /div >” to show where the drop down is meant to end!", "Personal Notes OoOoooOOOoo let’s go assignments!!", "Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester.", "As well as some personal notes on those assignments!", "Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys!", "Skill Quiz - Introduction to R", "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus", "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus", "Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus", "Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425)", "Intermediate Statistics (MATH 325)", "Experimental Design (MATH 326)", "Applied Linear Regression (MATH 425)", "R Help R Commands R Markdown Hints Data Sources Example Analyses", "Describing Data Graphical Summaries Numerical Summaries", "Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization", "Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus", "325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project)", "Good Example Analysis", "Poor Example Analysis"],
    "type": "page",
    "page_title": "Experimental Design Notes"
  },
  {
    "id": 2,
    "title": "📍 Cheat Sheets",
    "url": "326PaigesNotes.html#cheatsheets",
    "content": "Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts",
    "sentences": "R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts",
    "type": "section",
    "page_title": "Experimental Design Notes",
    "section_title": "Cheat Sheets"
  },
  {
    "id": 3,
    "title": "📍 Don’t forget to always load your libraries and to Knit often!!",
    "url": "326PaigesNotes.html#dontforgettoalwaysloadyourlibrariesandtoknitoften",
    "content": "Don’t forget to always load your libraries and to Knit often!! library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) This is the templet for the Hover Box Thingy There are three parts to the help box: The starter code < a href=“javascript:showhide(‘[Insert classifying name]’)”> < div class=“hoverchunk” > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions! < span class=“tooltipr” > [What shows up in the box] < span class=“tooltiprtext” > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box! & nbsp;& nbsp;& nbsp;& nbsp; < span class=“tooltiprtext” >Press Enter to run the code. < /span> & nbsp;…& nbsp; < span class=“tooltiprtext” >Click to View Output.< /span > < /span > < /div > < /a > < div id=“[The classifying name that you put in the starter code!]” style=“display:none;” > *When using, make sure you fix all the spaces when using and use “< /div >” to show where the drop down is meant to end!",
    "sentences": ["library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven)", "This is the templet for the Hover Box Thingy", "There are three parts to the help box: The starter code < a href=“javascript:showhide(‘[Insert classifying name]’)”> < div class=“hoverchunk” > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions!", "< span class=“tooltipr” > [What shows up in the box] < span class=“tooltiprtext” > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box!", "& nbsp;& nbsp;& nbsp;& nbsp; < span class=“tooltiprtext” >Press Enter to run the code.", "< /span> & nbsp;…& nbsp; < span class=“tooltiprtext” >Click to View Output.< /span > < /span > < /div > < /a > < div id=“[The classifying name that you put in the starter code!]” style=“display:none;” > *When using, make sure you fix all the spaces when using and use “< /div >” to show where the drop down is meant to end!"],
    "type": "section",
    "page_title": "Experimental Design Notes",
    "section_title": "Don’t forget to always load your libraries and to Knit often!!"
  },
  {
    "id": 4,
    "title": "📍 Personal Notes",
    "url": "326PaigesNotes.html#personalnotes",
    "content": "Personal Notes OoOoooOOOoo let’s go assignments!! Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester. As well as some personal notes on those assignments! Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys! Skill Quiz - Introduction to R",
    "sentences": ["OoOoooOOOoo let’s go assignments!!", "Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester.", "As well as some personal notes on those assignments!", "Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys!", "Skill Quiz - Introduction to R"],
    "type": "section",
    "page_title": "Experimental Design Notes",
    "section_title": "Personal Notes"
  },
  {
    "id": 5,
    "title": "📍 Week 1 | Welcome to the Course & Introduction to RStudio",
    "url": "326PaigesNotes.html#week1welcometothecourseintroductiontorstudio",
    "content": "Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys! Skill Quiz - Introduction to R",
    "sentences": ["We were setting up RStudio and doing cool little graphic thingys!", "Skill Quiz - Introduction to R"],
    "type": "section",
    "page_title": "Experimental Design Notes",
    "section_title": "Week 1 | Welcome to the Course & Introduction to RStudio"
  },
  {
    "id": 6,
    "title": "Applied Linear Regression Notes",
    "url": "425PaigesNotes.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Download Rmd Applied Linear Regression Notes (MATH 425) Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts Don’t forget to always load your libraries and to Knit often!! library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) library(alr4) Personal Notes OoOoooOOOoo let’s go assignments!! LETS GOOOOO!!!! Hint: Don’t worry about fully getting the code, always make sure you are actively listening and watching up front. Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester. As well as some personal notes on those assignments! Week 1 | Simple Linear Regression We were looking at “The Mathematical Model” and “Interpreting Model Parameters”! Skill Quiz - Simple Linear Regression Problem 1 Open the airquality dataset in R. Perform a regression of daily average Wind(\\(Y_i\\)) speed using the daily maximum Temperature (\\(X_i\\)) as the explanatory variable. Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{Wind} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Temp} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0,\\sigma^2) \\] Part (b) Fit and summarize a simple linear regression model for this data. # Type your code there templm <- lm(Wind ~ Temp, data=airquality) summary(templm) %>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 23.23 2.112 11 4.901e-21 Temp -0.1705 0.02693 -6.331 2.642e-09 Fitting linear model: Wind ~ Temp Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 153 3.142 0.2098 0.2045 Part (c) Type out the estimated equation for this regression model using your estimates found in part (b). \\[ \\hat{Y}_i = 23.234 + (-0.170)X_i \\] Part (d) Plot the data and the estimated regression model. # Type your code here ggplot(airquality, aes(x=Temp, y=Wind)) + geom_point(size=1.5, color=\"lightblue\", alpha= 0.5) + geom_smooth(method=\"lm\", formula=y~x, se=FALSE, size=0.5,color=\"skyblue\")+ theme_minimal() Part (e) Use your model to predict the average daily average Wind speed for an outside temperature of 72 degrees F. \\[ \\hat{Y}_i = 23.234 + (-0.170)(72) \\] # Type your code here 23.234 + (-0.170)*(72) ## [1] 10.994 Part (f) Write out an interpretation of the slope and intercept of your model. Are both meaningful for this data? Slope Interpretation: An increase of 1 degree F in the daily maximum Temperature(\\(X_i\\)) results in a 0.17 mph decrease in the average daily average Wind ($Y_i) speed. Is the slope meaningful?: Yes, the regression seems to be a good fit to this data and the slope is significant so the interpretation of the slope is meaningful for this data. Intercept Interpretation: When the daily maximum Temperature is 0 degrees F, the average daily average Wind speed is estimated to be 23.234 mph. Is the intercept meaningful?: Yes, the intercept is meaningful for this data because it is significant and an outside temperature of 0 degrees F is a meaningful situation. Problem 2 Open the mtcars dataset in R. Fit a regression model to the data that can be used to explain average gas mileage of a vehicle (mpg) (\\(Y_i\\)) using the weight (wt) (\\(X_i\\)) of the vehicle. Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{mpg} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{wt} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0,\\sigma^2) \\] Part (b) Fit and summarize a simple linear regression model for this data. carslm <- lm(mpg ~ wt, data=mtcars) summary(carslm)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Part (c) Type out the estimated equation for this regression model using your estimates found in part (b). \\[ \\hat{Y}_i = 37.29 + (-5.34)X_i \\] Part (d) Plot the data and the estimated regression model. # Type your code here ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point(size=1.5, color=\"lightblue\", alpha= 0.5) + geom_smooth(method=\"lm\", formula=y~x, se=FALSE, size=0.5,color=\"skyblue\")+ theme_minimal() Part (e) Use your model to predict the average gas mileage (mpg) for a vehicle that weighs 3,000 lbs. (Hint: ?mtcars) **Must convert 3000 lbs by dividing by 1000, thus it would make it 3* \\[ \\hat{Y}_i = 37.29 + (-5.34)(3) \\] ANSWER: # Type your code here 37.29 + (-5.34)*(3) ## [1] 21.27 Part (f) Write out an interpretation of the slope and intercept of your model. Are both meaningful for this data? Slope Interpretation: An increase of 1,000 lbs in the weight (\\(X_i\\)) of a vehicle results in a 5.34 mpg decrease in the average gas mileage of such vehicles(\\(Y_i\\)). Is the slope meaningful? : While the slope is significant, it only looks to be meaningful for vehicles that weigh between 2.5 thousand and 4 thousand pounds. Otherwise this regression does not seem to be a good fit for this data based on the scatterplot. Intercept Interpretation: The average gas mileage of vehicles that weigh nothing (0 lbs) is estimated to be 37.29 mpg. Is the intercept meaningful?: No, the intercept is not meaningful for this data because a vehicle with weight zero is not possible Before we can really trust the interpretation of and predictions from a regression model, there are important diagnostic checks to perform on the regression. These diagnostics are even more important to perform when p-values or confidence intervals are used as part of the analysis. In future weeks of this course, we will focus in greater detail on the technical details of regression: hypothesis tests, confidence intervals, and diagnostic checks. However, for the sake of completeness, the following problems have run through these technical details, even though we lack full understanding about them for the time being. Problem 3 Use your regression for the airquality data set in Problem 1 to complete the following “technical details” for this regression. Part (a) Create a (1) residuals vs. fitted-values plot, (2) Q-Q Plot of the residuals, and (3) residuals vs. order plot. # Type your code here par(mfrow=c(1,3)) plot(templm, which=1) qqPlot(templm$residuals, main=\"Q-Q Plot\",pch= 19, id=FALSE) plot(templm$residuals, ylab= \"Residuals\", main=\"Residuals vs Order\") Part (b) Explain, as best you understand currently, what each of these three plots show for this regression. Explanation: Everything looks pretty good. The residuals vs. fitted-values plot shows constant variance and a nice linear relation. The Q-Q Plot shows possible problems with normality because some dots go out of bounds, but is fairly good. The residuals vs. order plot shows no problems with time trends, so the error terms can be assumed to be independent. Part (c) Report the p-value for the test of these hypotheses for your regression. Intercept Hypotheses \\[ H_0: \\beta_0 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_0 \\neq 0 \\] P-VALUE = < 2e-16 Slope Hypotheses \\[ H_0: \\beta_1 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_1 \\neq 0 \\] P-VALUE = < 2.64e-09 Comment on whether or not we should trust each p-value based on your plots in Part (a). Should trust each p-value based on your plots in Part (a)? Intercept Hypotheses p-value? : Yes, it can be trusted because the diagnostic plots all checked out. Slope Hypotheses p-value? : Yes, it can be trusted because the diagnostic plots all checked out. Problem 4 Use your regression for the mtcars data set in Problem 2 to complete the following “technical details” for this regression. Part (a) Create a (1) residuals vs. fitted-values plot, (2) Q-Q Plot of the residuals, and (3) residuals vs. order plot. # Type your code here par(mfrow=c(1,3)) plot(carslm, which=1) qqPlot(carslm$residuals, main=\"Q-Q Plot\",pch= 19, id=FALSE) plot(carslm$residuals, ylab= \"Residuals\", main=\"Residuals vs Order\") Part (b) Explain, as best you understand currently, what each of these three plots show for this regression. Explanation: Everything looks somewhat questionable. The residuals vs. fitted-values plot shows a lack of linearity, which makes it hard to judge constant variance. The Q-Q Plot shows possible problems with normality because some dots go out of bounds, but is fairly good. The residuals vs. order plot shows a possible problem with time trends due to the slight rainbow pattern. Part (c) Report the p-value for the test of these hypotheses for your regression. Intercept Hypotheses \\[ H_0: \\beta_0 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_0 \\neq 0 \\] P-VALUE = < 2e-16 Slope Hypotheses \\[ H_0: \\beta_1 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_1 \\neq 0 \\] P-VALUE = 1.29e-10 Comment on whether or not we should trust each p-value based on your plots in Part (a). Should trust each p-value based on your plots in Part (a)? Intercept Hypotheses p-value? : No, it should not be trusted because of the lack of linearity and the distance zero is from the current data. Slope Hypotheses p-value? : No, it should not be fully trusted, though there is likely some sort of trend in the data, because of some problems in the diagnostic plots. Assessment Quiz - Simple Linear Regression Run the following commands in R. library(car) View(Davis) ?Davis Reduce the data to just the data for the males. Then perform a regression with weight as the response variable and height as the explanatory variable. Which of the following provides the estimated average weight of males that are 180 cm tall? Davey <- filter(Davis, sex == \"M\") daveylm <- lm(weight ~ height, data=Davey) summary(daveylm)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) -101.3 29.86 -3.393 0.001046 height 0.9956 0.1676 5.939 5.922e-08 Fitting linear model: weight ~ height Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 88 10.07 0.2908 0.2826 -101.33 +(0.9956)*180 = -101.33 +(0.9956)*180 ## [1] 77.878 Run the following commands in R. View(USArrests) ?USArrests Perform a regression using this data that explains the average number of murder arrests in cities (per 100,000 in 1973) using the number of assault arrests (per 100,000) in a city. Select the answer that provides the correct estimate of \\(\\beta_1\\) in the formula: \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i \\sim N(0, \\sigma^2)\\) for the USArrests regression described above. arrestlm <- lm(Murder ~ Assault, data=USArrests) summary(arrestlm)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 0.6317 0.8548 0.739 0.4635 Assault 0.04191 0.004507 9.298 2.596e-12 Fitting linear model: Murder ~ Assault Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 50 2.629 0.643 0.6356 Which of the following statements is a correct statement about the graphic shown below? Note: the “Line of Equality” shows where the line would need to be if the reported heights were equal (on average) to the actual measured heights. Dots that fall on this line show men that knew their actual height before they were officially measured. Answer: The average actual height gets closer to the reported height for taller men, while shorter men seem more likely to under-report their height, on average. Class Activity - Estimating Means & Variablility Open the airquality dataset in R View(airquality) ?airquality Part 1: Estimating the Mean and Spread of Y Creating Histogram graph ggplot(airquality, aes(x=Temp))+ geom_histogram(binwidth=5, fill=\"lightblue\", color=\"skyblue\") + labs(title=\"Maximum daily temperature in defees Fahrenheit at La Gaurdia Airport, NY, USA\", x=\"Temperature in degrees F\", y=\"Number of Days in Temperature Range\") favstats(airquality$Temp) %>% pander() min Q1 median Q3 max mean sd n missing 56 72 79 85 97 77.88 9.465 153 0 Spread is SO important! This tells us the guess of this thing that we are looking at, how good or bad we were in our guess! essentially, 2 standard deviations are a good guess (takes up a good percentage of the data) Part 2: Estimating the Mean and Spread of Y for Categorical X weathering <- ggplot(airquality, aes(x=factor(Month), y=Temp)) + geom_boxplot(fill=\"lightblue\", color=\"skyblue\") + labs(title=\"Maximum daily temp in degrees F at La Guardia Airport, NY, USA\") plot(weathering) favstats(Temp ~ Month, data=airquality) ## Month min Q1 median Q3 max mean ## 1 5 56 60.0 66 69.00 81 65.54839 ## 2 6 65 76.0 78 82.75 93 79.10000 ## 3 7 73 81.5 84 86.00 92 83.90323 ## 4 8 72 79.0 82 88.50 97 83.96774 ## 5 9 63 71.0 76 81.00 93 76.90000 ## sd n missing ## 1 6.854870 31 0 ## 2 6.598589 30 0 ## 3 4.315513 31 0 ## 4 6.585256 31 0 ## 5 8.355671 30 0 Part 3: Estimating the Mean and Spread of Y for Quantitative X dots : represents actual temperature line : represents prediction of temperature x : represents our explanatory variable (VERY important) ggplot(airquality, aes(x=Wind, y=Temp)) + geom_point(size=1.5, color=\"lightblue\", alpha= 0.5) + geom_smooth(method=\"lm\", formula=y~x, se=FALSE, size=0.5,color=\"skyblue\")+ theme_minimal() myweather <- lm(Temp ~ Wind, data = airquality) summary(myweather)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 90.13 2.052 43.92 6.69e-88 Wind -1.23 0.1944 -6.331 2.642e-09 Fitting linear model: Temp ~ Wind Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 153 8.442 0.2098 0.2045 Use your regression equation to compute the estimated mean Temp for a day (between May and September) with a morning average wind speed (7 AM to 10 AM) of 19 miles per hour: What is the standard error (estimated standard deviation) of the points around your regression equation? how much the dots will spread from the line (how much a dot will vary fr",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Download Rmd Applied Linear Regression Notes (MATH 425) Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts Don’t forget to always load your libraries and to Knit often!!", "library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) library(alr4) Personal Notes OoOoooOOOoo let’s go assignments!!", "LETS GOOOOO!!!!", "Hint: Don’t worry about fully getting the code, always make sure you are actively listening and watching up front.", "Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester.", "As well as some personal notes on those assignments!", "Week 1 | Simple Linear Regression We were looking at “The Mathematical Model” and “Interpreting Model Parameters”!", "Skill Quiz - Simple Linear Regression Problem 1 Open the airquality dataset in R.", "Perform a regression of daily average Wind(\\(Y_i\\)) speed using the daily maximum Temperature (\\(X_i\\)) as the explanatory variable.", "Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation.", "\\[ \\underbrace{Y_i}_\\text{Wind} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Temp} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0,\\sigma^2) \\] Part (b) Fit and summarize a simple linear regression model for this data.", "# Type your code there templm <- lm(Wind ~ Temp, data=airquality) summary(templm) %>% pander()   Estimate Std.", "Error t value Pr(>|t|) (Intercept) 23.23 2.112 11 4.901e-21 Temp -0.1705 0.02693 -6.331 2.642e-09 Fitting linear model: Wind ~ Temp Observations Residual Std.", "Error \\(R^2\\) Adjusted \\(R^2\\) 153 3.142 0.2098 0.2045 Part (c) Type out the estimated equation for this regression model using your estimates found in part (b).", "\\[ \\hat{Y}_i = 23.234 + (-0.170)X_i \\] Part (d) Plot the data and the estimated regression model.", "# Type your code here ggplot(airquality, aes(x=Temp, y=Wind)) + geom_point(size=1.5, color=\"lightblue\", alpha= 0.5) + geom_smooth(method=\"lm\", formula=y~x, se=FALSE, size=0.5,color=\"skyblue\")+ theme_minimal() Part (e) Use your model to predict the average daily average Wind speed for an outside temperature of 72 degrees F.", "\\[ \\hat{Y}_i = 23.234 + (-0.170)(72) \\] # Type your code here 23.234 + (-0.170)*(72) ## [1] 10.994 Part (f) Write out an interpretation of the slope and intercept of your model.", "Are both meaningful for this data?", "Slope Interpretation: An increase of 1 degree F in the daily maximum Temperature(\\(X_i\\)) results in a 0.17 mph decrease in the average daily average Wind ($Y_i) speed.", "Is the slope meaningful?: Yes, the regression seems to be a good fit to this data and the slope is significant so the interpretation of the slope is meaningful for this data.", "Intercept Interpretation: When the daily maximum Temperature is 0 degrees F, the average daily average Wind speed is estimated to be 23.234 mph.", "Is the intercept meaningful?: Yes, the intercept is meaningful for this data because it is significant and an outside temperature of 0 degrees F is a meaningful situation.", "Problem 2 Open the mtcars dataset in R.", "Fit a regression model to the data that can be used to explain average gas mileage of a vehicle (mpg) (\\(Y_i\\)) using the weight (wt) (\\(X_i\\)) of the vehicle.", "Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation.", "\\[ \\underbrace{Y_i}_\\text{mpg} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{wt} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0,\\sigma^2) \\] Part (b) Fit and summarize a simple linear regression model for this data.", "carslm <- lm(mpg ~ wt, data=mtcars) summary(carslm)%>%pander()   Estimate Std.", "Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std.", "Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Part (c) Type out the estimated equation for this regression model using your estimates found in part (b).", "\\[ \\hat{Y}_i = 37.29 + (-5.34)X_i \\] Part (d) Plot the data and the estimated regression model.", "# Type your code here ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point(size=1.5, color=\"lightblue\", alpha= 0.5) + geom_smooth(method=\"lm\", formula=y~x, se=FALSE, size=0.5,color=\"skyblue\")+ theme_minimal() Part (e) Use your model to predict the average gas mileage (mpg) for a vehicle that weighs 3,000 lbs.", "(Hint: ?mtcars) **Must convert 3000 lbs by dividing by 1000, thus it would make it 3* \\[ \\hat{Y}_i = 37.29 + (-5.34)(3) \\] ANSWER: # Type your code here 37.29 + (-5.34)*(3) ## [1] 21.27 Part (f) Write out an interpretation of the slope and intercept of your model.", "Are both meaningful for this data?", "Slope Interpretation: An increase of 1,000 lbs in the weight (\\(X_i\\)) of a vehicle results in a 5.34 mpg decrease in the average gas mileage of such vehicles(\\(Y_i\\)).", "Is the slope meaningful?", ": While the slope is significant, it only looks to be meaningful for vehicles that weigh between 2.5 thousand and 4 thousand pounds.", "Otherwise this regression does not seem to be a good fit for this data based on the scatterplot.", "Intercept Interpretation: The average gas mileage of vehicles that weigh nothing (0 lbs) is estimated to be 37.29 mpg.", "Is the intercept meaningful?: No, the intercept is not meaningful for this data because a vehicle with weight zero is not possible Before we can really trust the interpretation of and predictions from a regression model, there are important diagnostic checks to perform on the regression.", "These diagnostics are even more important to perform when p-values or confidence intervals are used as part of the analysis.", "In future weeks of this course, we will focus in greater detail on the technical details of regression: hypothesis tests, confidence intervals, and diagnostic checks.", "However, for the sake of completeness, the following problems have run through these technical details, even though we lack full understanding about them for the time being.", "Problem 3 Use your regression for the airquality data set in Problem 1 to complete the following “technical details” for this regression.", "Part (a) Create a (1) residuals vs. fitted-values plot, (2) Q-Q Plot of the residuals, and (3) residuals vs. order plot.", "# Type your code here par(mfrow=c(1,3)) plot(templm, which=1) qqPlot(templm$residuals, main=\"Q-Q Plot\",pch= 19, id=FALSE) plot(templm$residuals, ylab= \"Residuals\", main=\"Residuals vs Order\") Part (b) Explain, as best you understand currently, what each of these three plots show for this regression.", "Explanation: Everything looks pretty good.", "The residuals vs. fitted-values plot shows constant variance and a nice linear relation.", "The Q-Q Plot shows possible problems with normality because some dots go out of bounds, but is fairly good.", "The residuals vs. order plot shows no problems with time trends, so the error terms can be assumed to be independent.", "Part (c) Report the p-value for the test of these hypotheses for your regression."],
    "type": "page",
    "page_title": "Applied Linear Regression Notes"
  },
  {
    "id": 7,
    "title": "📍 Cheat Sheets",
    "url": "425PaigesNotes.html#cheatsheets",
    "content": "Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts",
    "sentences": "R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts",
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Cheat Sheets"
  },
  {
    "id": 8,
    "title": "📍 Don’t forget to always load your libraries and to Knit often!!",
    "url": "425PaigesNotes.html#dontforgettoalwaysloadyourlibrariesandtoknitoften",
    "content": "Don’t forget to always load your libraries and to Knit often!! library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) library(alr4)",
    "sentences": "library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) library(alr4)",
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Don’t forget to always load your libraries and to Knit often!!"
  },
  {
    "id": 9,
    "title": "📍 Weekly Assignments",
    "url": "425PaigesNotes.html#weeklyassignments",
    "content": "Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester. As well as some personal notes on those assignments! Week 1 | Simple Linear Regression We were looking at “The Mathematical Model” and “Interpreting Model Parameters”! Skill Quiz - Simple Linear Regression Problem 1 Open the airquality dataset in R. Perform a regression of daily average Wind(\\(Y_i\\)) speed using the daily maximum Temperature (\\(X_i\\)) as the explanatory variable. Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{Wind} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Temp} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0,\\sigma^2) \\] Part (b) Fit and summarize a simple linear regression model for this data. # Type your code there templm <- lm(Wind ~ Temp, data=airquality) summary(templm) %>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 23.23 2.112 11 4.901e-21 Temp -0.1705 0.02693 -6.331 2.642e-09 Fitting linear model: Wind ~ Temp Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 153 3.142 0.2098 0.2045 Part (c) Type out the estimated equation for this regression model using your estimates found in part (b). \\[ \\hat{Y}_i = 23.234 + (-0.170)X_i \\] Part (d) Plot the data and the estimated regression model. # Type your code here ggplot(airquality, aes(x=Temp, y=Wind)) + geom_point(size=1.5, color=\"lightblue\", alpha= 0.5) + geom_smooth(method=\"lm\", formula=y~x, se=FALSE, size=0.5,color=\"skyblue\")+ theme_minimal() Part (e) Use your model to predict the average daily average Wind speed for an outside temperature of 72 degrees F. \\[ \\hat{Y}_i = 23.234 + (-0.170)(72) \\] # Type your code here 23.234 + (-0.170)*(72) ## [1] 10.994 Part (f) Write out an interpretation of the slope and intercept of your model. Are both meaningful for this data? Slope Interpretation: An increase of 1 degree F in the daily maximum Temperature(\\(X_i\\)) results in a 0.17 mph decrease in the average daily average Wind ($Y_i) speed. Is the slope meaningful?: Yes, the regression seems to be a good fit to this data and the slope is significant so the interpretation of the slope is meaningful for this data. Intercept Interpretation: When the daily maximum Temperature is 0 degrees F, the average daily average Wind speed is estimated to be 23.234 mph. Is the intercept meaningful?: Yes, the intercept is meaningful for this data because it is significant and an outside temperature of 0 degrees F is a meaningful situation. Problem 2 Open the mtcars dataset in R. Fit a regression model to the data that can be used to explain average gas mileage of a vehicle (mpg) (\\(Y_i\\)) using the weight (wt) (\\(X_i\\)) of the vehicle. Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{mpg} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{wt} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0,\\sigma^2) \\] Part (b) Fit and summarize a simple linear regression model for this data. carslm <- lm(mpg ~ wt, data=mtcars) summary(carslm)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Part (c) Type out the estimated equation for this regression model using your estimates found in part (b). \\[ \\hat{Y}_i = 37.29 + (-5.34)X_i \\] Part (d) Plot the data and the estimated regression model. # Type your code here ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point(size=1.5, color=\"lightblue\", alpha= 0.5) + geom_smooth(method=\"lm\", formula=y~x, se=FALSE, size=0.5,color=\"skyblue\")+ theme_minimal() Part (e) Use your model to predict the average gas mileage (mpg) for a vehicle that weighs 3,000 lbs. (Hint: ?mtcars) **Must convert 3000 lbs by dividing by 1000, thus it would make it 3* \\[ \\hat{Y}_i = 37.29 + (-5.34)(3) \\] ANSWER: # Type your code here 37.29 + (-5.34)*(3) ## [1] 21.27 Part (f) Write out an interpretation of the slope and intercept of your model. Are both meaningful for this data? Slope Interpretation: An increase of 1,000 lbs in the weight (\\(X_i\\)) of a vehicle results in a 5.34 mpg decrease in the average gas mileage of such vehicles(\\(Y_i\\)). Is the slope meaningful? : While the slope is significant, it only looks to be meaningful for vehicles that weigh between 2.5 thousand and 4 thousand pounds. Otherwise this regression does not seem to be a good fit for this data based on the scatterplot. Intercept Interpretation: The average gas mileage of vehicles that weigh nothing (0 lbs) is estimated to be 37.29 mpg. Is the intercept meaningful?: No, the intercept is not meaningful for this data because a vehicle with weight zero is not possible Before we can really trust the interpretation of and predictions from a regression model, there are important diagnostic checks to perform on the regression. These diagnostics are even more important to perform when p-values or confidence intervals are used as part of the analysis. In future weeks of this course, we will focus in greater detail on the technical details of regression: hypothesis tests, confidence intervals, and diagnostic checks. However, for the sake of completeness, the following problems have run through these technical details, even though we lack full understanding about them for the time being. Problem 3 Use your regression for the airquality data set in Problem 1 to complete the following “technical details” for this regression. Part (a) Create a (1) residuals vs. fitted-values plot, (2) Q-Q Plot of the residuals, and (3) residuals vs. order plot. # Type your code here par(mfrow=c(1,3)) plot(templm, which=1) qqPlot(templm$residuals, main=\"Q-Q Plot\",pch= 19, id=FALSE) plot(templm$residuals, ylab= \"Residuals\", main=\"Residuals vs Order\") Part (b) Explain, as best you understand currently, what each of these three plots show for this regression. Explanation: Everything looks pretty good. The residuals vs. fitted-values plot shows constant variance and a nice linear relation. The Q-Q Plot shows possible problems with normality because some dots go out of bounds, but is fairly good. The residuals vs. order plot shows no problems with time trends, so the error terms can be assumed to be independent. Part (c) Report the p-value for the test of these hypotheses for your regression. Intercept Hypotheses \\[ H_0: \\beta_0 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_0 \\neq 0 \\] P-VALUE = < 2e-16 Slope Hypotheses \\[ H_0: \\beta_1 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_1 \\neq 0 \\] P-VALUE = < 2.64e-09 Comment on whether or not we should trust each p-value based on your plots in Part (a). Should trust each p-value based on your plots in Part (a)? Intercept Hypotheses p-value? : Yes, it can be trusted because the diagnostic plots all checked out. Slope Hypotheses p-value? : Yes, it can be trusted because the diagnostic plots all checked out. Problem 4 Use your regression for the mtcars data set in Problem 2 to complete the following “technical details” for this regression. Part (a) Create a (1) residuals vs. fitted-values plot, (2) Q-Q Plot of the residuals, and (3) residuals vs. order plot. # Type your code here par(mfrow=c(1,3)) plot(carslm, which=1) qqPlot(carslm$residuals, main=\"Q-Q Plot\",pch= 19, id=FALSE) plot(carslm$residuals, ylab= \"Residuals\", main=\"Residuals vs Order\") Part (b) Explain, as best you understand currently, what each of these three plots show for this regression. Explanation: Everything looks somewhat questionable. The residuals vs. fitted-values plot shows a lack of linearity, which makes it hard to judge constant variance. The Q-Q Plot shows possible problems with normality because some dots go out of bounds, but is fairly good. The residuals vs. order plot shows a possible problem with time trends due to the slight rainbow pattern. Part (c) Report the p-value for the test of these hypotheses for your regression. Intercept Hypotheses \\[ H_0: \\beta_0 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_0 \\neq 0 \\] P-VALUE = < 2e-16 Slope Hypotheses \\[ H_0: \\beta_1 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_1 \\neq 0 \\] P-VALUE = 1.29e-10 Comment on whether or not we should trust each p-value based on your plots in Part (a). Should trust each p-value based on your plots in Part (a)? Intercept Hypotheses p-value? : No, it should not be trusted because of the lack of linearity and the distance zero is from the current data. Slope Hypotheses p-value? : No, it should not be fully trusted, though there is likely some sort of trend in the data, because of some problems in the diagnostic plots. Assessment Quiz - Simple Linear Regression Run the following commands in R. library(car) View(Davis) ?Davis Reduce the data to just the data for the males. Then perform a regression with weight as the response variable and height as the explanatory variable. Which of the following provides the estimated average weight of males that are 180 cm tall? Davey <- filter(Davis, sex == \"M\") daveylm <- lm(weight ~ height, data=Davey) summary(daveylm)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) -101.3 29.86 -3.393 0.001046 height 0.9956 0.1676 5.939 5.922e-08 Fitting linear model: weight ~ height Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 88 10.07 0.2908 0.2826 -101.33 +(0.9956)*180 = -101.33 +(0.9956)*180 ## [1] 77.878 Run the following commands in R. View(USArrests) ?USArrests Perform a regression using this data that explains the average number of murder arrests in cities (per 100,000 in 1973) using the number of assault arrests (per 100,000) in a city. Select the answer that provides the correct estimate of \\(\\beta_1\\) in the formula: \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i \\sim N(0, \\sigma^2)\\) for the USArrests regression described above. arrestlm <- lm(Murder ~ Assault, data=USArrests) summary(arrestlm)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 0.6317 0.8548 0.739 0.4635 Assault 0.04191 0.004507 9.298 2.596e-12 Fitting linear model: Murder ~ Assault Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 50 2.629 0.643 0.6356 Which of the following statements is a correct statement about the graphic shown below? Note: the “Line of Equality” shows where the line would need to be if the reported heights were equal (on average) to the actual measured heights. Dots that fall on this line show men that knew their actual height before they were officially measured. Answer: The average actual height gets closer to the reported height for taller men, while shorter men seem more likely to under-report their height, on average. Class Activity - Estimating Means & Variablility Open the airquality dataset in R View(airquality) ?airquality Part 1: Estimating the Mean and Spread of Y Creating Histogram graph ggplot(airquality, aes(x=Temp))+ geom_histogram(binwidth=5, fill=\"lightblue\", color=\"skyblue\") + labs(title=\"Maximum daily temperature in defees Fahrenheit at La Gaurdia Airport, NY, USA\", x=\"Temperature in degrees F\", y=\"Number of Days in Temperature Range\") favstats(airquality$Temp) %>% pander() min Q1 median Q3 max mean sd n missing 56 72 79 85 97 77.88 9.465 153 0 Spread is SO important! This tells us the guess of this thing that we are looking at, how good or bad we were in our guess! essentially, 2 standard deviations are a good guess (takes up a good percentage of the data) Part 2: Estimating the Mean and Spread of Y for Categorical X weathering <- ggplot(airquality, aes(x=factor(Month), y=Temp)) + geom_boxplot(fill=\"lightblue\", color=\"skyblue\") + labs(title=\"Maximum daily temp in degrees F at La Guardia Airport, NY, USA\") plot(weathering) favstats(Temp ~ Month, data=airquality) ## Month min Q1 median Q3 max mean ## 1 5 56 60.0 66 69.00 81 65.54839 ## 2 6 65 76.0 78 82.75 93 79.10000 ## 3 7 73 81.5 84 86.00 92 83.90323 ## 4 8 72 79.0 82 88.50 97 83.96774 ## 5 9 63 71.0 76 81.00 93 76.90000 ## sd n missing ## 1 6.854870 31 0 ## 2 6.598589 30 0 ## 3 4.315513 31 0 ## 4 6.585256 31 0 ## 5 8.355671 30 0 Part 3: Estimating the Mean and Spread of Y for Quantitative X dots : represents actual temperature line : represents prediction of temperature x : represents our explanatory variable (VERY important) ggplot(airquality, aes(x=Wind, y=Temp)) + geom_point(size=1.5, color=\"lightblue\", alpha= 0.5) + geom_smooth(method=\"lm\", formula=y~x, se=FALSE, size=0.5,color=\"skyblue\")+ theme_minimal() myweather <- lm(Temp ~ Wind, data = airquality) summary(myweather)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 90.13 2.052 43.92 6.69e-88 Wind -1.23 0.1944 -6.331 2.642e-09 Fitting linear model: Temp ~ Wind Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 153 8.442 0.2098 0.2045 Use your regression equation to compute the estimated mean Temp for a day (between May and September) with a morning average wind speed (7 AM to 10 AM) of 19 miles per hour: What is the standard error (estimated standard deviation) of the points around your regression equation? how much the dots will spread from the line (how much a dot will vary from our prediction) Hint: Look for the “residual standard error” in your regression summary output. Class Activity - The Regression Model \\(Y_i\\) : \\(\\sigma\\) : control the tightness of the data (68%) \\(\\epsilon\\) : explanatory data value for individuality (gives us our variability, the dots departure from the line .aka the law) Part 2 : Simulating Data from a Regression Model residual standard error of this regression came out to be 8.442 the sample size consisted of n = 153 data points \\(\\hat{Y}_i\\) : the line that we found in the middle of the data (depends on the value of x) However, the “true regression equation” (or “true law”) governing the relationship between the Mean Max Temp and the Morning Wind Speed is still unknown to us. Suppose however that God revealed this true law to be governed by the equation: the data doesn’t have slope, the law that created the dots have slope! the slope is interpreted as the change in the average y-value for a one unit change in the x-value. we can’t tell what an individual will do, but we can tell what the law will do based on the individual",
    "sentences": ["The sections below show the different Skill Quizzes and Class Activities done throughout the semester.", "As well as some personal notes on those assignments!", "Week 1 | Simple Linear Regression We were looking at “The Mathematical Model” and “Interpreting Model Parameters”!", "Skill Quiz - Simple Linear Regression Problem 1 Open the airquality dataset in R.", "Perform a regression of daily average Wind(\\(Y_i\\)) speed using the daily maximum Temperature (\\(X_i\\)) as the explanatory variable.", "Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation.", "\\[ \\underbrace{Y_i}_\\text{Wind} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Temp} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0,\\sigma^2) \\] Part (b) Fit and summarize a simple linear regression model for this data.", "# Type your code there templm <- lm(Wind ~ Temp, data=airquality) summary(templm) %>% pander()   Estimate Std.", "Error t value Pr(>|t|) (Intercept) 23.23 2.112 11 4.901e-21 Temp -0.1705 0.02693 -6.331 2.642e-09 Fitting linear model: Wind ~ Temp Observations Residual Std.", "Error \\(R^2\\) Adjusted \\(R^2\\) 153 3.142 0.2098 0.2045 Part (c) Type out the estimated equation for this regression model using your estimates found in part (b)."],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Weekly Assignments"
  },
  {
    "id": 10,
    "title": "📍 Week 1 | Simple Linear Regression",
    "url": "425PaigesNotes.html#week1simplelinearregression",
    "content": "Week 1 | Simple Linear Regression We were looking at “The Mathematical Model” and “Interpreting Model Parameters”! Skill Quiz - Simple Linear Regression Problem 1 Open the airquality dataset in R. Perform a regression of daily average Wind(\\(Y_i\\)) speed using the daily maximum Temperature (\\(X_i\\)) as the explanatory variable. Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{Wind} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Temp} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0,\\sigma^2) \\] Part (b) Fit and summarize a simple linear regression model for this data. # Type your code there templm <- lm(Wind ~ Temp, data=airquality) summary(templm) %>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 23.23 2.112 11 4.901e-21 Temp -0.1705 0.02693 -6.331 2.642e-09 Fitting linear model: Wind ~ Temp Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 153 3.142 0.2098 0.2045 Part (c) Type out the estimated equation for this regression model using your estimates found in part (b). \\[ \\hat{Y}_i = 23.234 + (-0.170)X_i \\] Part (d) Plot the data and the estimated regression model. # Type your code here ggplot(airquality, aes(x=Temp, y=Wind)) + geom_point(size=1.5, color=\"lightblue\", alpha= 0.5) + geom_smooth(method=\"lm\", formula=y~x, se=FALSE, size=0.5,color=\"skyblue\")+ theme_minimal() Part (e) Use your model to predict the average daily average Wind speed for an outside temperature of 72 degrees F. \\[ \\hat{Y}_i = 23.234 + (-0.170)(72) \\] # Type your code here 23.234 + (-0.170)*(72) ## [1] 10.994 Part (f) Write out an interpretation of the slope and intercept of your model. Are both meaningful for this data? Slope Interpretation: An increase of 1 degree F in the daily maximum Temperature(\\(X_i\\)) results in a 0.17 mph decrease in the average daily average Wind ($Y_i) speed. Is the slope meaningful?: Yes, the regression seems to be a good fit to this data and the slope is significant so the interpretation of the slope is meaningful for this data. Intercept Interpretation: When the daily maximum Temperature is 0 degrees F, the average daily average Wind speed is estimated to be 23.234 mph. Is the intercept meaningful?: Yes, the intercept is meaningful for this data because it is significant and an outside temperature of 0 degrees F is a meaningful situation. Problem 2 Open the mtcars dataset in R. Fit a regression model to the data that can be used to explain average gas mileage of a vehicle (mpg) (\\(Y_i\\)) using the weight (wt) (\\(X_i\\)) of the vehicle. Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{mpg} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{wt} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0,\\sigma^2) \\] Part (b) Fit and summarize a simple linear regression model for this data. carslm <- lm(mpg ~ wt, data=mtcars) summary(carslm)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Part (c) Type out the estimated equation for this regression model using your estimates found in part (b). \\[ \\hat{Y}_i = 37.29 + (-5.34)X_i \\] Part (d) Plot the data and the estimated regression model. # Type your code here ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point(size=1.5, color=\"lightblue\", alpha= 0.5) + geom_smooth(method=\"lm\", formula=y~x, se=FALSE, size=0.5,color=\"skyblue\")+ theme_minimal() Part (e) Use your model to predict the average gas mileage (mpg) for a vehicle that weighs 3,000 lbs. (Hint: ?mtcars) **Must convert 3000 lbs by dividing by 1000, thus it would make it 3* \\[ \\hat{Y}_i = 37.29 + (-5.34)(3) \\] ANSWER: # Type your code here 37.29 + (-5.34)*(3) ## [1] 21.27 Part (f) Write out an interpretation of the slope and intercept of your model. Are both meaningful for this data? Slope Interpretation: An increase of 1,000 lbs in the weight (\\(X_i\\)) of a vehicle results in a 5.34 mpg decrease in the average gas mileage of such vehicles(\\(Y_i\\)). Is the slope meaningful? : While the slope is significant, it only looks to be meaningful for vehicles that weigh between 2.5 thousand and 4 thousand pounds. Otherwise this regression does not seem to be a good fit for this data based on the scatterplot. Intercept Interpretation: The average gas mileage of vehicles that weigh nothing (0 lbs) is estimated to be 37.29 mpg. Is the intercept meaningful?: No, the intercept is not meaningful for this data because a vehicle with weight zero is not possible Before we can really trust the interpretation of and predictions from a regression model, there are important diagnostic checks to perform on the regression. These diagnostics are even more important to perform when p-values or confidence intervals are used as part of the analysis. In future weeks of this course, we will focus in greater detail on the technical details of regression: hypothesis tests, confidence intervals, and diagnostic checks. However, for the sake of completeness, the following problems have run through these technical details, even though we lack full understanding about them for the time being. Problem 3 Use your regression for the airquality data set in Problem 1 to complete the following “technical details” for this regression. Part (a) Create a (1) residuals vs. fitted-values plot, (2) Q-Q Plot of the residuals, and (3) residuals vs. order plot. # Type your code here par(mfrow=c(1,3)) plot(templm, which=1) qqPlot(templm$residuals, main=\"Q-Q Plot\",pch= 19, id=FALSE) plot(templm$residuals, ylab= \"Residuals\", main=\"Residuals vs Order\") Part (b) Explain, as best you understand currently, what each of these three plots show for this regression. Explanation: Everything looks pretty good. The residuals vs. fitted-values plot shows constant variance and a nice linear relation. The Q-Q Plot shows possible problems with normality because some dots go out of bounds, but is fairly good. The residuals vs. order plot shows no problems with time trends, so the error terms can be assumed to be independent. Part (c) Report the p-value for the test of these hypotheses for your regression. Intercept Hypotheses \\[ H_0: \\beta_0 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_0 \\neq 0 \\] P-VALUE = < 2e-16 Slope Hypotheses \\[ H_0: \\beta_1 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_1 \\neq 0 \\] P-VALUE = < 2.64e-09 Comment on whether or not we should trust each p-value based on your plots in Part (a). Should trust each p-value based on your plots in Part (a)? Intercept Hypotheses p-value? : Yes, it can be trusted because the diagnostic plots all checked out. Slope Hypotheses p-value? : Yes, it can be trusted because the diagnostic plots all checked out. Problem 4 Use your regression for the mtcars data set in Problem 2 to complete the following “technical details” for this regression. Part (a) Create a (1) residuals vs. fitted-values plot, (2) Q-Q Plot of the residuals, and (3) residuals vs. order plot. # Type your code here par(mfrow=c(1,3)) plot(carslm, which=1) qqPlot(carslm$residuals, main=\"Q-Q Plot\",pch= 19, id=FALSE) plot(carslm$residuals, ylab= \"Residuals\", main=\"Residuals vs Order\") Part (b) Explain, as best you understand currently, what each of these three plots show for this regression. Explanation: Everything looks somewhat questionable. The residuals vs. fitted-values plot shows a lack of linearity, which makes it hard to judge constant variance. The Q-Q Plot shows possible problems with normality because some dots go out of bounds, but is fairly good. The residuals vs. order plot shows a possible problem with time trends due to the slight rainbow pattern. Part (c) Report the p-value for the test of these hypotheses for your regression. Intercept Hypotheses \\[ H_0: \\beta_0 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_0 \\neq 0 \\] P-VALUE = < 2e-16 Slope Hypotheses \\[ H_0: \\beta_1 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_1 \\neq 0 \\] P-VALUE = 1.29e-10 Comment on whether or not we should trust each p-value based on your plots in Part (a). Should trust each p-value based on your plots in Part (a)? Intercept Hypotheses p-value? : No, it should not be trusted because of the lack of linearity and the distance zero is from the current data. Slope Hypotheses p-value? : No, it should not be fully trusted, though there is likely some sort of trend in the data, because of some problems in the diagnostic plots.",
    "sentences": ["We were looking at “The Mathematical Model” and “Interpreting Model Parameters”!", "Skill Quiz - Simple Linear Regression Problem 1 Open the airquality dataset in R.", "Perform a regression of daily average Wind(\\(Y_i\\)) speed using the daily maximum Temperature (\\(X_i\\)) as the explanatory variable.", "Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation.", "\\[ \\underbrace{Y_i}_\\text{Wind} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Temp} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0,\\sigma^2) \\] Part (b) Fit and summarize a simple linear regression model for this data.", "# Type your code there templm <- lm(Wind ~ Temp, data=airquality) summary(templm) %>% pander()   Estimate Std.", "Error t value Pr(>|t|) (Intercept) 23.23 2.112 11 4.901e-21 Temp -0.1705 0.02693 -6.331 2.642e-09 Fitting linear model: Wind ~ Temp Observations Residual Std.", "Error \\(R^2\\) Adjusted \\(R^2\\) 153 3.142 0.2098 0.2045 Part (c) Type out the estimated equation for this regression model using your estimates found in part (b).", "\\[ \\hat{Y}_i = 23.234 + (-0.170)X_i \\] Part (d) Plot the data and the estimated regression model.", "# Type your code here ggplot(airquality, aes(x=Temp, y=Wind)) + geom_point(size=1.5, color=\"lightblue\", alpha= 0.5) + geom_smooth(method=\"lm\", formula=y~x, se=FALSE, size=0.5,color=\"skyblue\")+ theme_minimal() Part (e) Use your model to predict the average daily average Wind speed for an outside temperature of 72 degrees F."],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Week 1 | Simple Linear Regression"
  },
  {
    "id": 11,
    "title": "📍 Skill Quiz - Simple Linear Regression",
    "url": "425PaigesNotes.html#skillquizsimplelinearregression",
    "content": "Skill Quiz - Simple Linear Regression Open the airquality dataset in R. Perform a regression of daily average Wind(\\(Y_i\\)) speed using the daily maximum Temperature (\\(X_i\\)) as the explanatory variable. Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{Wind} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Temp} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0,\\sigma^2) \\] Fit and summarize a simple linear regression model for this data. # Type your code there templm <- lm(Wind ~ Temp, data=airquality) summary(templm) %>% pander() Type out the estimated equation for this regression model using your estimates found in part (b). \\[ \\hat{Y}_i = 23.234 + (-0.170)X_i \\] Plot the data and the estimated regression model. # Type your code here ggplot(airquality, aes(x=Temp, y=Wind)) + geom_point(size=1.5, color=\"lightblue\", alpha= 0.5) + geom_smooth(method=\"lm\", formula=y~x, se=FALSE, size=0.5,color=\"skyblue\")+ theme_minimal()",
    "sentences": ["Open the airquality dataset in R.", "Perform a regression of daily average Wind(\\(Y_i\\)) speed using the daily maximum Temperature (\\(X_i\\)) as the explanatory variable.", "Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation.", "\\[ \\underbrace{Y_i}_\\text{Wind} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Temp} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0,\\sigma^2) \\]", "Fit and summarize a simple linear regression model for this data.", "# Type your code there templm <- lm(Wind ~ Temp, data=airquality) summary(templm) %>% pander()", "Type out the estimated equation for this regression model using your estimates found in part (b).", "\\[ \\hat{Y}_i = 23.234 + (-0.170)X_i \\]", "Plot the data and the estimated regression model.", "# Type your code here ggplot(airquality, aes(x=Temp, y=Wind)) + geom_point(size=1.5, color=\"lightblue\", alpha= 0.5) + geom_smooth(method=\"lm\", formula=y~x, se=FALSE, size=0.5,color=\"skyblue\")+ theme_minimal()"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Skill Quiz - Simple Linear Regression"
  },
  {
    "id": 12,
    "title": "📍 Assessment Quiz - Simple Linear Regression",
    "url": "425PaigesNotes.html#assessmentquizsimplelinearregression",
    "content": "Assessment Quiz - Simple Linear Regression Run the following commands in R. library(car) View(Davis) ?Davis Reduce the data to just the data for the males. Then perform a regression with weight as the response variable and height as the explanatory variable. Which of the following provides the estimated average weight of males that are 180 cm tall? Davey <- filter(Davis, sex == \"M\") daveylm <- lm(weight ~ height, data=Davey) summary(daveylm)%>%pander() -101.33 +(0.9956)*180 = -101.33 +(0.9956)*180 Run the following commands in R. View(USArrests) ?USArrests",
    "sentences": ["Run the following commands in R.", "library(car) View(Davis) ?Davis", "Reduce the data to just the data for the males.", "Then perform a regression with weight as the response variable and height as the explanatory variable.", "Which of the following provides the estimated average weight of males that are 180 cm tall?", "Davey <- filter(Davis, sex == \"M\") daveylm <- lm(weight ~ height, data=Davey) summary(daveylm)%>%pander()", "-101.33 +(0.9956)*180 =", "-101.33 +(0.9956)*180", "Run the following commands in R.", "View(USArrests) ?USArrests"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Assessment Quiz - Simple Linear Regression"
  },
  {
    "id": 13,
    "title": "📍 Class Activity - Estimating Means & Variablility",
    "url": "425PaigesNotes.html#classactivityestimatingmeansvariablility",
    "content": "Class Activity - Estimating Means & Variablility Open the airquality dataset in R Part 1: Estimating the Mean and Spread of Y Creating Histogram graph ggplot(airquality, aes(x=Temp))+ geom_histogram(binwidth=5, fill=\"lightblue\", color=\"skyblue\") + labs(title=\"Maximum daily temperature in defees Fahrenheit at La Gaurdia Airport, NY, USA\", x=\"Temperature in degrees F\", y=\"Number of Days in Temperature Range\") favstats(airquality$Temp) %>% pander() Spread is SO important! This tells us the guess of this thing that we are looking at, how good or bad we were in our guess! essentially, 2 standard deviations are a good guess (takes up a good percentage of the data) Part 2: Estimating the Mean and Spread of Y for Categorical X weathering <- ggplot(airquality, aes(x=factor(Month), y=Temp)) + geom_boxplot(fill=\"lightblue\", color=\"skyblue\") + labs(title=\"Maximum daily temp in degrees F at La Guardia Airport, NY, USA\") plot(weathering)",
    "sentences": ["Open the airquality dataset in R", "Part 1: Estimating the Mean and Spread of Y", "Creating Histogram graph", "ggplot(airquality, aes(x=Temp))+ geom_histogram(binwidth=5, fill=\"lightblue\", color=\"skyblue\") + labs(title=\"Maximum daily temperature in defees Fahrenheit at La Gaurdia Airport, NY, USA\", x=\"Temperature in degrees F\", y=\"Number of Days in Temperature Range\")", "favstats(airquality$Temp) %>% pander()", "Spread is SO important!", "This tells us the guess of this thing that we are looking at, how good or bad we were in our guess!", "essentially, 2 standard deviations are a good guess (takes up a good percentage of the data)", "Part 2: Estimating the Mean and Spread of Y for Categorical X", "weathering <- ggplot(airquality, aes(x=factor(Month), y=Temp)) + geom_boxplot(fill=\"lightblue\", color=\"skyblue\") + labs(title=\"Maximum daily temp in degrees F at La Guardia Airport, NY, USA\") plot(weathering)"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Class Activity - Estimating Means & Variablility"
  },
  {
    "id": 14,
    "title": "📍 Class Activity - The Regression Model",
    "url": "425PaigesNotes.html#classactivitytheregressionmodel",
    "content": "Class Activity - The Regression Model \\(Y_i\\) : \\(\\sigma\\) : control the tightness of the data (68%) \\(\\epsilon\\) : explanatory data value for individuality (gives us our variability, the dots departure from the line .aka the law) Part 2 : Simulating Data from a Regression Model residual standard error of this regression came out to be 8.442 the sample size consisted of n = 153 data points \\(\\hat{Y}_i\\) : the line that we found in the middle of the data (depends on the value of x) However, the “true regression equation” (or “true law”) governing the relationship between the Mean Max Temp and the Morning Wind Speed is still unknown to us. Suppose however that God revealed this true law to be governed by the equation: the data doesn’t have slope, the law that created the dots have slope! the slope is interpreted as the change in the average y-value for a one unit change in the x-value. we can’t tell what an individual will do, but we can tell what the law will do based on the individual",
    "sentences": ["\\(Y_i\\) : \\(\\sigma\\) : control the tightness of the data (68%) \\(\\epsilon\\) : explanatory data value for individuality (gives us our variability, the dots departure from the line .aka the law)", "Part 2 : Simulating Data from a Regression Model", "residual standard error of this regression came out to be 8.442 the sample size consisted of n = 153 data points \\(\\hat{Y}_i\\) : the line that we found in the middle of the data (depends on the value of x)", "However, the “true regression equation” (or “true law”) governing the relationship between the Mean Max Temp and the Morning Wind Speed is still unknown to us.", "Suppose however that God revealed this true law to be governed by the equation:", "the data doesn’t have slope, the law that created the dots have slope!", "the slope is interpreted as the change in the average y-value for a one unit change in the x-value.", "we can’t tell what an individual will do, but we can tell what the law will do based on the individual"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Class Activity - The Regression Model"
  },
  {
    "id": 15,
    "title": "📍 Week 2 | Simple Linear Regression",
    "url": "425PaigesNotes.html#week2simplelinearregression",
    "content": "Week 2 | Simple Linear Regression Skill Quiz - Residuals, Sums of Squares, and R-squared datatable(Orange) `orangelm <- lm(circumference ~ age, data=Orange) summary(orangelm)%>%pander()` \\[\\hat{Y_i} = 17.40 + 0.106770 X_i\\] ggplot(Orange, aes(x = age, y = circumference))+ geom_point(color = \"orange\") + geom_smooth(method = \"lm\", formula = y~x, se = FALSE, color = \"chocolate\")+ theme_minimal() `SSE <- round(sum(residuals(orangelm)^2), 2) SSR <- round(sum((fitted(orangelm)- mean(Orange$circumference))^2), 2) SSTO <- round(sum((Orange\\(circumference - mean(Orange\\)circumference))^2), 2) R2 <- round(SSR / SSTO, 2) correlation <- sqrt(R2) if(coef(orangelm)[2]<0) correlation <- -correlation correlation <- round(correlation, 2) cat(“SSE:”, SSE, “”) cat(“SSR:”, SSR, “”) cat(“SSTO:”, SSTO, “”) cat(“R-squared (R2):”, R2, “”) cat(“Correlation (r):”, correlation, “”)` predict(orangelm, data.frame(age=1095, interval=\"prediction\")) datatable(mtcars) carwtlm <- lm(mpg ~ wt, data=mtcars) carcyllm <- lm(mpg ~ cyl, data= mtcars) carhplm <- lm(mpg ~ hp, data=mtcars) summary(carwtlm)%>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 summary(carcyllm) %>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 37.88 2.074 18.27 8.369e-18 cyl -2.876 0.3224 -8.92 6.113e-10 Fitting linear model: mpg ~ cyl Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.206 0.7262 0.7171 summary(carhplm)%>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 30.1 1.634 18.42 6.643e-18 hp -0.06823 0.01012 -6.742 1.788e-07 Fitting linear model: mpg ~ hp Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.863 0.6024 0.5892 ggplot(mtcars, aes(x=wt, y = mpg)) + geom_point() + geom_smooth(method = \"lm\", formula = y~x, se = FALSE, color = \"red\")+ labs(title = \"Explanatory variable is wt\") ggplot(mtcars, aes(x=cyl, y = mpg)) + geom_point() + geom_smooth(method = \"lm\", formula = y~x, se = FALSE, color = \"blue\")+ labs(title = \"Explanatory variable is cyl\") ggplot(mtcars, aes(x=hp, y = mpg)) + geom_point() + geom_smooth(method = \"lm\", formula = y~x, se = FALSE, color = \"green\")+ labs(title = \"Explanatory variable is hp\") # Just change the lms SSE.wt <- round(sum(residuals(carwtlm)^2), 2) # y value (prediciton) goes in the mtcar$Y parts! SSR.wt <- round(sum((fitted(carwtlm)- mean(mtcars$mpg))^2), 2) SSTO.wt <- round(sum((mtcars$mpg - mean(mtcars$mpg))^2), 2) R2.wt <- round(SSR.wt / SSTO.wt, 2) correlation.wt <- sqrt(R2.wt) if(coef(carwtlm)[2]<0) correlation.wt <- -correlation.wt correlation.wt <- round(correlation.wt, 2) cat(\"SSE:\", SSE.wt, \"\\n\") ## SSE: 278.32 cat(\"SSR:\", SSR.wt, \"\\n\") ## SSR: 847.73 cat(\"SSTO:\", SSTO.wt, \"\\n\") ## SSTO: 1126.05 cat(\"R-squared (R2):\", R2.wt, \"\\n\") ## R-squared (R2): 0.75 cat(\"Correlation (r):\", correlation.wt, \"\\n\") ## Correlation (r): -0.87 SSE.cyl <- round(sum(residuals(carcyllm)^2), 2) # y value goes in the mtcar$Y parts! SSR.cyl <- round(sum((fitted(carcyllm)- mean(mtcars$mpg))^2), 2) SSTO.cyl <- round(sum((mtcars$mpg - mean(mtcars$mpg))^2), 2) R2.cyl <- round(SSR.cyl / SSTO.cyl, 2) correlation.cyl <- sqrt(R2.cyl) if(coef(carcyllm)[2]<0) correlation.cyl <- -correlation.cyl correlation.cyl <- round(correlation.cyl, 2) cat(\"SSE:\", SSE.cyl, \"\\n\") ## SSE: 308.33 cat(\"SSR:\", SSR.cyl, \"\\n\") ## SSR: 817.71 cat(\"SSTO:\", SSTO.cyl, \"\\n\") ## SSTO: 1126.05 cat(\"R-squared (R2):\", R2.cyl, \"\\n\") ## R-squared (R2): 0.73 cat(\"Correlation (r):\", correlation.cyl, \"\\n\") ## Correlation (r): -0.85 SSE.hp <- round(sum(residuals(carhplm)^2), 2) # y value goes in the mtcar$Y parts! SSR.hp <- round(sum((fitted(carhplm)- mean(mtcars$mpg))^2), 2) SSTO.hp <- round(sum((mtcars$mpg - mean(mtcars$mpg))^2), 2) R2.hp <- round(SSR.hp / SSTO.hp, 2) correlation.hp <- sqrt(R2.hp) if(coef(carhplm)[2]<0) correlation.hp <- -correlation.hp correlation.hp <- round(correlation.hp, 2) cat(\"SSE:\", SSE.hp, \"\\n\") ## SSE: 447.67 cat(\"SSR:\", SSR.hp, \"\\n\") ## SSR: 678.37 cat(\"SSTO:\", SSTO.hp, \"\\n\") ## SSTO: 1126.05 cat(\"R-squared (R2):\", R2.hp, \"\\n\") ## R-squared (R2): 0.6 cat(\"Correlation (r):\", correlation.hp, \"\\n\") ## Correlation (r): -0.77 All together: # Combine the statistics into a data frame stats_table <- data.frame( Metric = c(\"SSE\", \"SSR\", \"SSTO\", \"R_squared\", \"Correlation\"), wt = c(SSE.wt, SSR.wt, SSTO.wt, R2.wt, correlation.wt), cyl = c(SSE.cyl, SSR.cyl, SSTO.cyl, R2.cyl, correlation.cyl), hp = c(SSE.hp, SSR.hp, SSTO.hp, R2.hp, correlation.hp) ) # Print the table print(stats_table) ## Metric wt cyl hp ## 1 SSE 278.32 308.33 447.67 ## 2 SSR 847.73 817.71 678.37 ## 3 SSTO 1126.05 1126.05 1126.05 ## 4 R_squared 0.75 0.73 0.60 ## 5 Correlation -0.87 -0.85 -0.77 # Optional: Format the table with nice rounding for display kable(stats_table, caption = \"Summary Statistics for mtcars dataset Regression Models\") Summary Statistics for mtcars dataset Regression Models Metric wt cyl hp SSE 278.32 308.33 447.67 SSR 847.73 817.71 678.37 SSTO 1126.05 1126.05 1126.05 R_squared 0.75 0.73 0.60 Correlation -0.87 -0.85 -0.77 par(mfrow=c(1,3)) plot(carwtlm, which=1) qqPlot(carwtlm, id=FALSE, main= \"Q-Q plot\", col=\"darkred\", col.lines = \"red\", pch = 16) plot(carwtlm$residuals, main=\"Residuals vs Order\") par(mfrow=c(1,3)) plot(carcyllm, which=1) qqPlot(carcyllm, id=FALSE, main= \"Q-Q plot\", col=\"darkblue\", col.lines = \"lightblue\", pch = 16) plot(carcyllm$residuals, main=\"Residuals vs Order\") par(mfrow=c(1,3)) plot(carhplm, which=1) qqPlot(carhplm, id=FALSE, main= \"Q-Q plot\", col=\"darkgreen\", col.lines = \"green\", pch = 16) plot(carhplm$residuals, main=\"Residuals vs Order\")",
    "sentences": ["Skill Quiz - Residuals, Sums of Squares, and R-squared datatable(Orange) `orangelm <- lm(circumference ~ age, data=Orange) summary(orangelm)%>%pander()` \\[\\hat{Y_i} = 17.40 + 0.106770 X_i\\] ggplot(Orange, aes(x = age, y = circumference))+ geom_point(color = \"orange\") + geom_smooth(method = \"lm\", formula = y~x, se = FALSE, color = \"chocolate\")+ theme_minimal() `SSE <- round(sum(residuals(orangelm)^2), 2) SSR <- round(sum((fitted(orangelm)- mean(Orange$circumference))^2), 2) SSTO <- round(sum((Orange\\(circumference - mean(Orange\\)circumference))^2), 2) R2 <- round(SSR / SSTO, 2) correlation <- sqrt(R2) if(coef(orangelm)[2]<0) correlation <- -correlation correlation <- round(correlation, 2) cat(“SSE:”, SSE, “”) cat(“SSR:”, SSR, “”) cat(“SSTO:”, SSTO, “”) cat(“R-squared (R2):”, R2, “”) cat(“Correlation (r):”, correlation, “”)` predict(orangelm, data.frame(age=1095, interval=\"prediction\")) datatable(mtcars) carwtlm <- lm(mpg ~ wt, data=mtcars) carcyllm <- lm(mpg ~ cyl, data= mtcars) carhplm <- lm(mpg ~ hp, data=mtcars) summary(carwtlm)%>% pander()   Estimate Std.", "Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std.", "Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 summary(carcyllm) %>% pander()   Estimate Std.", "Error t value Pr(>|t|) (Intercept) 37.88 2.074 18.27 8.369e-18 cyl -2.876 0.3224 -8.92 6.113e-10 Fitting linear model: mpg ~ cyl Observations Residual Std.", "Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.206 0.7262 0.7171 summary(carhplm)%>% pander()   Estimate Std.", "Error t value Pr(>|t|) (Intercept) 30.1 1.634 18.42 6.643e-18 hp -0.06823 0.01012 -6.742 1.788e-07 Fitting linear model: mpg ~ hp Observations Residual Std.", "Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.863 0.6024 0.5892 ggplot(mtcars, aes(x=wt, y = mpg)) + geom_point() + geom_smooth(method = \"lm\", formula = y~x, se = FALSE, color = \"red\")+ labs(title = \"Explanatory variable is wt\") ggplot(mtcars, aes(x=cyl, y = mpg)) + geom_point() + geom_smooth(method = \"lm\", formula = y~x, se = FALSE, color = \"blue\")+ labs(title = \"Explanatory variable is cyl\") ggplot(mtcars, aes(x=hp, y = mpg)) + geom_point() + geom_smooth(method = \"lm\", formula = y~x, se = FALSE, color = \"green\")+ labs(title = \"Explanatory variable is hp\") # Just change the lms SSE.wt <- round(sum(residuals(carwtlm)^2), 2) # y value (prediciton) goes in the mtcar$Y parts!", "SSR.wt <- round(sum((fitted(carwtlm)- mean(mtcars$mpg))^2), 2) SSTO.wt <- round(sum((mtcars$mpg - mean(mtcars$mpg))^2), 2) R2.wt <- round(SSR.wt / SSTO.wt, 2) correlation.wt <- sqrt(R2.wt) if(coef(carwtlm)[2]<0) correlation.wt <- -correlation.wt correlation.wt <- round(correlation.wt, 2) cat(\"SSE:\", SSE.wt, \"\\n\") ## SSE: 278.32 cat(\"SSR:\", SSR.wt, \"\\n\") ## SSR: 847.73 cat(\"SSTO:\", SSTO.wt, \"\\n\") ## SSTO: 1126.05 cat(\"R-squared (R2):\", R2.wt, \"\\n\") ## R-squared (R2): 0.75 cat(\"Correlation (r):\", correlation.wt, \"\\n\") ## Correlation (r): -0.87 SSE.cyl <- round(sum(residuals(carcyllm)^2), 2) # y value goes in the mtcar$Y parts!", "SSR.cyl <- round(sum((fitted(carcyllm)- mean(mtcars$mpg))^2), 2) SSTO.cyl <- round(sum((mtcars$mpg - mean(mtcars$mpg))^2), 2) R2.cyl <- round(SSR.cyl / SSTO.cyl, 2) correlation.cyl <- sqrt(R2.cyl) if(coef(carcyllm)[2]<0) correlation.cyl <- -correlation.cyl correlation.cyl <- round(correlation.cyl, 2) cat(\"SSE:\", SSE.cyl, \"\\n\") ## SSE: 308.33 cat(\"SSR:\", SSR.cyl, \"\\n\") ## SSR: 817.71 cat(\"SSTO:\", SSTO.cyl, \"\\n\") ## SSTO: 1126.05 cat(\"R-squared (R2):\", R2.cyl, \"\\n\") ## R-squared (R2): 0.73 cat(\"Correlation (r):\", correlation.cyl, \"\\n\") ## Correlation (r): -0.85 SSE.hp <- round(sum(residuals(carhplm)^2), 2) # y value goes in the mtcar$Y parts!", "SSR.hp <- round(sum((fitted(carhplm)- mean(mtcars$mpg))^2), 2) SSTO.hp <- round(sum((mtcars$mpg - mean(mtcars$mpg))^2), 2) R2.hp <- round(SSR.hp / SSTO.hp, 2) correlation.hp <- sqrt(R2.hp) if(coef(carhplm)[2]<0) correlation.hp <- -correlation.hp correlation.hp <- round(correlation.hp, 2) cat(\"SSE:\", SSE.hp, \"\\n\") ## SSE: 447.67 cat(\"SSR:\", SSR.hp, \"\\n\") ## SSR: 678.37 cat(\"SSTO:\", SSTO.hp, \"\\n\") ## SSTO: 1126.05 cat(\"R-squared (R2):\", R2.hp, \"\\n\") ## R-squared (R2): 0.6 cat(\"Correlation (r):\", correlation.hp, \"\\n\") ## Correlation (r): -0.77 All together: # Combine the statistics into a data frame stats_table <- data.frame( Metric = c(\"SSE\", \"SSR\", \"SSTO\", \"R_squared\", \"Correlation\"), wt = c(SSE.wt, SSR.wt, SSTO.wt, R2.wt, correlation.wt), cyl = c(SSE.cyl, SSR.cyl, SSTO.cyl, R2.cyl, correlation.cyl), hp = c(SSE.hp, SSR.hp, SSTO.hp, R2.hp, correlation.hp) ) # Print the table print(stats_table) ## Metric wt cyl hp ## 1 SSE 278.32 308.33 447.67 ## 2 SSR 847.73 817.71 678.37 ## 3 SSTO 1126.05 1126.05 1126.05 ## 4 R_squared 0.75 0.73 0.60 ## 5 Correlation -0.87 -0.85 -0.77 # Optional: Format the table with nice rounding for display kable(stats_table, caption = \"Summary Statistics for mtcars dataset Regression Models\") Summary Statistics for mtcars dataset Regression Models Metric wt cyl hp SSE 278.32 308.33 447.67 SSR 847.73 817.71 678.37 SSTO 1126.05 1126.05 1126.05 R_squared 0.75 0.73 0.60 Correlation -0.87 -0.85 -0.77 par(mfrow=c(1,3)) plot(carwtlm, which=1) qqPlot(carwtlm, id=FALSE, main= \"Q-Q plot\", col=\"darkred\", col.lines = \"red\", pch = 16) plot(carwtlm$residuals, main=\"Residuals vs Order\") par(mfrow=c(1,3)) plot(carcyllm, which=1) qqPlot(carcyllm, id=FALSE, main= \"Q-Q plot\", col=\"darkblue\", col.lines = \"lightblue\", pch = 16) plot(carcyllm$residuals, main=\"Residuals vs Order\") par(mfrow=c(1,3)) plot(carhplm, which=1) qqPlot(carhplm, id=FALSE, main= \"Q-Q plot\", col=\"darkgreen\", col.lines = \"green\", pch = 16) plot(carhplm$residuals, main=\"Residuals vs Order\")"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Week 2 | Simple Linear Regression"
  },
  {
    "id": 16,
    "title": "📍 Skill Quiz - Residuals, Sums of Squares, and R-squared",
    "url": "425PaigesNotes.html#skillquizresidualssumsofsquaresandrsquared",
    "content": "Skill Quiz - Residuals, Sums of Squares, and R-squared datatable(Orange) `orangelm <- lm(circumference ~ age, data=Orange) summary(orangelm)%>%pander()` \\[\\hat{Y_i} = 17.40 + 0.106770 X_i\\] ggplot(Orange, aes(x = age, y = circumference))+ geom_point(color = \"orange\") + geom_smooth(method = \"lm\", formula = y~x, se = FALSE, color = \"chocolate\")+ theme_minimal() `SSE <- round(sum(residuals(orangelm)^2), 2) SSR <- round(sum((fitted(orangelm)- mean(Orange$circumference))^2), 2) SSTO <- round(sum((Orange\\(circumference - mean(Orange\\)circumference))^2), 2) R2 <- round(SSR / SSTO, 2) correlation <- sqrt(R2) if(coef(orangelm)[2]<0) correlation <- -correlation",
    "sentences": ["datatable(Orange)", "`orangelm <- lm(circumference ~ age, data=Orange)", "summary(orangelm)%>%pander()`", "\\[\\hat{Y_i} = 17.40 + 0.106770 X_i\\]", "ggplot(Orange, aes(x = age, y = circumference))+ geom_point(color = \"orange\") + geom_smooth(method = \"lm\", formula = y~x, se = FALSE, color = \"chocolate\")+ theme_minimal()", "`SSE <- round(sum(residuals(orangelm)^2), 2)", "SSR <- round(sum((fitted(orangelm)- mean(Orange$circumference))^2), 2)", "SSTO <- round(sum((Orange\\(circumference - mean(Orange\\)circumference))^2), 2)", "R2 <- round(SSR / SSTO, 2)", "correlation <- sqrt(R2) if(coef(orangelm)[2]<0) correlation <- -correlation"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Skill Quiz - Residuals, Sums of Squares, and R-squared"
  },
  {
    "id": 17,
    "title": "📍 Assesment Quiz - Residuals, Sums of Squares, and R-squared",
    "url": "425PaigesNotes.html#assesmentquizresidualssumsofsquaresandrsquared",
    "content": "Assesment Quiz - Residuals, Sums of Squares, and R-squared A regression was performed for a sample of n = 5 data points. The y-values of the regression are: 3.78, 6.08, 6.65, 9.25, and 9.92. The residuals from the regression are: -0.266, 0.489, -0.486, 0.569, and -0.306. What is the R-squared value for this regression? y <- c(3.78, 6.08, 6.65, 9.25, 9.92) SSTO <- sum( (y - mean(y))^2 ) #SSTO = 24.83372 res <- c(-0.266, 0.489, -0.486, 0.569, -0.306) SSE <- sum(res^2) #SSE = 0.96347 rr <- 1 - SSE/SSTO print(rr) Open the mtcars data set in R. This data can be used to show that the displacement of the engine (disp) is positively correlated with the weight of the vehicle (wt). What is the R-squared value of this regression? mt.lm <- lm(disp ~ wt, data=mtcars) summary(mt.lm)%>%pander() Multiple R-squared: 0.7885",
    "sentences": ["A regression was performed for a sample of n = 5 data points.", "The y-values of the regression are: 3.78, 6.08, 6.65, 9.25, and 9.92.", "The residuals from the regression are: -0.266, 0.489, -0.486, 0.569, and -0.306.", "What is the R-squared value for this regression?", "y <- c(3.78, 6.08, 6.65, 9.25, 9.92) SSTO <- sum( (y - mean(y))^2 ) #SSTO = 24.83372 res <- c(-0.266, 0.489, -0.486, 0.569, -0.306) SSE <- sum(res^2) #SSE = 0.96347 rr <- 1 - SSE/SSTO print(rr)", "Open the mtcars data set in R.", "This data can be used to show that the displacement of the engine (disp) is positively correlated with the weight of the vehicle (wt).", "What is the R-squared value of this regression?", "mt.lm <- lm(disp ~ wt, data=mtcars) summary(mt.lm)%>%pander()", "Multiple R-squared: 0.7885"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Assesment Quiz - Residuals, Sums of Squares, and R-squared"
  },
  {
    "id": 18,
    "title": "📍 Class Activity - Residuals & Sum of Squares",
    "url": "425PaigesNotes.html#classactivityresidualssumofsquares",
    "content": "Class Activity - Residuals & Sum of Squares Part 1: Visualizing Residuals: What is a residual? A residual is the distance between the predicted value and the actual value In which direction (vertical, horizontal, diagonal) is a residual? The direction of a residual is vertical from the line. What happens to the magnitude of the residuals as you move the line to the “center” of the data? Or away from the data? The farther the line from the residuals, the bigger the distance. Part 2: Visualizing Sums of Squared Residuals What do the shaded “squares” that appeared next to each residual bar in the scatterplot represent?",
    "sentences": ["Part 1: Visualizing Residuals:", "What is a residual?", "A residual is the distance between the predicted value and the actual value", "In which direction (vertical, horizontal, diagonal) is a residual?", "The direction of a residual is vertical from the line.", "What happens to the magnitude of the residuals as you move the line to the “center” of the data?", "Or away from the data?", "The farther the line from the residuals, the bigger the distance.", "Part 2: Visualizing Sums of Squared Residuals", "What do the shaded “squares” that appeared next to each residual bar in the scatterplot represent?"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Class Activity - Residuals & Sum of Squares"
  },
  {
    "id": 19,
    "title": "📍 Class Activity - Sums of Squares and R-squared",
    "url": "425PaigesNotes.html#classactivitysumsofsquaresandrsquared",
    "content": "Class Activity - Sums of Squares and R-squared Part 1 - Sums of Squares First, what is a sum? Part 2 - Sums of Squares in Regression reduces variability dashed lines",
    "sentences": ["Part 1 - Sums of Squares", "First, what is a sum?", "Part 2 - Sums of Squares in Regression", "reduces variability dashed lines"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Class Activity - Sums of Squares and R-squared"
  },
  {
    "id": 20,
    "title": "📍 Class Activity - Residuals, Sums of Squares, and R-Squared in\r\nReview",
    "url": "425PaigesNotes.html#classactivityresidualssumsofsquaresandrsquaredinre",
    "content": "Class Activity - Residuals, Sums of Squares, and R-Squared in\r\nReview First, residuals are the key to obtaining the “least squares estimates” of the regression parameters \\(\\beta_0\\)and \\(\\beta_1\\) Second, residuals are an important part in measuring the \\(R^2\\) value of a regression, which is the proportion of variation in Y explained by the regression model. Third, residuals give insight about how much an individual of a given x-value differs from average in their y-value. Fourth, (and this one you haven’t seen yet) residuals can be used to estimate the variance parameter of a regression, i.e., in the equation \\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i \\text{where} \\sim N(0,\\sigma^2)\\] Follow along with your instructor on how this is done. Then, explain to a peer what MSE is, and how it is similar to the Variance formula in your Numerical Summaries page of your Math 325 Notebook. Fifth, (and we will see this one in more detail next week) residuals can be used to determine if a linear regression model is appropriate for a given data set.",
    "sentences": ["First, residuals are the key to obtaining the “least squares estimates” of the regression parameters \\(\\beta_0\\)and \\(\\beta_1\\) Second, residuals are an important part in measuring the \\(R^2\\) value of a regression, which is the proportion of variation in Y explained by the regression model.", "Third, residuals give insight about how much an individual of a given x-value differs from average in their y-value.", "Fourth, (and this one you haven’t seen yet) residuals can be used to estimate the variance parameter of a regression, i.e., in the equation", "\\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i \\text{where} \\sim N(0,\\sigma^2)\\]", "Follow along with your instructor on how this is done.", "Then, explain to a peer what MSE is, and how it is similar to the Variance formula in your Numerical Summaries page of your Math 325 Notebook.", "Fifth, (and we will see this one in more detail next week) residuals can be used to determine if a linear regression model is appropriate for a given data set."],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Class Activity - Residuals, Sums of Squares, and R-Squared in\r\nReview"
  },
  {
    "id": 21,
    "title": "📍 Class Activity - Diagnosing the Model",
    "url": "425PaigesNotes.html#classactivitydiagnosingthemodel",
    "content": "Class Activity - Diagnosing the Model lack linearity? -> EVERYTHING (because all our assumptions are made in the linear world) violated constant variance? -> DONT TRUST SIGMA (RSE) no normalitiy? -> ehhhhhhhhhhhhhhh putting bent things into a lens that brings us into the linear view (re-scaling our data) puts the data into a new world/space to see the data better likelihood changes the value of \\(\\mu\\) and how likely \\(\\mu\\) would be",
    "sentences": ["lack linearity?", "-> EVERYTHING (because all our assumptions are made in the linear world) violated constant variance?", "-> DONT TRUST SIGMA (RSE) no normalitiy?", "-> ehhhhhhhhhhhhhhh", "putting bent things into a lens that brings us into the linear view (re-scaling our data) puts the data into a new world/space to see the data better likelihood changes the value of \\(\\mu\\) and how likely \\(\\mu\\) would be"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Class Activity - Diagnosing the Model"
  },
  {
    "id": 22,
    "title": "📍 Week 3 | Diagnosing the Model & Model Tranformations",
    "url": "425PaigesNotes.html#week3diagnosingthemodelmodeltranformations",
    "content": "Week 3 | Diagnosing the Model & Model Tranformations Skill Quiz - Regression Diagnositcs & Transformation Problem 1: Open the Davis dataset in R, found in library(car). As stated in the help file for this data set, “The subjects were men and women engaged in regular exercise.” Perform a simple linear regression of the height of the individual based on their weight. PART (A) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{height} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{weight} + \\epsilon_i \\quad \\text{where} \\epsilon_i \\sim N(0, \\sigma^2) \\] PART (B) Plot a scatterplot of the data with your regression line overlaid. davelm <- lm(height ~ weight, data= Davis) plot(height ~ weight, data= Davis) abline(davelm) PART (C) Create a residuals vs fitted-values plot for this regression. What does this plot show? par(mfrow=c(1,3)) plot(davelm, which=1) What does the plot SHOW? - The point in the bottom right corner of the plot labeled “12” (observation 12) appears to be a dramatic outlier causing the regression line to be pulled towards it. PART (D) State and interpret the slope, y-intercept, and \\(R^2\\) of this regression. Are they meaningful for this data under the current regression? summary(davelm)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 160.1 3.747 42.73 6.862e-102 weight 0.1509 0.05551 2.718 0.007152 Fitting linear model: height ~ weight Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 200 11.82 0.03597 0.0311 Part Meaning Slope = 0.15 The slope is the increase int eh average height per one pound change in weight. Y- intercept = 160.09 The y-intercept is the average height of exercising individuals who have a weight of zero (To be put nicely, a bunch of baloga) \\(R^2\\) = 0.04 The proportion of variation in height explained by this regression (with the outlier included) is essentially zero Are these values meaningful for this data under the current regression? No, these values are not currently very meaningful because they are being strongly skewed by the presence of the outlier (observation number 12). PART (E) Run View(Davis) in your Console. What do you notice about observation #12 in this data set? It looks like they accidently reverse the weight and height. Perform a second regression for this data with observation #12 removed. Recreate the scatterplot of Part (b) with two regression lines showing this time. The first regression line should include the outlier. The second should exclude the outlier. Include a legend to show which line is which. daniel <- filter(Davis, weight != \"166\") dangdaniel <- lm(height ~ weight, data = daniel) ggplot(Davis, aes(x = weight, y = height)) + geom_point(size = 1.5, shape = 19, color = \"darkgrey\", alpha = 1) + geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, aes(color = \"Fitted Regression (with outlier)\"), size = 1.5) + geom_smooth(data = daniel, aes(x = weight, y = height, color = \"Fitted Regression (outlier removed)\"), method = \"lm\", formula = y ~ x, se = FALSE, size = 1.5) + scale_color_manual(values = c(\"Fitted Regression (with outlier)\" = \"gray\", \"Fitted Regression (outlier removed)\" = \"skyblue\")) + labs(color= \"Regression Lines\") + theme_classic() + theme(legend.position = c(0.11,0.13)) + labs(title = \"Exercising Individuals (Davis data set)\", x= \"Measured Weight of Individual in kg (weight)\", y= \"Measured Height of Individual in cm (height)\") PART (F) Compute the slope, y-intercept, and \\(R^2\\) value for the regression with the outlier removed. compare the results to the values when the outlier was present. summary(dangdaniel)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 136.8 2.029 67.45 4.466e-138 weight 0.5169 0.03044 16.98 2.009e-40 Fitting linear model: height ~ weight Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 199 5.716 0.594 0.592 WITHOUT outlier: Slope = 0.52 Y-intercept = 136.84 \\(R^2\\) = 0.59 WITH outlier: Slope = 0.15 Y-intercept = 160.09 \\(R^2\\) = 0.04 PART (G) Create a residuals vs fitted-values plot for the regression with the outlier removed. How do things look now? This plot shows possible slight problems with linearity (a slight, but consistent bend in the red line) but there do not appear to be any problems with the variance or with outliers. par(mfrow=c(1,3)) plot(dangdaniel, which=1) Problem 2: Open the Prestige data set found in library(car). Perform a regression that explains the 1971 average annual income from jobs according to their “Pineo-Porter prestige score for occupation, from a social survey conducted in the mod-1960’s.” PART (A) Plot the data and fitted simple linear regression line. presto <- lm(income ~ prestige, data= Prestige) ggplot(Prestige, aes(x = prestige, y=income)) + geom_point(size = 1.5, shape = 19, color = \"green\", alpha = 1) + geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, color = \"grey\")+ labs(title= \"Greater Prestige linked to Greater Income (Prestige data set)\", x= \"Prestige Ranking of Occupation(prestige)\", y=\"Average Annual Income USD (income)\")+ theme_classic() RSE is sigma! (apparently) PART (B) State the estimated values for \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\) for this regression. summary(presto)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) -1465 860.5 -1.703 0.09175 prestige 176.4 17.26 10.22 3.192e-17 Fitting linear model: income ~ prestige Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 102 2984 0.5111 0.5062 \\(\\beta_0\\) = -1465.03 \\(\\beta_1\\) = 176.43 \\(\\sigma\\) = 2984 PART (C) Create a residuals vs fitted-values plot and a Q-Q Plot of the residuals for this regression. par(mfrow=c(1,3)) plot(presto, which=1:2) PART (D) Comment on any difficulties the diagnostic plots in Part (c) reveal about the regression. Normality of the error terms is violated, as shown by the points going “out of bounds” in the Q-Q Plot. However, these problems are likely due to the increasing variance of the residuals shown in the residuals vs fitted-values plot. There even may be some problems with linearity of the data because of the three outliers in the top right of the graph pulling the regression line up (shown by the red line going down). Comment on which estimates of Part (b) are likely effected by these difficulties. The slope and estimate of the error variance are almost certainly being negatively effected by the increasing variance and three outliers Problem 3: Open the Burt data set from library(car). This data set is famous for being fraudulent, or fake. See ?Burt for more details. One of the first indicators that it was fraudulent was revealed by regressing IQbio ~ IQfoster. This regression was just a little too good to be real. (Note that for social science data, like this data, \\(R^2\\) values above 0.3 are impressive. Values above 0.7 are rare.) PART (A) Plot the data and fitted regression line. State the estimated values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\) as well as the \\(R^2\\) of the regression. ernie <- lm(IQbio ~ IQfoster, data= Burt) ggplot(Burt, aes(x= IQfoster, y = IQbio))+ geom_point(size = 1.5, shape = 19, color = \"wheat\", alpha=1.5) + geom_smooth(method = \"lm\",formula = y~x, se=FALSE, color = \"burlywood\")+ labs(title= \"Linked IQ? \\n (Burt data set)\", x=\"Twin IQ, Raised by Foster Parents (IQfoster)\", y= \"Twin IQ, Raised by Biological Parents (IQbio)\")+ theme_classic() PART (B) Create a (1) residuals vs. fitted-values plot, (2) Q-Q Plot of the residuals, and (3) residuals vs. order plot for this regression. Are any problems with regression assumption violations visible in these plots? par(mfrow=c(1,3)) plot(ernie, which=1:2) plot(ernie$residuals) PART (C) Comment on what the three diagnostic plots of Part (b) show for the regression. They show a fairly nice regression. There may be some slight difficulties with linearity due to the curved red line in the residuals vs fitted-values plot, and points 23 and 24 are somewhat far away from the rest of the data, but otherwise, things are impressively nice. Problem 4: Open the mtcars data set in R. Perform a regression of mpg explained by the displacement of the vehicle’s engine. PART (A) Plot the data and fitted regression line. State the estimated values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\) as well as the \\(R^2\\) of the regression. haveyouseenthecarsmovie <- lm(mpg ~ disp, data= mtcars) ggplot(mtcars, aes(x= disp, y= mpg))+ geom_point(size = 1.5, shape = 19, color = \"limegreen\", alpha = 2) + geom_smooth(method = \"lm\",formula = y~x, se = FALSE, color = \"limegreen\") + labs(title = \"Reduced Fuel Efficiency \\n (mtcars data set)\", x= \"Engine Displacement cu. (disp)\", y= \"Gas Mileage (mpg)\") + theme_classic() summary(haveyouseenthecarsmovie)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 29.6 1.23 24.07 3.577e-21 disp -0.04122 0.004712 -8.747 9.38e-10 Fitting linear model: mpg ~ disp Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.251 0.7183 0.709 \\(b_0\\) = 29.60 \\(b_1\\) = -0.04 Residual Standard Error (RSE) = 3.25 \\(R^2\\) = 0.72 PART (B) Create a (1) residuals vs. fitted-values plot, (2) Q-Q Plot of the residuals, and (3) residuals vs. order plot for this regression. Are any problems with regression assumption violations visible in these plots? par(mfrow=c(1,3)) plot(haveyouseenthecarsmovie, which=1) qqPlot(haveyouseenthecarsmovie$residuals, main=\"Q-Q Plot\", col.lines=\"blue\",pch= 19, id=FALSE) plot(haveyouseenthecarsmovie$residuals, ylab= \"Residuals\", main=\"Residuals vs Order\") PART (C) Comment on what the three diagnostic plots of Part (b) show for the validity of the values computed in Part (a). Residual vs. Fitted plot: violates the linearity assumption meaning our data lack linearity, everything is no longer meaningful Problem 5: Open the Orange data set found in R. Perform a regression that explains the circumference of the trunk of the orange tree as the tree ages. PART (A) Plot the data and fitted simple linear regression line. `ilikeapples <- lm(circumference ~ age, data=Orange) ggplot(Orange, aes(x= age, y= circumference))+ geom_point(size = 1.5, shape = 19, color = “orange”, alpha = 1.5) + geom_smooth(method = “lm”, formula = y~x, se= FALSE, color = “grey”) + labs(title = “Growth of Orange Trees”, x = “Age of Tree in Days”, y = “Circumference of Tree(mm)”)+ theme_classic()` PART (B) State the estimated values for \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\) for this regression. summary(ilikeapples)%>%pander() PART (C) Create a residuals vs fitted-values plot and a Q-Q Plot of the residuals for this regression. `par(mfrow=c(1,3)) plot(ilikeapples, which=1) qqPlot(ilikeapples$residuals, main=“Q-Q Plot”, col.lines=“blue”,pch= 19, id=FALSE) plot(ilikeapples$residuals, ylab= “Residuals”, main=“Residuals vs Order”)` PART (D) Comment on any difficulties the diagnostic plots in Part (c) reveal about the regression. Comment on which estimates of Part (b) are likely effected by these difficulties. Difficulties: Residuals vs Fitted: megaphoning!!!! RSE should not be considered meaningful as it will be too large on one end of the regression and too small on the other end PART (E) Perform a Box-Cox analysis of the regression. Which Y-transformation is suggested? boxCox(ilikeapples) The suggested y-transformation is 0.5 you will sqrt()! PART (F) Perform a regression with the transformed y-variable. Plot the regression in the transformed units. Diagnose the fit of the regression on the transformed data. `notmyapples <- lm(sqrt(circumference) ~ age, data=Orange) b.sqrt <- notmyapples$coef ggplot(Orange, aes(x= age, y= sqrt(circumference)))+ geom_point(size = 1.5, shape = 19, color = “orange”, alpha = 1.5) + geom_smooth(method = “lm”, formula = y~x, se= FALSE, color = “grey”) + labs(title = “Growth of Orange Trees”, x = “Age of Tree in Days”, y = “Circumference of Tree(mm)”)+ theme_classic() par(mfrow=c(1,3)) plot(notmyapples, which=1) qqPlot(notmyapples$residuals, main=“Q-Q Plot”, col.lines=“blue”,pch= 19, id=FALSE) plot(notmyapples$residuals, ylab= “Residuals”, main=“Residuals vs Order”)` Which of the following have actually become more problematic when comparing the diagnostic plots of the original regression to the diagnostic plots of the transformed regression? Linearity is more violated in the transformation regression than it was in the original units Normality is more violated in the transformed regression than it was in the original units PART (G) Write out the fitted model for \\(\\hat{Y}_i'\\). Then solve the transformed model back into the original units for \\(\\hat{Y}_i\\). Then compute the following. When \\(X_i = 500\\), then \\(\\hat{Y}_i = \\ldots\\). \\[ \\hat{Y}_i' = 5.3408 + 0.0055 X_i \\] \\[ \\sqrt{\\hat{Y}_i'} = 5.3408 + 0.0055 X_i \\] \\[ \\hat{Y}_i = (5.3408 + 0.0055 X_i)^2 \\] $ = $ (5.3408 + 0.0055 * 500)^2 ## [1] 65.46104 PART (H) Plot the data in the original units. Place the transformed line, back in the original units, on this plot. `plot(circumference ~ age, data=Orange, pch=16, col=“orangered”, main=“Growth of Orange Trees”, xlab=“Age of Tree in Days”, ylab=“Circumference of Tree (mm)”) abline(ilikeapples, col= “gray”) curve( (b.sqrt[1] + b.sqrt[2]*x)^2 , add=TRUE, col=“orangered”)`",
    "sentences": ["Skill Quiz - Regression Diagnositcs & Transformation Problem 1: Open the Davis dataset in R, found in library(car).", "As stated in the help file for this data set, “The subjects were men and women engaged in regular exercise.” Perform a simple linear regression of the height of the individual based on their weight.", "PART (A) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation.", "\\[ \\underbrace{Y_i}_\\text{height} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{weight} + \\epsilon_i \\quad \\text{where} \\epsilon_i \\sim N(0, \\sigma^2) \\] PART (B) Plot a scatterplot of the data with your regression line overlaid.", "davelm <- lm(height ~ weight, data= Davis) plot(height ~ weight, data= Davis) abline(davelm) PART (C) Create a residuals vs fitted-values plot for this regression.", "What does this plot show?", "par(mfrow=c(1,3)) plot(davelm, which=1) What does the plot SHOW?", "- The point in the bottom right corner of the plot labeled “12” (observation 12) appears to be a dramatic outlier causing the regression line to be pulled towards it.", "PART (D) State and interpret the slope, y-intercept, and \\(R^2\\) of this regression.", "Are they meaningful for this data under the current regression?"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Week 3 | Diagnosing the Model & Model Tranformations"
  },
  {
    "id": 23,
    "title": "📍 Skill Quiz - Regression Diagnositcs & Transformation",
    "url": "425PaigesNotes.html#skillquizregressiondiagnositcstransformation",
    "content": "Skill Quiz - Regression Diagnositcs & Transformation Open the Davis dataset in R, found in library(car). As stated in the help file for this data set, “The subjects were men and women engaged in regular exercise.” Perform a simple linear regression of the height of the individual based on their weight. Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{height} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{weight} + \\epsilon_i \\quad \\text{where} \\epsilon_i \\sim N(0, \\sigma^2) \\] Plot a scatterplot of the data with your regression line overlaid. davelm <- lm(height ~ weight, data= Davis) plot(height ~ weight, data= Davis) abline(davelm) Create a residuals vs fitted-values plot for this regression. What does this plot show? par(mfrow=c(1,3)) plot(davelm, which=1)",
    "sentences": ["Open the Davis dataset in R, found in library(car).", "As stated in the help file for this data set, “The subjects were men and women engaged in regular exercise.”", "Perform a simple linear regression of the height of the individual based on their weight.", "Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation.", "\\[ \\underbrace{Y_i}_\\text{height} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{weight} + \\epsilon_i \\quad \\text{where} \\epsilon_i \\sim N(0, \\sigma^2) \\]", "Plot a scatterplot of the data with your regression line overlaid.", "davelm <- lm(height ~ weight, data= Davis) plot(height ~ weight, data= Davis) abline(davelm)", "Create a residuals vs fitted-values plot for this regression.", "What does this plot show?", "par(mfrow=c(1,3)) plot(davelm, which=1)"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Skill Quiz - Regression Diagnositcs & Transformation"
  },
  {
    "id": 24,
    "title": "📍 Assesment Quiz - Diagnosing the Model and Model Transformations",
    "url": "425PaigesNotes.html#assesmentquizdiagnosingthemodelandmodeltransformat",
    "content": "Assesment Quiz - Diagnosing the Model and Model Transformations A regression is performed and the following plot created. Which of the following correctly shows the original scatterplot of this regression? A regression is performed using \\(\\hat{Y_i'} = Y_i^{-2}\\) and the summary output below is obtained. Select the appropriate equation for \\(\\hat{Y_i}\\) Answer: \\(\\hat{Y_i} = \\frac{1}{\\sqrt{830.13 - 10.93 X_i}}\\) Which of these regression functions depicted on the scatterplot is the one suggested by the Box-Cox transformation? kachow <- lm(dist ~ speed, data = cars) boxCox(kachow) lm.scoop <- lm(sqrt(dist) ~ speed, data = cars) b.scoop <- coef(lm.scoop) plot(dist ~ speed, data = cars) curve( (b.scoop[1] + b.scoop[2]*x)^2, add=TRUE, col = \"yellow\") abline(kachow) ggplot(Orange, aes(x=age, y=circumference)) + geom_point(color = \"orangered\") + stat_function(fun=function(x)(b.scoop[1] + b.scoop[2]*x)^2, color= \"yellow\") + theme_classic()",
    "sentences": ["A regression is performed and the following plot created.", "Which of the following correctly shows the original scatterplot of this regression?", "A regression is performed using \\(\\hat{Y_i'} = Y_i^{-2}\\) and the summary output below is obtained.", "Select the appropriate equation for \\(\\hat{Y_i}\\)", "Answer: \\(\\hat{Y_i} = \\frac{1}{\\sqrt{830.13 - 10.93 X_i}}\\)", "Which of these regression functions depicted on the scatterplot is the one suggested by the Box-Cox transformation?", "kachow <- lm(dist ~ speed, data = cars) boxCox(kachow)", "lm.scoop <- lm(sqrt(dist) ~ speed, data = cars) b.scoop <- coef(lm.scoop) plot(dist ~ speed, data = cars) curve( (b.scoop[1] + b.scoop[2]*x)^2, add=TRUE, col = \"yellow\") abline(kachow)", "ggplot(Orange, aes(x=age, y=circumference)) + geom_point(color = \"orangered\") + stat_function(fun=function(x)(b.scoop[1] + b.scoop[2]*x)^2, color= \"yellow\") + theme_classic()"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Assesment Quiz - Diagnosing the Model and Model Transformations"
  },
  {
    "id": 25,
    "title": "📍 Class Activity - Diagnosing the Model",
    "url": "425PaigesNotes.html#classactivitydiagnosingthemodel",
    "content": "Class Activity - Diagnosing the Model Note: located in your Linear Regression Tab of the Notebook, in the “Residual Plots & Regression Assumptions” Problems from Failed Assumptions: If these things are violated, what is ruined? Lack linearity? -> EVERYTHING (because all our assumptions are made in the linear world) Identifier: Residuals versus Fitted-values Plot are messed up, showing curved trend Unconstant Variance? -> DONT TRUST SIGMA (RSE) Identifier: Residuals versus Fitted values Plot shows a megaphone No Normalitiy? -> ehhhhhhhhhhhhhhh honestly, not a big problem Independence Assumption Violated? -> RSE is HUGE HUGE Identifier: Residuals vs Order has a trend, linear lookin’ putting bent things into a lens that brings us into the linear view (re-scaling our data) puts the data into a new world/space to see the data better likelihood changes the value of and how likely \\(\\mu\\) would be Three Regression Assumption Plots Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3",
    "sentences": ["Note: located in your Linear Regression Tab of the Notebook, in the “Residual Plots & Regression Assumptions”", "Problems from Failed Assumptions: If these things are violated, what is ruined?", "Lack linearity?", "-> EVERYTHING (because all our assumptions are made in the linear world) Identifier: Residuals versus Fitted-values Plot are messed up, showing curved trend Unconstant Variance?", "-> DONT TRUST SIGMA (RSE) Identifier: Residuals versus Fitted values Plot shows a megaphone No Normalitiy?", "-> ehhhhhhhhhhhhhhh honestly, not a big problem Independence Assumption Violated?", "-> RSE is HUGE HUGE Identifier: Residuals vs Order has a trend, linear lookin’", "putting bent things into a lens that brings us into the linear view (re-scaling our data) puts the data into a new world/space to see the data better likelihood changes the value of and how likely \\(\\mu\\) would be", "Three Regression Assumption Plots", "Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Class Activity - Diagnosing the Model"
  },
  {
    "id": 26,
    "title": "📍 Class Activity - Introduction to Transformations",
    "url": "425PaigesNotes.html#classactivityintroductiontotransformations",
    "content": "Class Activity - Introduction to Transformations Note: Find more in the Linear Regression Tab and find the “Transformation” section for more info boxCox(lm_name) is your bestest friend to give advice on what to do!",
    "sentences": ["Note: Find more in the Linear Regression Tab and find the “Transformation” section for more info", "boxCox(lm_name) is your bestest friend to give advice on what to do!"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Class Activity - Introduction to Transformations"
  },
  {
    "id": 27,
    "title": "📍 Class Activity - Graphing Transformations",
    "url": "425PaigesNotes.html#classactivitygraphingtransformations",
    "content": "Class Activity - Graphing Transformations Part 1 - Graphing Transformations Run the two of the following codes in R. `lm.log <- lm(log(circumference) ~ age, data=Orange) b.log <- coef(lm.log) lm.sqrt <-lm(sqrt(circumference) ~ age, data=Orange) b.sqrt <- coef(lm.sqrt) lm.1oy <- lm(1/circumference ~ age, data=Orange) b.1oy <- coef(lm.1oy) lm.y <- lm(circumference ~ age, data=Orange) b.y <- coef(lm.y) lm.2 <- lm(circumference^2 ~ age, data=Orange) b.2 <- coef(lm.2) lm.ss <- lm(sqrt(sqrt(circumference)) ~ age, data=Orange) b.ss <- coef(lm.ss) plot(circumference ~ age, data=Orange, pch=16, col=“orangered”, main=“Growth of Orange Trees”, xlab=“Age of Tree in Days”, ylab=“Circumference of Tree (mm)”) legend(“topleft”, legend=c(“1”,“2”,“3”,“4”,“5”,“6”), lty=1, col=c(1,2,3,4,5,6)) curve( exp(b.log[1] + b.log[2]*x) , add=TRUE, col=1)",
    "sentences": ["Part 1 - Graphing Transformations", "Run the two of the following codes in R.", "`lm.log <- lm(log(circumference) ~ age, data=Orange) b.log <- coef(lm.log)", "lm.sqrt <-lm(sqrt(circumference) ~ age, data=Orange) b.sqrt <- coef(lm.sqrt)", "lm.1oy <- lm(1/circumference ~ age, data=Orange) b.1oy <- coef(lm.1oy)", "lm.y <- lm(circumference ~ age, data=Orange) b.y <- coef(lm.y)", "lm.2 <- lm(circumference^2 ~ age, data=Orange) b.2 <- coef(lm.2)", "lm.ss <- lm(sqrt(sqrt(circumference)) ~ age, data=Orange) b.ss <- coef(lm.ss)", "plot(circumference ~ age, data=Orange, pch=16, col=“orangered”, main=“Growth of Orange Trees”, xlab=“Age of Tree in Days”, ylab=“Circumference of Tree (mm)”) legend(“topleft”, legend=c(“1”,“2”,“3”,“4”,“5”,“6”), lty=1, col=c(1,2,3,4,5,6))", "curve( exp(b.log[1] + b.log[2]*x) , add=TRUE, col=1)"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Class Activity - Graphing Transformations"
  },
  {
    "id": 28,
    "title": "📍 Week 4 | Hypothesis Tests for Model Parameters",
    "url": "425PaigesNotes.html#week4hypothesistestsformodelparameters",
    "content": "Week 4 | Hypothesis Tests for Model Parameters Skill Quiz - Hypothesis Test for Model Parameters Problem 1 Install the Ecdat library in R: install.packages(\"Ecdat\"). From library(Ecdat) open the Caschool data set in R. As stated in the help file for this data set, this data is a collection of measurements on 420 different school districts from California during the 1998-1999 school year. The school districts in California offer a reduced-price lunch program. This is in a way, a measure of the poverty of the student body of the school district. We will assume that the higher the percentage of participants, the greater the general level of poverty. The question is, does the poverty level (or at least the percentage of participation in the reduced-lunch program) predict how well the student body will perform overall on a standardized test? > ?Caschool > View(Caschool) Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{test score} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{lunch percentage} + \\epsilon_i \\] Part (b) Plot a scatterplot of the data with your regression line overlaid. Write out the fitted regression equation. library(Ecdat) lunch <- lm(testscr ~ mealpct,data= Caschool) plot(testscr ~ mealpct,data= Caschool) abline(lunch) \\[ \\hat{Y}_i = 681.43952 + (-0.61029)X_i \\] Part (c) Report the test statistics and p-values for the following hypotheses. \\[ \\begin{array}{l} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\\\ \\end{array} \\quad \\begin{array}{l} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\\\ \\end{array} \\] summary(lunch)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 681.4 0.8894 766.2 0 mealpct -0.6103 0.01701 -35.87 1.188e-129 Fitting linear model: testscr ~ mealpct Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 420 9.447 0.7548 0.7542 The test statistics in this case provide the number of standard errors that the estimated values of the parameter sits from the hypothesized value of the true parameter. Part (d) State the slope, y-intercept, and \\(R^2\\) of this regression. Further, provide 95% confidence intervals for the slope and intercept. Interpret the values. summary(lunch)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 681.4 0.8894 766.2 0 mealpct -0.6103 0.01701 -35.87 1.188e-129 Fitting linear model: testscr ~ mealpct Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 420 9.447 0.7548 0.7542 confint(lunch, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 679.6912170 683.1878250 ## mealpct -0.6437314 -0.5768403 As shown by the R-squared value the regression is fairly good at explaining the variablility in Y. This is further witness by how accurate the confidence intervals are for the slope and intercept. Part (e) Create a residuals vs fitted-values plot and Q-Q Plot of the residuals for this regression. What do these plots show? Answer: The residauls vs fitted-values plot shows a nice linear pattern and very constant variance with the exception of three possible outliers in points 367, 163, and 180 - The Q-Q Plot shows the residuals can very safely be assumed to be normally distributed. To prove that you made the plot, the dot in the top-right of the plot is labeled as point #367 par(mfrow=c(1,3)) plot(lunch, which= 1:2) plot(lunch$residuals) Problem 2 Open the Clothing data set from library(Ecdat). Although this data is from 1990, it contains two interesting variables (1) the total tsales of the clothing stores and (2) the average number of hours worked per employee during the year, hourspw. > ?Clothing > View(Clothing) Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{Total Sales} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Hours Per Employees} + \\epsilon_i \\] \\(Y_i\\) : Total Sales \\(X_i\\) : Hours Per Employee $_0 $ : Average Total Sales when Zero Hours are Worked per Employee \\(\\beta_1\\) : Change in Average Total Sales as Hours Worked per Employee increases by 1 \\(\\epsilon_i\\) : Each clothing companies difference from the average Total Sales Part (b) Plot a scatterplot of the data with your regression line overlaid. Write out the fitted regression equation. mahclothes <- lm(tsales ~ hourspw,data= Clothing) plot(tsales ~ hourspw,data= Clothing) abline(mahclothes) \\[ \\hat{Y}_i = b_0 + b_1 X_i + \\epsilon_i \\] \\(Y_i\\) : The estimated average Total Sales. \\(X_i\\) : The Hours Worked per Employee $b_0 $ : The estimated average Total Sales when the Hours Worked per Employee is zero. \\(b_1\\) : The estimated change in the average Total Sales per 1 hour increase in Hours Worked per Employee \\(\\epsilon_i\\) : Is not used in the fitted regression equation because it only belongs with the equation for the actual Yi values. Part (c) Report the test statistics and p-values given by your summary(…) in R for the following hypotheses. \\[ \\begin{array}{l} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\\\ \\end{array} \\quad \\begin{array}{l} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\\\ \\end{array} \\] summary(mahclothes)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 1745 67479 0.02585 0.9794 hourspw 43885 3320 13.22 2.535e-33 Fitting linear model: tsales ~ hourspw Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 400 487002 0.3051 0.3033 Part (d) Now, use your own calculations to obtain test statistics and p-values for the following hypotheses. You may find useful information on how to do this in the “Explanation” tab under “t Tests” from your Math 325 Notebook, Simple Linear Regression page. \\[ \\begin{array}{l} H_0: \\beta_0 = 1500 \\\\ H_a: \\beta_0 \\neq 1500 \\\\ \\end{array} \\quad \\begin{array}{l} H_0: \\beta_1 = 35000 \\\\ H_a: \\beta_1 \\neq 35000 \\\\ \\end{array} \\] Note that these hypotheses come from previous knowledge about clothing sales and employee hours. They state that in years past, the average annual sales when no employees worked any hours on average, was 1500. And that as average eployee hours worked increases by 1 hour, the average total annual sales increases by 35,000. The question now, is if the earning pattern has changed from what it used to be. \\[t = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}}\\] OR \\[t = \\frac{Estimate(b_0/b_1) -\\mu}{Std. Error(b_0/b_1)}\\] # Intercept (1745 - 1500)/67479 ## [1] 0.003630759 # Slope (43885 - 35000)/3320 ## [1] 2.676205 Intercept : Test Statistic: 0.003630759 P-value: Large (between 0.6 to 0.999) because test statistic is very small Slope : Test Statistic: 2.676205 P-value: Small (less than 0.01) because the test statistic is fairly large Part (e) State the slope, y-intercept, and \\(R^2\\) of this regression. Further, provide 95% confidence intervals for the slope and intercept. Interpret the values. summary(mahclothes)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 1745 67479 0.02585 0.9794 hourspw 43885 3320 13.22 2.535e-33 Fitting linear model: tsales ~ hourspw Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 400 487002 0.3051 0.3033 confint(mahclothes, level= 0.95)%>%pander()   2.5 % 97.5 % (Intercept) -130915 134404 hourspw 37358 50412 Part (f) Create a residuals vs fitted-values plot and Q-Q Plot of the residuals for this regression. What do these plots show? They show some possible difficulties with linearity and constant variance, but more importantly observation number 397 is an extreme outlier that should be removed. This outlier is causing the estimate of the variance of the error terms , called the MSE, to be much larger than it should be, thus causing the \\(R^2\\) value to be lower than it actually should be. In fact, removing the outlier and re-running the regression increases the \\(R^2\\) value to 0.393. (Round to 3 decimal places.) par(mfrow=c(1,3)) plot(mahclothes, which= 1:2) plot(mahclothes$residuals) Part (g) Do any x-transformations or y-transformations improve the regression? If so, which ones? The Box-Cox transformation suggests a 0.25 transformation on Y which is further inproved by a log transfromation on X. So the final plot looks like. boxCox(mahclothes) plot(tsales ~ log(hourspw),data= Clothing)",
    "sentences": ["Skill Quiz - Hypothesis Test for Model Parameters Problem 1 Install the Ecdat library in R: install.packages(\"Ecdat\").", "From library(Ecdat) open the Caschool data set in R.", "As stated in the help file for this data set, this data is a collection of measurements on 420 different school districts from California during the 1998-1999 school year.", "The school districts in California offer a reduced-price lunch program.", "This is in a way, a measure of the poverty of the student body of the school district.", "We will assume that the higher the percentage of participants, the greater the general level of poverty.", "The question is, does the poverty level (or at least the percentage of participation in the reduced-lunch program) predict how well the student body will perform overall on a standardized test?", "> ?Caschool > View(Caschool) Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation.", "\\[ \\underbrace{Y_i}_\\text{test score} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{lunch percentage} + \\epsilon_i \\] Part (b) Plot a scatterplot of the data with your regression line overlaid.", "Write out the fitted regression equation."],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Week 4 | Hypothesis Tests for Model Parameters"
  },
  {
    "id": 29,
    "title": "📍 Skill Quiz - Hypothesis Test for Model Parameters",
    "url": "425PaigesNotes.html#skillquizhypothesistestformodelparameters",
    "content": "Skill Quiz - Hypothesis Test for Model Parameters Problem 1 Install the Ecdat library in R: install.packages(\"Ecdat\"). From library(Ecdat) open the Caschool data set in R. As stated in the help file for this data set, this data is a collection of measurements on 420 different school districts from California during the 1998-1999 school year. The school districts in California offer a reduced-price lunch program. This is in a way, a measure of the poverty of the student body of the school district. We will assume that the higher the percentage of participants, the greater the general level of poverty. The question is, does the poverty level (or at least the percentage of participation in the reduced-lunch program) predict how well the student body will perform overall on a standardized test? > View(Caschool) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{test score} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{lunch percentage} + \\epsilon_i \\]",
    "sentences": ["Problem 1 Install the Ecdat library in R: install.packages(\"Ecdat\").", "From library(Ecdat) open the Caschool data set in R.", "As stated in the help file for this data set, this data is a collection of measurements on 420 different school districts from California during the 1998-1999 school year.", "The school districts in California offer a reduced-price lunch program.", "This is in a way, a measure of the poverty of the student body of the school district.", "We will assume that the higher the percentage of participants, the greater the general level of poverty.", "The question is, does the poverty level (or at least the percentage of participation in the reduced-lunch program) predict how well the student body will perform overall on a standardized test?", "> View(Caschool)", "Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation.", "\\[ \\underbrace{Y_i}_\\text{test score} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{lunch percentage} + \\epsilon_i \\]"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Skill Quiz - Hypothesis Test for Model Parameters"
  },
  {
    "id": 30,
    "title": "📍 Assessment Quiz - Hypothesis Testing",
    "url": "425PaigesNotes.html#assessmentquizhypothesistesting",
    "content": "Assessment Quiz - Hypothesis Testing Question 1: Below is the summary output from a simple linear regression performed in R. What is the value of the missing test statistic? Further Explanation: The t-value from a regression in R is found by taking the “Estimate” and dividing by the “Std. This is because R always uses the null hypothesis that the true parameter (for either the slope or the intercept) is zero. So the t-value is computed by t = (estimate - 0)/std. error = estimate/std. error In this case: 4.4133 / 1.2679 = 3.480795 which rounds to 3.481. Question 2: Below is the summary output from a simple linear regression performed in R. Compute the 95% confidence interval for the slope estimate. Answer: (7.701, 9.423)",
    "sentences": ["Question 1: Below is the summary output from a simple linear regression performed in R.", "What is the value of the missing test statistic?", "Further Explanation: The t-value from a regression in R is found by taking the “Estimate” and dividing by the “Std.", "This is because R always uses the null hypothesis that the true parameter (for either the slope or the intercept) is zero.", "So the t-value is computed by t = (estimate - 0)/std.", "error = estimate/std.", "error In this case: 4.4133 / 1.2679 = 3.480795 which rounds to 3.481.", "Question 2: Below is the summary output from a simple linear regression performed in R.", "Compute the 95% confidence interval for the slope estimate.", "Answer: (7.701, 9.423)"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Assessment Quiz - Hypothesis Testing"
  },
  {
    "id": 31,
    "title": "📍 Class Activity - Sampling Distributions of Model Parameters",
    "url": "425PaigesNotes.html#classactivitysamplingdistributionsofmodelparameter",
    "content": "Class Activity - Sampling Distributions of Model Parameters Brother Saunder’s P-value Explanation: - The probability of observing a test statistic/ witnessing data more extreme than what we’ve seen assuming our original belief is correct - how impossible things are looking based on your belief system - ex. how many heads at the flip of a coin would you have to see until you believe that what you are seeing is impossible Part 1 - Sampling Distributions Sample variability means that different samples taken from the same population will give different results Understanding how much these samples can vary is crucial for statistics to be useful the act of gathering many samples (there is a LOT of possible samples) A sampling distribution of a sample statistic is the showing of how sample statistics (like sample means) vary across different samples from the same population. Here are the key points: It’s crucial for making inferences about a population based on limited sample data Key characteristics: Center: The mean of the sampling distribution equals the population mean Spread: As sample size increases, the standard deviation of the sampling distribution decreases (Law of Large Numbers) Shape: If the population is normal or close to normal, the sampling distribution will be normal for any sample size. For skewed populations, the sampling distribution becomes normal when sample size is greater than 30 (Central Limit Theorem) This concept is fundamental for statistical inference and understanding how well our sample statistics represent the true population parameters. Part 2 - For Loops in R",
    "sentences": ["Brother Saunder’s P-value Explanation: - The probability of observing a test statistic/ witnessing data more extreme than what we’ve seen assuming our original belief is correct - how impossible things are looking based on your belief system - ex.", "how many heads at the flip of a coin would you have to see until you believe that what you are seeing is impossible", "Part 1 - Sampling Distributions", "Sample variability means that different samples taken from the same population will give different results Understanding how much these samples can vary is crucial for statistics to be useful the act of gathering many samples (there is a LOT of possible samples)", "A sampling distribution of a sample statistic is the showing of how sample statistics (like sample means) vary across different samples from the same population.", "Here are the key points:", "It’s crucial for making inferences about a population based on limited sample data Key characteristics: Center: The mean of the sampling distribution equals the population mean Spread: As sample size increases, the standard deviation of the sampling distribution decreases (Law of Large Numbers) Shape: If the population is normal or close to normal, the sampling distribution will be normal for any sample size.", "For skewed populations, the sampling distribution becomes normal when sample size is greater than 30 (Central Limit Theorem)", "This concept is fundamental for statistical inference and understanding how well our sample statistics represent the true population parameters.", "Part 2 - For Loops in R"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Class Activity - Sampling Distributions of Model Parameters"
  },
  {
    "id": 32,
    "title": "📍 Class Activity - Hypothesis Tests for Model Parameters",
    "url": "425PaigesNotes.html#classactivityhypothesistestsformodelparameters",
    "content": "Class Activity - Hypothesis Tests for Model Parameters Part 1 - Standard Errors Sample size, spread of the data, and MSE (no control over, nature controls that) - equations don’t care what \\(b_0\\) and \\(b_1\\) are",
    "sentences": ["Part 1 - Standard Errors", "Sample size, spread of the data, and MSE (no control over, nature controls that) - equations don’t care what \\(b_0\\) and \\(b_1\\) are"],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Class Activity - Hypothesis Tests for Model Parameters"
  },
  {
    "id": 33,
    "title": "📍 Week 5 | Confidence and Prediction Intervals",
    "url": "425PaigesNotes.html#week5confidenceandpredictionintervals",
    "content": "Week 5 | Confidence and Prediction Intervals Skill Quiz - Confidence and Prediction Intervals Problem 1 Install the alr3 library in R: install.packages(\"alr3\"). From library(alr3) open the BGSall data set in R. As stated in the help file for this data set, this data is a collection of measurements on “children born in 1928-29 in Berkeley, CA.” > ?BGSall > View(BGSall) A standing tradition is that if you measure a child when they are 2-years old, and double their height, this will give a good prediction on their final adult height. Let’s see if this is true. Perform a regression that could predict a child’s 18-year old height from their 2-year old height. Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{Height 18} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Height 2} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] Part (b) Plot a scatterplot of the data with your regression line overlaid. Write out the fitted regression equation. \\[ \\hat{Y}_i = 45.7966 + 1.441 X_i \\] Part (c) Report the test statistics and p-values for the following hypotheses. (The hypotheses about \\(\\beta_0\\) claim that at age 0-years, a child should have height 0 cm. The hypotheses about \\(\\beta_1\\) claim that height doubles from age 2 to 18.) \\[ \\begin{array}{l} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\\\ \\end{array} \\quad \\begin{array}{l} H_0: \\beta_1 = 2 \\\\ H_a: \\beta_1 \\neq 2 \\\\ \\end{array} \\] Part (d) State the slope, y-intercept, and \\(R^2\\) of this regression. Further, provide 95% confidence intervals for the slope and intercept. Interpret the values. Part (e) Create a residuals vs fitted-values plot and Q-Q Plot of the residuals for this regression. What do these plots show? Part (f) A certain stats teacher at BYU-Idaho has a son named Andrew who turns 2-years old in a couple of weeks. He is currently 2 feet 9 inches tall. Predict his 18-year old height in centimeters with 95% confidence. Problem 2 Open the wblake data set from library(alr4). > ?wblake > View(wblake) If you love fishing, then you might like this data. The goal of this data was to see if there was a link in the radius size of a key “Scale” of a fish and the overall “Length” of the fish. Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{Scale} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Length} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] Part (b) Plot a scatterplot of the data with your regression line overlaid. Write out the fitted regression equation. State the \\(R^2\\) value of the regression. fishy.lm <- lm(Scale ~ Length, data = wblake) summary(fishy.lm)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) -1.431 0.1357 -10.55 2.544e-23 Length 0.0378 0.0006644 56.9 3.706e-204 Fitting linear model: Scale ~ Length Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 439 0.9288 0.8811 0.8808 plot(Scale ~ Length, data = wblake) Part (c) Diagnose the regression with a residuals vs. fitted-values plot. Determine which Y-transformation is suggested for this data. par(mfrow=c(1,2)) plot(fishy.lm, which=1) qqPlot(fishy.lm$residuals, main=\"Q-Q Plot\", col=\"darkolivegreen\", col.lines=\"darkgreen\",pch= 19, id=FALSE) boxCox(fishy.lm) Part (d) Perform a regression of the form \\(Y' = Y^\\lambda\\). Use your answer to Part (c) to select \\(\\lambda\\). Plot the regression in the transformed space, \\(Y' \\sim X\\) and add the fitted regression to the plot. plot(sqrt(Scale) ~ Length, data = wblake) Part (e) State the slope, y-intercept, and \\(R^2\\) of this transformed regression. fisher.lm <- lm(sqrt(Scale) ~ Length, data = wblake) summary(fisher.lm)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 0.7882 0.02638 29.88 1.164e-107 Length 0.008111 0.0001292 62.79 8.07e-221 Fitting linear model: sqrt(Scale) ~ Length Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 439 0.1806 0.9002 0.9 Part (f) Create a residuals vs fitted-values plot and Q-Q Plot of the residuals for this trasnformed regression. Does this regression look better than the original? par(mfrow=c(1,2)) plot(fisher.lm, which=1) qqPlot(fisher.lm$residuals, main=\"Q-Q Plot\", col=\"darkolivegreen\", col.lines=\"darkgreen\",pch= 19, id=FALSE) Part (g) Untransform the fitted regression equation and draw it on a scatterplot of the original data. Include the original regression line on this plot. Part (h) Place two prediction intervals for the Scale radius when the Length of the fish is 250 on your scatterplot of the data in the original units. Show the prediction interval from the original regression in the original units. Show the prediction interval from the transformed regression back on the original units. predict(fishy.lm,data.frame(Length = 250), interval=\"prediction\") ## fit lwr upr ## 1 8.019889 6.19091 9.848869 predict(fisher.lm,data.frame(Length = 250), interval=\"prediction\")^2 ## fit lwr upr ## 1 7.93008 6.053607 10.0595",
    "sentences": ["Skill Quiz - Confidence and Prediction Intervals Problem 1 Install the alr3 library in R: install.packages(\"alr3\").", "From library(alr3) open the BGSall data set in R.", "As stated in the help file for this data set, this data is a collection of measurements on “children born in 1928-29 in Berkeley, CA.” > ?BGSall > View(BGSall) A standing tradition is that if you measure a child when they are 2-years old, and double their height, this will give a good prediction on their final adult height.", "Let’s see if this is true.", "Perform a regression that could predict a child’s 18-year old height from their 2-year old height.", "Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation.", "\\[ \\underbrace{Y_i}_\\text{Height 18} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Height 2} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] Part (b) Plot a scatterplot of the data with your regression line overlaid.", "Write out the fitted regression equation.", "\\[ \\hat{Y}_i = 45.7966 + 1.441 X_i \\] Part (c) Report the test statistics and p-values for the following hypotheses.", "(The hypotheses about \\(\\beta_0\\) claim that at age 0-years, a child should have height 0 cm."],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Week 5 | Confidence and Prediction Intervals"
  },
  {
    "id": 34,
    "title": "📍 Skill Quiz - Confidence and Prediction Intervals",
    "url": "425PaigesNotes.html#skillquizconfidenceandpredictionintervals",
    "content": "Skill Quiz - Confidence and Prediction Intervals Install the alr3 library in R: install.packages(\"alr3\"). From library(alr3) open the BGSall data set in R. As stated in the help file for this data set, this data is a collection of measurements on “children born in 1928-29 in Berkeley, CA.” A standing tradition is that if you measure a child when they are 2-years old, and double their height, this will give a good prediction on their final adult height. Let’s see if this is true. Perform a regression that could predict a child’s 18-year old height from their 2-year old height. Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{Height 18} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Height 2} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] Plot a scatterplot of the data with your regression line overlaid. Write out the fitted regression equation.",
    "sentences": ["Install the alr3 library in R: install.packages(\"alr3\").", "From library(alr3) open the BGSall data set in R.", "As stated in the help file for this data set, this data is a collection of measurements on “children born in 1928-29 in Berkeley, CA.”", "A standing tradition is that if you measure a child when they are 2-years old, and double their height, this will give a good prediction on their final adult height.", "Let’s see if this is true.", "Perform a regression that could predict a child’s 18-year old height from their 2-year old height.", "Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation.", "\\[ \\underbrace{Y_i}_\\text{Height 18} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Height 2} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\]", "Plot a scatterplot of the data with your regression line overlaid.", "Write out the fitted regression equation."],
    "type": "section",
    "page_title": "Applied Linear Regression Notes",
    "section_title": "Skill Quiz - Confidence and Prediction Intervals"
  },
  {
    "id": 35,
    "title": "Analysis Of Variance (ANOVA)",
    "url": "ANOVA.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Analysis Of Variance (ANOVA) library(readr) An ANOVA is for testing the equality of several means simultaneously. A single quantitative response variable is required with one or more qualitative explanatory variables, i.e., factors. Note: A factor is defined as a qualitative variable containing at least two categories. The categories of the factor are referred to as the “levels” of the factor. One-way ANOVA Each experimental unit is assigned to exactly one factor-level combination. Another way to say this is “one measurement per individual” (no repeated measures) and “equal numbers of individuals per group”. Overview An ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA can be assumed to be normally distributed when seen in a Q-Q Plot.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out in a Residuals versus fitted-values plot.) Hypotheses For a One-way ANOVA \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] Mathematical Model A typical model for a one-way ANOVA is of the form \\[ Y_{ij} = \\mu_i + \\epsilon_{ij} \\] where \\(\\mu_i\\) is the mean for level (group) \\(i\\), and \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) is the error term for each point \\(j\\) within level (group) \\(i\\). R Instructions Console Help Command: ?aov() myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. X is a qualitative variable (should have class(X) equal to factor or character. If it does not, use as.factor(X) inside the aov(Y ~ as.factor(X),...) command. YourDataSet is the name of your data set. Perform the ANOVA myaov <- aov(Y ~ X, data=YourDataSet) summary(myaov) Diagnose ANOVA Assumptions par(mfrow=c(1,2)) plot(myaov, which=1:2) Example Code Hover your mouse over the example codes to learn more. Perform the ANOVA chick.aov <-  Saves the results of the ANOVA test as an object named ‘chick.aov’. aov( ‘aov()’ is a function in R used to perform the ANOVA. weight Y is ‘weight’, which is a numeric variable from the chickwts dataset.  ~  ‘~’ is the tilde symbol used to separate the Y and X in a model formula. feed, X is ‘feed’, which is a qualitative variable in the chickwts dataset, or more specifically, a factor with six levels: “casein”, “horsebean”, and so on… Use str(chickwts) to see this.  data = chickwts) ‘chickwts’ is a dataset in R. summary( ‘summary()’ shows the results of the ANOVA. chick.aov) ‘chick.aov’ is the name of the ANOVA.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## feed 5 231129 46226 15.37 5.94e-10 ## Residuals 65 195556 3009 ## ## feed *** ## Residuals ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Diagnose the ANOVA par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow = c(1,2)) The mfrow parameter controls “multiple frames on a row”. In this case, the c(1,2) specifies 1 row of 2 plots. This will cause the two diagnostic plots to be placed side-by-side. plot( ‘plot’ is a R function for the plotting of R objects. chick.aov, ‘chick.aov’ is the name of the ANOVA.  which = 1:2) The which=1:2 selects “which” of 6 available plots we want to have graphed. In this case, 1 shows the Residuals vs Fitted, and 2 shows the Normal QQ-plot. Both are needed to check the ANOVA assumptions.  Click to View Output  Click to View Output. Explanation Analysis of variance (ANOVA) is often applied to the scenario of testing for the equality of three or more means from (possibly) separate normal distributions of data. The normality assumption is required. No matter the sample size. If the distributions are skewed then a nonparametric test should be applied instead of ANOVA. One-Way ANOVA One-way ANOVA is when a completely randomized design is used with a single factor of interest. A typical mathematical model for a one-way ANOVA is of the form \\[ Y_{ik} = \\mu_i + \\epsilon_{ik} \\quad (\\text{sometimes written}\\ Y_{ik} = \\mu + \\alpha_i + \\epsilon_{ik}) \\] where \\(\\mu_i\\) is the mean of each group (or level) \\(i\\) of a factor, and \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) is the error term. The plot below demonstrates what these symbols represent. Note that the notation \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) states that we are assuming the error term \\(\\epsilon_{ik}\\) is normally distributed with a mean of 0 and a standard deviation of \\(\\sigma\\). Hypotheses The aim of ANOVA is to determine which hypothesis is more plausible, that the means of the different distributions are all equal (the null), or that at least one group mean differs (the alternative). Mathematically, \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_m = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\quad \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\}. \\] In other words, the goal is to determine if it is more plausible that each of the \\(m\\) different samples (where each sample is of size \\(n\\)) came from the same normal distribution (this is what the null hypothesis claims) or that at least one of the samples (and possibly several or all) come from different normal distributions (this is what the alternative hypothesis claims). Visualizing the Hypotheses The first figure below demonstrates what a given scenario might look like when all \\(m=3\\) samples of data are from the same normal distribution. In this case, the null hypothesis \\(H_0\\) is true. Notice that the variability of the sample means is smaller than the variability of the points. The figure below shows what a given scenario might look like for \\(m=3\\) samples of data from three different normal distributions. In this case, the alternative hypothesis \\(H_a\\) is true. Notice that the variability of the sample means, i.e., \\((\\bar{x}_1,\\bar{x}_2,\\bar{x}_3)\\), is greater than the variability of the points. Explaining the Name The above plots are useful in understanding the mathematical details behind ANOVA and why it is called analysis of variance. Recall that variance is a measure of the spread of data. When data is very spread out, the variance is large. When the data is close together, the variance is small. ANOVA utilizes two important variances, the between groups variance and the within groups variance. Between groups variance–a measure of the variability in the sample means, the \\(\\bar{x}\\)’s. Within groups variance–a combined measure of the variability of the points within each sample. The plot below combines the information from the previous plots for ease of reference. It emphasizes the fact that when the null hypothesis is true, the points should have a large variance (be really spread out) while the sample means are relatively close together. On the other hand, when the points are relative close together within each sample and the sample means have a large variance (are really spread out) then the alternative hypothesis is true. This is the theory behind analysis of variance, or ANOVA. Calculating the Test Statistic, \\(F\\) The ratio of the “between groups variation” to the “within groups variation” provides the test statistic for ANOVA. Note that the test statistic of ANOVA is an \\(F\\) statistic. \\[ F = \\frac{\\text{Between groups variation}}{\\text{Within groups variation}} \\] It would be good to take a minute and review the \\(F\\) distribution. The \\(p\\)-value for ANOVA thus comes from an \\(F\\) distribution with parameters \\(p_1 = m-1\\) and \\(p_2 = n-m\\) where \\(m\\) is the number of samples and \\(n\\) is the total number of data points. A Deeper Look at Variance It is useful to take a few minutes and explain the word variance as well as mathematically define the terms “within group variance” and “between groups variance.” Variance is a statistical measure of the variability in data. The square root of the variance is called the standard deviation and is by far the more typical measure of spread. This is because standard deviation is easier to interpret. However, mathematically speaking, the variance is the more important measurement. As mentioned previously, the variance turns out to be the key to determining which hypothesis is the most plausible, \\(H_0\\) or \\(H_a\\), when several means are under consideration. There are two variances that are important for ANOVA, the “within groups variance” and the “between groups variance.” Recall that the formula for computing a sample variance is given by \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})^2}{n-1} \\quad\\leftarrow \\frac{\\text{sum of squares}}{\\text{degrees of freedom}} \\] This formula has a couple of important pieces that are so important they have been given special names. The \\(n-1\\) in the denominator of the formula is called the “degrees of freedom.” The other important part of this formula is the \\(\\sum_{i=1}^n(x_i - \\bar{x})^2\\), which is called the “sum of squared errors” or sometimes just the “sum of squares” or “SS” for short. Thus, the sample variance is calculated by computing a “sum of squares” and dividing this by the “degrees of freedom.” It turns out that this general approach works for many different contexts. Specifically, it allows us to compute the “within groups variance” and the “between groups variance.” To introduce the mathematical definitions of these two variances, we need to introduce some new notation. Let \\(\\bar{y}_{i\\bullet}\\) represent the sample mean of group \\(i\\) for \\(i=1,\\ldots,m\\). Let \\(n_i\\) denote the sample size in group \\(i\\). Let \\(\\bar{y}_{\\bullet\\bullet}\\) represent the sample mean of all \\(n = n_1+n_2+\\cdots+n_m\\) data points. The mathematical calculations for each of these variances is given as follows. \\[ \\text{Between groups variance} = \\frac{\\sum_{i=1}^m (\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2}{m-1} \\leftarrow \\frac{\\text{Between groups sum of squares}}{\\text{Between groups degrees of freedom}} \\] \\[ \\text{Within groups variance} = \\frac{\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2}{n-m} \\leftarrow \\frac{\\text{Within groups sum of squares}}{\\text{Within groups degrees of freedom}} \\] A Fabricated Example The following table provides three samples of data: A, B, and C. These samples were randomly generated from normal distributions using a computer. The true means \\(\\mu_1, \\mu_2\\), and \\(\\mu_3\\) of the normal distributions are thus known, but withheld from you at this point of the example. A B C 13.15457 13.17463 16.66831 12.65225 12.16277 15.54719 13.73061 12.76905 16.63074 14.43471 13.38524 15.06726 13.79728 12.02690 15.57534 13.88599 13.24651 15.99915 12.77753 12.58386 15.58995 13.81536 12.64615 16.99429 13.03635 12.52055 15.47153 14.26062 14.03566 16.13330 An ANOVA will be performed with the sample data to determine which hypothesis is more plausible: \\[ H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\in \\{1,\\ldots,m\\} \\] To perform an ANOVA, we must compute the between groups variance and the within groups variance. This requires the Between groups sums of squares, within groups sums of squares, between groups degrees of freedom, and the within groups degrees of freedom. Note that to get the sums of squares, we first had to calculate \\(\\bar{y}_{1\\bullet}\\), \\(\\bar{y}_{2\\bullet}\\), \\(\\bar{y}_{3\\bullet}\\), and \\(\\bar{y}_{\\bullet\\bullet}\\) where the 1, 2, 3 corresponds to Samples A, B, and C, respectively. After some work, we find these values to be \\[ \\bar{y}_{1\\bullet} = 13.55, \\quad \\bar{y}_{2\\bullet} = 12.86 \\quad \\bar{y}_{3\\bullet} = 15.97 \\] and \\[ \\bar{y}_{\\bullet\\bullet} = \\frac{13.55+12.86+15.97}{3} = 14.13 \\] Using these values we can then compute the between groups sum of squares and the within groups sum of squares according to the formulas stated previously. This process is very tedious and will not be demonstrated. Only the results are shown in the following table which summarizes all the important information.   Degrees of Freedom Sum of Squares Variance F-value p-value Between groups 2 53.3 26.67 70.2 2e-11 Within groups 27 10.3 0.38 ANOVA Table In general, the ANOVA table is created by   Degrees of Freedom Sum of Squares Variance F-value p-value Between groups \\(m-1\\) \\(\\sum_{i=1}^m n_i(\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) \\(\\frac{\\text{Between groups variance}}{\\text{Within groups variance}}\\) \\(F\\)-distribution tail probability Within groups \\(n-m\\) \\(\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) ANOVA Assumptions The requirements for an analysis of variance (the assumptions of the test) are two-fold and concern only the error terms, the \\(\\epsilon_{ik}\\). The errors are normally distributed. The variance of the errors is constant. Both of these assumptions were stated in the mathematical model where we assumed that \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\). Checking ANOVA Assumptions To check that the ANOVA assumptions are satisfied, it is required to check the data in each group for normality using QQ-Plots. Also, the sample variance of each group must be relatively constant. The fastest way to check these two assumptions is by analyzing the residuals. An ANOVA residual is defined as the difference between the observed value of \\(y_{ik}\\) and the mean \\(\\bar{y}_{i\\bullet}\\). Mathematically, \\[ r_{ik} = y_{ik} - \\bar{y}_{i\\bullet} \\] One QQ-Plot of the residuals will provide the necessary evidence to decide if it is reasonable to assume that the error terms are normally distributed. Also, the constant variance can be checked visually by using what is known as ",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Analysis Of Variance (ANOVA) library(readr) An ANOVA is for testing the equality of several means simultaneously.", "A single quantitative response variable is required with one or more qualitative explanatory variables, i.e., factors.", "Note: A factor is defined as a qualitative variable containing at least two categories.", "The categories of the factor are referred to as the “levels” of the factor.", "One-way ANOVA Each experimental unit is assigned to exactly one factor-level combination.", "Another way to say this is “one measurement per individual” (no repeated measures) and “equal numbers of individuals per group”.", "Overview An ANOVA is only appropriate when all of the following are satisfied.", "The sample(s) of data can be considered to be representative of their population(s).", "The data is normally distributed in each group.", "(This can safely be assumed to be satisfied when the residuals from the ANOVA can be assumed to be normally distributed when seen in a Q-Q Plot.) The population variance of each group can be assumed to be the same.", "(This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out in a Residuals versus fitted-values plot.) Hypotheses For a One-way ANOVA \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] Mathematical Model A typical model for a one-way ANOVA is of the form \\[ Y_{ij} = \\mu_i + \\epsilon_{ij} \\] where \\(\\mu_i\\) is the mean for level (group) \\(i\\), and \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) is the error term for each point \\(j\\) within level (group) \\(i\\).", "R Instructions Console Help Command: ?aov() myaov is some name you come up with to store the results of the aov() test.", "Y must be a “numeric” vector of the quantitative response variable.", "X is a qualitative variable (should have class(X) equal to factor or character.", "If it does not, use as.factor(X) inside the aov(Y ~ as.factor(X),...) command.", "YourDataSet is the name of your data set.", "Perform the ANOVA myaov <- aov(Y ~ X, data=YourDataSet) summary(myaov) Diagnose ANOVA Assumptions par(mfrow=c(1,2)) plot(myaov, which=1:2) Example Code Hover your mouse over the example codes to learn more.", "Perform the ANOVA chick.aov <-  Saves the results of the ANOVA test as an object named ‘chick.aov’.", "aov( ‘aov()’ is a function in R used to perform the ANOVA.", "weight Y is ‘weight’, which is a numeric variable from the chickwts dataset.", " ~  ‘~’ is the tilde symbol used to separate the Y and X in a model formula.", "feed, X is ‘feed’, which is a qualitative variable in the chickwts dataset, or more specifically, a factor with six levels: “casein”, “horsebean”, and so on… Use str(chickwts) to see this.", " data = chickwts) ‘chickwts’ is a dataset in R.", "summary( ‘summary()’ shows the results of the ANOVA.", "chick.aov) ‘chick.aov’ is the name of the ANOVA.", "    Press Enter to run the code if you have typed it in yourself.", "You can also click here to view the output.", " Click to View Output  Click to View Output.", "## Df Sum Sq Mean Sq F value Pr(>F) ## feed 5 231129 46226 15.37 5.94e-10 ## Residuals 65 195556 3009 ## ## feed *** ## Residuals ## --- ## Signif.", "codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Diagnose the ANOVA par( ‘par’ is a R function that can be used to set or query graphical parameters.", "mfrow = c(1,2)) The mfrow parameter controls “multiple frames on a row”.", "In this case, the c(1,2) specifies 1 row of 2 plots.", "This will cause the two diagnostic plots to be placed side-by-side.", "plot( ‘plot’ is a R function for the plotting of R objects.", "chick.aov, ‘chick.aov’ is the name of the ANOVA.", " which = 1:2) The which=1:2 selects “which” of 6 available plots we want to have graphed.", "In this case, 1 shows the Residuals vs Fitted, and 2 shows the Normal QQ-plot.", "Both are needed to check the ANOVA assumptions.", " Click to View Output  Click to View Output.", "Explanation Analysis of variance (ANOVA) is often applied to the scenario of testing for the equality of three or more means from (possibly) separate normal distributions of data.", "The normality assumption is required.", "No matter the sample size.", "If the distributions are skewed then a nonparametric test should be applied instead of ANOVA.", "One-Way ANOVA One-way ANOVA is when a completely randomized design is used with a single factor of interest.", "A typical mathematical model for a one-way ANOVA is of the form \\[ Y_{ik} = \\mu_i + \\epsilon_{ik} \\quad (\\text{sometimes written}\\ Y_{ik} = \\mu + \\alpha_i + \\epsilon_{ik}) \\] where \\(\\mu_i\\) is the mean of each group (or level) \\(i\\) of a factor, and \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) is the error term.", "The plot below demonstrates what these symbols represent.", "Note that the notation \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) states that we are assuming the error term \\(\\epsilon_{ik}\\) is normally distributed with a mean of 0 and a standard deviation of \\(\\sigma\\).", "Hypotheses The aim of ANOVA is to determine which hypothesis is more plausible, that the means of the different distributions are all equal (the null), or that at least one group mean differs (the alternative).", "Mathematically, \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_m = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\quad \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\}.", "\\] In other words, the goal is to determine if it is more plausible that each of the \\(m\\) different samples (where each sample is of size \\(n\\)) came from the same normal distribution (this is what the null hypothesis claims) or that at least one of the samples (and possibly several or all) come from different normal distributions (this is what the alternative hypothesis claims)."],
    "type": "page",
    "page_title": "Analysis Of Variance (ANOVA)"
  },
  {
    "id": 36,
    "title": "📍 One-way ANOVA",
    "url": "ANOVA.html#onewayanova",
    "content": "One-way ANOVA Each experimental unit is assigned to exactly one factor-level combination. Another way to say this is “one measurement per individual” (no repeated measures) and “equal numbers of individuals per group”. Overview An ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA can be assumed to be normally distributed when seen in a Q-Q Plot.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out in a Residuals versus fitted-values plot.) Hypotheses For a One-way ANOVA \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] Mathematical Model A typical model for a one-way ANOVA is of the form \\[ Y_{ij} = \\mu_i + \\epsilon_{ij} \\] where \\(\\mu_i\\) is the mean for level (group) \\(i\\), and \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) is the error term for each point \\(j\\) within level (group) \\(i\\). R Instructions Console Help Command: ?aov() myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. X is a qualitative variable (should have class(X) equal to factor or character. If it does not, use as.factor(X) inside the aov(Y ~ as.factor(X),...) command. YourDataSet is the name of your data set. Perform the ANOVA myaov <- aov(Y ~ X, data=YourDataSet) summary(myaov) Diagnose ANOVA Assumptions par(mfrow=c(1,2)) plot(myaov, which=1:2) Example Code Hover your mouse over the example codes to learn more. Perform the ANOVA chick.aov <-  Saves the results of the ANOVA test as an object named ‘chick.aov’. aov( ‘aov()’ is a function in R used to perform the ANOVA. weight Y is ‘weight’, which is a numeric variable from the chickwts dataset.  ~  ‘~’ is the tilde symbol used to separate the Y and X in a model formula. feed, X is ‘feed’, which is a qualitative variable in the chickwts dataset, or more specifically, a factor with six levels: “casein”, “horsebean”, and so on… Use str(chickwts) to see this.  data = chickwts) ‘chickwts’ is a dataset in R. summary( ‘summary()’ shows the results of the ANOVA. chick.aov) ‘chick.aov’ is the name of the ANOVA.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## feed 5 231129 46226 15.37 5.94e-10 ## Residuals 65 195556 3009 ## ## feed *** ## Residuals ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Diagnose the ANOVA par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow = c(1,2)) The mfrow parameter controls “multiple frames on a row”. In this case, the c(1,2) specifies 1 row of 2 plots. This will cause the two diagnostic plots to be placed side-by-side. plot( ‘plot’ is a R function for the plotting of R objects. chick.aov, ‘chick.aov’ is the name of the ANOVA.  which = 1:2) The which=1:2 selects “which” of 6 available plots we want to have graphed. In this case, 1 shows the Residuals vs Fitted, and 2 shows the Normal QQ-plot. Both are needed to check the ANOVA assumptions.  Click to View Output  Click to View Output.",
    "sentences": ["Each experimental unit is assigned to exactly one factor-level combination.", "Another way to say this is “one measurement per individual” (no repeated measures) and “equal numbers of individuals per group”.", "Overview An ANOVA is only appropriate when all of the following are satisfied.", "The sample(s) of data can be considered to be representative of their population(s).", "The data is normally distributed in each group.", "(This can safely be assumed to be satisfied when the residuals from the ANOVA can be assumed to be normally distributed when seen in a Q-Q Plot.) The population variance of each group can be assumed to be the same.", "(This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out in a Residuals versus fitted-values plot.) Hypotheses For a One-way ANOVA \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] Mathematical Model A typical model for a one-way ANOVA is of the form \\[ Y_{ij} = \\mu_i + \\epsilon_{ij} \\] where \\(\\mu_i\\) is the mean for level (group) \\(i\\), and \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) is the error term for each point \\(j\\) within level (group) \\(i\\).", "R Instructions Console Help Command: ?aov() myaov is some name you come up with to store the results of the aov() test.", "Y must be a “numeric” vector of the quantitative response variable.", "X is a qualitative variable (should have class(X) equal to factor or character."],
    "type": "section",
    "page_title": "Analysis Of Variance (ANOVA)",
    "section_title": "One-way ANOVA"
  },
  {
    "id": 37,
    "title": "📍 Overview",
    "url": "ANOVA.html#overview",
    "content": "Overview An ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA can be assumed to be normally distributed when seen in a Q-Q Plot.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out in a Residuals versus fitted-values plot.) Hypotheses For a One-way ANOVA \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] Mathematical Model A typical model for a one-way ANOVA is of the form \\[ Y_{ij} = \\mu_i + \\epsilon_{ij} \\] where \\(\\mu_i\\) is the mean for level (group) \\(i\\), and \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) is the error term for each point \\(j\\) within level (group) \\(i\\).",
    "sentences": ["An ANOVA is only appropriate when all of the following are satisfied.", "The sample(s) of data can be considered to be representative of their population(s).", "The data is normally distributed in each group.", "(This can safely be assumed to be satisfied when the residuals from the ANOVA can be assumed to be normally distributed when seen in a Q-Q Plot.) The population variance of each group can be assumed to be the same.", "(This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out in a Residuals versus fitted-values plot.) Hypotheses For a One-way ANOVA \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] Mathematical Model A typical model for a one-way ANOVA is of the form \\[ Y_{ij} = \\mu_i + \\epsilon_{ij} \\] where \\(\\mu_i\\) is the mean for level (group) \\(i\\), and \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) is the error term for each point \\(j\\) within level (group) \\(i\\)."],
    "type": "section",
    "page_title": "Analysis Of Variance (ANOVA)",
    "section_title": "Overview"
  },
  {
    "id": 38,
    "title": "📍 R Instructions",
    "url": "ANOVA.html#rinstructions",
    "content": "R Instructions Console Help Command: ?aov() myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. X is a qualitative variable (should have class(X) equal to factor or character. If it does not, use as.factor(X) inside the aov(Y ~ as.factor(X),...) command. YourDataSet is the name of your data set. Perform the ANOVA myaov <- aov(Y ~ X, data=YourDataSet) summary(myaov) Diagnose ANOVA Assumptions par(mfrow=c(1,2)) plot(myaov, which=1:2) Example Code Hover your mouse over the example codes to learn more. Perform the ANOVA chick.aov <-  Saves the results of the ANOVA test as an object named ‘chick.aov’. aov( ‘aov()’ is a function in R used to perform the ANOVA. weight Y is ‘weight’, which is a numeric variable from the chickwts dataset.  ~  ‘~’ is the tilde symbol used to separate the Y and X in a model formula. feed, X is ‘feed’, which is a qualitative variable in the chickwts dataset, or more specifically, a factor with six levels: “casein”, “horsebean”, and so on… Use str(chickwts) to see this.  data = chickwts) ‘chickwts’ is a dataset in R. summary( ‘summary()’ shows the results of the ANOVA. chick.aov) ‘chick.aov’ is the name of the ANOVA.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## feed 5 231129 46226 15.37 5.94e-10 ## Residuals 65 195556 3009 ## ## feed *** ## Residuals ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Diagnose the ANOVA par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow = c(1,2)) The mfrow parameter controls “multiple frames on a row”. In this case, the c(1,2) specifies 1 row of 2 plots. This will cause the two diagnostic plots to be placed side-by-side. plot( ‘plot’ is a R function for the plotting of R objects. chick.aov, ‘chick.aov’ is the name of the ANOVA.  which = 1:2) The which=1:2 selects “which” of 6 available plots we want to have graphed. In this case, 1 shows the Residuals vs Fitted, and 2 shows the Normal QQ-plot. Both are needed to check the ANOVA assumptions.  Click to View Output  Click to View Output.",
    "sentences": ["Console Help Command: ?aov() myaov is some name you come up with to store the results of the aov() test.", "Y must be a “numeric” vector of the quantitative response variable.", "X is a qualitative variable (should have class(X) equal to factor or character.", "If it does not, use as.factor(X) inside the aov(Y ~ as.factor(X),...) command.", "YourDataSet is the name of your data set.", "Perform the ANOVA myaov <- aov(Y ~ X, data=YourDataSet) summary(myaov) Diagnose ANOVA Assumptions par(mfrow=c(1,2)) plot(myaov, which=1:2) Example Code Hover your mouse over the example codes to learn more.", "Perform the ANOVA chick.aov <-  Saves the results of the ANOVA test as an object named ‘chick.aov’.", "aov( ‘aov()’ is a function in R used to perform the ANOVA.", "weight Y is ‘weight’, which is a numeric variable from the chickwts dataset.", " ~  ‘~’ is the tilde symbol used to separate the Y and X in a model formula."],
    "type": "section",
    "page_title": "Analysis Of Variance (ANOVA)",
    "section_title": "R Instructions"
  },
  {
    "id": 39,
    "title": "📍 Explanation",
    "url": "ANOVA.html#explanation",
    "content": "Explanation Analysis of variance (ANOVA) is often applied to the scenario of testing for the equality of three or more means from (possibly) separate normal distributions of data. The normality assumption is required. No matter the sample size. If the distributions are skewed then a nonparametric test should be applied instead of ANOVA. One-Way ANOVA One-way ANOVA is when a completely randomized design is used with a single factor of interest. A typical mathematical model for a one-way ANOVA is of the form \\[ Y_{ik} = \\mu_i + \\epsilon_{ik} \\quad (\\text{sometimes written}\\ Y_{ik} = \\mu + \\alpha_i + \\epsilon_{ik}) \\] where \\(\\mu_i\\) is the mean of each group (or level) \\(i\\) of a factor, and \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) is the error term. The plot below demonstrates what these symbols represent. Note that the notation \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) states that we are assuming the error term \\(\\epsilon_{ik}\\) is normally distributed with a mean of 0 and a standard deviation of \\(\\sigma\\). Hypotheses The aim of ANOVA is to determine which hypothesis is more plausible, that the means of the different distributions are all equal (the null), or that at least one group mean differs (the alternative). Mathematically, \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_m = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\quad \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\}. \\] In other words, the goal is to determine if it is more plausible that each of the \\(m\\) different samples (where each sample is of size \\(n\\)) came from the same normal distribution (this is what the null hypothesis claims) or that at least one of the samples (and possibly several or all) come from different normal distributions (this is what the alternative hypothesis claims). Visualizing the Hypotheses The first figure below demonstrates what a given scenario might look like when all \\(m=3\\) samples of data are from the same normal distribution. In this case, the null hypothesis \\(H_0\\) is true. Notice that the variability of the sample means is smaller than the variability of the points. The figure below shows what a given scenario might look like for \\(m=3\\) samples of data from three different normal distributions. In this case, the alternative hypothesis \\(H_a\\) is true. Notice that the variability of the sample means, i.e., \\((\\bar{x}_1,\\bar{x}_2,\\bar{x}_3)\\), is greater than the variability of the points. Explaining the Name The above plots are useful in understanding the mathematical details behind ANOVA and why it is called analysis of variance. Recall that variance is a measure of the spread of data. When data is very spread out, the variance is large. When the data is close together, the variance is small. ANOVA utilizes two important variances, the between groups variance and the within groups variance. Between groups variance–a measure of the variability in the sample means, the \\(\\bar{x}\\)’s. Within groups variance–a combined measure of the variability of the points within each sample. The plot below combines the information from the previous plots for ease of reference. It emphasizes the fact that when the null hypothesis is true, the points should have a large variance (be really spread out) while the sample means are relatively close together. On the other hand, when the points are relative close together within each sample and the sample means have a large variance (are really spread out) then the alternative hypothesis is true. This is the theory behind analysis of variance, or ANOVA. Calculating the Test Statistic, \\(F\\) The ratio of the “between groups variation” to the “within groups variation” provides the test statistic for ANOVA. Note that the test statistic of ANOVA is an \\(F\\) statistic. \\[ F = \\frac{\\text{Between groups variation}}{\\text{Within groups variation}} \\] It would be good to take a minute and review the \\(F\\) distribution. The \\(p\\)-value for ANOVA thus comes from an \\(F\\) distribution with parameters \\(p_1 = m-1\\) and \\(p_2 = n-m\\) where \\(m\\) is the number of samples and \\(n\\) is the total number of data points. A Deeper Look at Variance It is useful to take a few minutes and explain the word variance as well as mathematically define the terms “within group variance” and “between groups variance.” Variance is a statistical measure of the variability in data. The square root of the variance is called the standard deviation and is by far the more typical measure of spread. This is because standard deviation is easier to interpret. However, mathematically speaking, the variance is the more important measurement. As mentioned previously, the variance turns out to be the key to determining which hypothesis is the most plausible, \\(H_0\\) or \\(H_a\\), when several means are under consideration. There are two variances that are important for ANOVA, the “within groups variance” and the “between groups variance.” Recall that the formula for computing a sample variance is given by \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})^2}{n-1} \\quad\\leftarrow \\frac{\\text{sum of squares}}{\\text{degrees of freedom}} \\] This formula has a couple of important pieces that are so important they have been given special names. The \\(n-1\\) in the denominator of the formula is called the “degrees of freedom.” The other important part of this formula is the \\(\\sum_{i=1}^n(x_i - \\bar{x})^2\\), which is called the “sum of squared errors” or sometimes just the “sum of squares” or “SS” for short. Thus, the sample variance is calculated by computing a “sum of squares” and dividing this by the “degrees of freedom.” It turns out that this general approach works for many different contexts. Specifically, it allows us to compute the “within groups variance” and the “between groups variance.” To introduce the mathematical definitions of these two variances, we need to introduce some new notation. Let \\(\\bar{y}_{i\\bullet}\\) represent the sample mean of group \\(i\\) for \\(i=1,\\ldots,m\\). Let \\(n_i\\) denote the sample size in group \\(i\\). Let \\(\\bar{y}_{\\bullet\\bullet}\\) represent the sample mean of all \\(n = n_1+n_2+\\cdots+n_m\\) data points. The mathematical calculations for each of these variances is given as follows. \\[ \\text{Between groups variance} = \\frac{\\sum_{i=1}^m (\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2}{m-1} \\leftarrow \\frac{\\text{Between groups sum of squares}}{\\text{Between groups degrees of freedom}} \\] \\[ \\text{Within groups variance} = \\frac{\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2}{n-m} \\leftarrow \\frac{\\text{Within groups sum of squares}}{\\text{Within groups degrees of freedom}} \\] A Fabricated Example The following table provides three samples of data: A, B, and C. These samples were randomly generated from normal distributions using a computer. The true means \\(\\mu_1, \\mu_2\\), and \\(\\mu_3\\) of the normal distributions are thus known, but withheld from you at this point of the example. A B C 13.15457 13.17463 16.66831 12.65225 12.16277 15.54719 13.73061 12.76905 16.63074 14.43471 13.38524 15.06726 13.79728 12.02690 15.57534 13.88599 13.24651 15.99915 12.77753 12.58386 15.58995 13.81536 12.64615 16.99429 13.03635 12.52055 15.47153 14.26062 14.03566 16.13330 An ANOVA will be performed with the sample data to determine which hypothesis is more plausible: \\[ H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\in \\{1,\\ldots,m\\} \\] To perform an ANOVA, we must compute the between groups variance and the within groups variance. This requires the Between groups sums of squares, within groups sums of squares, between groups degrees of freedom, and the within groups degrees of freedom. Note that to get the sums of squares, we first had to calculate \\(\\bar{y}_{1\\bullet}\\), \\(\\bar{y}_{2\\bullet}\\), \\(\\bar{y}_{3\\bullet}\\), and \\(\\bar{y}_{\\bullet\\bullet}\\) where the 1, 2, 3 corresponds to Samples A, B, and C, respectively. After some work, we find these values to be \\[ \\bar{y}_{1\\bullet} = 13.55, \\quad \\bar{y}_{2\\bullet} = 12.86 \\quad \\bar{y}_{3\\bullet} = 15.97 \\] and \\[ \\bar{y}_{\\bullet\\bullet} = \\frac{13.55+12.86+15.97}{3} = 14.13 \\] Using these values we can then compute the between groups sum of squares and the within groups sum of squares according to the formulas stated previously. This process is very tedious and will not be demonstrated. Only the results are shown in the following table which summarizes all the important information.   Degrees of Freedom Sum of Squares Variance F-value p-value Between groups 2 53.3 26.67 70.2 2e-11 Within groups 27 10.3 0.38 ANOVA Table In general, the ANOVA table is created by   Degrees of Freedom Sum of Squares Variance F-value p-value Between groups \\(m-1\\) \\(\\sum_{i=1}^m n_i(\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) \\(\\frac{\\text{Between groups variance}}{\\text{Within groups variance}}\\) \\(F\\)-distribution tail probability Within groups \\(n-m\\) \\(\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) ANOVA Assumptions The requirements for an analysis of variance (the assumptions of the test) are two-fold and concern only the error terms, the \\(\\epsilon_{ik}\\). The errors are normally distributed. The variance of the errors is constant. Both of these assumptions were stated in the mathematical model where we assumed that \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\). Checking ANOVA Assumptions To check that the ANOVA assumptions are satisfied, it is required to check the data in each group for normality using QQ-Plots. Also, the sample variance of each group must be relatively constant. The fastest way to check these two assumptions is by analyzing the residuals. An ANOVA residual is defined as the difference between the observed value of \\(y_{ik}\\) and the mean \\(\\bar{y}_{i\\bullet}\\). Mathematically, \\[ r_{ik} = y_{ik} - \\bar{y}_{i\\bullet} \\] One QQ-Plot of the residuals will provide the necessary evidence to decide if it is reasonable to assume that the error terms are normally distributed. Also, the constant variance can be checked visually by using what is known as a residuals versus fitted values plot. For the Fabricated Example above, the QQ-Plot and residuals versus fitted values plots show the two assumptions of ANOVA appear to be satisfied.",
    "sentences": ["Analysis of variance (ANOVA) is often applied to the scenario of testing for the equality of three or more means from (possibly) separate normal distributions of data.", "The normality assumption is required.", "No matter the sample size.", "If the distributions are skewed then a nonparametric test should be applied instead of ANOVA.", "One-Way ANOVA One-way ANOVA is when a completely randomized design is used with a single factor of interest.", "A typical mathematical model for a one-way ANOVA is of the form \\[ Y_{ik} = \\mu_i + \\epsilon_{ik} \\quad (\\text{sometimes written}\\ Y_{ik} = \\mu + \\alpha_i + \\epsilon_{ik}) \\] where \\(\\mu_i\\) is the mean of each group (or level) \\(i\\) of a factor, and \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) is the error term.", "The plot below demonstrates what these symbols represent.", "Note that the notation \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) states that we are assuming the error term \\(\\epsilon_{ik}\\) is normally distributed with a mean of 0 and a standard deviation of \\(\\sigma\\).", "Hypotheses The aim of ANOVA is to determine which hypothesis is more plausible, that the means of the different distributions are all equal (the null), or that at least one group mean differs (the alternative).", "Mathematically, \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_m = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\quad \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\}."],
    "type": "section",
    "page_title": "Analysis Of Variance (ANOVA)",
    "section_title": "Explanation"
  },
  {
    "id": 40,
    "title": "📍 Two-way ANOVA",
    "url": "ANOVA.html#twowayanova",
    "content": "Two-way ANOVA Overview A two-way ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses. Writing out the hypotheses can be very involved depending on whether you use the official “effects model” notation (very mathematically correct) or a simplified “means model” notation (which isn’t very mathematically correct, but gets the idea across in an acceptable way). Means Model Effects Model The first set of hypotheses are a “one-way” set of hypotheses for the first factor of the ANOVA. Factor: X1 with say, levels \\(A\\) and \\(B\\). \\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a “one-way” set of hypotheses, but for the second factor of the ANOVA. Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\mu_C = \\mu_D = \\mu_E = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one}\\ i\\in\\{1=C,2=D,3=E\\} \\] The third set of hypotheses are the most interesting hypotheses in a two-way ANOVA. These are called the interaction hypotheses. They test to see if the levels of one of the factors, say \\(X1\\), impact \\(Y\\) differently for the differing levels of the other factor, \\(X2\\). The hypotheses read formally as \\[ H_0: \\text{The effect of the first factor on Y} \\\\ \\text{is the same for all levels of the second factor.} \\] \\[ H_a: \\text{The effect of the first factor on Y is not the same} \\\\ \\text{for all levels of the second factor.} \\] A mathematically correct way to state the two-way ANOVA model is with the equation \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] In this model, \\(\\mu\\) is the grand mean (which is the average Y-value ignoring all information contained in the factors); \\(\\alpha_i\\) is the first factor \\(X1\\) with levels \\(A\\) and \\(B\\) (though there could be more levels in \\(X1\\) depending on your data); \\(\\beta_j\\) is the second factor with levels \\(C\\), \\(D\\), and \\(E\\) (though there could be fewer or more levels to this factor depending on your data); \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors which has \\(2\\times3=6\\) (may differ for your data) levels; and \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) is the normally distributed error term for each point \\(k\\) found within level \\(i\\) of \\(X1\\) and level \\(j\\) of \\(X2\\). This model allows us to more formally state the hypotheses as First factor \\(X1\\) having say, levels \\(A\\) and \\(B\\). \\[ H_0: \\alpha_A = \\alpha_B = 0 \\] \\[ H_a: \\alpha_i \\neq 0 \\ \\text{for at least one}\\ i\\in\\{1=A,2=B\\} \\] Second factor \\(X2\\) with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\beta_C = \\beta_D = \\beta_E = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j\\in\\{1=C,2=D,3=E\\} \\] Does the effect of the first factor (\\(X1\\)) change for the different levels of the second factor (\\(X2\\))? In other words, is there an interaction between the two factors \\(X1\\) and \\(X2\\)? \\[ H_0: \\alpha\\beta_{ij} = 0 \\ \\text{for all } i,j \\] \\[ H_a: \\alpha\\beta_{ij} \\neq 0 \\ \\text{for at least one } i,j \\]",
    "sentences": ["Overview A two-way ANOVA is only appropriate when all of the following are satisfied.", "The sample(s) of data can be considered to be representative of their population(s).", "The data is normally distributed in each group.", "(This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same.", "(This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses.", "Writing out the hypotheses can be very involved depending on whether you use the official “effects model” notation (very mathematically correct) or a simplified “means model” notation (which isn’t very mathematically correct, but gets the idea across in an acceptable way).", "Means Model Effects Model The first set of hypotheses are a “one-way” set of hypotheses for the first factor of the ANOVA.", "Factor: X1 with say, levels \\(A\\) and \\(B\\).", "\\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a “one-way” set of hypotheses, but for the second factor of the ANOVA.", "Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\)."],
    "type": "section",
    "page_title": "Analysis Of Variance (ANOVA)",
    "section_title": "Two-way ANOVA"
  },
  {
    "id": 41,
    "title": "📍 Overview",
    "url": "ANOVA.html#overview",
    "content": "Overview A two-way ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses. Writing out the hypotheses can be very involved depending on whether you use the official “effects model” notation (very mathematically correct) or a simplified “means model” notation (which isn’t very mathematically correct, but gets the idea across in an acceptable way). Means Model Effects Model The first set of hypotheses are a “one-way” set of hypotheses for the first factor of the ANOVA. Factor: X1 with say, levels \\(A\\) and \\(B\\). \\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a “one-way” set of hypotheses, but for the second factor of the ANOVA. Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\mu_C = \\mu_D = \\mu_E = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one}\\ i\\in\\{1=C,2=D,3=E\\} \\] The third set of hypotheses are the most interesting hypotheses in a two-way ANOVA. These are called the interaction hypotheses. They test to see if the levels of one of the factors, say \\(X1\\), impact \\(Y\\) differently for the differing levels of the other factor, \\(X2\\). The hypotheses read formally as \\[ H_0: \\text{The effect of the first factor on Y} \\\\ \\text{is the same for all levels of the second factor.} \\] \\[ H_a: \\text{The effect of the first factor on Y is not the same} \\\\ \\text{for all levels of the second factor.} \\] A mathematically correct way to state the two-way ANOVA model is with the equation \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] In this model, \\(\\mu\\) is the grand mean (which is the average Y-value ignoring all information contained in the factors); \\(\\alpha_i\\) is the first factor \\(X1\\) with levels \\(A\\) and \\(B\\) (though there could be more levels in \\(X1\\) depending on your data); \\(\\beta_j\\) is the second factor with levels \\(C\\), \\(D\\), and \\(E\\) (though there could be fewer or more levels to this factor depending on your data); \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors which has \\(2\\times3=6\\) (may differ for your data) levels; and \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) is the normally distributed error term for each point \\(k\\) found within level \\(i\\) of \\(X1\\) and level \\(j\\) of \\(X2\\). This model allows us to more formally state the hypotheses as First factor \\(X1\\) having say, levels \\(A\\) and \\(B\\). \\[ H_0: \\alpha_A = \\alpha_B = 0 \\] \\[ H_a: \\alpha_i \\neq 0 \\ \\text{for at least one}\\ i\\in\\{1=A,2=B\\} \\] Second factor \\(X2\\) with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\beta_C = \\beta_D = \\beta_E = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j\\in\\{1=C,2=D,3=E\\} \\] Does the effect of the first factor (\\(X1\\)) change for the different levels of the second factor (\\(X2\\))? In other words, is there an interaction between the two factors \\(X1\\) and \\(X2\\)? \\[ H_0: \\alpha\\beta_{ij} = 0 \\ \\text{for all } i,j \\] \\[ H_a: \\alpha\\beta_{ij} \\neq 0 \\ \\text{for at least one } i,j \\]",
    "sentences": ["A two-way ANOVA is only appropriate when all of the following are satisfied.", "The sample(s) of data can be considered to be representative of their population(s).", "The data is normally distributed in each group.", "(This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same.", "(This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses.", "Writing out the hypotheses can be very involved depending on whether you use the official “effects model” notation (very mathematically correct) or a simplified “means model” notation (which isn’t very mathematically correct, but gets the idea across in an acceptable way).", "Means Model Effects Model The first set of hypotheses are a “one-way” set of hypotheses for the first factor of the ANOVA.", "Factor: X1 with say, levels \\(A\\) and \\(B\\).", "\\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a “one-way” set of hypotheses, but for the second factor of the ANOVA.", "Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\)."],
    "type": "section",
    "page_title": "Analysis Of Variance (ANOVA)",
    "section_title": "Overview"
  },
  {
    "id": 42,
    "title": "📍 R Instructions",
    "url": "ANOVA.html#rinstructions",
    "content": "R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ X1+X2+X1:X2, data=YourDataSet) Perform the ANOVA summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. Note that factors X3, X4, and so on could also be added + to the model if desired, but this would create a three-way, or four-way ANOVA model, and so on. X1:X2 denotes the interaction of the factors X1 and X2. It is not required, but is usually included. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. warp.aov <-  Saves the results of the ANOVA test as an object named ‘warp.aov’. aov( ‘aov()’ is a function in R used to perform the ANOVA. breaks \\(Y\\) is ‘breaks’, which is a numeric variable from the warpbreaks dataset.  ~  ‘~’ is the tilde symbol used to separate the left- and right-hand side in a model formula. wool  The first factor \\(X1\\) is ‘wool’, which is a qualitative variable in the warpbreaks dataset. In this case, wool is a factor with two levels. Use str(warpbreaks) to see this. + tension  The second factor \\(X2\\) is ‘tension’, which is another qualitative variable in the warpbreaks dataset. In this case, tension is a factor with three levels. Use str(warpbreaks) to see this. + wool:tension, The interaction of the two factors: wool and tension.  data = warpbreaks) ‘warpbreaks’ is a dataset in R. summary( ‘summary()’ shows the results of the ANOVA. warp.aov) ‘warp.aov’ is the name of the ANOVA.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## wool 1 451 450.7 3.765 0.058213 ## tension 2 2034 1017.1 8.498 0.000693 ## wool:tension 2 1003 501.4 4.189 0.021044 ## Residuals 48 5745 119.7 ## ## wool . ## tension *** ## wool:tension * ## Residuals ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow = c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. plot( ‘plot’ is a R function for the plotting of R objects. warp.aov, ‘warp.aov’ is the name of the ANOVA.  which = 1:2) Will show the Residuals vs Fitted and the Normal QQ-plot to check the ANOVA assumptions.  Click to View Output  Click to View Output.",
    "sentences": ["Console Help Command: ?aov() myaov <- aov(Y ~ X1+X2+X1:X2, data=YourDataSet) Perform the ANOVA summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test.", "Y must be a “numeric” vector of the quantitative response variable.", "X1 is a qualitative variable (should have class(X1) equal to factor or character.", "If it does not, use as.factor(X1) inside the aov() command.", "X2 is a second qualitative variable that should also be either a factor or a character vector.", "Note that factors X3, X4, and so on could also be added + to the model if desired, but this would create a three-way, or four-way ANOVA model, and so on.", "X1:X2 denotes the interaction of the factors X1 and X2.", "It is not required, but is usually included.", "YourDataSet is the name of your data set.", "Example Code Hover your mouse over the example codes to learn more."],
    "type": "section",
    "page_title": "Analysis Of Variance (ANOVA)",
    "section_title": "R Instructions"
  },
  {
    "id": 43,
    "title": "📍 Explanation",
    "url": "ANOVA.html#explanation",
    "content": "Explanation Hypotheses in Two-way ANOVA The hypotheses that can be tested in a two-way ANOVA that includes an interaction term are three-fold. Hypotheses about \\(\\alpha\\) where \\(\\alpha\\) has \\(m\\) levels. \\[ H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_m = 0 \\] \\[ H_a: \\alpha_i \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\} \\] Hypotheses about \\(\\beta\\) where \\(\\beta\\) has \\(q\\) levels. \\[ H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_q = 0 \\] \\[ H_a: \\beta_j \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,q\\} \\] Hypotheses about the interaction term \\(\\alpha\\beta\\). \\[ H_0: \\text{the effect of one factor is the same across all levels of the other factor} \\] \\[ H_a: \\text{the effect of one factor differs for at least one level of the other factor} \\] Expanding the ANOVA Model It turns out that more can be done with ANOVA than simply checking to see if the means of several groups differ. Reconsider the mathematical model of two-way ANOVA. \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] This model could be expanded to include any number of new terms in the model. The power of this approach is in the several questions (hypotheses) that can be posed to data simultaneously. What happens if your ANOVA fails both requirement tests? Example: BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") ## Rows: 204 Columns: 3 ## ── Column specification ────────────────────────── ## Delimiter: \",\" ## chr (2): Semester, Participation ## dbl (1): FinalGrade ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. BAanova <- aov(FinalGrade ~ Semester + Participation + Semester:Participation, data=BAFinalGrades) par(mfrow=c(1,2)) plot(BAanova, which=1:2, pch=16) The residual plot shows that the constant (equal) variance assumption raises some concern as the points within each group have varying spread. The spread of each group seems to gets closer and closer as we go from left to right as well as some straying point from the more clustered groups. Thus, showing the variance of the residuals are not constant and do not accurately show a relationship between each variable. For the Q-Q plot, it shows that the residuals of the data is not quite normally distributed. There are visible points at the lower and upper ends of the graph deviating away form the diagonal line, which represents a normal distribution. Thus, showing us that the residuals of the data are not normal distributed. Although both graphs indicate that the data fails to meet both requirements, this primarily means our findings may be less reliable due to the abnormalities in the data. As a result, we should interpret the results cautiously as we proceed with our Two-Way ANOVA test!",
    "sentences": ["Hypotheses in Two-way ANOVA The hypotheses that can be tested in a two-way ANOVA that includes an interaction term are three-fold.", "Hypotheses about \\(\\alpha\\) where \\(\\alpha\\) has \\(m\\) levels.", "\\[ H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_m = 0 \\] \\[ H_a: \\alpha_i \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\} \\] Hypotheses about \\(\\beta\\) where \\(\\beta\\) has \\(q\\) levels.", "\\[ H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_q = 0 \\] \\[ H_a: \\beta_j \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,q\\} \\] Hypotheses about the interaction term \\(\\alpha\\beta\\).", "\\[ H_0: \\text{the effect of one factor is the same across all levels of the other factor} \\] \\[ H_a: \\text{the effect of one factor differs for at least one level of the other factor} \\] Expanding the ANOVA Model It turns out that more can be done with ANOVA than simply checking to see if the means of several groups differ.", "Reconsider the mathematical model of two-way ANOVA.", "\\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] This model could be expanded to include any number of new terms in the model.", "The power of this approach is in the several questions (hypotheses) that can be posed to data simultaneously.", "What happens if your ANOVA fails both requirement tests?", "Example: BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") ## Rows: 204 Columns: 3 ## ── Column specification ────────────────────────── ## Delimiter: \",\" ## chr (2): Semester, Participation ## dbl (1): FinalGrade ## ## ℹ Use `spec()` to retrieve the full column specification for this data."],
    "type": "section",
    "page_title": "Analysis Of Variance (ANOVA)",
    "section_title": "Explanation"
  },
  {
    "id": 44,
    "title": "📍 Block Design",
    "url": "ANOVA.html#blockdesign",
    "content": "Block Design Repeated measures or other factors that group individuals into similar groups (blocks) are included in the study design. Overview A typical model for a block design is of the form \\[ Y_{lijk} = \\mu + B_l + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijlk} \\] where \\(\\mu\\) is the grand mean, \\(B_l\\) is the blocking factor, \\(\\alpha_i\\) is one factor with at least two levels, \\(\\beta_j\\) is another factor with at least two levels, \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors, and \\(\\epsilon_{ijlk} \\sim N(0,\\sigma^2)\\) is the error term. Only one block and one factor is required. Multiple blocks and multiple factors are allowed. It is not required to include interaction terms. The error term is always required. R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ Block+X1+X2+X1:X2, data=YourDataSet) Perform the test summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. Block is a qualitative variable that is not of direct interest, but is included in the model to account for variability in the data. It should have class(Block) equal to either factor or character. Use as.factor() if it does not. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. If it does not, use as.factor(X2). Note that factors C, D, and so on could also be added + to the model if desired. X1:X2 denotes the interaction of the factors X1 and X2. It is not required and should only be included if the interaction term is of interest. YourDataSet is the name of your data set.",
    "sentences": ["Repeated measures or other factors that group individuals into similar groups (blocks) are included in the study design.", "Overview A typical model for a block design is of the form \\[ Y_{lijk} = \\mu + B_l + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijlk} \\] where \\(\\mu\\) is the grand mean, \\(B_l\\) is the blocking factor, \\(\\alpha_i\\) is one factor with at least two levels, \\(\\beta_j\\) is another factor with at least two levels, \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors, and \\(\\epsilon_{ijlk} \\sim N(0,\\sigma^2)\\) is the error term.", "Only one block and one factor is required.", "Multiple blocks and multiple factors are allowed.", "It is not required to include interaction terms.", "The error term is always required.", "R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ Block+X1+X2+X1:X2, data=YourDataSet) Perform the test summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test.", "Y must be a “numeric” vector of the quantitative response variable.", "Block is a qualitative variable that is not of direct interest, but is included in the model to account for variability in the data.", "It should have class(Block) equal to either factor or character."],
    "type": "section",
    "page_title": "Analysis Of Variance (ANOVA)",
    "section_title": "Block Design"
  },
  {
    "id": 45,
    "title": "📍 Overview",
    "url": "ANOVA.html#overview",
    "content": "Overview A typical model for a block design is of the form \\[ Y_{lijk} = \\mu + B_l + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijlk} \\] where \\(\\mu\\) is the grand mean, \\(B_l\\) is the blocking factor, \\(\\alpha_i\\) is one factor with at least two levels, \\(\\beta_j\\) is another factor with at least two levels, \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors, and \\(\\epsilon_{ijlk} \\sim N(0,\\sigma^2)\\) is the error term. Only one block and one factor is required. Multiple blocks and multiple factors are allowed. It is not required to include interaction terms. The error term is always required.",
    "sentences": ["A typical model for a block design is of the form \\[ Y_{lijk} = \\mu + B_l + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijlk} \\] where \\(\\mu\\) is the grand mean, \\(B_l\\) is the blocking factor, \\(\\alpha_i\\) is one factor with at least two levels, \\(\\beta_j\\) is another factor with at least two levels, \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors, and \\(\\epsilon_{ijlk} \\sim N(0,\\sigma^2)\\) is the error term.", "Only one block and one factor is required.", "Multiple blocks and multiple factors are allowed.", "It is not required to include interaction terms.", "The error term is always required."],
    "type": "section",
    "page_title": "Analysis Of Variance (ANOVA)",
    "section_title": "Overview"
  },
  {
    "id": 46,
    "title": "📍 R Instructions",
    "url": "ANOVA.html#rinstructions",
    "content": "R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ Block+X1+X2+X1:X2, data=YourDataSet) Perform the test summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. Block is a qualitative variable that is not of direct interest, but is included in the model to account for variability in the data. It should have class(Block) equal to either factor or character. Use as.factor() if it does not. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. If it does not, use as.factor(X2). Note that factors C, D, and so on could also be added + to the model if desired. X1:X2 denotes the interaction of the factors X1 and X2. It is not required and should only be included if the interaction term is of interest. YourDataSet is the name of your data set.",
    "sentences": ["Console Help Command: ?aov() myaov <- aov(Y ~ Block+X1+X2+X1:X2, data=YourDataSet) Perform the test summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test.", "Y must be a “numeric” vector of the quantitative response variable.", "Block is a qualitative variable that is not of direct interest, but is included in the model to account for variability in the data.", "It should have class(Block) equal to either factor or character.", "Use as.factor() if it does not.", "X1 is a qualitative variable (should have class(X1) equal to factor or character.", "If it does not, use as.factor(X1) inside the aov() command.", "X2 is a second qualitative variable that should also be either a factor or a character vector.", "If it does not, use as.factor(X2).", "Note that factors C, D, and so on could also be added + to the model if desired."],
    "type": "section",
    "page_title": "Analysis Of Variance (ANOVA)",
    "section_title": "R Instructions"
  },
  {
    "id": 47,
    "title": "Chi-squared Tests",
    "url": "ChiSquaredTests.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Chi-squared Tests Association testing for two qualitative variables with at least two levels to each variable. Chi-squared Test of Independence Overview A method of comparing observed counts to the expected counts in a contingency table to decide if two qualitative variables are associated (alternative hypothesis) or are independent, i.e., not associated (null hypothesis). Typically, all expected counts are required to be 5 or greater for the test to be appropriate. However, the test is still appropriate if all expected counts are at least 1 and the average of the expected counts is at least 5. R Instructions Console Help Command: ?chisq.test() To Run the Test chisq.test(x) x must be a table or a matrix. To Make a Table If you have a data set that you want to turn into a table, use the table command. x The name of your created table to go in the chisq.test() code.  <-  This is the “left arrow” assignment operator that stores the results of your table() code into x. table( table is an R function used to tabulate how many times each value occrus in a given dataset. Dataset$FirstVariable, Use your dataset to call in a row variable.  Dataset$SecondVariable) Use your dataset to call in a column variable. x <- table(mtcars$am, mtcars$cyl) pander(x)   4 6 8 0 3 4 12 1 8 3 2 #Rename rows and columns of your table: rownames(x) <- c(\"Automatic\", \"Manual\") colnames(x) <- c(\"4 cyl\", \"6 cyl\", \"8 cyl\") pander(x)   4 cyl 6 cyl 8 cyl Automatic 3 4 12 Manual 8 3 2 Click open the example code above for details on how to rename rows and columns in a table. To Make a Matrix If you have tally counts of your observations, enter them into RStudio using a matrix. x The name of your created matrix to go in the chisq.test() code.  <-  This is the “left arrow” assignment operator that stores the results of your cbind() code into x. cbind( cbind stands for”column bind” and is a function that joins together different c() vectors to make them become columns of a table. `Farm 1` = The name of the first column you want in the matrix. To include spaces in the name, use the back-ticks ` and `.  c(Pigs = 10, Cats = 5, Dogs = 28, Roosters = 3), Specify each of the values by name in a c(…) function to be stored in your first column, in this case, Farm 1.  `Farm 2` = The name of the second column you want in the matrix.  c(Pigs = 15, Cats = 3, Dogs = 8, Roosters = 1) Each new column must contain entries for each named element. Technically, you can leave off the names in all but the first column as only the names from the first column will be used. pander(x) Print the matrix to the screen using pander(…).   Farm 1 Farm 2 Pigs 10 15 Cats 5 3 Dogs 28 8 Roosters 3 1 Or, notice how you could rbind instead with slightly different results… x The name of your created matrix to go in the chisq.test() code.  <-  This is the “left arrow” assignment operator that stores the results of your cbind() code into x. rbind( rbind stands for”row bind” and is a function that joins together different c() vectors to make them become rows of a table. `Farm 1` = The name of the first column you want in the matrix. To include spaces in the name, use the back-ticks ` and `.  c(Pigs = 10, Cats = 5, Dogs = 28, Roosters = 3), Specify each of the values by name in a c(…) function to be stored in your first column, in this case, Farm 1.  `Farm 2` = The name of the second column you want in the matrix.  c(Pigs = 15, Cats = 3, Dogs = 8, Roosters = 1) Each new column must contain entries for each named element. Technically, you can leave off the names in all but the first column as only the names from the first column will be used. pander(x) Print the matrix to the screen using pander(…).   Pigs Cats Dogs Roosters Farm 1 10 5 28 3 Farm 2 15 3 8 1 Diagnostics yourNamedTestResults This is some name you come up with that will become the R object that stores the results of your Chi-squated test chisq.test() command.  <-  This is the “left arrow” assignment operator that stores the results of your chisq.test() code into yourNamedTestResults. chisq.test( chisq.test() is an R function that stands for “Chi-Squared Test”. It performs the Chi-squared test for x. x) x is either a matrix or a table. Pearson’s Chi-squared test The name of the test being performed. data: x The data you are using for the Chi-Squared test. x is either a table or matrix that you created. X-squared = 9.2956, The test statistic of the test follows a chi-squared distribution with the given degrees of freedom. See the end of the “Making Inference” page of the book to learn more about the Chi-squared Parametric Distribution.  df = 3, The degrees of freedom of the test. It is found by (number of rows - 1)x(number of columns - 1)  p-value = 0.02561 The p-value of the test is found by computing the probability of being more extreme than given test statistic. In R, you can compute this yourself using the code pchisq(9.2956, 3, lower.tail=FALSE). yourNamedTestResults This is some name you come up with that will become the R object that stores the results of your Chi-squared test chisq.test() command. $ This allows you to access various elements from the test that was performed. expected The expected counts of the chi-squared test. These are computed for each observed count by using the equation (row tota)*(column total)/(total total). Pigs EXPLANATION.   Cats EXPLANATION.   Dogs EXPLANATION.   Roosters EXPLANATION. Farm 1 EXPLANATION.   15.753425 EXPLANATION.   5.041096 EXPLANATION. 22.68493 EXPLANATION. 2.520548 EXPLANATION. Farm 2 EXPLANATION.   9.246575 EXPLANATION.   2.958904 EXPLANATION. 13.31507 EXPLANATION. 1.479452 EXPLANATION. Interpretation: Pearson Residuals yourNamedTestResults This is some name you come up with that will become the R object that stores the results of your Chi-squated test chisq.test() command. $ This allows you to access various elements from the test that was performed. residuals This grabs the residuals for each observation in the Chi-squared test. Pigs EXPLANATION.   Cats EXPLANATION.   Dogs EXPLANATION.   Roosters EXPLANATION. Farm 1 EXPLANATION.   -1.449569 EXPLANATION.   -0.01830357 EXPLANATION. 1.115938 EXPLANATION. 0.3019936 EXPLANATION. Farm 2 EXPLANATION.   1.892065 EXPLANATION.   0.02389092 EXPLANATION. -1.456589 EXPLANATION. -0.3941801 EXPLANATION. Explanation To demonstrate the theory behind the Chi-squared Test, consider the following data about the survival of the Titanic passengers as a specific example. The rows of the following contingency table (a table of counts) show the Class of the passenger, while the columns show the Survival of the passenger. Note that each passenger is placed in exactly one Class and one level of Survival. Also, the two factors, Class and Survival, are qualitative variables with at least two levels. Thus, the data meets the basic requirements of the Chi-squared Test. No Yes 1st 122 203 2nd 167 118 3rd 528 178 Crew 673 212 The Hypotheses The hypotheses of the Chi-squared Test are written as verbal statements. \\(H_0\\): The row variable and column variable are independent. \\(H_a\\): The row variable and column variable are associated (not independent). For the Titanic data, these hypotheses would be written as \\(H_0\\): Class and Survival are independent. \\(H_a\\): Class and Survival are associated. The Model Recall that to obtain a \\(p\\)-value for a hypothesis test two things are required, 1) a test statistic, and 2) the distribution of the test statistic under the null hypothesis. The test statistic for the Chi-squared Test is given by the formula \\[ \\chi^2 = \\sum_{i=1}^m \\frac{(O_i - E_i)^2}{E_i} \\] where \\(O_i\\) represents each of the \\(m\\) observed counts and \\(E_i\\) represents the \\(m\\) expected counts. Note that \\(m = r\\times c\\), were \\(r\\) represents the number of rows in the contingency table and \\(c\\) represents the number of columns. The expected counts are obtained by the formula \\[ E_i = \\frac{\\text{(row total)(column total)}}{\\text{(total total)}} \\] The \\(\\chi^2\\) test statistic can be assumed to follow a chi-squared distribution with degrees of freedom \\(p = (r-1)\\times(c-1)\\) as long as the distribution of counts meets either of the following requirements. All expected counts are greater than five. OR All expected counts are greater than one, and the average of the expected counts is at least five. Where the \\(\\chi^2\\) statistic Comes From Consider the following graphic of the Titanic data. The data suggests that Survival of passengers appears to be associated with the Class of the passenger (the alternative hypothesis). This is due to the fact that more 1st class passengers survived than perished but far more 3rd class and Crew members perished than survived. It seems that the chances of survival were much greater for the 1st and 2nd class passengers than the 3rd class and Crew. Thus, survival appears to be associated with class. If the null hypothesis were true for the Titanic data, then we would expect to see roughly the same percentage of survival across all classes. In other words, if Class and Survival were independent, we would expect to see plots more like the theoretical ones depicted below. The theoretical plot above depicts the expected counts for each Class and Survival combination. To see where these expected counts come from, note that there are a total of 2,201 passengers recorded for the Titanic data (the sum of all values in the table). Further, there are a total of \\(122+203 = 325\\) passengers that were 1st class, \\(167 + 118 = 285\\) passengers in 2nd class, \\(528 + 178 = 706\\) passengers in 3rd class, and \\(673 + 212 = 885\\) Crew members. Thus, we have the following percentage of each class of passenger. Note that these percentages were obtained by taking the row totals and dividing them by the “total total”. Class Percentage 1st 0.1476602 2nd 0.1294866 3rd 0.3207633 Crew 0.4020900 Thus, if the null hypothesis is true, we would “expect” these percentages to remain consistent across both categories of Survival. Since there were \\(122+167+528+673 = 1490\\) passengers that did not survive and \\(203+118+178+212 =711\\) passengers that survived, we have the following expected counts. The counts under Survived were obtained by multiplying the number of passengers that survived, \\(711\\) by each of the percentages for the different classes. The counts under Perished were obtained by multiplying the number of passengers that perished, \\(1,490\\) by each of the percentages for the different classes. Class Percentage Survived Perished 1st 0.1476602 104.98637 220.0136 2nd 0.1294866 92.06497 192.9350 3rd 0.3207633 228.06270 477.9373 Crew 0.4020900 285.88596 599.1140 Notice that the above process followed the procedure of dividing the row totals by the total total and then multiplying that percentage to the column totals. Because the order of multiplication and division does not matter, we can phrase this as (row total)(column total) / (total total). These provide the counts we would expect if the null hypothesis were true. When the observed counts differ dramatically from the expected counts, we will reject the null hypothesis. To measure how dramatically the observed counts differ from the expected counts we use the formula \\[ \\chi^2 = \\sum_{i=1}^m \\frac{(O_i - E_i)^2}{E_i} \\] which is the formula that was stated previously. Notice that \\(\\chi^2\\) will become large when the observed counts differ dramatically from the expected counts, and will be relatively small when the expected counts and observed counts are similar. The \\(p\\)-value for the \\(\\chi^2\\) statistic can be calculated using a chi-squared distribution with degrees of freedom equal to the product of (the number of rows - 1) and (the number of columns - 1). Interpretation If the null hypothesis is true, then the interpretation is simple, the two variables are independent. End of story. However, when the null hypothesis is rejected and the alternative is concluded, it becomes interesting to interpret the results because all we know now is that the two variables are somehow associated. One way to interpret the results is to consider the individual values of \\[ \\frac{(O_i-E_i)^2}{E_i} \\] which, when square-rooted are sometimes called the Pearson residuals. \\[ \\sqrt{\\frac{(O_i-E_i)^2}{E_i}} = \\frac{(O_i-E_i)}{\\sqrt{E_i}} \\] The Pearson residuals allow a quick understanding of which observed counts are responsible for the \\(\\chi^2\\) statistic being large. They also show the direction in which the observed counts differ from the expected counts. For the Titanic data the Pearson residuals are given in the following table. Note that the most dramatic depatures in the observed counts (from the expected counts) were for the 1st Class survivors (Pearson residual = 9.566) and the 1st class passengers that perished (Pearson residual = -6.608). These tell us that far more 1st class passengers survived than we would have expected and far fewer perished than we expected. The next interesting scenario is for the Crew members, where more than expected perished and fewer than expected survived. A similar story to a lesser degree is true for the 3rd class passengers. The 2nd class passengers were similar to the 1st class passengers (more survived and less perished than expected) but to a far lesser degree. No Yes 1st -6.607873 9.565772 2nd -1.867159 2.702959 3rd 2.289965 -3.315027 Crew 3.018611 -4.369840 Examples: movies | StudentRatings | HairEyeColor Nonparametric Chi-squared Test Overview The test to perform when all observed counts are at least 1. The hypotheses and interpretation are the same as the Chi-Squared Test of Independence. The distribution of the test statistic is calculated in a nonparametric way. R Instructions Console Help Command: ?chisq.test() chisq.test( chisq.test() is an R function that stands for “Chi-Squared Test”. It performs the Chi-squared test for x. x, x is either a matrix or a table. simulate.p.value=TRUE) EXPLANATION. Pearson’s Chi-squared test with simulated p-value (based on 2000 replicates) EXPLANATION. data: x The data you are using ",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Chi-squared Tests Association testing for two qualitative variables with at least two levels to each variable.", "Chi-squared Test of Independence Overview A method of comparing observed counts to the expected counts in a contingency table to decide if two qualitative variables are associated (alternative hypothesis) or are independent, i.e., not associated (null hypothesis).", "Typically, all expected counts are required to be 5 or greater for the test to be appropriate.", "However, the test is still appropriate if all expected counts are at least 1 and the average of the expected counts is at least 5.", "R Instructions Console Help Command: ?chisq.test() To Run the Test chisq.test(x) x must be a table or a matrix.", "To Make a Table If you have a data set that you want to turn into a table, use the table command.", "x The name of your created table to go in the chisq.test() code.", " <-  This is the “left arrow” assignment operator that stores the results of your table() code into x.", "table( table is an R function used to tabulate how many times each value occrus in a given dataset.", "Dataset$FirstVariable, Use your dataset to call in a row variable.", " Dataset$SecondVariable) Use your dataset to call in a column variable.", "x <- table(mtcars$am, mtcars$cyl) pander(x)   4 6 8 0 3 4 12 1 8 3 2 #Rename rows and columns of your table: rownames(x) <- c(\"Automatic\", \"Manual\") colnames(x) <- c(\"4 cyl\", \"6 cyl\", \"8 cyl\") pander(x)   4 cyl 6 cyl 8 cyl Automatic 3 4 12 Manual 8 3 2 Click open the example code above for details on how to rename rows and columns in a table.", "To Make a Matrix If you have tally counts of your observations, enter them into RStudio using a matrix.", "x The name of your created matrix to go in the chisq.test() code.", " <-  This is the “left arrow” assignment operator that stores the results of your cbind() code into x.", "cbind( cbind stands for”column bind” and is a function that joins together different c() vectors to make them become columns of a table.", "`Farm 1` = The name of the first column you want in the matrix.", "To include spaces in the name, use the back-ticks ` and `.", " c(Pigs = 10, Cats = 5, Dogs = 28, Roosters = 3), Specify each of the values by name in a c(…) function to be stored in your first column, in this case, Farm 1.", " `Farm 2` = The name of the second column you want in the matrix.", " c(Pigs = 15, Cats = 3, Dogs = 8, Roosters = 1) Each new column must contain entries for each named element.", "Technically, you can leave off the names in all but the first column as only the names from the first column will be used.", "pander(x) Print the matrix to the screen using pander(…).", "  Farm 1 Farm 2 Pigs 10 15 Cats 5 3 Dogs 28 8 Roosters 3 1 Or, notice how you could rbind instead with slightly different results… x The name of your created matrix to go in the chisq.test() code.", " <-  This is the “left arrow” assignment operator that stores the results of your cbind() code into x.", "rbind( rbind stands for”row bind” and is a function that joins together different c() vectors to make them become rows of a table.", "`Farm 1` = The name of the first column you want in the matrix.", "To include spaces in the name, use the back-ticks ` and `.", " c(Pigs = 10, Cats = 5, Dogs = 28, Roosters = 3), Specify each of the values by name in a c(…) function to be stored in your first column, in this case, Farm 1.", " `Farm 2` = The name of the second column you want in the matrix.", " c(Pigs = 15, Cats = 3, Dogs = 8, Roosters = 1) Each new column must contain entries for each named element.", "Technically, you can leave off the names in all but the first column as only the names from the first column will be used.", "pander(x) Print the matrix to the screen using pander(…).", "  Pigs Cats Dogs Roosters Farm 1 10 5 28 3 Farm 2 15 3 8 1 Diagnostics yourNamedTestResults This is some name you come up with that will become the R object that stores the results of your Chi-squated test chisq.test() command.", " <-  This is the “left arrow” assignment operator that stores the results of your chisq.test() code into yourNamedTestResults.", "chisq.test( chisq.test() is an R function that stands for “Chi-Squared Test”.", "It performs the Chi-squared test for x.", "x) x is either a matrix or a table.", "Pearson’s Chi-squared test The name of the test being performed.", "data: x The data you are using for the Chi-Squared test.", "x is either a table or matrix that you created.", "X-squared = 9.2956, The test statistic of the test follows a chi-squared distribution with the given degrees of freedom.", "See the end of the “Making Inference” page of the book to learn more about the Chi-squared Parametric Distribution.", " df = 3, The degrees of freedom of the test.", "It is found by (number of rows - 1)x(number of columns - 1)  p-value = 0.02561 The p-value of the test is found by computing the probability of being more extreme than given test statistic.", "In R, you can compute this yourself using the code pchisq(9.2956, 3, lower.tail=FALSE).", "yourNamedTestResults This is some name you come up with that will become the R object that stores the results of your Chi-squared test chisq.test() command.", "$ This allows you to access various elements from the test that was performed.", "expected The expected counts of the chi-squared test.", "These are computed for each observed count by using the equation (row tota)*(column total)/(total total)."],
    "type": "page",
    "page_title": "Chi-squared Tests"
  },
  {
    "id": 48,
    "title": "📍 Chi-squared Test of\r\nIndependence",
    "url": "ChiSquaredTests.html#chisquaredtestofindependence",
    "content": "Chi-squared Test of\r\nIndependence Examples: movies | StudentRatings | HairEyeColor",
    "sentences": "Examples: movies | StudentRatings | HairEyeColor",
    "type": "section",
    "page_title": "Chi-squared Tests",
    "section_title": "Chi-squared Test of\r\nIndependence"
  },
  {
    "id": 49,
    "title": "📍 Overview",
    "url": "ChiSquaredTests.html#overview",
    "content": "Overview A method of comparing observed counts to the expected counts in a contingency table to decide if two qualitative variables are associated (alternative hypothesis) or are independent, i.e., not associated (null hypothesis). Typically, all expected counts are required to be 5 or greater for the test to be appropriate. However, the test is still appropriate if all expected counts are at least 1 and the average of the expected counts is at least 5.",
    "sentences": ["A method of comparing observed counts to the expected counts in a contingency table to decide if two qualitative variables are associated (alternative hypothesis) or are independent, i.e., not associated (null hypothesis).", "Typically, all expected counts are required to be 5 or greater for the test to be appropriate.", "However, the test is still appropriate if all expected counts are at least 1 and the average of the expected counts is at least 5."],
    "type": "section",
    "page_title": "Chi-squared Tests",
    "section_title": "Overview"
  },
  {
    "id": 50,
    "title": "📍 R Instructions",
    "url": "ChiSquaredTests.html#rinstructions",
    "content": "R Instructions Console Help Command: ?chisq.test() To Run the Test chisq.test(x) x must be a table or a matrix. To Make a Table If you have a data set that you want to turn into a table, use the table command. x The name of your created table to go in the chisq.test() code.  <-  This is the “left arrow” assignment operator that stores the results of your table() code into x. table( table is an R function used to tabulate how many times each value occrus in a given dataset. Dataset$FirstVariable, Use your dataset to call in a row variable.  Dataset$SecondVariable) Use your dataset to call in a column variable. x <- table(mtcars$am, mtcars$cyl) pander(x)   4 6 8 0 3 4 12 1 8 3 2 #Rename rows and columns of your table: rownames(x) <- c(\"Automatic\", \"Manual\") colnames(x) <- c(\"4 cyl\", \"6 cyl\", \"8 cyl\") pander(x)   4 cyl 6 cyl 8 cyl Automatic 3 4 12 Manual 8 3 2 Click open the example code above for details on how to rename rows and columns in a table. To Make a Matrix If you have tally counts of your observations, enter them into RStudio using a matrix. x The name of your created matrix to go in the chisq.test() code.  <-  This is the “left arrow” assignment operator that stores the results of your cbind() code into x. cbind( cbind stands for”column bind” and is a function that joins together different c() vectors to make them become columns of a table. `Farm 1` = The name of the first column you want in the matrix. To include spaces in the name, use the back-ticks ` and `.  c(Pigs = 10, Cats = 5, Dogs = 28, Roosters = 3), Specify each of the values by name in a c(…) function to be stored in your first column, in this case, Farm 1.  `Farm 2` = The name of the second column you want in the matrix.  c(Pigs = 15, Cats = 3, Dogs = 8, Roosters = 1) Each new column must contain entries for each named element. Technically, you can leave off the names in all but the first column as only the names from the first column will be used. pander(x) Print the matrix to the screen using pander(…).   Farm 1 Farm 2 Pigs 10 15 Cats 5 3 Dogs 28 8 Roosters 3 1 Or, notice how you could rbind instead with slightly different results… x The name of your created matrix to go in the chisq.test() code.  <-  This is the “left arrow” assignment operator that stores the results of your cbind() code into x. rbind( rbind stands for”row bind” and is a function that joins together different c() vectors to make them become rows of a table. `Farm 1` = The name of the first column you want in the matrix. To include spaces in the name, use the back-ticks ` and `.  c(Pigs = 10, Cats = 5, Dogs = 28, Roosters = 3), Specify each of the values by name in a c(…) function to be stored in your first column, in this case, Farm 1.  `Farm 2` = The name of the second column you want in the matrix.  c(Pigs = 15, Cats = 3, Dogs = 8, Roosters = 1) Each new column must contain entries for each named element. Technically, you can leave off the names in all but the first column as only the names from the first column will be used. pander(x) Print the matrix to the screen using pander(…).   Pigs Cats Dogs Roosters Farm 1 10 5 28 3 Farm 2 15 3 8 1",
    "sentences": ["Console Help Command: ?chisq.test()", "To Run the Test chisq.test(x) x must be a table or a matrix.", "To Make a Table If you have a data set that you want to turn into a table, use the table command.", "x The name of your created table to go in the chisq.test() code.", " <-  This is the “left arrow” assignment operator that stores the results of your table() code into x.", "table( table is an R function used to tabulate how many times each value occrus in a given dataset.", "Dataset$FirstVariable, Use your dataset to call in a row variable.", " Dataset$SecondVariable) Use your dataset to call in a column variable.", "x <- table(mtcars$am, mtcars$cyl) pander(x)   4 6 8 0 3 4 12 1 8 3 2 #Rename rows and columns of your table: rownames(x) <- c(\"Automatic\", \"Manual\") colnames(x) <- c(\"4 cyl\", \"6 cyl\", \"8 cyl\") pander(x)   4 cyl 6 cyl 8 cyl Automatic 3 4 12 Manual 8 3 2 Click open the example code above for details on how to rename rows and columns in a table.", "To Make a Matrix If you have tally counts of your observations, enter them into RStudio using a matrix."],
    "type": "section",
    "page_title": "Chi-squared Tests",
    "section_title": "R Instructions"
  },
  {
    "id": 51,
    "title": "📍 Explanation",
    "url": "ChiSquaredTests.html#explanation",
    "content": "Explanation To demonstrate the theory behind the Chi-squared Test, consider the following data about the survival of the Titanic passengers as a specific example. The rows of the following contingency table (a table of counts) show the Class of the passenger, while the columns show the Survival of the passenger. Note that each passenger is placed in exactly one Class and one level of Survival. Also, the two factors, Class and Survival, are qualitative variables with at least two levels. Thus, the data meets the basic requirements of the Chi-squared Test. No Yes 1st 122 203 2nd 167 118 3rd 528 178 Crew 673 212 The Hypotheses The hypotheses of the Chi-squared Test are written as verbal statements. \\(H_0\\): The row variable and column variable are independent. \\(H_a\\): The row variable and column variable are associated (not independent). For the Titanic data, these hypotheses would be written as \\(H_0\\): Class and Survival are independent. \\(H_a\\): Class and Survival are associated.",
    "sentences": ["To demonstrate the theory behind the Chi-squared Test, consider the following data about the survival of the Titanic passengers as a specific example.", "The rows of the following contingency table (a table of counts) show the Class of the passenger, while the columns show the Survival of the passenger.", "Note that each passenger is placed in exactly one Class and one level of Survival.", "Also, the two factors, Class and Survival, are qualitative variables with at least two levels.", "Thus, the data meets the basic requirements of the Chi-squared Test.", "No Yes 1st 122 203 2nd 167 118 3rd 528 178 Crew 673 212", "The Hypotheses The hypotheses of the Chi-squared Test are written as verbal statements.", "\\(H_0\\): The row variable and column variable are independent.", "\\(H_a\\): The row variable and column variable are associated (not independent).", "For the Titanic data, these hypotheses would be written as \\(H_0\\): Class and Survival are independent."],
    "type": "section",
    "page_title": "Chi-squared Tests",
    "section_title": "Explanation"
  },
  {
    "id": 52,
    "title": "📍 Nonparametric Chi-squared\r\nTest",
    "url": "ChiSquaredTests.html#nonparametricchisquaredtest",
    "content": "Nonparametric Chi-squared\r\nTest Examples: Cowlick",
    "sentences": "Examples: Cowlick",
    "type": "section",
    "page_title": "Chi-squared Tests",
    "section_title": "Nonparametric Chi-squared\r\nTest"
  },
  {
    "id": 53,
    "title": "📍 Overview",
    "url": "ChiSquaredTests.html#overview",
    "content": "Overview The test to perform when all observed counts are at least 1. The hypotheses and interpretation are the same as the Chi-Squared Test of Independence. The distribution of the test statistic is calculated in a nonparametric way.",
    "sentences": ["The test to perform when all observed counts are at least 1.", "The hypotheses and interpretation are the same as the Chi-Squared Test of Independence.", "The distribution of the test statistic is calculated in a nonparametric way."],
    "type": "section",
    "page_title": "Chi-squared Tests",
    "section_title": "Overview"
  },
  {
    "id": 54,
    "title": "📍 R Instructions",
    "url": "ChiSquaredTests.html#rinstructions",
    "content": "R Instructions Console Help Command: ?chisq.test() Pearson’s Chi-squared test with simulated p-value (based on 2000 replicates) EXPLANATION. data: x The data you are using for the Chi-Squared test. x is either a table or matrix that you created. X-squared = 9.2956, EXPLANATION.  df = NA, EXPLANATION.  p-value = 0.02799 EXPLANATION.",
    "sentences": ["Console Help Command: ?chisq.test()", "Pearson’s Chi-squared test with simulated p-value (based on 2000 replicates) EXPLANATION.", "data: x The data you are using for the Chi-Squared test.", "x is either a table or matrix that you created.", "X-squared = 9.2956, EXPLANATION.", " df = NA, EXPLANATION.", " p-value = 0.02799 EXPLANATION."],
    "type": "section",
    "page_title": "Chi-squared Tests",
    "section_title": "R Instructions"
  },
  {
    "id": 55,
    "title": "📍 Explanation",
    "url": "ChiSquaredTests.html#explanation",
    "content": "Explanation The nonparametric chi-squared test is very similar to the chi-squared test. The hypotheses, expected counts, and test statistic are all exactly the same as the chi-squared test. The only difference is that the test statistic is not assumed to follow a parametric chi-squared distribution. The Hypotheses \\(H_0\\): The row variable and column variable are independent. \\(H_a\\): The row variable and column variable are associated. The Model The test statistic for the Nonparametric Chi-squared Test is the same as the Chi-Squared Test. \\[ \\chi^2 = \\sum_{i=1}^m \\frac{(O_i - E_i)^2}{E_i} \\] where \\(O_i\\) represents each of the \\(m\\) observed counts and \\(E_i\\) represents the \\(m\\) expected counts. Note that \\(m = r\\times c\\), were \\(r\\) represents the number of rows in the contingency table and \\(c\\) represents the number of columns. The expected counts are also still obtained by the formula \\[ E_i = \\frac{\\text{(row total)(column total)}}{\\text{(total total)}} \\] However, the \\(\\chi^2\\) test statistic is no longer assumed to follow a chi-squared distribution. Instead, a nonparametric distribution of the \\(\\chi^2\\) test statistic is generated using the following logic using the assumption that the null hypothesis is true. This is how it works.",
    "sentences": ["The nonparametric chi-squared test is very similar to the chi-squared test.", "The hypotheses, expected counts, and test statistic are all exactly the same as the chi-squared test.", "The only difference is that the test statistic is not assumed to follow a parametric chi-squared distribution.", "The Hypotheses \\(H_0\\): The row variable and column variable are independent.", "\\(H_a\\): The row variable and column variable are associated.", "The Model The test statistic for the Nonparametric Chi-squared Test is the same as the Chi-Squared Test.", "\\[ \\chi^2 = \\sum_{i=1}^m \\frac{(O_i - E_i)^2}{E_i} \\] where \\(O_i\\) represents each of the \\(m\\) observed counts and \\(E_i\\) represents the \\(m\\) expected counts.", "Note that \\(m = r\\times c\\), were \\(r\\) represents the number of rows in the contingency table and \\(c\\) represents the number of columns.", "The expected counts are also still obtained by the formula \\[ E_i = \\frac{\\text{(row total)(column total)}}{\\text{(total total)}} \\] However, the \\(\\chi^2\\) test statistic is no longer assumed to follow a chi-squared distribution.", "Instead, a nonparametric distribution of the \\(\\chi^2\\) test statistic is generated using the following logic using the assumption that the null hypothesis is true."],
    "type": "section",
    "page_title": "Chi-squared Tests",
    "section_title": "Explanation"
  },
  {
    "id": 56,
    "title": "Data Sources",
    "url": "DataSources.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Data Sources Kaggle UCI Machine Learning Repository U.S. Open Data Portal EU Open Data Portal Google Public Data Explorer U.S. Health Data Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) Good Example Analysis Poor Example Analysis High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Outlier Theory Assignment UCI Machine Learning Repository UCI Machine Learning Repository Open Data Portal Open Data Portal Google Public Data Explorer Google Public Data Explorer",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Data Sources Kaggle UCI Machine Learning Repository U.S.", "Open Data Portal EU Open Data Portal Google Public Data Explorer U.S.", "Health Data", "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus", "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus", "Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus", "Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425)", "Intermediate Statistics (MATH 325)", "Experimental Design (MATH 326)", "Applied Linear Regression (MATH 425)", "R Help R Commands R Markdown Hints Data Sources Example Analyses", "Describing Data Graphical Summaries Numerical Summaries", "Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization", "Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus", "325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project)", "Good Example Analysis", "Poor Example Analysis", "High School Seniors (Independent Samples t-Test)", "Recalling Words (Wilcoxon Test)", "Food (Kruskal-Wallis Test)", "My Simple Linear Regression", "Weather (Multiple Linear Regression)", "My Logistic Regression", "Testing Center (Consulting Project)", "425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus", "Predicting Rexburg, Idaho Weather", "Residuals, Sums of Squares, and R-Squared", "Acura TL Selling Prices", "Outlier Theory Assignment", "UCI Machine Learning Repository", "UCI Machine Learning Repository", "Open Data Portal", "Open Data Portal", "Google Public Data Explorer", "Google Public Data Explorer"],
    "type": "page",
    "page_title": "Data Sources"
  },
  {
    "id": 57,
    "title": "Getting Started Tutorial",
    "url": "GettingStartedTutorial.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Getting Started Tutorial Student: “How do I learn R?” Teacher: “By using it.” Student: “But I don’t know how to use it.” Teacher: “Just try it anyway. Suddenly you’ll understand.” Most people that are new to using the “R Software” ask the question, “How do I learn R?” The answer is simple: “start using it.” Really. Just start using it, even when you have no idea what you are doing, and suddenly you will start to learn R. So, here we go. The more you use it, the more you will know. This textbook (the “Statistics-Notebook”) follows a simple learning model: Hover your mouse over codeYes, just like that. By hovering over “Codes” you will get instructions on what that code does. to read about it. Click on a line of codeHovering is a good start, try clicking on this one. to see what it does. Try typing the code into RStudio yourself to actually start learning R. (This is the most important step! Avoid copying and pasting codes, and type them instead. The more you type codes yourself, even though it is slow and prone to mistakes, the more you will learn.) In summary, the most successful students in Math 325 follow the pattern: Example Codes For each of the following examples: (1) hover, (2) click, and (3) try. Before you begin working on these Example Codes, ensure you have RStudio open. It should look like this: Example 1 Remember, “Hover” the code first, then click, then try. View( The “View” R function (with a CAPTIAL ‘V’ in View) allows us to view a data set. When run, this function will open up a new tab in RStudio showing the data set. cars cars is a data set that is in R. R has datasets that are available for anyone to use. You can see them using the data() command. It would be good to explore View() for a few different datasets. ) Always be sure to end your function with closing parantheses.     Press Enter to run the code.  Click to Show Tutorial  Click to see a full tutorial on the “View()” command. Good work clicking on Example Code 1. But… Did you remember to first hover your mouse over the View(cars) example code? Good job if you did. If not, go do that right now before continuing. R allows you to work with data. So, the first step to understanding R is to open a dataset and begin exploring some data. If you type the data() command into your Console of RStudio you will see a list of “built in” data sets that come with R. Go ahead and type data() into your R Console (and then press Return or Enter) right now to try it yourself. The cars data set is one of the options that is available within the list from the data() command. This is a fun “historic” data set from the 1920’s. It shows the speed and stopping distance of cars from the 1920’s. To see the cars data set in RStudio, use the View command as follows. Open RStudio. Type the command View(cars) into your Console. Press Enter or Return and the following output should appear within RStudio. Be sure to TRY IT yourself, if you haven’t already. Good work if you did. It’s the only way to learn R. Ask someone for help if you aren’t sure how to get started. Try View(airquality) as well. Example 2 mean( An R function “mean()” that will compute the mean of a quantitative column of data from a data set. cars cars is the name of a data set in R. Any data set can be used instead by simply typing the name of that data set instead of cars. $ The $ sign is a powerful operator in R. The $ sign allows you to access, or “purchase,” any column from a data set. Try typing cars$ into R and notice how a selection menu appears with options dist and speed. dist dist is one of the two columns from the cars data set. By typing cars$dist we are essentially pulling that column of data out of the data set, and then computing the mean of that column with mean(cars$dist). ) Closing parenthesis for the mean() function.     Press Enter to run the code.  Click to Show Output  Click to see output. Did you remember to first hover your mouse over the mean(cars$dist) example code? Good job if you did. If not, go do that right now before continuing. Recall that you “Viewed” the cars data set in the first example code. If you look carefully at this data set, you should notice that there are two columns labeled speed and dist in this data set. Each row of the data set represents a vehicle that was going a certain speed (in miles per hour) and once the breaks were applied, the distance the vehicle took to stop was also recorded (in feet). This data was recorded for 50 different vehicles. So it might make sense to compute something like the average (or mean) distance it took vehicles to stop. This is done with the mean( ) function. Notice how the $ sign is used to access a column called dist from the cars data set. Think of the data set cars as a store, and if you bring your money $ then you can “buy” the column dist from that data set. You may also be interested in trying any of the following: sd(cars$dist) var(cars$dist) median(cars$dist) min(cars$dist) max(cars$dist) length(cars$dist) sum(cars$dist) If you are curious, you can begin exploring the Numerical Summaries page of your Statistics-Notebook to learn more. But we won’t officially get to that as a class until later this week. So you don’t need to worry about it for now. Example 3 plot( The plot(...) function allows us to create a plot (usually a scatterplot) in R. dist dist is the name of a column from the cars data set. This is going to be the Y-variable of the plot. The Y-variable always comes first in the plot(Y ~ X) command.  ~  ~ is found on the upper-left key of your keyboard. It is called the “tilde” or “tilda” symbol. It is used to state a relationship between Y and X: Y ~ X. speed speed is the name of a column. This is going to be the X-variable of the plot. The X-variable always comes after the ~ in the plot(Y ~ X) command. ,  The comma , is used to separate each entry within a command. data=cars The data= statement is used to tell R which data set the columns of “dist” and “speed” come from. In this case, the cars data set. ,  The comma , is used to separate each entry within a command. col=“skyblue” The col= statement is used to tell R which color to use in the plot. Type colors() in your R Console to see what options there are. This code is using the \"skyblue\" color. Color names are always placed in quotes \" \". ,  The comma , is used to separate each entry within a command. pch=16 The pch= statement is used to tell R which plotting character to use in the plot. Type ?pch in your R Console to see what options there are. (You’ll need to scroll down in the help file that appears until you get to the 'pch values' section. ) Closing parenthesis for the plot(…) function.     Press Enter to run the code.  Click to Show Output  Click to View Output. Study the image below. Try changing the col=\"skyblue\" statement to col=\"firebrick\" instead. What do you notice? Try changing the pch=16 statement to pch=4 or any other number from 1 to 25. What do you notice? Hint: pressing the “up” arrow on your keyboard brings up the last command you typed into the Console. You’re all done for now. Completion Code: abc123R Return to the R Commands page of the Statistics-Notebook. This is “the end” of the Getting Started tutorial. To find the “completion code” you will need to study, and “click open” each of the example codes above. Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) Good Example Analysis Poor Example Analysis High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Outlier Theory Assignment Getting Started Tutorial Student: “How do I learn R?” Teacher: “By using it.” Student: “But I don’t know how to use it.” Teacher: “Just try it anyway. Suddenly you’ll understand.” Most people that are new to using the “R Software” ask the question, “How do I learn R?” The answer is simple: “start using it.” Really. Just start using it, even when you have no idea what you are doing, and suddenly you will start to learn R. So, here we go. The more you use it, the more you will know. This textbook (the “Statistics-Notebook”) follows a simple learning model: Hover your mouse over codeYes, just like that. By hovering over “Codes” you will get instructions on what that code does. to read about it. Click on a line of codeHovering is a good start, try clicking on this one. to see what it does. Try typing the code into RStudio yourself to actually start learning R. (This is the most important step! Avoid copying and pasting codes, and type them instead. The more you type codes yourself, even though it is slow and prone to mistakes, the more you will learn.) In summary, the most successful students in Math 325 follow the pattern: For each of the following examples: (1) hover, (2) click, and (3) try. Before you begin working on these Example Codes, ensure you have RStudio open. It should look like this: Remember, “Hover” the code first, then click, then try. View( The “View” R function (with a CAPTIAL ‘V’ in View) allows us to view a data set. When run, this function will open up a new tab in RStudio showing the data set. cars cars is a data set that is in R. R has datasets that are available for anyone to use. You can see them using the data() command. It would be good to explore View() for a few different datasets. ) Always be sure to end your function with closing parantheses.     Press Enter to run the code.  Click to Show Tutorial  Click to see a full tutorial on the “View()” command. View( The “View” R function",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Getting Started Tutorial Student: “How do I learn R?” Teacher: “By using it.” Student: “But I don’t know how to use it.” Teacher: “Just try it anyway.", "Suddenly you’ll understand.” Most people that are new to using the “R Software” ask the question, “How do I learn R?” The answer is simple: “start using it.” Really.", "Just start using it, even when you have no idea what you are doing, and suddenly you will start to learn R.", "So, here we go.", "The more you use it, the more you will know.", "This textbook (the “Statistics-Notebook”) follows a simple learning model: Hover your mouse over codeYes, just like that.", "By hovering over “Codes” you will get instructions on what that code does.", "to read about it.", "Click on a line of codeHovering is a good start, try clicking on this one.", "to see what it does.", "Try typing the code into RStudio yourself to actually start learning R.", "(This is the most important step!", "Avoid copying and pasting codes, and type them instead.", "The more you type codes yourself, even though it is slow and prone to mistakes, the more you will learn.) In summary, the most successful students in Math 325 follow the pattern: Example Codes For each of the following examples: (1) hover, (2) click, and (3) try.", "Before you begin working on these Example Codes, ensure you have RStudio open.", "It should look like this: Example 1 Remember, “Hover” the code first, then click, then try.", "View( The “View” R function (with a CAPTIAL ‘V’ in View) allows us to view a data set.", "When run, this function will open up a new tab in RStudio showing the data set.", "cars cars is a data set that is in R.", "R has datasets that are available for anyone to use.", "You can see them using the data() command.", "It would be good to explore View() for a few different datasets.", ") Always be sure to end your function with closing parantheses.", "    Press Enter to run the code.", " Click to Show Tutorial  Click to see a full tutorial on the “View()” command.", "Good work clicking on Example Code 1.", "But… Did you remember to first hover your mouse over the View(cars) example code?", "Good job if you did.", "If not, go do that right now before continuing.", "R allows you to work with data.", "So, the first step to understanding R is to open a dataset and begin exploring some data.", "If you type the data() command into your Console of RStudio you will see a list of “built in” data sets that come with R.", "Go ahead and type data() into your R Console (and then press Return or Enter) right now to try it yourself.", "The cars data set is one of the options that is available within the list from the data() command.", "This is a fun “historic” data set from the 1920’s.", "It shows the speed and stopping distance of cars from the 1920’s.", "To see the cars data set in RStudio, use the View command as follows.", "Open RStudio.", "Type the command View(cars) into your Console.", "Press Enter or Return and the following output should appear within RStudio.", "Be sure to TRY IT yourself, if you haven’t already.", "Good work if you did.", "It’s the only way to learn R.", "Ask someone for help if you aren’t sure how to get started.", "Try View(airquality) as well.", "Example 2 mean( An R function “mean()” that will compute the mean of a quantitative column of data from a data set.", "cars cars is the name of a data set in R.", "Any data set can be used instead by simply typing the name of that data set instead of cars.", "$ The $ sign is a powerful operator in R.", "The $ sign allows you to access, or “purchase,” any column from a data set."],
    "type": "page",
    "page_title": "Getting Started Tutorial"
  },
  {
    "id": 58,
    "title": "Graphical Summaries",
    "url": "GraphicalSummaries.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Graphical Summaries There are many ways to display data. The fundamental idea is that the graphical depiction of data should communicate the truth the data has to offer about the situation of interest. Histograms 1 Quantitative Variable Overview Great for showing the distribution of data for a single quantitative variable when the sample size is large. Dotplots are a good alternative for smaller sample sizes. Gives a good feel for the mean and standard deviation of the data. R Instructions Base R ggplot2 plotly To make a histogram in R use the function: hist(object) object must be quantitative data. R refers to this as a “numeric vector.” Usually this will be the column of a dataset accessed with the $ sign by hist(dataSetName$columnName). Type ?hist in your R Console to open the help file in R. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic histogram hist An R function “hist” used to create a histogram. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the hist function.     Press Enter to run the code.  …  Click to View Output. Change Color hist(airquality$Temp,  This code was explained in the first example code. col=“skyblue” col= allows us to specify the color of the plot using a named color. The name of the color must be placed in quotations. Type “colors()” in R to see color options. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Add Titles hist(airquality$Temp This part was explained in the first example code. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. col=“skyblue”col= allows us to specify the color of the plot using a named color. The name of the color must be placed in quotations. Type “colors()” in R to see color options. ,  A comma must always be used to separate additional commands. xlab=“Temperature” xlab= stands for “x label.” Use it to specify the text to print on the plot under the x-axis. The desired text must always be in quotations. ,  A comma must always be used to separate additional commands. main=“La Guardia Airport Daily Mean Temperatures” main= lets us specify the “main” title to be placed above the plot. The desired text must always be placed in quotations. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. To make a histogram in R using the ggplot approach, first ensure library(ggplot2) is loaded. Then, ggplot(data, aes(x=column)) +   geom_histogram() data is the name of your dataset. column is a column of data from your dataset that is quantitative. The aesthetic helper function aes(x= ) is how you tell the gpplot to make the x-axis become your column of data. The geometry helper function geom_histogram() causes the ggplot to become a histogram. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic Histogram ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp “x=” declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_histogram() The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used.     Press Enter to run the code.  …  Click to View Output. Change Bin Width and Color ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp “x=” declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_histogram( The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used. binwidth=5,  The “binwidth” command controls the width of the bars in the histogram. fill=“skyblue”, The “fill” command controls the color of the insides of each bar. color=“black” The “color” command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function.     Press Enter to run the code.  …  Click to View Output. Add Titles ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp “x=” declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_histogram( The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used. binwidth=5,  The “binwidth” command controls the width of the bars in the histogram. fill=“skyblue”, The “fill” command controls the color of the insides of each bar. color=“black” The “color” command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function.  +  The addition symbol + is used to add further elements to the ggplot.    labs( The “labs” function is used to add labels to the plot, like a main title, x-label and y-label. title=“La Guardia Airport Daily Mean Temperature”,  The “title=” command allows you to control the main title at the top of the graphic. x=“Temperature”,  The “x=” command allows you to control the x-label of the graphic. y=“Number of Days” The “y=” command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function.     Press Enter to run the code.  …  Click to View Output. Gallery See some ideas from past students… Show/Hide Gallery Hover to see code. Copy the code into R. Play with it. Modify it to create your own graph. ggplot(airquality, aes(x = Temp)) + geom_histogram(binwidth=5, fill = \"skyblue\", color = \"black\") + labs(title = \"La Guardia Airport Daily Mean Temperature\",    x = \"Temperature\", y = \"Number of Days\") + theme_bw() + theme(plot.title = element_text(size = 14, face = \"bold\", hjust = .5),    panel.grid.major = element_line(color = \"grey80\")) ggplot(airquality) + geom_histogram(aes(x = Temp, fill = Temp > median(Temp)), color = \"black\", binwidth = 2) + labs(title = \"La Guardia Airport Daily Mean Temperature\",    x = \"Temperature\", y = \"Number of Days\") + theme_bw() + theme(plot.title = element_text(size = 14, face = \"bold\", hjust = .5),    panel.grid.major = element_line(color = \"grey80\"),    legend.position = \"none\") + geom_vline(xintercept = 72, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_vline(xintercept = 85, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 71, y = 10, label = \"Q1\") + geom_text(x = 86, y = 10, label = \"Q3\") ggplot(airquality, aes(x = Temp)) + geom_histogram(aes(fill = Temp > median(Temp)), color = \"black\", binwidth = 2) + labs(title = \"La Guardia Airport Daily Mean Temperature\",    x = \"Temperature\", y = \"Number of Days\", fill = \"Temperature\") + theme_bw() + theme(plot.title = element_text(size = 10, face = \"bold\", hjust = .5),    panel.grid.major = element_line(color = \"grey80\")) + scale_fill_manual(values = c(\"steelblue2\",\"brown1\"),labels=c(\"< 79 Degrees\",\"> 79 Degrees\")) + geom_vline(xintercept = 72, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_vline(xintercept = 85, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 71, y = 10, label = \"Q1\") + geom_text(x = 86, y = 10, label = \"Q3\") ggplot(airquality, aes(x = Temp)) + geom_histogram(aes(fill = Temp > median(Temp)), color = \"black\", binwidth = 3) + labs(title = \"Temperature of La Guardia Airport by Month\",    x = \"Temperature\", y = \"Number of Days\", fill = \"Temperature\") + theme_bw() + theme(plot.title = element_text(size = 16, face = \"bold\", hjust = .5),    panel.grid.major = element_line(color = \"grey80\")) + scale_fill_manual(values = c(\"steelblue2\",\"brown1\"),labels=c(\"< 79 Degrees\",\"> 79 Degrees\")) + facet_grid(~Month) + geom_vline(xintercept = 77.88, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 69, y = 10, label = \"Mean = \\n 77.88\") To make a histogram in plotly first load library(plotly) Then, use the function: plot_ly(dataName, x=~columnName, type=\"histogram\") dataName is the name of a data set columnName must be the name of a column of quantitative data. R refers to this as a “numeric vector.” type=\"histogram\" tells the plot_ly(…) function to create a histogram. Visit plotly.com/r/histograms for more details. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic histogram plot_ly An R function “plot_ly” from library(plotly) used to create any plotly plot. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality,  “airquality” is a dataset. Type “View(airquality)” in R to see it. x= The x= allows us to declare which column of the data set will become the x-axis of the histogram. ~Temp,   “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. The ~ is required before column names inside all plot_ly(…) commands. type=“histogram” This option tells the plot_ly(…) function what “type” of graph to make. In this case, a histogram. )Closing parenthsis for the plot_ly function.     Press Enter to run the code.  …  Click to View Output. Change Color plot_ly(airquality, x=~Temp, type=“histogram”,  This code was explained in the first example code. marker=list( this “list(…)” of options that will be specified will effect the bars of the histogram. color = “skyblue”,  this will change the color of the bars to skyblue. line = list(,  this opens a list of options to specify for the “lines” around the “markers.” color = “darkgray”,  this will change the color of the lines around the bars to darkgray. width = 2 this will change the width of the lines around the bars to 2 pixels. Too really see what this does, change it to something crazy like 10. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Add Titles plot_ly(airquality$Temp, type=“histogram”,  This code was explained in the first example code. marker=list( this “list(” of options that will be specified will effect the bars of the histogram. color = “skyblue”,  this will change the color of the bars to skyblue. line = list(,  this opens a list of options to specify for the “lines” around the “markers.” color = “darkgray”,  this will change the color of the lines around the bars to darkgray. width = 10 this will change the width of the lines around the bars to 10 pixels, which is rather large really. Using a width=2 is probably better. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis.  %>% The pipe operator passes the completed plot_ly(…) code into the layout(…) function. layout( The layout(…) function is used for specifying details about the axes and their labels. title=“La Guardia Airport Daily Mean Temperatures” This declares a main title for the top of the graph. xaxis=list( This declares a list of options to be specified for the xaxis. The same can be done for the yaxis(…). title=“Temperature in Degrees F” This declares a title underneath the x-axis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Explanation Histograms group data that are close to each other into “bins” (the vertical bars in the plot). The height of a bin is determined by the number of data points that are contained within the bin. For example, if we group together all the sections of the book of scripture known as the Doctrine and Covenants that occurred in a given year (Jan. 1st - Dec. 31st) then we get the following counts. Year Number of Sections 1823 1 1824 0 1825 0 1826 0 1827 0 1828 1 1829 16 1830 19 1831 37 1832 16 1833 12 1834 5 1835 3 1836 4 1837 1 1838 8 1839 3 1840 0 1841 3 1842 2 1843 ",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Graphical Summaries There are many ways to display data.", "The fundamental idea is that the graphical depiction of data should communicate the truth the data has to offer about the situation of interest.", "Histograms 1 Quantitative Variable Overview Great for showing the distribution of data for a single quantitative variable when the sample size is large.", "Dotplots are a good alternative for smaller sample sizes.", "Gives a good feel for the mean and standard deviation of the data.", "R Instructions Base R ggplot2 plotly To make a histogram in R use the function: hist(object) object must be quantitative data.", "R refers to this as a “numeric vector.” Usually this will be the column of a dataset accessed with the $ sign by hist(dataSetName$columnName).", "Type ?hist in your R Console to open the help file in R.", "Example Code Hover your mouse over the example codes to learn more.", "Click on them to see what they create.", "Basic histogram hist An R function “hist” used to create a histogram.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset.", "Type “View(airquality)” in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.", ")Closing parenthsis for the hist function.", "    Press Enter to run the code.", " …  Click to View Output.", "Change Color hist(airquality$Temp,  This code was explained in the first example code.", "col=“skyblue” col= allows us to specify the color of the plot using a named color.", "The name of the color must be placed in quotations.", "Type “colors()” in R to see color options.", ") Functions always end with a closing parenthesis.", "    Press Enter to run the code.", " …  Click to View Output.", "Add Titles hist(airquality$Temp This part was explained in the first example code.", ",  The comma allows us to specify optional commands to the function.", "The space after the comma is not required.", "It just looks nice.", "col=“skyblue”col= allows us to specify the color of the plot using a named color.", "The name of the color must be placed in quotations.", "Type “colors()” in R to see color options.", ",  A comma must always be used to separate additional commands.", "xlab=“Temperature” xlab= stands for “x label.” Use it to specify the text to print on the plot under the x-axis.", "The desired text must always be in quotations.", ",  A comma must always be used to separate additional commands.", "main=“La Guardia Airport Daily Mean Temperatures” main= lets us specify the “main” title to be placed above the plot.", "The desired text must always be placed in quotations.", ") Functions must always end with a closing parenthesis.", "    Press Enter to run the code.", " …  Click to View Output.", "To make a histogram in R using the ggplot approach, first ensure library(ggplot2) is loaded.", "Then, ggplot(data, aes(x=column)) +   geom_histogram() data is the name of your dataset.", "column is a column of data from your dataset that is quantitative.", "The aesthetic helper function aes(x= ) is how you tell the gpplot to make the x-axis become your column of data.", "The geometry helper function geom_histogram() causes the ggplot to become a histogram.", "Example Code Hover your mouse over the example codes to learn more.", "Click on them to see what they create."],
    "type": "page",
    "page_title": "Graphical Summaries"
  },
  {
    "id": 59,
    "title": "📍 Histograms",
    "url": "GraphicalSummaries.html#histograms",
    "content": "Histograms 1 Quantitative Variable Overview Great for showing the distribution of data for a single quantitative variable when the sample size is large. Dotplots are a good alternative for smaller sample sizes. Gives a good feel for the mean and standard deviation of the data. R Instructions Base R ggplot2 plotly To make a histogram in R use the function: hist(object) object must be quantitative data. R refers to this as a “numeric vector.” Usually this will be the column of a dataset accessed with the $ sign by hist(dataSetName$columnName). Type ?hist in your R Console to open the help file in R. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic histogram hist An R function “hist” used to create a histogram. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the hist function.     Press Enter to run the code.  …  Click to View Output. Change Color hist(airquality$Temp,  This code was explained in the first example code. col=“skyblue” col= allows us to specify the color of the plot using a named color. The name of the color must be placed in quotations. Type “colors()” in R to see color options. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Add Titles hist(airquality$Temp This part was explained in the first example code. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. col=“skyblue”col= allows us to specify the color of the plot using a named color. The name of the color must be placed in quotations. Type “colors()” in R to see color options. ,  A comma must always be used to separate additional commands. xlab=“Temperature” xlab= stands for “x label.” Use it to specify the text to print on the plot under the x-axis. The desired text must always be in quotations. ,  A comma must always be used to separate additional commands. main=“La Guardia Airport Daily Mean Temperatures” main= lets us specify the “main” title to be placed above the plot. The desired text must always be placed in quotations. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. To make a histogram in R using the ggplot approach, first ensure library(ggplot2) is loaded. Then, ggplot(data, aes(x=column)) +   geom_histogram() data is the name of your dataset. column is a column of data from your dataset that is quantitative. The aesthetic helper function aes(x= ) is how you tell the gpplot to make the x-axis become your column of data. The geometry helper function geom_histogram() causes the ggplot to become a histogram. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic Histogram ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp “x=” declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_histogram() The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used.     Press Enter to run the code.  …  Click to View Output. Change Bin Width and Color ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp “x=” declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_histogram( The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used. binwidth=5,  The “binwidth” command controls the width of the bars in the histogram. fill=“skyblue”, The “fill” command controls the color of the insides of each bar. color=“black” The “color” command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function.     Press Enter to run the code.  …  Click to View Output. Add Titles ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp “x=” declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_histogram( The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used. binwidth=5,  The “binwidth” command controls the width of the bars in the histogram. fill=“skyblue”, The “fill” command controls the color of the insides of each bar. color=“black” The “color” command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function.  +  The addition symbol + is used to add further elements to the ggplot.    labs( The “labs” function is used to add labels to the plot, like a main title, x-label and y-label. title=“La Guardia Airport Daily Mean Temperature”,  The “title=” command allows you to control the main title at the top of the graphic. x=“Temperature”,  The “x=” command allows you to control the x-label of the graphic. y=“Number of Days” The “y=” command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function.     Press Enter to run the code.  …  Click to View Output. Gallery See some ideas from past students… Show/Hide Gallery Hover to see code. Copy the code into R. Play with it. Modify it to create your own graph. ggplot(airquality, aes(x = Temp)) + geom_histogram(binwidth=5, fill = \"skyblue\", color = \"black\") + labs(title = \"La Guardia Airport Daily Mean Temperature\",    x = \"Temperature\", y = \"Number of Days\") + theme_bw() + theme(plot.title = element_text(size = 14, face = \"bold\", hjust = .5),    panel.grid.major = element_line(color = \"grey80\")) ggplot(airquality) + geom_histogram(aes(x = Temp, fill = Temp > median(Temp)), color = \"black\", binwidth = 2) + labs(title = \"La Guardia Airport Daily Mean Temperature\",    x = \"Temperature\", y = \"Number of Days\") + theme_bw() + theme(plot.title = element_text(size = 14, face = \"bold\", hjust = .5),    panel.grid.major = element_line(color = \"grey80\"),    legend.position = \"none\") + geom_vline(xintercept = 72, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_vline(xintercept = 85, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 71, y = 10, label = \"Q1\") + geom_text(x = 86, y = 10, label = \"Q3\") ggplot(airquality, aes(x = Temp)) + geom_histogram(aes(fill = Temp > median(Temp)), color = \"black\", binwidth = 2) + labs(title = \"La Guardia Airport Daily Mean Temperature\",    x = \"Temperature\", y = \"Number of Days\", fill = \"Temperature\") + theme_bw() + theme(plot.title = element_text(size = 10, face = \"bold\", hjust = .5),    panel.grid.major = element_line(color = \"grey80\")) + scale_fill_manual(values = c(\"steelblue2\",\"brown1\"),labels=c(\"< 79 Degrees\",\"> 79 Degrees\")) + geom_vline(xintercept = 72, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_vline(xintercept = 85, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 71, y = 10, label = \"Q1\") + geom_text(x = 86, y = 10, label = \"Q3\") ggplot(airquality, aes(x = Temp)) + geom_histogram(aes(fill = Temp > median(Temp)), color = \"black\", binwidth = 3) + labs(title = \"Temperature of La Guardia Airport by Month\",    x = \"Temperature\", y = \"Number of Days\", fill = \"Temperature\") + theme_bw() + theme(plot.title = element_text(size = 16, face = \"bold\", hjust = .5),    panel.grid.major = element_line(color = \"grey80\")) + scale_fill_manual(values = c(\"steelblue2\",\"brown1\"),labels=c(\"< 79 Degrees\",\"> 79 Degrees\")) + facet_grid(~Month) + geom_vline(xintercept = 77.88, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 69, y = 10, label = \"Mean = \\n 77.88\") To make a histogram in plotly first load library(plotly) Then, use the function: plot_ly(dataName, x=~columnName, type=\"histogram\") dataName is the name of a data set columnName must be the name of a column of quantitative data. R refers to this as a “numeric vector.” type=\"histogram\" tells the plot_ly(…) function to create a histogram. Visit plotly.com/r/histograms for more details. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic histogram plot_ly An R function “plot_ly” from library(plotly) used to create any plotly plot. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality,  “airquality” is a dataset. Type “View(airquality)” in R to see it. x= The x= allows us to declare which column of the data set will become the x-axis of the histogram. ~Temp,   “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. The ~ is required before column names inside all plot_ly(…) commands. type=“histogram” This option tells the plot_ly(…) function what “type” of graph to make. In this case, a histogram. )Closing parenthsis for the plot_ly function.     Press Enter to run the code.  …  Click to View Output. Change Color plot_ly(airquality, x=~Temp, type=“histogram”,  This code was explained in the first example code. marker=list( this “list(…)” of options that will be specified will effect the bars of the histogram. color = “skyblue”,  this will change the color of the bars to skyblue. line = list(,  this opens a list of options to specify for the “lines” around the “markers.” color = “darkgray”,  this will change the color of the lines around the bars to darkgray. width = 2 this will change the width of the lines around the bars to 2 pixels. Too really see what this does, change it to something crazy like 10. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Add Titles plot_ly(airquality$Temp, type=“histogram”,  This code was explained in the first example code. marker=list( this “list(” of options that will be specified will effect the bars of the histogram. color = “skyblue”,  this will change the color of the bars to skyblue. line = list(,  this opens a list of options to specify for the “lines” around the “markers.” color = “darkgray”,  this will change the color of the lines around the bars to darkgray. width = 10 this will change the width of the lines around the bars to 10 pixels, which is rather large really. Using a width=2 is probably better. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis.  %>% The pipe operator passes the completed plot_ly(…) code into the layout(…) function. layout( The layout(…) function is used for specifying details about the axes and their labels. title=“La Guardia Airport Daily Mean Temperatures” This declares a main title for the top of the graph. xaxis=list( This declares a list of options to be specified for the xaxis. The same can be done for the yaxis(…). title=“Temperature in Degrees F” This declares a title underneath the x-axis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output.",
    "sentences": ["1 Quantitative Variable", "Overview Great for showing the distribution of data for a single quantitative variable when the sample size is large.", "Dotplots are a good alternative for smaller sample sizes.", "Gives a good feel for the mean and standard deviation of the data.", "R Instructions Base R ggplot2 plotly To make a histogram in R use the function: hist(object) object must be quantitative data.", "R refers to this as a “numeric vector.” Usually this will be the column of a dataset accessed with the $ sign by hist(dataSetName$columnName).", "Type ?hist in your R Console to open the help file in R.", "Example Code Hover your mouse over the example codes to learn more.", "Click on them to see what they create.", "Basic histogram hist An R function “hist” used to create a histogram."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Histograms"
  },
  {
    "id": 60,
    "title": "📍 Overview",
    "url": "GraphicalSummaries.html#overview",
    "content": "Overview Great for showing the distribution of data for a single quantitative variable when the sample size is large. Dotplots are a good alternative for smaller sample sizes. Gives a good feel for the mean and standard deviation of the data.",
    "sentences": ["Great for showing the distribution of data for a single quantitative variable when the sample size is large.", "Dotplots are a good alternative for smaller sample sizes.", "Gives a good feel for the mean and standard deviation of the data."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 61,
    "title": "📍 R Instructions",
    "url": "GraphicalSummaries.html#rinstructions",
    "content": "R Instructions Base R ggplot2 plotly To make a histogram in R use the function: hist(object) object must be quantitative data. R refers to this as a “numeric vector.” Usually this will be the column of a dataset accessed with the $ sign by hist(dataSetName$columnName). Type ?hist in your R Console to open the help file in R. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic histogram hist An R function “hist” used to create a histogram. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the hist function.     Press Enter to run the code.  …  Click to View Output. Change Color hist(airquality$Temp,  This code was explained in the first example code. col=“skyblue” col= allows us to specify the color of the plot using a named color. The name of the color must be placed in quotations. Type “colors()” in R to see color options. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Add Titles hist(airquality$Temp This part was explained in the first example code. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. col=“skyblue”col= allows us to specify the color of the plot using a named color. The name of the color must be placed in quotations. Type “colors()” in R to see color options. ,  A comma must always be used to separate additional commands. xlab=“Temperature” xlab= stands for “x label.” Use it to specify the text to print on the plot under the x-axis. The desired text must always be in quotations. ,  A comma must always be used to separate additional commands. main=“La Guardia Airport Daily Mean Temperatures” main= lets us specify the “main” title to be placed above the plot. The desired text must always be placed in quotations. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. To make a histogram in R using the ggplot approach, first ensure library(ggplot2) is loaded. Then, ggplot(data, aes(x=column)) +   geom_histogram() data is the name of your dataset. column is a column of data from your dataset that is quantitative. The aesthetic helper function aes(x= ) is how you tell the gpplot to make the x-axis become your column of data. The geometry helper function geom_histogram() causes the ggplot to become a histogram. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic Histogram ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp “x=” declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_histogram() The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used.     Press Enter to run the code.  …  Click to View Output. Change Bin Width and Color ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp “x=” declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_histogram( The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used. binwidth=5,  The “binwidth” command controls the width of the bars in the histogram. fill=“skyblue”, The “fill” command controls the color of the insides of each bar. color=“black” The “color” command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function.     Press Enter to run the code.  …  Click to View Output. Add Titles ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp “x=” declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_histogram( The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used. binwidth=5,  The “binwidth” command controls the width of the bars in the histogram. fill=“skyblue”, The “fill” command controls the color of the insides of each bar. color=“black” The “color” command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function.  +  The addition symbol + is used to add further elements to the ggplot.    labs( The “labs” function is used to add labels to the plot, like a main title, x-label and y-label. title=“La Guardia Airport Daily Mean Temperature”,  The “title=” command allows you to control the main title at the top of the graphic. x=“Temperature”,  The “x=” command allows you to control the x-label of the graphic. y=“Number of Days” The “y=” command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function.     Press Enter to run the code.  …  Click to View Output. Gallery See some ideas from past students… Show/Hide Gallery Hover to see code. Copy the code into R. Play with it. Modify it to create your own graph. ggplot(airquality, aes(x = Temp)) + geom_histogram(binwidth=5, fill = \"skyblue\", color = \"black\") + labs(title = \"La Guardia Airport Daily Mean Temperature\",    x = \"Temperature\", y = \"Number of Days\") + theme_bw() + theme(plot.title = element_text(size = 14, face = \"bold\", hjust = .5),    panel.grid.major = element_line(color = \"grey80\")) ggplot(airquality) + geom_histogram(aes(x = Temp, fill = Temp > median(Temp)), color = \"black\", binwidth = 2) + labs(title = \"La Guardia Airport Daily Mean Temperature\",    x = \"Temperature\", y = \"Number of Days\") + theme_bw() + theme(plot.title = element_text(size = 14, face = \"bold\", hjust = .5),    panel.grid.major = element_line(color = \"grey80\"),    legend.position = \"none\") + geom_vline(xintercept = 72, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_vline(xintercept = 85, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 71, y = 10, label = \"Q1\") + geom_text(x = 86, y = 10, label = \"Q3\") ggplot(airquality, aes(x = Temp)) + geom_histogram(aes(fill = Temp > median(Temp)), color = \"black\", binwidth = 2) + labs(title = \"La Guardia Airport Daily Mean Temperature\",    x = \"Temperature\", y = \"Number of Days\", fill = \"Temperature\") + theme_bw() + theme(plot.title = element_text(size = 10, face = \"bold\", hjust = .5),    panel.grid.major = element_line(color = \"grey80\")) + scale_fill_manual(values = c(\"steelblue2\",\"brown1\"),labels=c(\"< 79 Degrees\",\"> 79 Degrees\")) + geom_vline(xintercept = 72, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_vline(xintercept = 85, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 71, y = 10, label = \"Q1\") + geom_text(x = 86, y = 10, label = \"Q3\") ggplot(airquality, aes(x = Temp)) + geom_histogram(aes(fill = Temp > median(Temp)), color = \"black\", binwidth = 3) + labs(title = \"Temperature of La Guardia Airport by Month\",    x = \"Temperature\", y = \"Number of Days\", fill = \"Temperature\") + theme_bw() + theme(plot.title = element_text(size = 16, face = \"bold\", hjust = .5),    panel.grid.major = element_line(color = \"grey80\")) + scale_fill_manual(values = c(\"steelblue2\",\"brown1\"),labels=c(\"< 79 Degrees\",\"> 79 Degrees\")) + facet_grid(~Month) + geom_vline(xintercept = 77.88, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 69, y = 10, label = \"Mean = \\n 77.88\") To make a histogram in plotly first load library(plotly) Then, use the function: plot_ly(dataName, x=~columnName, type=\"histogram\") dataName is the name of a data set columnName must be the name of a column of quantitative data. R refers to this as a “numeric vector.” type=\"histogram\" tells the plot_ly(…) function to create a histogram. Visit plotly.com/r/histograms for more details. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic histogram plot_ly An R function “plot_ly” from library(plotly) used to create any plotly plot. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality,  “airquality” is a dataset. Type “View(airquality)” in R to see it. x= The x= allows us to declare which column of the data set will become the x-axis of the histogram. ~Temp,   “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. The ~ is required before column names inside all plot_ly(…) commands. type=“histogram” This option tells the plot_ly(…) function what “type” of graph to make. In this case, a histogram. )Closing parenthsis for the plot_ly function.     Press Enter to run the code.  …  Click to View Output. Change Color plot_ly(airquality, x=~Temp, type=“histogram”,  This code was explained in the first example code. marker=list( this “list(…)” of options that will be specified will effect the bars of the histogram. color = “skyblue”,  this will change the color of the bars to skyblue. line = list(,  this opens a list of options to specify for the “lines” around the “markers.” color = “darkgray”,  this will change the color of the lines around the bars to darkgray. width = 2 this will change the width of the lines around the bars to 2 pixels. Too really see what this does, change it to something crazy like 10. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Add Titles plot_ly(airquality$Temp, type=“histogram”,  This code was explained in the first example code. marker=list( this “list(” of options that will be specified will effect the bars of the histogram. color = “skyblue”,  this will change the color of the bars to skyblue. line = list(,  this opens a list of options to specify for the “lines” around the “markers.” color = “darkgray”,  this will change the color of the lines around the bars to darkgray. width = 10 this will change the width of the lines around the bars to 10 pixels, which is rather large really. Using a width=2 is probably better. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis.  %>% The pipe operator passes the completed plot_ly(…) code into the layout(…) function. layout( The layout(…) function is used for specifying details about the axes and their labels. title=“La Guardia Airport Daily Mean Temperatures” This declares a main title for the top of the graph. xaxis=list( This declares a list of options to be specified for the xaxis. The same can be done for the yaxis(…). title=“Temperature in Degrees F” This declares a title underneath the x-axis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output.",
    "sentences": ["Base R ggplot2 plotly To make a histogram in R use the function: hist(object) object must be quantitative data.", "R refers to this as a “numeric vector.” Usually this will be the column of a dataset accessed with the $ sign by hist(dataSetName$columnName).", "Type ?hist in your R Console to open the help file in R.", "Example Code Hover your mouse over the example codes to learn more.", "Click on them to see what they create.", "Basic histogram hist An R function “hist” used to create a histogram.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset.", "Type “View(airquality)” in R to see it."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 62,
    "title": "📍 Explanation",
    "url": "GraphicalSummaries.html#explanation",
    "content": "Explanation Histograms group data that are close to each other into “bins” (the vertical bars in the plot). The height of a bin is determined by the number of data points that are contained within the bin. For example, if we group together all the sections of the book of scripture known as the Doctrine and Covenants that occurred in a given year (Jan. 1st - Dec. 31st) then we get the following counts. Year Number of Sections 1823 1 1824 0 1825 0 1826 0 1827 0 1828 1 1829 16 1830 19 1831 37 1832 16 1833 12 1834 5 1835 3 1836 4 1837 1 1838 8 1839 3 1840 0 1841 3 1842 2 1843 4 1844 1 1845 0 1846 0 1847 1 *Note that Section 138 occurred in 1918 and is removed from this example. In this example, each “bin” spans 365 days (Jan. 1 - Dec. 31 of each year). Since “dates” can be used as quantitative data, it makes sense to make a histogram of these data. (Remember, histograms are only for quantitative data.) Notice in the bins above that the left edge of the bin is on the year the data corresponds with. The right edge of the bin lands on the following year. For example, the first bin has left edge on 1823 and right edge on 1824. Since there was one revelation in 1823, this bin has a height of 1. The bin that has 1831 on the left and 1832 on the right shows that 37 revelations occurred in 1831. It is powerful to notice the amount of revelations occurring around 1830, the year the Church of Jesus Christ of Latter-day Saints was organized.",
    "sentences": ["Histograms group data that are close to each other into “bins” (the vertical bars in the plot).", "The height of a bin is determined by the number of data points that are contained within the bin.", "For example, if we group together all the sections of the book of scripture known as the Doctrine and Covenants that occurred in a given year (Jan. 1st - Dec. 31st) then we get the following counts.", "Year Number of Sections 1823 1 1824 0 1825 0 1826 0 1827 0 1828 1 1829 16 1830 19 1831 37 1832 16 1833 12 1834 5 1835 3 1836 4 1837 1 1838 8 1839 3 1840 0 1841 3 1842 2 1843 4 1844 1 1845 0 1846 0 1847 1 *Note that Section 138 occurred in 1918 and is removed from this example.", "In this example, each “bin” spans 365 days (Jan. 1 - Dec. 31 of each year).", "Since “dates” can be used as quantitative data, it makes sense to make a histogram of these data.", "(Remember, histograms are only for quantitative data.) Notice in the bins above that the left edge of the bin is on the year the data corresponds with.", "The right edge of the bin lands on the following year.", "For example, the first bin has left edge on 1823 and right edge on 1824.", "Since there was one revelation in 1823, this bin has a height of 1."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Explanation"
  },
  {
    "id": 63,
    "title": "📍 Boxplots",
    "url": "GraphicalSummaries.html#boxplots",
    "content": "Boxplots 1 Quantitative Variable | 2+ Groups Overview Graphical depiction of the five-number summary. Great for comparing the distributions of data across several groups or categories. Provides a quick visual understanding of the location of the median as well as the range of the data. Can be useful in showing outliers. Sample size should be larger than at least five, or computing the five-number summary is not very meaningful. Side-by-side dotplots are a good alternative for smaller sample sizes. R Instructions Base R ggplot2 plotly To make a boxplot in R use the function: boxplot(object) To make side-by-side boxplots: boxplot(object ~ group, data=NameOfYourData, ...) object must be quantitative data. R refers to this as a “numeric vector.” group must be qualitative data. R refers to this as either a “character vector” or a “factor.” However, a “numeric vector” can also act as a qualitative variable. NameOfYourData is the name of the dataset containing object and group. implies there are many other options that can be given to the boxplot() function. Type ?boxplot in your R Console for more details. Example Code Basic Single Boxplot boxplot An R function “boxplot” used to create boxplots. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code.  …  Click to View Output. More Useful… Basic Side-by-Side Boxplot boxplot An R function “boxplot” used to create boxplots. ( Parenthesis to begin the function. Must touch the last letter of the function. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.  ~  The ~ is used to tell R that you want one boxplot of the quantitative variable (“Temp”) for each group found in the qualitative variable (“Month”). Month “Month” is a qualitative variable (in this case a “numeric vector” defining months by 5, 6, 7, 8, and 9) from the “airquality” dataset. , The “,” is required to start specifying additional commands for the “boxplot()” function. data=airquality data= is used to tell R that the “Temp” and “Month” variables are located in the airquality dataset. Without this, R will not know where to find “Temp” and “Month” and the command will give an error. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Add Names under each Box boxplot An R function “boxplot” used to create boxplots. ( Parenthesis to begin the function. Must touch the last letter of the function. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.  ~  The ~ is used to tell R that you want one boxplot of the quantitative variable (“Temp”) for each group found in the qualitative variable (“Month”). Month “Month” is a qualitative variable (in this case a “numeric vector” defining months by 5, 6, 7, 8, and 9) from the “airquality” dataset. , The “,” is required to start specifying additional commands for the “boxplot()” function. data=airquality data= is used to tell R that the “Temp” and “Month” variables are located in the airquality dataset. Without this, R will not know where to find “Temp” and “Month” and the command will give an error. , The “,” is required to start specifying additional commands for the “boxplot()” function. names=c(“May”,“June”,“July”,“Aug”,“Sep”) names= is used to tell R what labels to place on the x-axis below each boxplot. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Add Color and Labels boxplot(Temp ~ Month, data=airquality This code was explained in the previous example code. ,  The comma is used to separate each additional command to a function. xlab=“Month of the Year” xlab= stands for “x label.” Use it to specify the text to print on the plot under the x-axis. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. ylab=“Temperature” ylab= stands for “y label.” Use it to specify the text to print on the plot next to the y-axis. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. main=“La Guardia Airport Daily Temperatures” main= stands for the “main label” of the plot, which is placed at the top center of the plot. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. col=“wheat” col= stands for the “color” of the plot. The color name “wheat” is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. To make a boxplot in R using the ggplot approach, first ensure library(ggplot2) is loaded. Then, ggplot(data, aes(x=groupsColumn, y=dataColumn) +   geom_boxplot() data is the name of your dataset. groupsColumn is a column of data from your dataset that is qualitative and represents the groups that should each have a boxplot. dataColumn is a column of data from your dataset that is quantitative. The aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your groupsColumn of data, the y-axis become your dataColumn. Note if groupsColumn is not a factor, use factor(groupsColumn) instead. The geometry helper function geom_boxplot() causes the ggplot to become a boxplot. Example Code Basic Single Boxplot ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the y-axis should become. y=Temp “y=” declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_boxplot() The “geom_boxplot()” function causes the ggplot to become a boxplot. There are many other “geom_” functions that could be used.     Press Enter to run the code.  …  Click to View Output. Side-by-side Boxplot and Color Change ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=factor(Month),  “x=” declares which variable will become the x-axis of the graphic. Since Month is “numeric” we must use “factor(Month)” instead of just “Month”. y=Temp “y=” declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_boxplot( The “geom_boxplot()” function causes the ggplot to become a boxplot. There are many other “geom_” functions that could be used. fill=“skyblue”,  The “fill” command controls the color of the insides of each box in the boxplot. color=“black” The “color” command controls the color of the edges of each box. )Closing parenthsis for the geom_boxplot function.     Press Enter to run the code.  …  Click to View Output. Add Labels ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=factor(Month),  “x=” declares which variable will become the x-axis of the graphic. Since Month is “numeric” we must use “factor(Month)” instead of just “Month”. y=Temp “y=” declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_boxplot( The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used. fill=“skyblue”,  The “fill” command controls the color of the insides of each box. color=“black” The “color” command controls the color of the edges of each box. )Closing parenthsis for the geom_boxplot function.  +  The addition symbol + is used to add further elements to the ggplot.    labs( The “labs” function is used to add labels to the plot, like a main title, x-label and y-label. title=“La Guardia Airport Daily Mean Temperature”,  The “title=” command allows you to control the main title at the top of the graphic. x=“Month of the Year”,  The “x=” command allows you to control the x-label of the graphic. y=“Daily Mean Temperature” The “y=” command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function.     Press Enter to run the code.  …  Click to View Output. Gallery See what past students have done… Click to view. Hover to see code. ggplot(data = mtcars, aes(x = as.factor(cyl), y = mpg, fill=as.factor(cyl))) + geom_boxplot() + stat_summary(fun.y = mean, geom = \"errorbar\", aes(ymax = ..y.., ymin = ..y..),    width = .75, linetype = \"dashed\", color=\"firebrick\") + theme_light() + theme(panel.grid.major=element_blank()) + scale_fill_brewer(palette=\"Dark2\") + geom_jitter(width=0.1, height=0) + labs(title = \"Miles Per Gallon Based on Cylinders\",    x=\"Number of Cylinders\",    fill=\"Cylinders\",    y=\"Miles Per Gallon\") ggplot(data = ToothGrowth, aes(x = as.factor(dose), y = len, fill=as.factor(dose))) + geom_boxplot( ) + facet_wrap(~supp) + theme_bw() + scale_fill_manual(values=c(\"#999999\", \"#E69F00\", \"#56B4E9\")) + geom_jitter(width=0.1, height=0) + labs(title = \"Tooth Length Based on Doses    According to Supplement Type\",    fill=\"Doses\",    x=\"Dosage Amount(mg)\",    y=\"Tooth Length\" ) To make a histogram in plotly first load library(plotly) Then, use the function: plot_ly(dataName, y=~columnNameY, x=~columnNameX, type=\"box\") dataName is the name of a data set columnNameY must be the name of a column of quantitative data. R refers to this as a “numeric vector.” This will become the y-axis of the plot. columnNameX must be the name of a column of qualitative data. This will provide the “groups” forming each individual box in the boxplot. type=\"box\" tells the plot_ly(…) function to create a boxplot. Visit plotly.com/r/box-plots for more details. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic Boxplot plot_ly An R function “plot_ly” from library(plotly) used to create any plotly plot. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality,  “airquality” is a dataset. Type “View(airquality)” in R to see it. y= The y= allows us to declare which column of the data set will become the y-axis of the boxplot. In other words, the quantitative data we are interested in studying for each group. ~Temp,   “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. The ~ is required before column names inside all plot_ly(…) commands. x= The x= allows us to declare which column of the data set will become the x-axis of the boxplot. In other words, the “groups” forming each separate box in the boxplot. ~as.factor(Month),   since “Month” is a quantitative variable (numeric vector) from the “airquality” dataset we have to change it to a “factor” which forces R to treat it as a qualitative (groups) variable. The ~ is required before column names inside all plot_ly(…) commands. type=“box” This option tells the plot_ly(…) function what “type” of graph to make. In this case, a boxplot. )Closing parenthsis for the plot_ly function.     Press Enter to run the code.  …  Click to View Output. Change Color plot_ly(airquality, y=~Temp, x=~as.factor(Month), type=“box”,  This code was explained in the first example code. fillcolor=“skyblue”,  this changes the fill color of the boxes in the boxplot to the color specified, in this case “skyblue.” line=list(color=“darkgray”, width=3),  this “list(…)” of options that will be specified will effect the edges of the boxes in the boxplot. We are changing their color to “darkgray” and their width to 3 pixels wide. marker=list( this “list(…)” of options that will be specified will effect the outlying dots shown in the boxplots beyond the “fences” of each box. color = “orange”,  this will change the color of the dots to orange. line = list(,  this opens a list of options to specify for the “lines” around the “markers.” color = “red”,  this will change the color of the lines around the outlier dots to red. width = 1 this will change the width of the lines around the outlier dots to 1 pixel. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Add Titles plot_ly(airquality, y=~Temp, x=~as.factor(Month), type=“box”, fillcolor=“skyblue”, line=list(color=“darkgray”, width=3), marker = list(color=“orange”, line = list(color=“red”, width=1)))  This code was explained in the above example code. %>% the pipe operator sends the completed plot_ly(…) code into the layout function. layout( The layout(…) function is used for specifying details about the axes and their labels. title=“La Guardia Airport Daily Mean Temperatures” This declares a main title for the top of the graph. xaxis=list( This declares a list of options to be specified for the xaxis. The same can be done for the yaxis(…). title=“Month of the Year” This declares a title underneath the x-axis. ),  Functions always end with a closing parenthesis. yaxis=list( This declares a list of options to be specified for the y-axis. title=“Temperature in Degrees F” This declares a title beside the y-axis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output.",
    "sentences": ["1 Quantitative Variable | 2+ Groups", "Overview Graphical depiction of the five-number summary.", "Great for comparing the distributions of data across several groups or categories.", "Provides a quick visual understanding of the location of the median as well as the range of the data.", "Can be useful in showing outliers.", "Sample size should be larger than at least five, or computing the five-number summary is not very meaningful.", "Side-by-side dotplots are a good alternative for smaller sample sizes.", "R Instructions Base R ggplot2 plotly To make a boxplot in R use the function: boxplot(object) To make side-by-side boxplots: boxplot(object ~ group, data=NameOfYourData, ...) object must be quantitative data.", "R refers to this as a “numeric vector.” group must be qualitative data.", "R refers to this as either a “character vector” or a “factor.” However, a “numeric vector” can also act as a qualitative variable."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Boxplots"
  },
  {
    "id": 64,
    "title": "📍 Overview",
    "url": "GraphicalSummaries.html#overview",
    "content": "Overview Graphical depiction of the five-number summary. Great for comparing the distributions of data across several groups or categories. Provides a quick visual understanding of the location of the median as well as the range of the data. Can be useful in showing outliers. Sample size should be larger than at least five, or computing the five-number summary is not very meaningful. Side-by-side dotplots are a good alternative for smaller sample sizes.",
    "sentences": ["Graphical depiction of the five-number summary.", "Great for comparing the distributions of data across several groups or categories.", "Provides a quick visual understanding of the location of the median as well as the range of the data.", "Can be useful in showing outliers.", "Sample size should be larger than at least five, or computing the five-number summary is not very meaningful.", "Side-by-side dotplots are a good alternative for smaller sample sizes."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 65,
    "title": "📍 R Instructions",
    "url": "GraphicalSummaries.html#rinstructions",
    "content": "R Instructions Base R ggplot2 plotly To make a boxplot in R use the function: boxplot(object) To make side-by-side boxplots: boxplot(object ~ group, data=NameOfYourData, ...) object must be quantitative data. R refers to this as a “numeric vector.” group must be qualitative data. R refers to this as either a “character vector” or a “factor.” However, a “numeric vector” can also act as a qualitative variable. NameOfYourData is the name of the dataset containing object and group. implies there are many other options that can be given to the boxplot() function. Type ?boxplot in your R Console for more details. Example Code Basic Single Boxplot boxplot An R function “boxplot” used to create boxplots. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code.  …  Click to View Output. More Useful… Basic Side-by-Side Boxplot boxplot An R function “boxplot” used to create boxplots. ( Parenthesis to begin the function. Must touch the last letter of the function. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.  ~  The ~ is used to tell R that you want one boxplot of the quantitative variable (“Temp”) for each group found in the qualitative variable (“Month”). Month “Month” is a qualitative variable (in this case a “numeric vector” defining months by 5, 6, 7, 8, and 9) from the “airquality” dataset. , The “,” is required to start specifying additional commands for the “boxplot()” function. data=airquality data= is used to tell R that the “Temp” and “Month” variables are located in the airquality dataset. Without this, R will not know where to find “Temp” and “Month” and the command will give an error. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Add Names under each Box boxplot An R function “boxplot” used to create boxplots. ( Parenthesis to begin the function. Must touch the last letter of the function. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.  ~  The ~ is used to tell R that you want one boxplot of the quantitative variable (“Temp”) for each group found in the qualitative variable (“Month”). Month “Month” is a qualitative variable (in this case a “numeric vector” defining months by 5, 6, 7, 8, and 9) from the “airquality” dataset. , The “,” is required to start specifying additional commands for the “boxplot()” function. data=airquality data= is used to tell R that the “Temp” and “Month” variables are located in the airquality dataset. Without this, R will not know where to find “Temp” and “Month” and the command will give an error. , The “,” is required to start specifying additional commands for the “boxplot()” function. names=c(“May”,“June”,“July”,“Aug”,“Sep”) names= is used to tell R what labels to place on the x-axis below each boxplot. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Add Color and Labels boxplot(Temp ~ Month, data=airquality This code was explained in the previous example code. ,  The comma is used to separate each additional command to a function. xlab=“Month of the Year” xlab= stands for “x label.” Use it to specify the text to print on the plot under the x-axis. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. ylab=“Temperature” ylab= stands for “y label.” Use it to specify the text to print on the plot next to the y-axis. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. main=“La Guardia Airport Daily Temperatures” main= stands for the “main label” of the plot, which is placed at the top center of the plot. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. col=“wheat” col= stands for the “color” of the plot. The color name “wheat” is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. To make a boxplot in R using the ggplot approach, first ensure library(ggplot2) is loaded. Then, ggplot(data, aes(x=groupsColumn, y=dataColumn) +   geom_boxplot() data is the name of your dataset. groupsColumn is a column of data from your dataset that is qualitative and represents the groups that should each have a boxplot. dataColumn is a column of data from your dataset that is quantitative. The aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your groupsColumn of data, the y-axis become your dataColumn. Note if groupsColumn is not a factor, use factor(groupsColumn) instead. The geometry helper function geom_boxplot() causes the ggplot to become a boxplot. Example Code Basic Single Boxplot ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the y-axis should become. y=Temp “y=” declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_boxplot() The “geom_boxplot()” function causes the ggplot to become a boxplot. There are many other “geom_” functions that could be used.     Press Enter to run the code.  …  Click to View Output. Side-by-side Boxplot and Color Change ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=factor(Month),  “x=” declares which variable will become the x-axis of the graphic. Since Month is “numeric” we must use “factor(Month)” instead of just “Month”. y=Temp “y=” declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_boxplot( The “geom_boxplot()” function causes the ggplot to become a boxplot. There are many other “geom_” functions that could be used. fill=“skyblue”,  The “fill” command controls the color of the insides of each box in the boxplot. color=“black” The “color” command controls the color of the edges of each box. )Closing parenthsis for the geom_boxplot function.     Press Enter to run the code.  …  Click to View Output. Add Labels ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=factor(Month),  “x=” declares which variable will become the x-axis of the graphic. Since Month is “numeric” we must use “factor(Month)” instead of just “Month”. y=Temp “y=” declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_boxplot( The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used. fill=“skyblue”,  The “fill” command controls the color of the insides of each box. color=“black” The “color” command controls the color of the edges of each box. )Closing parenthsis for the geom_boxplot function.  +  The addition symbol + is used to add further elements to the ggplot.    labs( The “labs” function is used to add labels to the plot, like a main title, x-label and y-label. title=“La Guardia Airport Daily Mean Temperature”,  The “title=” command allows you to control the main title at the top of the graphic. x=“Month of the Year”,  The “x=” command allows you to control the x-label of the graphic. y=“Daily Mean Temperature” The “y=” command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function.     Press Enter to run the code.  …  Click to View Output. Gallery See what past students have done… Click to view. Hover to see code. ggplot(data = mtcars, aes(x = as.factor(cyl), y = mpg, fill=as.factor(cyl))) + geom_boxplot() + stat_summary(fun.y = mean, geom = \"errorbar\", aes(ymax = ..y.., ymin = ..y..),    width = .75, linetype = \"dashed\", color=\"firebrick\") + theme_light() + theme(panel.grid.major=element_blank()) + scale_fill_brewer(palette=\"Dark2\") + geom_jitter(width=0.1, height=0) + labs(title = \"Miles Per Gallon Based on Cylinders\",    x=\"Number of Cylinders\",    fill=\"Cylinders\",    y=\"Miles Per Gallon\") ggplot(data = ToothGrowth, aes(x = as.factor(dose), y = len, fill=as.factor(dose))) + geom_boxplot( ) + facet_wrap(~supp) + theme_bw() + scale_fill_manual(values=c(\"#999999\", \"#E69F00\", \"#56B4E9\")) + geom_jitter(width=0.1, height=0) + labs(title = \"Tooth Length Based on Doses    According to Supplement Type\",    fill=\"Doses\",    x=\"Dosage Amount(mg)\",    y=\"Tooth Length\" ) To make a histogram in plotly first load library(plotly) Then, use the function: plot_ly(dataName, y=~columnNameY, x=~columnNameX, type=\"box\") dataName is the name of a data set columnNameY must be the name of a column of quantitative data. R refers to this as a “numeric vector.” This will become the y-axis of the plot. columnNameX must be the name of a column of qualitative data. This will provide the “groups” forming each individual box in the boxplot. type=\"box\" tells the plot_ly(…) function to create a boxplot. Visit plotly.com/r/box-plots for more details. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic Boxplot plot_ly An R function “plot_ly” from library(plotly) used to create any plotly plot. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality,  “airquality” is a dataset. Type “View(airquality)” in R to see it. y= The y= allows us to declare which column of the data set will become the y-axis of the boxplot. In other words, the quantitative data we are interested in studying for each group. ~Temp,   “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. The ~ is required before column names inside all plot_ly(…) commands. x= The x= allows us to declare which column of the data set will become the x-axis of the boxplot. In other words, the “groups” forming each separate box in the boxplot. ~as.factor(Month),   since “Month” is a quantitative variable (numeric vector) from the “airquality” dataset we have to change it to a “factor” which forces R to treat it as a qualitative (groups) variable. The ~ is required before column names inside all plot_ly(…) commands. type=“box” This option tells the plot_ly(…) function what “type” of graph to make. In this case, a boxplot. )Closing parenthsis for the plot_ly function.     Press Enter to run the code.  …  Click to View Output. Change Color plot_ly(airquality, y=~Temp, x=~as.factor(Month), type=“box”,  This code was explained in the first example code. fillcolor=“skyblue”,  this changes the fill color of the boxes in the boxplot to the color specified, in this case “skyblue.” line=list(color=“darkgray”, width=3),  this “list(…)” of options that will be specified will effect the edges of the boxes in the boxplot. We are changing their color to “darkgray” and their width to 3 pixels wide. marker=list( this “list(…)” of options that will be specified will effect the outlying dots shown in the boxplots beyond the “fences” of each box. color = “orange”,  this will change the color of the dots to orange. line = list(,  this opens a list of options to specify for the “lines” around the “markers.” color = “red”,  this will change the color of the lines around the outlier dots to red. width = 1 this will change the width of the lines around the outlier dots to 1 pixel. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Add Titles plot_ly(airquality, y=~Temp, x=~as.factor(Month), type=“box”, fillcolor=“skyblue”, line=list(color=“darkgray”, width=3), marker = list(color=“orange”, line = list(color=“red”, width=1)))  This code was explained in the above example code. %>% the pipe operator sends the completed plot_ly(…) code into the layout function. layout( The layout(…) function is used for specifying details about the axes and their labels. title=“La Guardia Airport Daily Mean Temperatures” This declares a main title for the top of the graph. xaxis=list( This declares a list of options to be specified for the xaxis. The same can be done for the yaxis(…). title=“Month of the Year” This declares a title underneath the x-axis. ),  Functions always end with a closing parenthesis. yaxis=list( This declares a list of options to be specified for the y-axis. title=“Temperature in Degrees F” This declares a title beside the y-axis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output.",
    "sentences": ["Base R ggplot2 plotly To make a boxplot in R use the function: boxplot(object) To make side-by-side boxplots: boxplot(object ~ group, data=NameOfYourData, ...) object must be quantitative data.", "R refers to this as a “numeric vector.” group must be qualitative data.", "R refers to this as either a “character vector” or a “factor.” However, a “numeric vector” can also act as a qualitative variable.", "NameOfYourData is the name of the dataset containing object and group.", "implies there are many other options that can be given to the boxplot() function.", "Type ?boxplot in your R Console for more details.", "Example Code Basic Single Boxplot boxplot An R function “boxplot” used to create boxplots.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 66,
    "title": "📍 Explanation",
    "url": "GraphicalSummaries.html#explanation",
    "content": "Explanation Understanding how a boxplot is created is the best way to understand what the boxplot shows. How Boxplots are Made The five-number summary is computed. A box is drawn with one edge located at the first quartile and the opposite edge located at the third quartile. This box is then divided into two boxes by placing another line inside the box at the location of the median. The maximum value and minimum value are marked on the plot. Whiskers are drawn from the first quartile out towards the minimum and from the third quartile out towards the maximum. If the minimum or maximum is too far away, then the whisker is ended early. Any points beyond the line ending the whisker are marked on the plot as dots. This helps identify possible outliers in the data.",
    "sentences": ["Understanding how a boxplot is created is the best way to understand what the boxplot shows.", "How Boxplots are Made The five-number summary is computed.", "A box is drawn with one edge located at the first quartile and the opposite edge located at the third quartile.", "This box is then divided into two boxes by placing another line inside the box at the location of the median.", "The maximum value and minimum value are marked on the plot.", "Whiskers are drawn from the first quartile out towards the minimum and from the third quartile out towards the maximum.", "If the minimum or maximum is too far away, then the whisker is ended early.", "Any points beyond the line ending the whisker are marked on the plot as dots.", "This helps identify possible outliers in the data."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Explanation"
  },
  {
    "id": 67,
    "title": "📍 Dot Plots",
    "url": "GraphicalSummaries.html#dotplots",
    "content": "Dot Plots 1 Quantitative Variable | 2+ Groups Overview Depicts the actual values of each data point. Best for small sample sizes or for datasets where there are lots of repeated values. Histograms or boxplots are better alternatives for large sample sizes when there are few repeated values. Great for comparing the distribution of data across several groups or categories. R Instructions Base R ggplot2 plotly To make a dot plot in Base R use the code: stripchart(object) For side-by-side dotplots: stripchart(object ~ group, data=NameOfYourData) object must be a quantitative (or ordinal) variable, what R refers to as a “numeric vector.” group is a qualitative variable, which in R can be either a “character vector” or a “factor.” NameOfYourData is the name of the dataset containing object and group. Example Code stripchart An R function “stripchart” used to create a dot plot. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. , The “,” is required to start specifying additional commands for the function. method=“stack”method= allows us to choose from the options “overplot”, “jitter”, and “stack”. The “stack” option stacks mutliple points that occur at the same location on top of each other. You can try the code yourself to see what “overplot” and “jitter” do. )Closing parenthsis for the function.     Press Enter to run the code.  …  Click to View Output. stripchart An R function “stripchart” used to create dot plots. ( Parenthesis to begin the function. Must touch the last letter of the function. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.  ~  The ~ is used to tell R that you want a dot plot of the quantitative variable (“Temp”) for each group found in the qualitative variable (“Month”). Month “Month” is a qualitative variable (in this case a “numeric vector” defining months by 5, 6, 7, 8, and 9) from the “airquality” dataset. , The “,” is required to start specifying additional commands for the function. data=airquality data= is used to tell R that the “Temp” and “Month” variables are located in the airquality dataset. Without this, R will not know where to find “Temp” and “Month” and the command will give an error. , The “,” is required to start specifying additional commands for the function. method=“stack”method= allows us to choose from the options “overplot”, “jitter”, and “stack”. The “stack” option stacks mutliple points that occur at the same location on top of each other. You can try the code yourself to see what “overplot” and “jitter” do. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. stripchart(Temp ~ Month This part of the code was explained already in the example code directly above this one. , The “,” is required to start specifying additional commands for the function. data=airquality data= is used to tell R that the “Temp” and “Month” variables are located in the airquality dataset. Without this, R will not know where to find “Temp” and “Month” and the command will give an error. , The “,” is required to start specifying additional commands for the function. method=“stack”method= allows us to choose from the options “overplot”, “jitter”, and “stack”. The “stack” option stacks mutliple points that occur at the same location on top of each other. You can try the code yourself to see what “overplot” and “jitter” do. ,  The comma is used to separate each additional command to a function. ylab=“Month of the Year” ylab= stands for “y label.” Use it to specify the text to print on the plot next to the y-axis. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. xlab=“Temperature” xlab= stands for “x label.” Use it to specify the text to print on the plot below the x-axis. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. main=“La Guardia Airport Daily Temperatures” main= stands for the “main label” of the plot, which is placed at the top center of the plot. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. col=“sienna” col= stands for the “color” of the plot. The color name “sienna” is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. ,  The comma is used to separate each additional command to a function. pch=16 pch= stands for the “plotting character” of the plot. This plot uses the filled circle (option 16) as the plotting character. The options are 0, 1, 2, …, 25. Type ?pch in the R Console, and scroll down the help file half way to see what each option does. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. To make a dot plot in R using the ggplot approach, first ensure: library(ggplot2) is loaded. Then, ggplot(data, aes(x=groupsColumn, y=dataColumn) +   geom_dotplot() data is the name of your dataset. groupsColumn is a column of data from your dataset that is qualitative and represents the groups that should each have a boxplot. dataColumn is a column of data from your dataset that is quantitative. The aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your groupsColumn of data, the y-axis become your dataColumn. Note if groupsColumn is not a factor, use factor(groupsColumn) instead. The geometry helper function geom_dotplot() causes the ggplot to become a dot plot. Example Code Click to view. Hover to learn. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the y-axis should become. x=Temp “x=” declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_dotplot() The “geom_dotplot()” function causes the ggplot to become a dot plot. There are many other “geom_” functions that could be used.     Press Enter to run the code.  …  Click to View Output. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=factor(Month),  “x=” declares which variable will become the x-axis of the graphic. Use factor(Month) to change “Month”, which is numeric, into categories. y=Temp “y=” declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_dotplot( The “geom_dotplot()” function causes the ggplot to become a dot plot. There are many other “geom_” functions that could be used. binaxis = “y”,  This tells the function that the y=Temp statement should be used as the quantitative data. stackdir = “up”,  This causes the dots to be stacked on top of each other. position = “dodge”,  This causes the dots to not overalap, i.e., “dodge each other.” dotsize = 0.75,  Controls the size of the dots. You can make them larger with numbers greater than 1 and smaller with numbers less than 1. binwidth = 0.5 Controls how the dots are grouped, similar to the bins in a histogram. )Closing parenthsis for the geom_dotplot function.     Press Enter to run the code.  …  Click to View Output. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=factor(Month),  “x=” declares which variable will become the x-axis of the graphic. Use factor(Month) to change “Month”, which is numeric, into categories. y=Temp “y=” declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    coord_flip( ) The “coord_flip()” function causes the ggplot to reverse the axes when drawing the plot. However, all commands must be given as if the plot were to be drawn without coord_flip(), then coord_flip() is applied.  +  The addition symbol + is used to add further elements to the ggplot.    geom_dotplot( The “geom_dotplot()” function causes the ggplot to become a dot plot. There are many other “geom_” functions that could be used. binaxis = “y”,  This tells the function that the y=Temp statement should be used as the quantitative data. stackdir = “up”,  This causes the dots to be stacked on top of each other. position = “dodge”,  This causes the dots to not overalap, i.e., “dodge each other.” dotsize = 0.75,  Controls the size of the dots. You can make them larger with numbers greater than 1 and smaller with numbers less than 1. binwidth = 0.5 Controls how the dots are grouped, similar to the bins in a histogram. )Closing parenthsis for the geom_dotplot function.  +  The addition symbol + is used to add further elements to the ggplot.    labs( The “labs” function is used to add labels to the plot, like a main title, x-label and y-label. title=“La Guardia Airport Daily Mean Temperature”,  The “title=” command allows you to control the main title at the top of the graphic. x=“Month of the Year”,  The “x=” command allows you to control the x-label of the graphic. y=“Daily Mean Temperature” The “y=” command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function.     Press Enter to run the code.  …  Click to View Output. Not yet available.",
    "sentences": ["1 Quantitative Variable | 2+ Groups", "Overview Depicts the actual values of each data point.", "Best for small sample sizes or for datasets where there are lots of repeated values.", "Histograms or boxplots are better alternatives for large sample sizes when there are few repeated values.", "Great for comparing the distribution of data across several groups or categories.", "R Instructions Base R ggplot2 plotly To make a dot plot in Base R use the code: stripchart(object) For side-by-side dotplots: stripchart(object ~ group, data=NameOfYourData) object must be a quantitative (or ordinal) variable, what R refers to as a “numeric vector.” group is a qualitative variable, which in R can be either a “character vector” or a “factor.” NameOfYourData is the name of the dataset containing object and group.", "Example Code stripchart An R function “stripchart” used to create a dot plot.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Dot Plots"
  },
  {
    "id": 68,
    "title": "📍 Overview",
    "url": "GraphicalSummaries.html#overview",
    "content": "Overview Depicts the actual values of each data point. Best for small sample sizes or for datasets where there are lots of repeated values. Histograms or boxplots are better alternatives for large sample sizes when there are few repeated values. Great for comparing the distribution of data across several groups or categories.",
    "sentences": ["Depicts the actual values of each data point.", "Best for small sample sizes or for datasets where there are lots of repeated values.", "Histograms or boxplots are better alternatives for large sample sizes when there are few repeated values.", "Great for comparing the distribution of data across several groups or categories."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 69,
    "title": "📍 R Instructions",
    "url": "GraphicalSummaries.html#rinstructions",
    "content": "R Instructions Base R ggplot2 plotly To make a dot plot in Base R use the code: stripchart(object) For side-by-side dotplots: stripchart(object ~ group, data=NameOfYourData) object must be a quantitative (or ordinal) variable, what R refers to as a “numeric vector.” group is a qualitative variable, which in R can be either a “character vector” or a “factor.” NameOfYourData is the name of the dataset containing object and group. Example Code stripchart An R function “stripchart” used to create a dot plot. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. , The “,” is required to start specifying additional commands for the function. method=“stack”method= allows us to choose from the options “overplot”, “jitter”, and “stack”. The “stack” option stacks mutliple points that occur at the same location on top of each other. You can try the code yourself to see what “overplot” and “jitter” do. )Closing parenthsis for the function.     Press Enter to run the code.  …  Click to View Output. stripchart An R function “stripchart” used to create dot plots. ( Parenthesis to begin the function. Must touch the last letter of the function. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.  ~  The ~ is used to tell R that you want a dot plot of the quantitative variable (“Temp”) for each group found in the qualitative variable (“Month”). Month “Month” is a qualitative variable (in this case a “numeric vector” defining months by 5, 6, 7, 8, and 9) from the “airquality” dataset. , The “,” is required to start specifying additional commands for the function. data=airquality data= is used to tell R that the “Temp” and “Month” variables are located in the airquality dataset. Without this, R will not know where to find “Temp” and “Month” and the command will give an error. , The “,” is required to start specifying additional commands for the function. method=“stack”method= allows us to choose from the options “overplot”, “jitter”, and “stack”. The “stack” option stacks mutliple points that occur at the same location on top of each other. You can try the code yourself to see what “overplot” and “jitter” do. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. stripchart(Temp ~ Month This part of the code was explained already in the example code directly above this one. , The “,” is required to start specifying additional commands for the function. data=airquality data= is used to tell R that the “Temp” and “Month” variables are located in the airquality dataset. Without this, R will not know where to find “Temp” and “Month” and the command will give an error. , The “,” is required to start specifying additional commands for the function. method=“stack”method= allows us to choose from the options “overplot”, “jitter”, and “stack”. The “stack” option stacks mutliple points that occur at the same location on top of each other. You can try the code yourself to see what “overplot” and “jitter” do. ,  The comma is used to separate each additional command to a function. ylab=“Month of the Year” ylab= stands for “y label.” Use it to specify the text to print on the plot next to the y-axis. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. xlab=“Temperature” xlab= stands for “x label.” Use it to specify the text to print on the plot below the x-axis. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. main=“La Guardia Airport Daily Temperatures” main= stands for the “main label” of the plot, which is placed at the top center of the plot. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. col=“sienna” col= stands for the “color” of the plot. The color name “sienna” is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. ,  The comma is used to separate each additional command to a function. pch=16 pch= stands for the “plotting character” of the plot. This plot uses the filled circle (option 16) as the plotting character. The options are 0, 1, 2, …, 25. Type ?pch in the R Console, and scroll down the help file half way to see what each option does. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. To make a dot plot in R using the ggplot approach, first ensure: library(ggplot2) is loaded. Then, ggplot(data, aes(x=groupsColumn, y=dataColumn) +   geom_dotplot() data is the name of your dataset. groupsColumn is a column of data from your dataset that is qualitative and represents the groups that should each have a boxplot. dataColumn is a column of data from your dataset that is quantitative. The aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your groupsColumn of data, the y-axis become your dataColumn. Note if groupsColumn is not a factor, use factor(groupsColumn) instead. The geometry helper function geom_dotplot() causes the ggplot to become a dot plot. Example Code Click to view. Hover to learn. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the y-axis should become. x=Temp “x=” declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_dotplot() The “geom_dotplot()” function causes the ggplot to become a dot plot. There are many other “geom_” functions that could be used.     Press Enter to run the code.  …  Click to View Output. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=factor(Month),  “x=” declares which variable will become the x-axis of the graphic. Use factor(Month) to change “Month”, which is numeric, into categories. y=Temp “y=” declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_dotplot( The “geom_dotplot()” function causes the ggplot to become a dot plot. There are many other “geom_” functions that could be used. binaxis = “y”,  This tells the function that the y=Temp statement should be used as the quantitative data. stackdir = “up”,  This causes the dots to be stacked on top of each other. position = “dodge”,  This causes the dots to not overalap, i.e., “dodge each other.” dotsize = 0.75,  Controls the size of the dots. You can make them larger with numbers greater than 1 and smaller with numbers less than 1. binwidth = 0.5 Controls how the dots are grouped, similar to the bins in a histogram. )Closing parenthsis for the geom_dotplot function.     Press Enter to run the code.  …  Click to View Output. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=factor(Month),  “x=” declares which variable will become the x-axis of the graphic. Use factor(Month) to change “Month”, which is numeric, into categories. y=Temp “y=” declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    coord_flip( ) The “coord_flip()” function causes the ggplot to reverse the axes when drawing the plot. However, all commands must be given as if the plot were to be drawn without coord_flip(), then coord_flip() is applied.  +  The addition symbol + is used to add further elements to the ggplot.    geom_dotplot( The “geom_dotplot()” function causes the ggplot to become a dot plot. There are many other “geom_” functions that could be used. binaxis = “y”,  This tells the function that the y=Temp statement should be used as the quantitative data. stackdir = “up”,  This causes the dots to be stacked on top of each other. position = “dodge”,  This causes the dots to not overalap, i.e., “dodge each other.” dotsize = 0.75,  Controls the size of the dots. You can make them larger with numbers greater than 1 and smaller with numbers less than 1. binwidth = 0.5 Controls how the dots are grouped, similar to the bins in a histogram. )Closing parenthsis for the geom_dotplot function.  +  The addition symbol + is used to add further elements to the ggplot.    labs( The “labs” function is used to add labels to the plot, like a main title, x-label and y-label. title=“La Guardia Airport Daily Mean Temperature”,  The “title=” command allows you to control the main title at the top of the graphic. x=“Month of the Year”,  The “x=” command allows you to control the x-label of the graphic. y=“Daily Mean Temperature” The “y=” command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function.     Press Enter to run the code.  …  Click to View Output. Not yet available.",
    "sentences": ["Base R ggplot2 plotly To make a dot plot in Base R use the code: stripchart(object) For side-by-side dotplots: stripchart(object ~ group, data=NameOfYourData) object must be a quantitative (or ordinal) variable, what R refers to as a “numeric vector.” group is a qualitative variable, which in R can be either a “character vector” or a “factor.” NameOfYourData is the name of the dataset containing object and group.", "Example Code stripchart An R function “stripchart” used to create a dot plot.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset.", "Type “View(airquality)” in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.", ", The “,” is required to start specifying additional commands for the function.", "method=“stack”method= allows us to choose from the options “overplot”, “jitter”, and “stack”."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 70,
    "title": "📍 Scatterplots",
    "url": "GraphicalSummaries.html#scatterplots",
    "content": "Scatterplots 2 Quantitative Variables Overview Depicts the actual values of the data points, which are \\((x,y)\\) pairs. Works well for small or large sample sizes. Visualizes well the correlation between the two variables. Should be used in linear regression contexts whenever possible. R Instructions Base R ggplot2 plotly To make a scatterplot in R use the code: plot(y ~ x, data=NameOfYourData) y is the quantitative response variable, i.e., “numeric vector.” x is the quantitative explanatory variable, i.e., “numeric vector.” NameOfYourData is the name of the dataset containing y and x. Note: plot(object) where object is a “numeric vector” will create a time series plot, which is sometimes useful. Example Code plot An R function “plot” used to create a scatterplot, or in this case a time series plot because only one quantitative variable is being supplied to the function. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. , The “,” is required to start specifying additional commands for the function. type=“l”type= allows us to choose from the options “p” for points, “l” for lines, and “b” for both. There are also other options that could be chosen, type  ?plot in the R Console to learn about them. )Closing parenthsis for the function.     Press Enter to run the code.  …  Click to View Output. plot An R function “plot” used to create a scatterplot. ( Parenthesis to begin the function. Must touch the last letter of the function. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset that is being used as the response variable (y-axis) for this plot.  ~  The ~ is used to tell R that you want a scatterplot with the quantitative variable “Temp” on the y-axis and the qauntitative variable “Month” on the x-axis. Wind “Wind” is a quantitative variable (numeric vector) from the “airquality” dataset that is being used as the explanatory variable (x-axis) for this plot. , The “,” is required to start specifying additional commands for the function. data=airquality data= is used to tell R that the “Temp” and “Month” variables are located in the airquality dataset. Without this, R will not know where to find “Temp” and “Month” and the command will give an error. , The “,” is required to start specifying additional commands for the function. pch=8 pch= stands for the “plotting character” of the plot. This plot uses the star shape (option 8) as the plotting character. The options are 0, 1, 2, …, 25. Type ?pch in the R Console, and scroll down the help file half way to see what each option does. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. plot(Temp ~ Wind This part of the code was explained already in the example code directly above this one. , The “,” is required to start specifying additional commands for the function. data=airquality data= is used to tell R that the “Temp” and “Month” variables are located in the airquality dataset. Without this, R will not know where to find “Temp” and “Month” and the command will give an error. , The “,” is required to start specifying additional commands for the function. xlab=“Daily Wind Speed (mph)” xlab= stands for “x label.” Use it to specify the text to print on the plot below the x-axis. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. ylab=“Temperature” ylab= stands for “y label.” Use it to specify the text to print on the plot next to the y-axis. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. main=“La Guardia Airport (May - Sep)” main= stands for the “main label” of the plot, which is placed at the top center of the plot. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. col=“ivory3” col= stands for the “color” of the plot. The color name “ivory3” is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. ,  The comma is used to separate each additional command to a function. pch=18 pch= stands for the “plotting character” of the plot. This plot uses the filled diamond (option 18) as the plotting character. The options are 0, 1, 2, …, 25. Type ?pch in the R Console, and scroll down the help file half way to see what each option does. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. pch Options To make a scatterplot in R using the ggplot approach, first ensure: library(ggplot2) is loaded. Then, ggplot(data, aes(x=dataColumn1, y=dataColumn2) +   geom_point() data is the name of your dataset. dataColumn1 is a column of data from your dataset that is quantitative and will be used as the explanatory variable. dataColumn2 is a column of data from your dataset that is quantitative and will be used as the response variable. The aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your dataColumn1 of data, the y-axis become your dataColumn2. The geometry helper function geom_point() causes the ggplot to become a scatterplot. Example Code Click to view. Hover to learn. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Wind,  “x=” declares which variable will become the x-axis of the graphic, the explanatory variable. y=Temp “y=” declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_point( The “geom_point()” function causes the ggplot to become a scatterplot. There are many other “geom_” functions that could be used. )Closing parenthsis for the geom_point function.     Press Enter to run the code.  …  Click to View Output. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Wind,  “x=” declares which variable will become the x-axis of the graphic, the explanatory variable. y=Temp “y=” declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_point( The “geom_point()” function causes the ggplot to become a scatterplot. There are many other “geom_” functions that could be used. color = “ivory3”,  Controls the color of the dots. pch = 18 Controls the type of plotting character to be used in the plot. )Closing parenthsis for the geom_point function.  +  The addition symbol + is used to add further elements to the ggplot.    labs( The “labs” function is used to add labels to the plot, like a main title, x-label and y-label. title=“La Guardia Airport (May - Sep)”,  The “title=” command allows you to control the main title at the top of the graphic. x=“Daily Average Wind Speed (mph)”,  The “x=” command allows you to control the x-label of the graphic. y=“Daily Mean Temperature” The “y=” command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function.  +  The addition symbol + is used to add further elements to the ggplot.   theme_bw()Changes the “theme” or look of the plot to “black” and “white”.     Press Enter to run the code.  …  Click to View Output. To make a scatterplot in R using the plotly approach, first ensure: library(plotly) is loaded. Then, plot_ly(data, x= ~dataColumn1, y= ~dataColumn2) data is the name of your dataset. dataColumn1 is a column of data from your dataset that is quantitative and will be used as the explanatory variable. dataColumn2 is a column of data from your dataset that is quantitative and will be used as the response variable. Example Code plot_ly(airquality, x= ~Wind, y= ~Temp) plot_ly(KidsFeet, x= ~length, y= ~width, color= ~sex, size= ~birthmonth, text= ~paste(\"Name:\", name, \"\\n\", \"Birth-Month:\", birthmonth), colors=c(\"skyblue\",\"hotpink\")) %>% layout(title=\"KidsFeet dataset\", xaxis=list(title=\"Length of the longer foot in cm\"), yaxis=list(title=\"Width of the longer foot in cm\"))",
    "sentences": ["2 Quantitative Variables", "Overview Depicts the actual values of the data points, which are \\((x,y)\\) pairs.", "Works well for small or large sample sizes.", "Visualizes well the correlation between the two variables.", "Should be used in linear regression contexts whenever possible.", "R Instructions Base R ggplot2 plotly To make a scatterplot in R use the code: plot(y ~ x, data=NameOfYourData) y is the quantitative response variable, i.e., “numeric vector.” x is the quantitative explanatory variable, i.e., “numeric vector.” NameOfYourData is the name of the dataset containing y and x.", "Note: plot(object) where object is a “numeric vector” will create a time series plot, which is sometimes useful.", "Example Code plot An R function “plot” used to create a scatterplot, or in this case a time series plot because only one quantitative variable is being supplied to the function.", "( Parenthesis to begin the function.", "Must touch the last letter of the function."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Scatterplots"
  },
  {
    "id": 71,
    "title": "📍 Overview",
    "url": "GraphicalSummaries.html#overview",
    "content": "Overview Depicts the actual values of the data points, which are \\((x,y)\\) pairs. Works well for small or large sample sizes. Visualizes well the correlation between the two variables. Should be used in linear regression contexts whenever possible.",
    "sentences": ["Depicts the actual values of the data points, which are \\((x,y)\\) pairs.", "Works well for small or large sample sizes.", "Visualizes well the correlation between the two variables.", "Should be used in linear regression contexts whenever possible."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 72,
    "title": "📍 R Instructions",
    "url": "GraphicalSummaries.html#rinstructions",
    "content": "R Instructions Base R ggplot2 plotly To make a scatterplot in R use the code: plot(y ~ x, data=NameOfYourData) y is the quantitative response variable, i.e., “numeric vector.” x is the quantitative explanatory variable, i.e., “numeric vector.” NameOfYourData is the name of the dataset containing y and x. Note: plot(object) where object is a “numeric vector” will create a time series plot, which is sometimes useful. Example Code plot An R function “plot” used to create a scatterplot, or in this case a time series plot because only one quantitative variable is being supplied to the function. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. , The “,” is required to start specifying additional commands for the function. type=“l”type= allows us to choose from the options “p” for points, “l” for lines, and “b” for both. There are also other options that could be chosen, type  ?plot in the R Console to learn about them. )Closing parenthsis for the function.     Press Enter to run the code.  …  Click to View Output. plot An R function “plot” used to create a scatterplot. ( Parenthesis to begin the function. Must touch the last letter of the function. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset that is being used as the response variable (y-axis) for this plot.  ~  The ~ is used to tell R that you want a scatterplot with the quantitative variable “Temp” on the y-axis and the qauntitative variable “Month” on the x-axis. Wind “Wind” is a quantitative variable (numeric vector) from the “airquality” dataset that is being used as the explanatory variable (x-axis) for this plot. , The “,” is required to start specifying additional commands for the function. data=airquality data= is used to tell R that the “Temp” and “Month” variables are located in the airquality dataset. Without this, R will not know where to find “Temp” and “Month” and the command will give an error. , The “,” is required to start specifying additional commands for the function. pch=8 pch= stands for the “plotting character” of the plot. This plot uses the star shape (option 8) as the plotting character. The options are 0, 1, 2, …, 25. Type ?pch in the R Console, and scroll down the help file half way to see what each option does. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. plot(Temp ~ Wind This part of the code was explained already in the example code directly above this one. , The “,” is required to start specifying additional commands for the function. data=airquality data= is used to tell R that the “Temp” and “Month” variables are located in the airquality dataset. Without this, R will not know where to find “Temp” and “Month” and the command will give an error. , The “,” is required to start specifying additional commands for the function. xlab=“Daily Wind Speed (mph)” xlab= stands for “x label.” Use it to specify the text to print on the plot below the x-axis. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. ylab=“Temperature” ylab= stands for “y label.” Use it to specify the text to print on the plot next to the y-axis. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. main=“La Guardia Airport (May - Sep)” main= stands for the “main label” of the plot, which is placed at the top center of the plot. The desired text must always be contained in quotes. ,  The comma is used to separate each additional command to a function. col=“ivory3” col= stands for the “color” of the plot. The color name “ivory3” is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. ,  The comma is used to separate each additional command to a function. pch=18 pch= stands for the “plotting character” of the plot. This plot uses the filled diamond (option 18) as the plotting character. The options are 0, 1, 2, …, 25. Type ?pch in the R Console, and scroll down the help file half way to see what each option does. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. pch Options To make a scatterplot in R using the ggplot approach, first ensure: library(ggplot2) is loaded. Then, ggplot(data, aes(x=dataColumn1, y=dataColumn2) +   geom_point() data is the name of your dataset. dataColumn1 is a column of data from your dataset that is quantitative and will be used as the explanatory variable. dataColumn2 is a column of data from your dataset that is quantitative and will be used as the response variable. The aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your dataColumn1 of data, the y-axis become your dataColumn2. The geometry helper function geom_point() causes the ggplot to become a scatterplot. Example Code Click to view. Hover to learn. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Wind,  “x=” declares which variable will become the x-axis of the graphic, the explanatory variable. y=Temp “y=” declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_point( The “geom_point()” function causes the ggplot to become a scatterplot. There are many other “geom_” functions that could be used. )Closing parenthsis for the geom_point function.     Press Enter to run the code.  …  Click to View Output. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Wind,  “x=” declares which variable will become the x-axis of the graphic, the explanatory variable. y=Temp “y=” declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_point( The “geom_point()” function causes the ggplot to become a scatterplot. There are many other “geom_” functions that could be used. color = “ivory3”,  Controls the color of the dots. pch = 18 Controls the type of plotting character to be used in the plot. )Closing parenthsis for the geom_point function.  +  The addition symbol + is used to add further elements to the ggplot.    labs( The “labs” function is used to add labels to the plot, like a main title, x-label and y-label. title=“La Guardia Airport (May - Sep)”,  The “title=” command allows you to control the main title at the top of the graphic. x=“Daily Average Wind Speed (mph)”,  The “x=” command allows you to control the x-label of the graphic. y=“Daily Mean Temperature” The “y=” command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function.  +  The addition symbol + is used to add further elements to the ggplot.   theme_bw()Changes the “theme” or look of the plot to “black” and “white”.     Press Enter to run the code.  …  Click to View Output. To make a scatterplot in R using the plotly approach, first ensure: library(plotly) is loaded. Then, plot_ly(data, x= ~dataColumn1, y= ~dataColumn2) data is the name of your dataset. dataColumn1 is a column of data from your dataset that is quantitative and will be used as the explanatory variable. dataColumn2 is a column of data from your dataset that is quantitative and will be used as the response variable. Example Code plot_ly(airquality, x= ~Wind, y= ~Temp) plot_ly(KidsFeet, x= ~length, y= ~width, color= ~sex, size= ~birthmonth, text= ~paste(\"Name:\", name, \"\\n\", \"Birth-Month:\", birthmonth), colors=c(\"skyblue\",\"hotpink\")) %>% layout(title=\"KidsFeet dataset\", xaxis=list(title=\"Length of the longer foot in cm\"), yaxis=list(title=\"Width of the longer foot in cm\"))",
    "sentences": ["Base R ggplot2 plotly To make a scatterplot in R use the code: plot(y ~ x, data=NameOfYourData) y is the quantitative response variable, i.e., “numeric vector.” x is the quantitative explanatory variable, i.e., “numeric vector.” NameOfYourData is the name of the dataset containing y and x.", "Note: plot(object) where object is a “numeric vector” will create a time series plot, which is sometimes useful.", "Example Code plot An R function “plot” used to create a scatterplot, or in this case a time series plot because only one quantitative variable is being supplied to the function.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset.", "Type “View(airquality)” in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.", ", The “,” is required to start specifying additional commands for the function."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 73,
    "title": "📍 Bar Charts",
    "url": "GraphicalSummaries.html#barcharts",
    "content": "Bar Charts 1 (or 2) Qualitative Variable(s) Overview Depicts the number of occurrances for each category, or level, of the qualitative variable. Similar to a histogram, but there is no natural way to order the bars. Thus the white-space between each bar. It is called a Pareto chart if the bars are ordered from tallest to shortest. Clustered and stacked bar charts are often used to display information for two qualitative variables simultaneously. R Instructions Base R ggplot2 plotly To make a bar chart in R use the code: barplot(heights) heights must be a “numeric vector” that contains the heights for each bar that will be drawn in the plot. Note: both the c() and table() functions can be used to specify the heights. The example codes below demonstrate. Example Code Using the c() function. barplot barplot is an R function used to create a bar chart. ( Parenthesis to begin the barplot function. Must touch the last letter of the function. c c is an R function used to concatenate a list of values together into a “vector.” It is being used here to specify the heights of the 4 bars in the bar plot. ( Parenthesis to begin the c function. Must touch the last letter of the function. 10,5,28,3 This list of numbers will be joined together into a single “vector.” There is no limit on the number of entries that can be put into such a list. )Closing parenthsis for the c() function. , The “,” is required to start specifying additional commands for the barplot function. col=“gray24” col= stands for the “color” of the plot. The color name “gray24” is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. )Closing parenthsis for the barplot function.     Press Enter to run the code.  …  Click to View Output. barplot barplot is an R function used to create a bar chart. ( Parenthesis to begin the barplot function. Must touch the last letter of the function. c c is an R function used to concatenate a list of values together into a “vector.” It is being used here to specify the heights of the 4 bars in the bar plot. ( Parenthesis to begin the c function. Must touch the last letter of the function. Pigs=10,Cats=5,Dogs=28,Roosters=3 This named list of numbers will be joined together into a single “vector.” There is no limit on the number of entries that can be put into such a list. Notice how the names show up as the labels for each bin in the bar chart. )Closing parenthsis for the c() function. , The “,” is required to start specifying additional commands for the barplot function. col=“gray44” col= stands for the “color” of the plot. The color name “gray44” is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. )Closing parenthsis for the barplot function.     Press Enter to run the code.  …  Click to View Output. barplot( barplot is an R function used to create a bar chart. rbind( rbind stands for “row bind” and is a function that joins together different c() vectors to make them become rows of a table. `Farm 1`=c(Pigs=10,Cats=5,Dogs=28,Roosters=3) Notice how this c() vector of named values is being named “Farm 1.” The tick marks ` ` are required to specify a name of a vector that has a space in it. If the name was just Farm1 (without a space) then the tick marks would not be needed. Since `Farm 1` is the first vector in the rbind() function, it will become the first row of the resulting table that rbind() will create. , The “,” is required to specify additional c() vectors for the rbind() function. `Farm 2`=c(Pigs=15,Cats=3,Dogs=8,Roosters=1) Notice how this c() vector of named values is being named “Farm 2.” It will become the second row of the table created by rbind(). )Closing parenthsis for the rbind() function. , The “,” is required to specify additional commands for the barplot function. col=c(“gray84”,“gray44”) col= stands for the “color” of the plot. Here two colors: “gray84” and “gray44” are being passed to the col= option by using the c() function. Notice how these two colors are used in the resulting bar chart. , The “,” is required to specify additional commands for the barplot function. beside=TRUE beside= can be set to either TRUE or FALSE. When it is TRUE, the bars are clustered side-by-side. When it is set to FALSE, the bars are stacked on top of each other. Typically, beside=TRUE is preferred. , The “,” is required to specify additional commands for the barplot function. legend.text=TRUE legend.text=TRUE allows for the legend to be placed on the barplot. )Closing parenthsis for the barplot function.     Press Enter to run the code.  …  Click to View Output. Using the table() function. barplot barplot is an R function used to create a bar chart. ( Parenthesis to begin the function. Must touch the last letter of the function. table table is an R function used to tabulate how many times each value occurs in a given dataset. It is being used here to specify the heights of the bars in the bar chart. ( Parenthesis to begin the function. Must touch the last letter of the function. mtcars “mtcars” is a dataset. Type “View(mtcars)” in R to see it. $ The $ allows us to access any variable from the mtcars dataset. cyl “cyl” is a qualitative variable (in this case actually a numeric vector acting as a qualitative variable) from the “mtcars” dataset. It represents the number of cylinders the vehicle’s engine has. )Closing parenthsis for the table() function. , The “,” is required to start specifying additional commands for the barplot function. col=“cornsilk” col= stands for the “color” of the plot. The color name “cornsilk” is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. )Closing parenthsis for the barplot function.     Press Enter to run the code.  …  Click to View Output. barplot( barplot is an R function used to create a bar chart. table( table is an R function used to tabulate how many times each pair of values occurs in a given dataset. It is being used here to specify the heights of the bars in this clustered bar chart. mtcars$am “mtcars” is a dataset and the $ sign is being used to access the “am” variable from that dataset. Note that “am” is being used as a qualitative variable, but is actually a numeric vector acting as a qualitative variable. It denotes whether the vehicle is an automatic (0) or manual (1) transmission. , The “,” is required to specify additional variables for the table() function. mtcars$cyl “mtcars” is a dataset and the $ sign is being used to access the “cyl” variable from that dataset. The “cyl” variable gives the cylinders of the vehicle’s engine as either 4, 6, or 8. So even though it is numeric, it can be used as a qualitative variable. )Closing parenthsis for the table() function. , The “,” is required to start specifying additional commands for the barplot function. beside=TRUEbeside= is an optional command to the barplot() function. When TRUE, the bars are placed next to each other. When FALSE, the bars are stacked on top of each other. , The “,” is required to specify additional commands for the barplot function. col=c(“firebrick”,“snow1”) col= stands for the “color” of the plot. The colors of “firebrick” and “snow1” are being passed to the col= option using the c() function. , The “,” is required to specify additional commands for the barplot function. legend.text=TRUE legend.text=TRUE allows for the legend to be placed on the barplot. , The “,” is required to specify additional commands for the barplot function. xlab=“Cylinders” xlab= stands for “x label.” Use it to specify the text to print on the plot below the x-axis. The desired text must always be contained in quotes. )Closing parenthsis for the barplot function.     Press Enter to run the code.  …  Click to View Output. To make a bar chart in R using the ggplot approach, first ensure: library(ggplot2) is loaded. Then, ggplot(data, aes(x=groupsColumn, y=countsColumn) +   geom_bar() data is the name of your dataset. groupsColumn is a column of data from your dataset that is qualitative and represents the groups that should each have a bar in the barplot. countsColumn is a column of data from your dataset that contains the counts of how many times each group has been observed. The aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your groupsColumn of data, the y-axis become your countsColumn. Note if groupsColumn is not a factor, use factor(groupsColumn) instead. The geometry helper function geom_bar() causes the ggplot to become a bar chart. Example Code Manually building the counts data. FarmAnimals <- data.frame(animal = c(“pigs”,“cats”,“dogs”,“Roosters”), count = c(10,5,28,3)) This code creates a data set manually called FarmAnimals using the data.frame() function. Notice that there are two columns in this dataset: “animal” and “count”. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. FarmAnimals “FarmAnimals” is a dataset we just created. Type “View(FarmAnimals)” in R after running the above code to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. animal,  Declares which variable will become the x-axis of the graphic, the explanatory variable. count,  Declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_col( ) The “geom_col()” function is being used here instead of “geom_bar()” because this is a very simple bar chart for just one groups column.     Press Enter to run the code.  …  Click to View Output. FarmAnimals <- data.frame(animal = c(“pigs”,“pigs”,“cats”,“cats”,“dogs”,“dogs”,“Roosters”,“Roosters”), count = c(6,4,2,3,18,10,2,1), farm = c(“farm1”,“farm2”,“farm1”,“farm2”,“farm1”,“farm2”,“farm1”,“farm2”)) This code creates a data set manually called FarmAnimals using the data.frame() function. Notice that there are two columns in this dataset: “animal” and “count”. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. FarmAnimals “FarmAnimals” is a dataset we just created. Type “View(FarmAnimals)” in R after running the above code to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x = animal,  Declares which variable will become the x-axis of the graphic, the explanatory variable. y = count,  Declares which variable will become the y-axis of the graphic. fill = farm,  Declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_bar( The “geom_bar” function tells the ggplot() to become a bar chart. stat = “identity”, Tells the ggplot to use the counts as listed in the counts column. position = “dodge”, Causes the bars in the barchart to be side-by-side rather than stacked. color = “black”, Controls the colors of the borders of the bars in the plot. )Closing parenthsis for the geom_bar() function.     Press Enter to run the code.  …  Click to View Output. Using an existing dataset directly. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. mtcars “mtcars” is a dataset in R. Type “View(mtcars)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x = factor(cyl) Declares which variable will become the x-axis of the graphic. Use factor(columnName) when the column consists of numbers to turn it into groups. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_bar( The “geom_bar” function tells the ggplot() to become a bar chart. fill = “cornsilk”, Controls the colors of the insides of the bars in the plot. color = “black” Controls the colors of the borders of the bars in the plot. )Closing parenthsis for the geom_bar() function.     Press Enter to run the code.  …  Click to View Output. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. mtcars “mtcars” is a dataset in R. Type “View(mtcars)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x = factor(cyl),  Declares which variable will become the x-axis of the graphic. Use factor(columnName) when the column consists of numbers to turn it into groups. fill = factor(am),  Declares which variable will become the x-axis of the graphic. Use factor(columnName) when the column consists of numbers to turn it into groups. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_bar( The “geom_bar” function tells the ggplot() to become a bar chart. position = “dodge”, Causes the bars to be side-by-side instead of stacked. color = “black” Controls the colors of the borders of the bars in the plot. )Closing parenthsis for the geom_bar() function.  +  The addition symbol + is used to add further elements to the ggplot.    labs(x=“Cylinders”) The “labs” function is being used to add a title to the x-axis only. title=“main title” and y=“y title” could also be used.     Press Enter to run the code.  …  Click to View Output. Not yet available.",
    "sentences": ["1 (or 2) Qualitative Variable(s)", "Overview Depicts the number of occurrances for each category, or level, of the qualitative variable.", "Similar to a histogram, but there is no natural way to order the bars.", "Thus the white-space between each bar.", "It is called a Pareto chart if the bars are ordered from tallest to shortest.", "Clustered and stacked bar charts are often used to display information for two qualitative variables simultaneously.", "R Instructions Base R ggplot2 plotly To make a bar chart in R use the code: barplot(heights) heights must be a “numeric vector” that contains the heights for each bar that will be drawn in the plot.", "Note: both the c() and table() functions can be used to specify the heights.", "The example codes below demonstrate.", "Example Code Using the c() function."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Bar Charts"
  },
  {
    "id": 74,
    "title": "📍 Overview",
    "url": "GraphicalSummaries.html#overview",
    "content": "Overview Depicts the number of occurrances for each category, or level, of the qualitative variable. Similar to a histogram, but there is no natural way to order the bars. Thus the white-space between each bar. It is called a Pareto chart if the bars are ordered from tallest to shortest. Clustered and stacked bar charts are often used to display information for two qualitative variables simultaneously.",
    "sentences": ["Depicts the number of occurrances for each category, or level, of the qualitative variable.", "Similar to a histogram, but there is no natural way to order the bars.", "Thus the white-space between each bar.", "It is called a Pareto chart if the bars are ordered from tallest to shortest.", "Clustered and stacked bar charts are often used to display information for two qualitative variables simultaneously."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 75,
    "title": "📍 R Instructions",
    "url": "GraphicalSummaries.html#rinstructions",
    "content": "R Instructions Base R ggplot2 plotly To make a bar chart in R use the code: barplot(heights) heights must be a “numeric vector” that contains the heights for each bar that will be drawn in the plot. Note: both the c() and table() functions can be used to specify the heights. The example codes below demonstrate. Example Code Using the c() function. barplot barplot is an R function used to create a bar chart. ( Parenthesis to begin the barplot function. Must touch the last letter of the function. c c is an R function used to concatenate a list of values together into a “vector.” It is being used here to specify the heights of the 4 bars in the bar plot. ( Parenthesis to begin the c function. Must touch the last letter of the function. 10,5,28,3 This list of numbers will be joined together into a single “vector.” There is no limit on the number of entries that can be put into such a list. )Closing parenthsis for the c() function. , The “,” is required to start specifying additional commands for the barplot function. col=“gray24” col= stands for the “color” of the plot. The color name “gray24” is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. )Closing parenthsis for the barplot function.     Press Enter to run the code.  …  Click to View Output. barplot barplot is an R function used to create a bar chart. ( Parenthesis to begin the barplot function. Must touch the last letter of the function. c c is an R function used to concatenate a list of values together into a “vector.” It is being used here to specify the heights of the 4 bars in the bar plot. ( Parenthesis to begin the c function. Must touch the last letter of the function. Pigs=10,Cats=5,Dogs=28,Roosters=3 This named list of numbers will be joined together into a single “vector.” There is no limit on the number of entries that can be put into such a list. Notice how the names show up as the labels for each bin in the bar chart. )Closing parenthsis for the c() function. , The “,” is required to start specifying additional commands for the barplot function. col=“gray44” col= stands for the “color” of the plot. The color name “gray44” is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. )Closing parenthsis for the barplot function.     Press Enter to run the code.  …  Click to View Output. barplot( barplot is an R function used to create a bar chart. rbind( rbind stands for “row bind” and is a function that joins together different c() vectors to make them become rows of a table. `Farm 1`=c(Pigs=10,Cats=5,Dogs=28,Roosters=3) Notice how this c() vector of named values is being named “Farm 1.” The tick marks ` ` are required to specify a name of a vector that has a space in it. If the name was just Farm1 (without a space) then the tick marks would not be needed. Since `Farm 1` is the first vector in the rbind() function, it will become the first row of the resulting table that rbind() will create. , The “,” is required to specify additional c() vectors for the rbind() function. `Farm 2`=c(Pigs=15,Cats=3,Dogs=8,Roosters=1) Notice how this c() vector of named values is being named “Farm 2.” It will become the second row of the table created by rbind(). )Closing parenthsis for the rbind() function. , The “,” is required to specify additional commands for the barplot function. col=c(“gray84”,“gray44”) col= stands for the “color” of the plot. Here two colors: “gray84” and “gray44” are being passed to the col= option by using the c() function. Notice how these two colors are used in the resulting bar chart. , The “,” is required to specify additional commands for the barplot function. beside=TRUE beside= can be set to either TRUE or FALSE. When it is TRUE, the bars are clustered side-by-side. When it is set to FALSE, the bars are stacked on top of each other. Typically, beside=TRUE is preferred. , The “,” is required to specify additional commands for the barplot function. legend.text=TRUE legend.text=TRUE allows for the legend to be placed on the barplot. )Closing parenthsis for the barplot function.     Press Enter to run the code.  …  Click to View Output. Using the table() function. barplot barplot is an R function used to create a bar chart. ( Parenthesis to begin the function. Must touch the last letter of the function. table table is an R function used to tabulate how many times each value occurs in a given dataset. It is being used here to specify the heights of the bars in the bar chart. ( Parenthesis to begin the function. Must touch the last letter of the function. mtcars “mtcars” is a dataset. Type “View(mtcars)” in R to see it. $ The $ allows us to access any variable from the mtcars dataset. cyl “cyl” is a qualitative variable (in this case actually a numeric vector acting as a qualitative variable) from the “mtcars” dataset. It represents the number of cylinders the vehicle’s engine has. )Closing parenthsis for the table() function. , The “,” is required to start specifying additional commands for the barplot function. col=“cornsilk” col= stands for the “color” of the plot. The color name “cornsilk” is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes. )Closing parenthsis for the barplot function.     Press Enter to run the code.  …  Click to View Output. barplot( barplot is an R function used to create a bar chart. table( table is an R function used to tabulate how many times each pair of values occurs in a given dataset. It is being used here to specify the heights of the bars in this clustered bar chart. mtcars$am “mtcars” is a dataset and the $ sign is being used to access the “am” variable from that dataset. Note that “am” is being used as a qualitative variable, but is actually a numeric vector acting as a qualitative variable. It denotes whether the vehicle is an automatic (0) or manual (1) transmission. , The “,” is required to specify additional variables for the table() function. mtcars$cyl “mtcars” is a dataset and the $ sign is being used to access the “cyl” variable from that dataset. The “cyl” variable gives the cylinders of the vehicle’s engine as either 4, 6, or 8. So even though it is numeric, it can be used as a qualitative variable. )Closing parenthsis for the table() function. , The “,” is required to start specifying additional commands for the barplot function. beside=TRUEbeside= is an optional command to the barplot() function. When TRUE, the bars are placed next to each other. When FALSE, the bars are stacked on top of each other. , The “,” is required to specify additional commands for the barplot function. col=c(“firebrick”,“snow1”) col= stands for the “color” of the plot. The colors of “firebrick” and “snow1” are being passed to the col= option using the c() function. , The “,” is required to specify additional commands for the barplot function. legend.text=TRUE legend.text=TRUE allows for the legend to be placed on the barplot. , The “,” is required to specify additional commands for the barplot function. xlab=“Cylinders” xlab= stands for “x label.” Use it to specify the text to print on the plot below the x-axis. The desired text must always be contained in quotes. )Closing parenthsis for the barplot function.     Press Enter to run the code.  …  Click to View Output. To make a bar chart in R using the ggplot approach, first ensure: library(ggplot2) is loaded. Then, ggplot(data, aes(x=groupsColumn, y=countsColumn) +   geom_bar() data is the name of your dataset. groupsColumn is a column of data from your dataset that is qualitative and represents the groups that should each have a bar in the barplot. countsColumn is a column of data from your dataset that contains the counts of how many times each group has been observed. The aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your groupsColumn of data, the y-axis become your countsColumn. Note if groupsColumn is not a factor, use factor(groupsColumn) instead. The geometry helper function geom_bar() causes the ggplot to become a bar chart. Example Code Manually building the counts data. FarmAnimals <- data.frame(animal = c(“pigs”,“cats”,“dogs”,“Roosters”), count = c(10,5,28,3)) This code creates a data set manually called FarmAnimals using the data.frame() function. Notice that there are two columns in this dataset: “animal” and “count”. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. FarmAnimals “FarmAnimals” is a dataset we just created. Type “View(FarmAnimals)” in R after running the above code to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. animal,  Declares which variable will become the x-axis of the graphic, the explanatory variable. count,  Declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_col( ) The “geom_col()” function is being used here instead of “geom_bar()” because this is a very simple bar chart for just one groups column.     Press Enter to run the code.  …  Click to View Output. FarmAnimals <- data.frame(animal = c(“pigs”,“pigs”,“cats”,“cats”,“dogs”,“dogs”,“Roosters”,“Roosters”), count = c(6,4,2,3,18,10,2,1), farm = c(“farm1”,“farm2”,“farm1”,“farm2”,“farm1”,“farm2”,“farm1”,“farm2”)) This code creates a data set manually called FarmAnimals using the data.frame() function. Notice that there are two columns in this dataset: “animal” and “count”. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. FarmAnimals “FarmAnimals” is a dataset we just created. Type “View(FarmAnimals)” in R after running the above code to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x = animal,  Declares which variable will become the x-axis of the graphic, the explanatory variable. y = count,  Declares which variable will become the y-axis of the graphic. fill = farm,  Declares which variable will become the y-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_bar( The “geom_bar” function tells the ggplot() to become a bar chart. stat = “identity”, Tells the ggplot to use the counts as listed in the counts column. position = “dodge”, Causes the bars in the barchart to be side-by-side rather than stacked. color = “black”, Controls the colors of the borders of the bars in the plot. )Closing parenthsis for the geom_bar() function.     Press Enter to run the code.  …  Click to View Output. Using an existing dataset directly. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. mtcars “mtcars” is a dataset in R. Type “View(mtcars)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x = factor(cyl) Declares which variable will become the x-axis of the graphic. Use factor(columnName) when the column consists of numbers to turn it into groups. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_bar( The “geom_bar” function tells the ggplot() to become a bar chart. fill = “cornsilk”, Controls the colors of the insides of the bars in the plot. color = “black” Controls the colors of the borders of the bars in the plot. )Closing parenthsis for the geom_bar() function.     Press Enter to run the code.  …  Click to View Output. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. mtcars “mtcars” is a dataset in R. Type “View(mtcars)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x = factor(cyl),  Declares which variable will become the x-axis of the graphic. Use factor(columnName) when the column consists of numbers to turn it into groups. fill = factor(am),  Declares which variable will become the x-axis of the graphic. Use factor(columnName) when the column consists of numbers to turn it into groups. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_bar( The “geom_bar” function tells the ggplot() to become a bar chart. position = “dodge”, Causes the bars to be side-by-side instead of stacked. color = “black” Controls the colors of the borders of the bars in the plot. )Closing parenthsis for the geom_bar() function.  +  The addition symbol + is used to add further elements to the ggplot.    labs(x=“Cylinders”) The “labs” function is being used to add a title to the x-axis only. title=“main title” and y=“y title” could also be used.     Press Enter to run the code.  …  Click to View Output. Not yet available.",
    "sentences": ["Base R ggplot2 plotly To make a bar chart in R use the code: barplot(heights) heights must be a “numeric vector” that contains the heights for each bar that will be drawn in the plot.", "Note: both the c() and table() functions can be used to specify the heights.", "The example codes below demonstrate.", "Example Code Using the c() function.", "barplot barplot is an R function used to create a bar chart.", "( Parenthesis to begin the barplot function.", "Must touch the last letter of the function.", "c c is an R function used to concatenate a list of values together into a “vector.” It is being used here to specify the heights of the 4 bars in the bar plot.", "( Parenthesis to begin the c function.", "Must touch the last letter of the function."],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 76,
    "title": "📍 Custom Plots",
    "url": "GraphicalSummaries.html#customplots",
    "content": "Custom Plots Creativity Required Overview Sometimes no standard plot sufficiently describes the data. In these cases, the only guideline is the one stated originally, “the graphical depiction of data should communicate the truth the data has to offer about the situation of interest.” R Examples You should add links to examples you find of interesting plots made in R. Here is the R Code for the graphic to the left: plot(density(CO2$uptake[CO2$Type==\"Quebec\"]), main=\"\", col='skyblue4', xlab=\"\", ylab=\"\", xaxt='n', yaxt='n') lines(density(CO2$uptake[CO2$Type==\"Mississippi\"]), col='firebrick')",
    "sentences": ["Creativity Required", "Overview Sometimes no standard plot sufficiently describes the data.", "In these cases, the only guideline is the one stated originally, “the graphical depiction of data should communicate the truth the data has to offer about the situation of interest.”", "R Examples You should add links to examples you find of interesting plots made in R.", "Here is the R Code for the graphic to the left: plot(density(CO2$uptake[CO2$Type==\"Quebec\"]), main=\"\", col='skyblue4', xlab=\"\", ylab=\"\", xaxt='n', yaxt='n') lines(density(CO2$uptake[CO2$Type==\"Mississippi\"]), col='firebrick')"],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Custom Plots"
  },
  {
    "id": 77,
    "title": "📍 Overview",
    "url": "GraphicalSummaries.html#overview",
    "content": "Overview Sometimes no standard plot sufficiently describes the data. In these cases, the only guideline is the one stated originally, “the graphical depiction of data should communicate the truth the data has to offer about the situation of interest.”",
    "sentences": ["Sometimes no standard plot sufficiently describes the data.", "In these cases, the only guideline is the one stated originally, “the graphical depiction of data should communicate the truth the data has to offer about the situation of interest.”"],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 78,
    "title": "📍 R Examples",
    "url": "GraphicalSummaries.html#rexamples",
    "content": "R Examples You should add links to examples you find of interesting plots made in R. Here is the R Code for the graphic to the left: plot(density(CO2$uptake[CO2$Type==\"Quebec\"]), main=\"\", col='skyblue4', xlab=\"\", ylab=\"\", xaxt='n', yaxt='n') lines(density(CO2$uptake[CO2$Type==\"Mississippi\"]), col='firebrick')",
    "sentences": ["You should add links to examples you find of interesting plots made in R.", "Here is the R Code for the graphic to the left: plot(density(CO2$uptake[CO2$Type==\"Quebec\"]), main=\"\", col='skyblue4', xlab=\"\", ylab=\"\", xaxt='n', yaxt='n') lines(density(CO2$uptake[CO2$Type==\"Mississippi\"]), col='firebrick')"],
    "type": "section",
    "page_title": "Graphical Summaries",
    "section_title": "R Examples"
  },
  {
    "id": 79,
    "title": "Statistics Notebook",
    "url": "index.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Statistics Notebook This page, and all pages of this notebook, are meant to be customized to become a useful Guide to Statistical Analysis in R for your current and future self. Sections Paige’s Notes : Notes taken throughout different statistic based classes. R Help : R Studio commands, structuring tips, and tricks. Describing Data : R Studio commands for graphically and numerically describing data. Making Inference : Deep dives into the different statistical tests. Analyses : Statistical analyses that utilizes the tools of collecting, analyzing, and interpreting data to result in informed decision-making. Search Bar <input id=\"search-input\" class=\"search-input\" type=\"search\" placeholder=\"Search notes, sections, examples…\" autocomplete=\"off\" /> <button class=\"search-btn\" id=\"search-go\" title=\"Search\">Search<\/button> <button class=\"clear-btn\" id=\"search-clear\" title=\"Clear\">Clear<\/button> <span id=\"search-count\">Type to search.<\/span> <span>Tip: use ↑/↓ then Enter to open<\/span> Table of Contents These are the statistical tools used to explore and interpret different data types. One Quantitative Response Variable Y Graphics Y is a single quantitative variable of interest. This would be like “heights” of BYU-Idaho students. Test(s) used for this category: One Sample T-test Paired Sample T-test Wilcoxon Signed- Rank Test Used for one quantitative variable. Pretty simple tbh. Questions that this answers: How long are 4th grader’s feet? What is the average length of feet in the KidsFeet dataset? These are the best graphics to use: Examples : What is the mean temperature at Airport? (One Sample) can be done with histogram or dot plot! plot_ly(airquality, x=~Temp, type=\"histogram\", marker=list(color=\"skyblue\", line=list(color=\"darkgray\", width=10))) %>% layout(title=\"La Guardia Airport Daily Mean Temperatures\", xaxis=list(title=\"Temperature in Degrees F\")) stripchart(airquality$Temp, method=\"stack\", xlab= \"Temperature (F)\", main=\"La Guardia Airport Daily Mean Temperature\") You can change the method to “overplot”, “jitter”, or “stack” What is the median temperature of La Guardia Airport? (One Sample) boxplot (airquality$Temp, ylab=\"Temperature (F)\", main=\"La Guardia Airport Temperature Median\") Tests One Sample t Test A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable. Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average? Is human body temperature really 98.6&deg; F on average? Do I spend less than $3 a day,on average, purchasing snacks? Requirements This test is only appropriate when both of the following are satisfied. The sample is representative of the population. (Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal. This is a safe assumption when either: the population data can be assumed to be normally distributed using a Q-Q Plot OR the size of the sample (n) that was taken from the population is large (at least n > 30, but “large” really depends on how badly the data is skewed). If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population. If the requirements are not met, then that doesn’t mean the results of the test are necessarily bad, but there is no guarantee that they are good. Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits R Instructions Console Help Command: ?t.test() t.test(NameOfYourData$Y, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set, like mtcars or KidsFeet. Y must be a “numeric” vector of quantitative data. YourNull is the numeric value from your null hypothesis for \\(\\mu\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(NameOfYourData$Y) Example Code Hover your mouse over the example codes to learn more. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,  ‘mpg’ is Y, a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,  The numeric value from the null hypothesis is 20 meaning \\(\\mu=20\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1−α.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. One Sample t-test EXPLANATION. data: mtcars$mpg EXPLANATION. t = 0.08506, EXPLANATION.  df = 31, EXPLANATION.  p-value = 0.9328 EXPLANATION. alternative hypothesis: true mean is not equal to 20 EXPLANATION. 95 percent confidence interval: EXPLANATION.  17.91768 EXPLANATION.  22.26357 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION.  20.09062 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg) ‘mpg’ is a quantitative variable (numeric vector) from the mtcars dataset.  Click to Show Output  Click to View Output. ## [1] 20 18 Explanation When we want to check if a claim about the average of a group(population mean \\(\\mu\\)) is true, we often use a test called the “one sample t test”. This test works well when we can assume that the data follows a normal pattern and that we’ve picked our sample randomly from the bigger group we’re interested in. In the figure below, the null hypothesis \\(H_0: \\mu = \\mu_0\\) is represented by the normal distribution (gray) centered at \\(\\mu_0\\). - Note that \\(\\mu_0\\) is just some specified number. - This shows how the null hypothesis represents the assumption about the center of the distribution of the data. After a hypothesis (null) is established and an alternative hypothesis similarly declared, a simple random sample of data of size \\(n\\) is obtained from the population of interest. - In the plot above, this is depicted by the points (blue dots) which are centered around their sample mean \\(\\bar{x}\\). Above the points (blue dots) is another bell-shaped curve (blue dashed line). This curve shows that the alternative hypothesis might fit the data better than the null hypothesis. The role of the one sample t test is to measure the probability of a sample mean being as extreme or more extreme from the hypothesized value of \\(\\mu_0\\) than the one observed assuming the null hypothesis is true. - This probability is of course the p-value of the test. - This works because the sampling distribution of the sample mean has been assumed to be normal. In this case, the distribution of the test statistic t, \\[ t = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\] is known to follow a t distribution with \\(n-1\\) degrees of freedom. *The mathematics that provide this result are phenominal! You can consult any advanced statistical textbook for the details. The p-value of the one sample t test represents the probability that the test statistic \\(t\\) is as extreme or more extreme than the one observed according to a t-distribution with \\(n-1\\) degrees of freedom. If the probability (the p-value) is close enough to zero (smaller than \\(\\alpha\\)), then it is determined that the most plausible hypothesis is the alternative hypothesis, and thus the null is “rejected” in favor of the alternative. Paired Samples t Test The paired samples t test is used when a value is hypothesized for the popluation mean of the differences, \\(\\mu_d\\), obtained from paired observations. Overview Questions The Paired Samples t Test can be used to answer questions like: From pre-test to post-test is there an improvement on average in the subjects? How much taller are husbands than their wives, on average? Do hospital patients that are carefully matched together according to reason for being in the hospital, age, gender, ethnicity, height, and weight show increased stay times in the hospital when infected with a nosocomial infection compared to those who were not infected? Requirements The test is only appropriate when both of the following are satisfied. The sample of differences is representative of the population differences. The sampling distribution of the sample mean of the differences \\(\\bar{d}\\) (\\(\\bar{x}\\) of the differences) can be assumed to be normal. (This second requirement can be assumed to be satisfied when (a) the differences themselves can be assumed to be normal from a Q-Q Plot, or (b) when the sample size \\(n\\) of the differences is large.) Hypotheses Math Code $$ H_0: \\mu_d = 0 $$ $$ H_a: \\mu_d \\neq 0 $$ \\(H_0: \\mu_d = \\text{some number, but typically 0}\\)\\(H_a: \\mu_d \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepPaired studentPaired R Instructions Console Help Command: ?t.test() Option 1: t.test(NameOfYourData$Y1, NameOfYourData$Y2, paired = TRUE, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set like sleep or mtcars or KidsFeet. Y1 must be a “numeric” vector that represents the quantitative data from the first sample of data. Y2 must be a “numeric” vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y1 - Y2) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the “group1” data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the “group2” data from the sleep data set t.test( ‘t.test’ is an R function that performs one and two sample t-tests. sleep2$extra,  A numeric vector that represents the hours of extra sleep that the group had with drug 2. sleep1$extra,  A numeric vector that represents the hours of extra sleep that the same group had with drug 1. paired=TRUE,  Indicates that this is a paired t-Test. This will cause the subtraction of sleep2$extra - sleep1$extra to be performed to obtain the paired differences. To cause the subtraction to occur in the other order, reverse the order sleep1$extra, sleep2$extra occur in the t.test(…) function. mu = 0,  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. Paired t-test EXPLANATION. data: sleep2$extra and sleep1$extra EXPLANATION. t = 4.0621, EXPLANATION.  df = 9, EXPLANATION.  p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  0.7001142 EXPLANATION.  2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of the differences EXPLANATION.   1.58 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. sleep2$extra The hours of extra sleep that the group had with drug 2.  -  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. ) Closing parenthesis for qqPlot(…) function.  Click to Show Output  Click to View Output. ## [1] 9 5 Option 2: Compute the differences yourself instead of using paired=TRUE. differences = NameOfYourData$Y1 - NameOfYourData$Y2 t.test(differences, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a “numeric” vector that represents the quantitative data from the first sample of data. Y2 must be a “numeric” vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. differences are the resulting differences obtained from subtracting Y1 - Y2. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(differences) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the “group1” data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the “group2” data from the sleep data set differences <-  Saved the computed differences to an object called ‘differences’. sleep2$extra The hours of extra sleep that t",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Statistics Notebook This page, and all pages of this notebook, are meant to be customized to become a useful Guide to Statistical Analysis in R for your current and future self.", "Sections Paige’s Notes : Notes taken throughout different statistic based classes.", "R Help : R Studio commands, structuring tips, and tricks.", "Describing Data : R Studio commands for graphically and numerically describing data.", "Making Inference : Deep dives into the different statistical tests.", "Analyses : Statistical analyses that utilizes the tools of collecting, analyzing, and interpreting data to result in informed decision-making.", "Search Bar <input id=\"search-input\" class=\"search-input\" type=\"search\" placeholder=\"Search notes, sections, examples…\" autocomplete=\"off\" /> <button class=\"search-btn\" id=\"search-go\" title=\"Search\">Search<\/button> <button class=\"clear-btn\" id=\"search-clear\" title=\"Clear\">Clear<\/button> <span id=\"search-count\">Type to search.<\/span> <span>Tip: use ↑/↓ then Enter to open<\/span> Table of Contents These are the statistical tools used to explore and interpret different data types.", "One Quantitative Response Variable Y Graphics Y is a single quantitative variable of interest.", "This would be like “heights” of BYU-Idaho students.", "Test(s) used for this category: One Sample T-test Paired Sample T-test Wilcoxon Signed- Rank Test Used for one quantitative variable.", "Pretty simple tbh.", "Questions that this answers: How long are 4th grader’s feet?", "What is the average length of feet in the KidsFeet dataset?", "These are the best graphics to use: Examples : What is the mean temperature at Airport?", "(One Sample) can be done with histogram or dot plot!", "plot_ly(airquality, x=~Temp, type=\"histogram\", marker=list(color=\"skyblue\", line=list(color=\"darkgray\", width=10))) %>% layout(title=\"La Guardia Airport Daily Mean Temperatures\", xaxis=list(title=\"Temperature in Degrees F\")) stripchart(airquality$Temp, method=\"stack\", xlab= \"Temperature (F)\", main=\"La Guardia Airport Daily Mean Temperature\") You can change the method to “overplot”, “jitter”, or “stack” What is the median temperature of La Guardia Airport?", "(One Sample) boxplot (airquality$Temp, ylab=\"Temperature (F)\", main=\"La Guardia Airport Temperature Median\") Tests One Sample t Test A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable.", "Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average?", "Is human body temperature really 98.6&deg; F on average?", "Do I spend less than $3 a day,on average, purchasing snacks?", "Requirements This test is only appropriate when both of the following are satisfied.", "The sample is representative of the population.", "(Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal.", "This is a safe assumption when either: the population data can be assumed to be normally distributed using a Q-Q Plot OR the size of the sample (n) that was taken from the population is large (at least n > 30, but “large” really depends on how badly the data is skewed).", "If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population.", "If the requirements are not met, then that doesn’t mean the results of the test are necessarily bad, but there is no guarantee that they are good.", "Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits R Instructions Console Help Command: ?t.test() t.test(NameOfYourData$Y, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set, like mtcars or KidsFeet.", "Y must be a “numeric” vector of quantitative data.", "YourNull is the numeric value from your null hypothesis for \\(\\mu\\).", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Testing Assumptions library(car) qqPlot(NameOfYourData$Y) Example Code Hover your mouse over the example codes to learn more.", "t.test( ‘t.test’ is an R function that performs one and two sample t-tests.", "mtcars ‘mtcars’ is a dataset.", "Type ‘View(mtcars)’ in R to view the dataset.", "$ The $ allows us to access any variable from the mtcars dataset.", "mpg,  ‘mpg’ is Y, a quantitative variable (numeric vector) from the mtcars dataset.", "mu = 20,  The numeric value from the null hypothesis is 20 meaning \\(\\mu=20\\).", "alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu\\neq20\\).", "conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1−α.", "    Press Enter to run the code if you have typed it in yourself.", "You can also click here to view the output.", " Click to Show Output  Click to View Output.", "One Sample t-test EXPLANATION.", "data: mtcars$mpg EXPLANATION.", "t = 0.08506, EXPLANATION.", " df = 31, EXPLANATION.", " p-value = 0.9328 EXPLANATION.", "alternative hypothesis: true mean is not equal to 20 EXPLANATION."],
    "type": "page",
    "page_title": "Statistics Notebook"
  },
  {
    "id": 80,
    "title": "📍 Sections",
    "url": "index.html#sections",
    "content": "Sections Paige’s Notes : Notes taken throughout different statistic based classes. R Help : R Studio commands, structuring tips, and tricks. Describing Data : R Studio commands for graphically and numerically describing data. Making Inference : Deep dives into the different statistical tests. Analyses : Statistical analyses that utilizes the tools of collecting, analyzing, and interpreting data to result in informed decision-making.",
    "sentences": ["Paige’s Notes : Notes taken throughout different statistic based classes.", "R Help : R Studio commands, structuring tips, and tricks.", "Describing Data : R Studio commands for graphically and numerically describing data.", "Making Inference : Deep dives into the different statistical tests.", "Analyses : Statistical analyses that utilizes the tools of collecting, analyzing, and interpreting data to result in informed decision-making."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Sections"
  },
  {
    "id": 81,
    "title": "📍 Search Bar",
    "url": "index.html#searchbar",
    "content": "Search Bar <input id=\"search-input\" class=\"search-input\" type=\"search\" placeholder=\"Search notes, sections, examples…\" autocomplete=\"off\" /> <button class=\"search-btn\" id=\"search-go\" title=\"Search\">Search<\/button> <button class=\"clear-btn\" id=\"search-clear\" title=\"Clear\">Clear<\/button> <span id=\"search-count\">Type to search.<\/span> <span>Tip: use ↑/↓ then Enter to open<\/span>",
    "sentences": "<input id=\"search-input\" class=\"search-input\" type=\"search\" placeholder=\"Search notes, sections, examples…\" autocomplete=\"off\" /> <button class=\"search-btn\" id=\"search-go\" title=\"Search\">Search<\/button> <button class=\"clear-btn\" id=\"search-clear\" title=\"Clear\">Clear<\/button> <span id=\"search-count\">Type to search.<\/span> <span>Tip: use ↑/↓ then Enter to open<\/span>",
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Search Bar"
  },
  {
    "id": 82,
    "title": "📍 One Quantitative Response Variable Y",
    "url": "index.html#onequantitativeresponsevariabley",
    "content": "One Quantitative Response Variable Y Graphics Y is a single quantitative variable of interest. This would be like “heights” of BYU-Idaho students. Test(s) used for this category: One Sample T-test Paired Sample T-test Wilcoxon Signed- Rank Test Used for one quantitative variable. Pretty simple tbh. Questions that this answers: How long are 4th grader’s feet? What is the average length of feet in the KidsFeet dataset? These are the best graphics to use: Examples : What is the mean temperature at Airport? (One Sample) can be done with histogram or dot plot! plot_ly(airquality, x=~Temp, type=\"histogram\", marker=list(color=\"skyblue\", line=list(color=\"darkgray\", width=10))) %>% layout(title=\"La Guardia Airport Daily Mean Temperatures\", xaxis=list(title=\"Temperature in Degrees F\")) stripchart(airquality$Temp, method=\"stack\", xlab= \"Temperature (F)\", main=\"La Guardia Airport Daily Mean Temperature\") You can change the method to “overplot”, “jitter”, or “stack” What is the median temperature of La Guardia Airport? (One Sample) boxplot (airquality$Temp, ylab=\"Temperature (F)\", main=\"La Guardia Airport Temperature Median\")",
    "sentences": ["Graphics Y is a single quantitative variable of interest.", "This would be like “heights” of BYU-Idaho students.", "Test(s) used for this category: One Sample T-test Paired Sample T-test Wilcoxon Signed- Rank Test Used for one quantitative variable.", "Pretty simple tbh.", "Questions that this answers: How long are 4th grader’s feet?", "What is the average length of feet in the KidsFeet dataset?", "These are the best graphics to use: Examples : What is the mean temperature at Airport?", "(One Sample) can be done with histogram or dot plot!", "plot_ly(airquality, x=~Temp, type=\"histogram\", marker=list(color=\"skyblue\", line=list(color=\"darkgray\", width=10))) %>% layout(title=\"La Guardia Airport Daily Mean Temperatures\", xaxis=list(title=\"Temperature in Degrees F\")) stripchart(airquality$Temp, method=\"stack\", xlab= \"Temperature (F)\", main=\"La Guardia Airport Daily Mean Temperature\") You can change the method to “overplot”, “jitter”, or “stack” What is the median temperature of La Guardia Airport?", "(One Sample) boxplot (airquality$Temp, ylab=\"Temperature (F)\", main=\"La Guardia Airport Temperature Median\")"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "One Quantitative Response Variable Y"
  },
  {
    "id": 83,
    "title": "📍 Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single quantitative variable of interest. This would be like “heights” of BYU-Idaho students. Test(s) used for this category: One Sample T-test Paired Sample T-test Wilcoxon Signed- Rank Test Used for one quantitative variable. Pretty simple tbh. Questions that this answers: How long are 4th grader’s feet? What is the average length of feet in the KidsFeet dataset? These are the best graphics to use: Examples : What is the mean temperature at Airport? (One Sample) can be done with histogram or dot plot! plot_ly(airquality, x=~Temp, type=\"histogram\", marker=list(color=\"skyblue\", line=list(color=\"darkgray\", width=10))) %>% layout(title=\"La Guardia Airport Daily Mean Temperatures\", xaxis=list(title=\"Temperature in Degrees F\")) stripchart(airquality$Temp, method=\"stack\", xlab= \"Temperature (F)\", main=\"La Guardia Airport Daily Mean Temperature\") You can change the method to “overplot”, “jitter”, or “stack” What is the median temperature of La Guardia Airport? (One Sample) boxplot (airquality$Temp, ylab=\"Temperature (F)\", main=\"La Guardia Airport Temperature Median\")",
    "sentences": ["Y is a single quantitative variable of interest.", "This would be like “heights” of BYU-Idaho students.", "Test(s) used for this category: One Sample T-test Paired Sample T-test Wilcoxon Signed- Rank Test Used for one quantitative variable.", "Pretty simple tbh.", "Questions that this answers: How long are 4th grader’s feet?", "What is the average length of feet in the KidsFeet dataset?", "These are the best graphics to use: Examples : What is the mean temperature at Airport?", "(One Sample) can be done with histogram or dot plot!", "plot_ly(airquality, x=~Temp, type=\"histogram\", marker=list(color=\"skyblue\", line=list(color=\"darkgray\", width=10))) %>% layout(title=\"La Guardia Airport Daily Mean Temperatures\", xaxis=list(title=\"Temperature in Degrees F\")) stripchart(airquality$Temp, method=\"stack\", xlab= \"Temperature (F)\", main=\"La Guardia Airport Daily Mean Temperature\") You can change the method to “overplot”, “jitter”, or “stack” What is the median temperature of La Guardia Airport?", "(One Sample) boxplot (airquality$Temp, ylab=\"Temperature (F)\", main=\"La Guardia Airport Temperature Median\")"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 84,
    "title": "📍 Tests",
    "url": "index.html#tests",
    "content": "Tests One Sample t Test A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable. Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average? Is human body temperature really 98.6&deg; F on average? Do I spend less than $3 a day,on average, purchasing snacks? Requirements This test is only appropriate when both of the following are satisfied. The sample is representative of the population. (Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal. This is a safe assumption when either: the population data can be assumed to be normally distributed using a Q-Q Plot OR the size of the sample (n) that was taken from the population is large (at least n > 30, but “large” really depends on how badly the data is skewed). If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population. If the requirements are not met, then that doesn’t mean the results of the test are necessarily bad, but there is no guarantee that they are good. Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits R Instructions Console Help Command: ?t.test() t.test(NameOfYourData$Y, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set, like mtcars or KidsFeet. Y must be a “numeric” vector of quantitative data. YourNull is the numeric value from your null hypothesis for \\(\\mu\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(NameOfYourData$Y) Example Code Hover your mouse over the example codes to learn more. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,  ‘mpg’ is Y, a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,  The numeric value from the null hypothesis is 20 meaning \\(\\mu=20\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1−α.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. One Sample t-test EXPLANATION. data: mtcars$mpg EXPLANATION. t = 0.08506, EXPLANATION.  df = 31, EXPLANATION.  p-value = 0.9328 EXPLANATION. alternative hypothesis: true mean is not equal to 20 EXPLANATION. 95 percent confidence interval: EXPLANATION.  17.91768 EXPLANATION.  22.26357 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION.  20.09062 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg) ‘mpg’ is a quantitative variable (numeric vector) from the mtcars dataset.  Click to Show Output  Click to View Output. ## [1] 20 18 Explanation When we want to check if a claim about the average of a group(population mean \\(\\mu\\)) is true, we often use a test called the “one sample t test”. This test works well when we can assume that the data follows a normal pattern and that we’ve picked our sample randomly from the bigger group we’re interested in. In the figure below, the null hypothesis \\(H_0: \\mu = \\mu_0\\) is represented by the normal distribution (gray) centered at \\(\\mu_0\\). - Note that \\(\\mu_0\\) is just some specified number. - This shows how the null hypothesis represents the assumption about the center of the distribution of the data. After a hypothesis (null) is established and an alternative hypothesis similarly declared, a simple random sample of data of size \\(n\\) is obtained from the population of interest. - In the plot above, this is depicted by the points (blue dots) which are centered around their sample mean \\(\\bar{x}\\). Above the points (blue dots) is another bell-shaped curve (blue dashed line). This curve shows that the alternative hypothesis might fit the data better than the null hypothesis. The role of the one sample t test is to measure the probability of a sample mean being as extreme or more extreme from the hypothesized value of \\(\\mu_0\\) than the one observed assuming the null hypothesis is true. - This probability is of course the p-value of the test. - This works because the sampling distribution of the sample mean has been assumed to be normal. In this case, the distribution of the test statistic t, \\[ t = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\] is known to follow a t distribution with \\(n-1\\) degrees of freedom. *The mathematics that provide this result are phenominal! You can consult any advanced statistical textbook for the details. The p-value of the one sample t test represents the probability that the test statistic \\(t\\) is as extreme or more extreme than the one observed according to a t-distribution with \\(n-1\\) degrees of freedom. If the probability (the p-value) is close enough to zero (smaller than \\(\\alpha\\)), then it is determined that the most plausible hypothesis is the alternative hypothesis, and thus the null is “rejected” in favor of the alternative. Paired Samples t Test The paired samples t test is used when a value is hypothesized for the popluation mean of the differences, \\(\\mu_d\\), obtained from paired observations. Overview Questions The Paired Samples t Test can be used to answer questions like: From pre-test to post-test is there an improvement on average in the subjects? How much taller are husbands than their wives, on average? Do hospital patients that are carefully matched together according to reason for being in the hospital, age, gender, ethnicity, height, and weight show increased stay times in the hospital when infected with a nosocomial infection compared to those who were not infected? Requirements The test is only appropriate when both of the following are satisfied. The sample of differences is representative of the population differences. The sampling distribution of the sample mean of the differences \\(\\bar{d}\\) (\\(\\bar{x}\\) of the differences) can be assumed to be normal. (This second requirement can be assumed to be satisfied when (a) the differences themselves can be assumed to be normal from a Q-Q Plot, or (b) when the sample size \\(n\\) of the differences is large.) Hypotheses Math Code $$ H_0: \\mu_d = 0 $$ $$ H_a: \\mu_d \\neq 0 $$ \\(H_0: \\mu_d = \\text{some number, but typically 0}\\)\\(H_a: \\mu_d \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepPaired studentPaired R Instructions Console Help Command: ?t.test() Option 1: t.test(NameOfYourData$Y1, NameOfYourData$Y2, paired = TRUE, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set like sleep or mtcars or KidsFeet. Y1 must be a “numeric” vector that represents the quantitative data from the first sample of data. Y2 must be a “numeric” vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y1 - Y2) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the “group1” data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the “group2” data from the sleep data set t.test( ‘t.test’ is an R function that performs one and two sample t-tests. sleep2$extra,  A numeric vector that represents the hours of extra sleep that the group had with drug 2. sleep1$extra,  A numeric vector that represents the hours of extra sleep that the same group had with drug 1. paired=TRUE,  Indicates that this is a paired t-Test. This will cause the subtraction of sleep2$extra - sleep1$extra to be performed to obtain the paired differences. To cause the subtraction to occur in the other order, reverse the order sleep1$extra, sleep2$extra occur in the t.test(…) function. mu = 0,  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. Paired t-test EXPLANATION. data: sleep2$extra and sleep1$extra EXPLANATION. t = 4.0621, EXPLANATION.  df = 9, EXPLANATION.  p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  0.7001142 EXPLANATION.  2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of the differences EXPLANATION.   1.58 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. sleep2$extra The hours of extra sleep that the group had with drug 2.  -  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. ) Closing parenthesis for qqPlot(…) function.  Click to Show Output  Click to View Output. ## [1] 9 5 Option 2: Compute the differences yourself instead of using paired=TRUE. differences = NameOfYourData$Y1 - NameOfYourData$Y2 t.test(differences, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a “numeric” vector that represents the quantitative data from the first sample of data. Y2 must be a “numeric” vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. differences are the resulting differences obtained from subtracting Y1 - Y2. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(differences) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the “group1” data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the “group2” data from the sleep data set differences <-  Saved the computed differences to an object called ‘differences’. sleep2$extra The hours of extra sleep that the group had with drug 2.  -  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. differences,  ‘differences’ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2. mu = 0,  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. One Sample t-test EXPLANATION. data: differences EXPLANATION. t = 4.0621, EXPLANATION.  df = 9, EXPLANATION.  p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  0.7001142 EXPLANATION.  2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION.   1.58 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. differences) ‘differences’ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2.  Click to Show Output  Click to View Output. ## [1] 9 5 Explanation The paired samples t test considers the single mean of all the differences from the paired values. Thus, the paired samples t test essentially becomes a one sample t test on the differences between paired observations. Hence the requirement is that the sampling distribution of the sample mean of the differences, \\(\\bar{d}\\), can be assumed to be normally distributed. (It is also required that the obtained differences represent a simple random sample of the full population of possible differences.) The paired samples t test is similar to the independent samples t test scenario, except that there is extra information that allows values from one sample to be paired with a value from the other sample. This pairing of values allows for a more direct analysis of the change or difference individuals experience between the two samples. The points in the plot below demonstrate how points are paired together, and the only thing of interest are the differences between the paired points. Wilcoxon Signed-Rank Test For testing hypotheses about the value of the median of: one sample of quantitative data or one set of differences from paired data. Box plots + Dot plots are great to show off this kind of data (because it shows the median!) Median line closer to the bottom of the box plot = Right Skewed Median line is closer to the top of the box plot = Left Skewed Overview The nonparametric equivalent of the paired-samples t test as well as the one-sample t test. Best for smaller sample sizes where the distribution of the data is not normal The t test is more appropriate when the data is normal or when the sample size is large. While the test will work in most scenarios it suffers slightly when ties (repeated values) are present in the data. If many ties are present in the data, the test is not appropriate. If only a few ties are present, the test is still appropriate. This test is similar to the paired-samples t test and the one-sample t test, but it doesn’t assume the data follows a normal distribution (nonparametric equivalent) It works best when: - You have a small number of samples - Your data doesn’t follow a normal distribution ** The t test is better when your data is normal or when you have a lot of samples.This test usually works fine, but it can have problems if you have many repeated values in your data. If there are just a few repeated values, it’s still okay to use. Hypotheses Originally created to test hypotheses about the value of the median works as well for the mean when the distribution of the data is symmetrical. One Sample of Data Math Code $$ H_0: \\text{Median} = \\text{(Some Number)} $$ $$ H_a: \\text{Median} \\neq \\text{(Same Number)} $$ \\(H_0: \\text{Median} = \\text{(Some Number)}\\) \\(H_a: \\text{Median} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{(Some Number)}\\) Paired Samples of Data Math Code $$ H_0: \\text{median of differences} = 0 $$ $$ H_a: \\text{median of differences} \\neq 0 $$ \\(H_0: \\text{median of differences} = 0\\) \\(H_a: \\text{median of differences} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ 0\\) Examples: sleep, CornHeights R Instructions Console Help Command: ?wilcox.test() Paired Data wilcox.test(Y1, Y2, mu = YourNull, alternative = YourAlternative, paired = TRUE, conf.level = 0.95) Y1 must be a “numeric” vector. One set of measurements from the pair. Y2 also a “numeric” vector. Other set of measurements from the pair. YourNull is the numeric value from your null hypothesis for the median of differences from the paired data. Usually zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests. sleep$extra[sleep$group==1],  The hours of extra sleep that the group had with drug 2. sleep$extra[sleep$group==2],  The hours of extra sleep that the same group had with drug 1. mu = 0,  The numeric value from the null hypothesis for the median of differences from the paired data is 0 meaning the null hypothesis is \\(\\text{median of differences} = 0\\). paired=TRUE,  This command forces a “paired” samples test to be performed. alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\text{median of differences} \\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Wilcoxon signed rank test with continuity correction The phrase “with continuity correction” implies that instead of using the “exact” distribution of the test statistic a “normal approximation” was used instead to compute the p-value. Further, a small correction was made to allow for the change from the “discrete” exact distribution to the “continuous normal distribution” when calculating the p-value. data: sleep$extra[sleep$group == 1] and sleep$extra[sleep$group == 2] This statement of the output just reminds you of the code you used to perform the test. The important thing is recognizing that the first group listed is “Group 1” and the second group listed is “Group 2.” This is especially important when using alternative hypotheses of “less” or “greater” as the order is always “Group 1” is “less” than “Group 2” or “Group 1” is “greater” than “Group 2.” V = 0, This is the test statistic of the test, i.e., the sum of the ranks from the positive group minus the minimum sum of ranks possible.  p-value = 0.009091 This is the p-value of the test. If no warning is displayed when the test is run, then this is the “exact” p-value from the non-parametric Wilcoxon Test Statistic distribution. Sometimes a message will appear stating “Cannot compute exact p-value with ties” or other similar messages. In those cases, the p-value is still considered valid even though it is obtained through a normal approximation to the exact distribution. alternative hypothesis: true location shift is not equal to 0 This reports that the alternative hypothesis was “two-sided.” If the alternative had been “less” or “greater” the wording would change accordingly. One Sample wilcox.test(object, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object must be a “numeric” vector. YourNull is the numeric value from your null hypothesis for the median (even though it says “mu”). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,  ‘mpg’ is a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,  The numeric value from the null hypothesis is 20 meaning \\(\\mu = 20\\). alternative = “two.sided”,  The alternative is “two.sided” meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1−α.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Wilcoxon signed rank test with continuity correction This reports on the type of test performed. The phrase “with continuity correction” implies the normal approximation was used when calculating the p-value of the test. data: mtcars$mpg This print-out reminds us that the mpg column of the mtcars data was used as “Y” in the test. V = 249, The test statistic of the test.  p-value = 0.7863 The p-value of the test. alternative hypothesis: true location is not equal to 20 The words “not equal” tell us this was a two-sided test. Had it been a one-sided test, either the word “less” or the word “greater” would have appeared instead of “not equal.” Explanation In many cases it is of interest to perform a hypothesis test about the location of the center of a distribution of data. The Wilcoxon Signed Rank Test allows a nonparametric approach to doing this. The Wilcoxon Signed-Rank Test covers two important scenarios: One sample of data from a population. (Not very common.) The differences obtained from paired data. (Very common.) The Wilcoxon methods are most easily explained through examples, beginning with the paired data for which the method was originally created. ** Scroll down for the One Sample Example if that is what you are really interested in. However, it is still recommended that you read the paired data example first. Paired Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon. Background Height differences “between cross- and self- fertilized corn plants of the same pair” were collected. The experiment hypothesized that the center of the distribution of the height differences would be zero, with the alternative being that the center was not zero. The result of the data collection was 15 height differences: Differences: 14, 56, 60, 16, 6, 8, -48, 49, 24, 28, 29, 41, -67, 23, 75 Step 1 The first step of the Wilcoxon Signed Rank Test is to order the differences from smallest magnitude to largest magnitude. Negative signs are essentially ignored at this point and only magnitudes of the numbers matter. Sorted Differences: 6, 8, 14, 16, 23, 24, 28, 29, 41, -48, 49, 56, 60, -67, 75 Step 2 The next step is to rank the ordered values. Negative signs are attached to the ranks corresponding to negative numbers.   Differences: 6 8 14 16 23 24 28 29 41 -48 49 56 60 -67 75 Ranks: 1 2 3 4 5 6 7 8 9 -10 11 12 13 -14 15 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=15\\). Step 3 The ranks are then put into two groups. Negative Ranks Positive Ranks -10, -14 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15 Step 4 One of the groups is summed, usually the group with the fewest observations. Only the absolute values of the ranks are summed. Sum of Negative Ranks: \\(\\left|-10\\right| + \\left|-14\\right| = 24\\) The sum of the ranks becomes the test statistic of the Wilcoxon Test. The test statistic is sometimes called \\(W\\) or \\(V\\) or \\(U\\). Step 5 The \\(p\\)-value of the test is then obtained by computing the probability of the test statistic being as extreme or more extreme than the one obtained. This is done by first computing the probability of all possible values the test statistic could have obtained using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=15\\) ranks, the possible sums of ranks range from 0 to 120 and hit every integer in between, i.e., \\(1, 2, 3, \\ldots, 120\\). (Note, if summing the negative ranks these sums would technically all be negative.) To verify that \\(120\\) is the largest sum possible for \\(n=15\\) ranks, note that: \\(1+15 = 16\\), \\(2 + 14 = 16\\), \\(3+13 = 16\\), \\(4+12=16\\), \\(5+11=16\\), \\(6+10=16\\), \\(7+9=16\\), and finally that \\(8 = \\frac{16}{2}\\). Thus, there are 7 sums of 16 and one sum of \\(\\frac{16}{2}\\). This could be said in a mathematically equivalent way by stating there are \\(\\frac{14}{2}\\) sums of 16 and one sum of \\(\\frac{16}{2}\\). By multiplication this gives \\[ \\frac{14}{2}\\cdot\\frac{16}{1} + \\frac{1}{1}\\cdot\\frac{16}{2} = \\frac{14\\cdot16 + 1\\cdot16}{2} = \\frac{15\\cdot16}{2} = \\frac{n(n+1)}{2} = 120 \\] The probability of each sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 32,768 total different groups of ranks possible when there are \\(n=15\\) ranks.) For example, a sum of 1 can happen only one way, only the rank of 1 is in the group. A sum of 2 can also only happen 1 way. The sum of 3 however, can happen two ways: we could have the ranks of 1 and 2 in the group, or just the rank of 3 in the group. A similar counting technique is then implemented for each possible sum. After all the calculations are performed, the distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(24\\) (or its opposite of \\(120-24=96\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of (the absolute value of) negative ranks as extreme or more extreme than \\(24\\) is \\(p=0.04126\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would reject the null hypothesis that the center of the distribution of differences is zero. We conclude that the center of the distribution is greater than zero because the sum of negative ranks is much smaller than we expected under the zero center hypothesis (the null). Thus, there is sufficient evidence to conclude that the centers of the distributions of “cross- and self-fertilized corn plants” heights are not equal. One is greater than the other. Notice how the following dot plot shows that the differences are in favor of the cross-fertilized plants (the first group in the subtraction) being taller. This is true even though two self-fertilized plants were much taller than their cross-fertilized counterpart (the two negative differences). Comment If the distribution of differences is symmetric, then the hypotheses can be written as \\[ H_0: \\mu = 0 \\] \\[ H_a: \\mu \\neq 0 \\] If the distribution is skewed, then the hypotheses technically refer to the median instead of the mean and should be written as \\[ H_0: \\text{median} = 0 \\] \\[ H_a: \\text{median} \\neq 0 \\] One Sample Example {#one} The idea behind the one sample Wilcoxon Signed Rank test is nearly identical to the paired data. The only change is that the median must be subtracted from all observed values to obtain the differences. Note that the mean is equal to the median when data is symmetric. Background Suppose we are interested in testing to see if the median hourly wage of BYU-Idaho students during their off-track employment is equal to the minimum wage in Idaho, $7.25 an hour as of January 1st, 2015. Five randomly sampled hourly wages from BYU-Idaho Math 221B students provides the following data. Wages: $6.00, $9.00, $8.10, $18.00, $10.45 The differences are then obtained by subtracting the hypothesized value for the median (or mean if the data is symmetric) from all observations. Differences: -1.25, 1.75, 0.85, 10.75, 3.20 Note: from this point down, the wording of this example is identical to the paired data example (above) with the numbers changed to match \\(n=5\\). It is useful to continue reading to reinforce the idea of the Wilcoxon Signed Rank Test, but no new knowledge will be presented. Step 1 The first step of the Wilcoxon Signed Rank Test is to order the differences from smallest magnitude to largest magnitude. Negative signs are essentially ignored at this point and only magnitudes of the numbers matter. Sorted Differences: 0.85, -1.25, 1.75, 3.20, 10.75 Step 2 The next step is to rank the ordered values. Negative signs are attached to the ranks corresponding to negative numbers. Ranks: 1, -2, 3, 4, 5 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=5\\). Step 3 The ranks are then put into two groups. Negative Ranks Positive Ranks -2 1, 3, 4, 5 Step 4 One of the groups is summed, usually the group with the fewest observations. Sum of Negative Ranks: \\(\\left|-2\\right| = 2\\) Step 5 The \\(p\\)-value of the test is then obtained by computing the probabilities of all possible sums using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=5\\) ranks, the possible sums of ranks range from 0 to 15 and hit every integer in between, i.e., \\(1, 2, 3, \\ldots, 15\\). (Note, if summing the negative ranks these sums would technically all be negative.) To verify that \\(15\\) is the largest sum possible for \\(n=5\\) ranks, note that: \\(1+5 = 6\\), \\(2 + 4 = 6\\), and finally \\(3 = \\frac{6}{2}\\). Thus, there are 2 sums of 6 and one sum of \\(\\frac{6}{2}\\). This could be said in a mathematically equivalent way by stating there are \\(\\frac{4}{2}\\) sums of 6 and one sum of \\(\\frac{6}{2}\\). By multiplication this gives \\[ \\frac{4}{2}\\cdot\\frac{6}{1} + \\frac{1}{1}\\cdot\\frac{6}{2} = \\frac{4\\cdot6 + 1\\cdot6}{2} = \\frac{5\\cdot6}{2} = \\frac{n(n+1)}{2} = 15 \\] The probability of each sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 32 total different groups of ranks possible when there are \\(n=5\\) ranks.) For example, a sum of 1 can happen only one way, only the rank of 1 is in the group. A sum of 2 can also only happen 1 way. The sum of 3 however, can happen two ways: we could have the ranks of 1 and 2 in the group, or just the rank of 3 in the group. A similar counting technique is then implemented for each possible sum. After all the calculations are performed, the distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(2\\) (or its opposite of \\(15-2=13\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of negative ranks as extreme or more extreme than \\(-2\\) is \\(p=0.1875\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would fail to reject the null hypothesis that the center of the distribution of differences is zero. We will continue to assume the null hypothesis was true, that the median off-track hourly wage of BYU-Idaho students is the same as the Idaho minimum wage. Final Comment Notice that when the sample size is large the distribution of the test statistic can be approximated by a normal distribution. Most software applications use this approximation when the sample size is over \\(50\\), i.e., for \\(n\\geq50\\) because computing all the possible sums of ranks becomes incredibly time consuming. Thus, for large sample sizes the results will be almost identical whether the Wilcoxon Tests or a t Test is used. Quantitative Y | Categorical X (2 Groups) Graphics Y is a single quantitative variable of interest. This would be like “heights” of BYU-Idaho students. X is a qualitative (categorical) variable of interest like “gender” that has just two groups “A” and “B”. So this logo represents situtations where we would want to compare heights of male (group A) and female (group B) students. Test(s) used for this category: Independent Samples T-test Analysis : High School Seniors t Test Wilcoxon Rank Sum (Mann- Whitney Test) Analysis : The Benefits of Word Recall Strategies Used for the comparison of two groups, Quantitative Y (ex. height, weight, money, distance) and Categorical X (ex. gender, birth month, phone number) Questions that this answers: How long are boys feet and how long are girls feet? Do boys (B) or girls (G) have longer feet, on average, in the KidsFeet dataset? These are the best graphics to use: Examples: How long are boys feet and how long are girls feet? (Independent Sample) Can use box or dot plot! plot_ly(KidsFeet, y=~length, x=~as.factor(sex),type=\"box\",color=~sex, colors=c(\"lightblue\",\"pink\")) %>% layout( title=\"4th Grader's Average Foot Size\", xaxis=list(title=\"Gender\"),yaxis=list(title=\"Length (cm)\")) ggplot(KidsFeet, aes(x=factor(sex),y=length, fill=sex)) + coord_flip() + geom_dotplot(binaxis=\"y\", stackdir=\"up\", position=\"dodge\", dotsize=0.45, binwidth=0.5) + scale_fill_manual(values=c(\"B\"=\"lightblue\",\"G\"=\"pink\")) + labs(title= \"Kennedy 4th Grader's Foot Size\", x=\"Length (cm)\", y=\"Gender\") Does the Meshed or Before approach have any positive benefits on memory recall when it comes to remembering content? (Wilcoxon Rank Sum Test) Dot plot over box plot shows the distribution and the individuals within each distribution Tests Independent Samples t Test The independent samples t test is used when a value is hypothesized for the difference between two (possibly) different population means, \\(\\mu_1 - \\mu_2\\). finds the mean of each data set, then subtracts those means for their difference two groups don’t depend on each other’s results Overview Questions The Independent Samples t Test can be used to answer questions like: Are boys taller than girls on average? Do students who show up to class everyday get higher scores on average than those who don’t? Do you take more steps on average on weekdays or on weekends? Requirements The test is only appropriate when both of the following are satisfied. Both samples are representative of the population. (Simple random samples are the best way to do this.) The sampling distribution of the difference of the sample means \\((\\bar{x}_1 - \\bar{x}_2)\\) can be assumed to be normal. This is a safe assumption when the sample size of each group is \\(30\\) or greater or when the population data from each group can be assumed to be normal with a Q-Q Plot. Hypotheses Math Code $$ H_0: \\mu_\\text{label 1} - \\mu_\\text{label 2} = 13.2 $$ $$ H_a: \\mu_\\text{label 1} - \\mu_\\text{label 2} \\neq 13.2 $$ \\(H_0: \\mu_1 - \\mu_2 = \\text{some number, but typically 0}\\) \\(H_a: \\mu_1 - \\mu_2 \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepInd student1 student2 R Instructions Console Help Command: ?t.test() There are two ways to perform the test. Option 1: t.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a “numeric” vector from YourData that represents the data for both samples. X must be a “factor” or “character” vector from YourData that represents the group assignment for each observation. There can only be two groups specified in this column of data. YourNull is the numeric value from your null hypothesis for \\(\\mu_1-\\mu_2\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y ~ X, data=YourData) Example Code Hover your mouse over the example codes to learn more. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. length  ‘length’ is a quantitative variable (numeric vector). ~  ‘~’ is the tilde symbol. sex,  ‘sex’ is a ‘factor’ or ‘character’ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet,  ‘KidsFeet’ is a dataset in library(mosaic). Type View(KidsFeet) to view it. mu = 0,  The numeric value from the null hypothesis for μ1-μ2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = “two.sided”,  The alternative is “two-sided” meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\)     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. Welch Two Sample t-test EXPLANATION. data: length by sex EXPLANATION. t = 1.9174, EXPLANATION.  df = 36.275, EXPLANATION.  p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  -0.04502067 EXPLANATION.  1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean in group B mean in group G EXPLANATION.   25.10500 EXPLANATION.   24.32105 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. length  ‘length’ is a quantitative variable (numeric vector). ~  ‘~’ is the tilde symbol. sex,  ‘sex’ is a “factor” or “character” vector that represents the group assignment for each observation. There are two groups. data=KidsFeet) ‘KidsFeet’ is a dataset in library(mosaic). Type View(KidsFeet) to view it.  Click to Show Output  Click to View Output. Option 2: t.test(NameOfYourData$Y1, NameOfYourData$Y2, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a “numeric” vector that represents the quantitative data from the first sample. Y2 must be a “numeric” vector that represents the quantitative data from the second sample. YourNull is the numeric value from your null hypothesis for the difference of \\(\\mu_1-\\mu_2\\). This is typically zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) par(mfrow=c(1,2)) qqPlot(NameOfYourData$Y1) qqPlot(NameOfYourData$Y2) Example Code Hover your mouse over the example codes to learn more. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. KidsFeet$length[KidsFeet$sex == “B”],  A numeric vector that represents the quantitative data or the foot length for the first sample of data which in this case is the boys. KidsFeet$length[KidsFeet$sex == “G”],  A numeric vector that represents the quantitative data or the foot length for the second sample of data which in this case is the girls. mu = 0,  The numeric value from the null hypothesis for μ1-μ2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = “two.sided”,  The alternative is “two-sided” meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\)     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Welch Two Sample t-test EXPLANATION. data: KidsFeet$length[KidsFeet$sex == “B”] and KidsFeet$length[KidsFeet$sex == “G”] EXPLANATION. t = 1.9174, EXPLANATION.  df = 36.275, EXPLANATION.  p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  -0.04502067 EXPLANATION.  1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean of x mean of y EXPLANATION.   25.10500 EXPLANATION.   24.32105 EXPLANATION. par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow=c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. KidsFeet$length[KidsFeet$sex == “B”]) A numeric vector that represents the quantitative data or the foot length for the first sample of data which in this case is the boys. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. KidsFeet$length[KidsFeet$sex == “G”]) A numeric vector that represents the quantitative data or the foot length for the second sample of data which in this case is the girls.  …  Click to View Output. ## [1] 9 8 ## [1] 18 7 Explanation The first figure below depicts the scenario where the difference in means of two separate normal distributions is non-zero. - In other words, the two distributions have different means, \\(\\mu_1\\) and \\(\\mu_2\\), respectively. - It is worth emphasizing that the values of \\(\\mu_1\\) and \\(\\mu_2\\) are unknown to the researcher. The only thing observed are two separate samples of data (blue dots) of sizes \\(n_1\\) and \\(n_2\\), respectively. For the scenario depicted, the null hypothesis that \\(H_0: \\mu_1 - \\mu_2 = 0\\) (i.e., that \\(\\mu_1=\\mu_2\\)) is rejected in favor of the alternative that \\(H_a: \\mu_1 - \\mu_2 \\neq 0\\) based on the sample data observed. - This dicision would be correct as the true difference in means, \\(\\mu_1-\\mu_2\\) is non-zero in this case. When the null hypothesis is true, that \\(H_0: \\mu_1 - \\mu_2 = 0\\) : - the test statistic \\(t\\) that is obtained by measuring the distance between the two sample means, \\(\\bar{x}_1-\\bar{x}_2\\) - appropriately standardizing the result follows a \\(t\\) distribution with degrees of freedom less than or equal to \\(n_1+n_2-2\\). - Thus, the \\(p\\)-value of the independent samples \\(t\\) test is obtained by using this \\(t\\) distribution to calculate the probability of a test statistic \\(t\\) being as extreme or more extreme than the one observed assuming the null hypothesis is true. \\[ t = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{s_1/n_1+s_2/n_2 }} \\] Wilcoxon Rank Sum (Mann-Whitney) Test For testing the equality of the medians of two (possibly different) distributions of a quantitative variable. Box plots + Dot plots are great to show off this kind of data (because it shows the median!) Median line closer to the bottom of the box plot = Right Skewed Median line is closer to the top of the box plot = Left Skewed Overview The nonparametric equivalent of the Independent Samples t Test. Can also be used when data is ordered (ordinal) but does not have an exact measurement. For example, first place, second place, and so on. The Independent Samples t Test is more appropriate when the distributions are normal, or when the sample size for each sample is large. The test is negatively affected when there are ties (repeated values) present in the data, but the results are still useful if there are relatively few ties. Hypotheses Originally designed to test for the equality of medians from two identically shaped distributions. Math Code $$ H_0: \\text{difference in medians} = 0 $$ $$ H_a: \\text{difference in medians} \\neq 0 $$ \\(H_0: \\text{difference in medians} = 0\\) \\(H_a: \\text{difference in medians} \\neq 0\\) However, the test also allows for the more general hypotheses that one distribution is stochastically greater than the other. \\(H_0: \\text{the distributions are stochastically equal}\\) \\(H_a: \\text{one distribution is stochastically greater than the other}\\) If these hypotheses are used, then the distributions do not have to be identically distributed. (Note: Men’s heights are stochastically greater than women’s heights because men are generally taller than women, but not all men are taller than all women.) Examples: BugSpray, MoralIntegration R Instructions Console Help Command: ?wilcox.test() There are two ways to perform the test. Option 1: wilcox.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a “numeric” vector from YourData that represents the data for both samples. X must be a “factor” or “character” vector from YourData that represents the group assignment for each observation. There can only be two groups specified in this column of data. YourNull is the numeric value from your null hypothesis for the difference in medians from the two groups. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests. length  ‘length’ is a quantitative variable (numeric vector). ~  ‘~’ is the tilde symbol. sex,  ‘sex’ is a ‘factor’ or ‘character’ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet,  ‘KidsFeet’ is a dataset in library(mosaic). Type View(KidsFeet) to view it. mu = 0,  The numeric value from the null hypothesis for the difference in medians from the two groups is 0 meaning the null hypothesis is \\(\\text{difference in medians} = 0\\) alternative = “two.sided”,  The alternative is “two-sided” meaning the alternative hypothesis is \\(\\text{difference in medians} \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\)     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Wilcoxon rank sum test with continuity correction This states the test that was performed and that a normal approximation to the test statistic was used instead of the exact distribution. data: length by sex This states that the length column was split into the two groups found in the “sex” column. Unfortunately, it forgets to remind us that the test used the KidsFeet data set. V = 252, The test statistic of the test. In this case, the sum of the ranks from the alphabetically first group minus the minimum sum of ranks possible.  p-value = 0.0836 The p-value of the test. alternative hypothesis: true location shift is not equal to 0 This reports that the test used an alternative hypothesis of “not equal” (a two-sided test). Further, the phrase “true location shift” emphasizes that the Wilcoxon Rank Sum Test is testing to see if one distribution is shifted higher or lower than the other. Option 2: wilcox.test(object1, object2, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object1 must be a “numeric” vector that represents the first sample of data. obejct2 must be a “numeric” vector that represents the second sample of data. YourNull is the numeric value from your null hypothesis for the difference in medians from the two groups. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests. KidsFeet$length[KidsFeet$sex == “B”],  A numeric vector of foot length for the first sample of data or for the group of boys. KidsFeet$length[KidsFeet$sex == “G”],  A numeric vector of foot length for the second sample of data or for the group of girls. mu = 0,  The numeric value from the null hypothesis for the difference in medians from the two groups is 0 meaning the null hypothesis is \\(\\text{difference in medians} = 0\\) alternative = “two.sided”,  The alternative is “two-sided” meaning the alternative hypothesis is \\(\\text{difference in medians} \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\)     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Wilcoxon rank sum test with continuity correction This states the type of test performed. data: KidsFeet$length[KidsFeet$sex == “B”] and KidsFeet$length[KidsFeet$sex == “G”] Reminds you what data was used for the test and points out that the first set of data listed is “Group 1” and the second set of data listed is “Group 2.” V = 252, The test statistic of the test.  p-value = 0.0836 The p-value of the test. alternative hypothesis: true location shift is not equal to 0 The alternative hypothesis of the test. The phrase “not equal” implies a “two-sided” alternative was used. Explanation In many cases it is of interest to perform a hypothesis test concerning the equality of the centers of two (possibly different) distributions. In other words, an independent samples test. The Wilcoxon Rank Sum Test allows a nonparametric approach to doing this. It is often considered the nonparametric equivalent of the independent samples t test. The method is most easily explained through an example. The theory behind it is very similar to the theory behind the Wilcoxon Signed-Rank Test. Independent Samples Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon. Background The percent of flies (bugs) killed from two different concentrations of a certain spray were recorded from 16 different trials, 8 trials per treatment concentration. The experiment hypothesized that the center of the distributions of the percent killed by either concentration were the same. In other words, that both treatments were equally effective. The alternative hypothesis was that the treatments differed in their effectiveness. Spray Concentration Percent Killed A 68, 68, 59, 72, 64, 67, 70, 74 B 60, 67, 61, 62, 67, 63, 56, 58 Step 1 The first step of the Wilcoxon Rank Sum Test is to order all the data from smallest magnitude to largest magnitude, while keeping track of the group. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Step 2 The next step is to rank the ordered values. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=16\\). Any ranks that are tied need to have the average rank assigned to each of those that are tied. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Rank 1 2 3 4 5 6 7 8 10 10 10 12.5 12.5 14 15 16 Step 3 The ranks are then returned to their original groups. Ranks of Spray A Ranks of Spray B 3, 8, 10, 12.5, 12.5, 14, 15, 16 1, 2, 4, 5, 6, 7, 10, 10 Step 4 The ranks are summed for one of the groups. (It does not matter which group.) Sum of Ranks for Spray A: \\[ 3+8+10+12.5+12.5+14+15+16 = 91 \\] Step 5 The \\(p\\)-value of the test is then obtained by computing the probabilities of all possible sums that one group can achieve using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=16\\) ranks, with just \\(8\\) of the ranks assigned to one group, the possible sums of ranks range from \\(36\\) to \\(100\\) and include every integer in between, i.e., \\(36, 37, 38, \\ldots, 100\\). Note that the smallest sum would be obtained if the ranks 1-8 were in the group. The largest sum would be obtained if the ranks 9-16 were in the group. The probability of each possible sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 12,870 total different sets of ranks possible when there are \\(n=16\\) ranks and \\(8\\) are assigned to one group.) The distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(91\\) (or its opposite of \\(136-91=45\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of negative ranks as extreme or more extreme than \\(91\\) is \\(p=0.01476\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would reject the null hypothesis that the difference in the center of the distributions is zero. We conclude that the Spray Concentration A is more effective at killing flies (bugs). Comment The hypotheses for the Wilcoxon Rank Sum Test are difficult to write out in simple mathematical statements. The test is often referred to as a test of medians, but goes deeper than this. Technically, it allows us to determine if one distribution is stochastically larger than another. In other words, if one distribution typically gives larger values than does another distribution. If the distributions are identically shaped and have the same spread, then this implies the medians (and means) are different. Thus, the hypotheses for the test can be written mathematically as \\[ H_0: \\text{difference in medians} = 0 \\] \\[ H_a: \\text{difference in medians} \\neq 0 \\] or even as \\[ H_0: \\mu_1-\\mu_2 = 0 \\] \\[ H_a: \\mu_1-\\mu_2 \\neq 0 \\] However, it is important to remember that the estimated difference in location parameters that results from the test does not provide a measurement on either of these things. However, the \\(p\\)-value can lead us to determine whether to reject, or fail to reject the null, whichever of the above hypotheses is used. As stated in the R help file for this test ?wilcox.test() “the estimator for the difference in location parameters does not estimate the difference in medians (a common misconception) but rather the median of the difference between a sample from [the first population] and a sample from [the second population].” These are technical details that most people ignore without encountering too much difficulty. However, it does remind us that the t test is more easily interpreted whenever it is appropriate to use that test. Final Comment Notice that when the sample size is large the distribution of the test statistic can be approximated by a normal distribution. Most software applications use this approximation when the sample size is over \\(50\\), i.e., for \\(n\\geq50\\) because computing all the possible sums of ranks becomes incredibly time consuming. Thus, for large sample sizes the results will be almost identical whether the Wilcoxon Tests or a t Test is used. Quantitative Y | Categorical X (3+ Groups) Graphics Y is a single quantitative variable of interest, like “heights” of BYU-Idaho students. X is a categorical (qualitative) variable like which Math 221 you took, 221A, 221B, or 221C. In other words, X has three or more groups. So “Classrank” could be X, with groups “Freshman”, “Sophomore”, “Junior”, and “Senior”. Test(s) used for this category: One-Way ANOVA (means) Block Design Kruskal-Wallis Rank Sum Test (distributions) Analysis : College Student’s Food Perception Used comparing several groups based on means or distributions: How long are their feet based of width and gender? Are there certain months of the year that are associated with children having longer feet, on average, than others? Does a college student’s perception of food result in a change in their weight? (Kruskal-Wallis Rank Sum Test) # Read data food <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv\") # Data transformation and cleaning food_cleaned <- food %>% # Convert weight to numeric right away mutate(weight = as.numeric(weight)) %>% # Remove rows with NA weight or missing drink values filter(!is.na(weight) & !is.na(drink)) %>% # Create a new column `Food Perception Score` mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) # Create plots boxplot(weight ~ `Food Perception Score`, data=food_cleaned, col=\"lightgreen\", xlab=\"Food Perception Score\", main=\"Weight of College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=food_cleaned, pch=16, vertical=TRUE, add=TRUE, col=\"palegreen4\") Tests One-way ANOVA Each experimental unit is assigned to exactly one factor-level combination. Another way to say this is “one measurement per individual” (no repeated measures) and “equal numbers of individuals per group”. Overview An ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA can be assumed to be normally distributed when seen in a Q-Q Plot.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out in a Residuals versus fitted-values plot.) comparing means Hypotheses For a One-way ANOVA \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] Mathematical Model A typical model for a one-way ANOVA is of the form \\[ Y_{ij} = \\mu_i + \\epsilon_{ij} \\] where \\(\\mu_i\\) is the mean for level (group) \\(i\\), and \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) is the error term for each point \\(j\\) within level (group) \\(i\\). R Instructions Console Help Command: ?aov() myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. X is a qualitative variable (should have class(X) equal to factor or character. If it does not, use as.factor(X) inside the aov(Y ~ as.factor(X),...) command. YourDataSet is the name of your data set. Perform the ANOVA myaov <- aov(Y ~ X, data=YourDataSet) summary(myaov) Diagnose ANOVA Assumptions par(mfrow=c(1,2)) plot(myaov, which=1:2) Example Code Hover your mouse over the example codes to learn more. Perform the ANOVA chick.aov <-  Saves the results of the ANOVA test as an object named ‘chick.aov’. aov( ‘aov()’ is a function in R used to perform the ANOVA. weight Y is ‘weight’, which is a numeric variable from the chickwts dataset.  ~  ‘~’ is the tilde symbol used to separate the Y and X in a model formula. feed, X is ‘feed’, which is a qualitative variable in the chickwts dataset, or more specifically, a factor with six levels: “casein”, “horsebean”, and so on… Use str(chickwts) to see this.  data = chickwts) ‘chickwts’ is a dataset in R. summary( ‘summary()’ shows the results of the ANOVA. chick.aov) ‘chick.aov’ is the name of the ANOVA.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## feed 5 231129 46226 15.37 5.94e-10 ## Residuals 65 195556 3009 ## ## feed *** ## Residuals ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Diagnose the ANOVA par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow = c(1,2)) The mfrow parameter controls “multiple frames on a row”. In this case, the c(1,2) specifies 1 row of 2 plots. This will cause the two diagnostic plots to be placed side-by-side. plot( ‘plot’ is a R function for the plotting of R objects. chick.aov, ‘chick.aov’ is the name of the ANOVA.  which = 1:2) The which=1:2 selects “which” of 6 available plots we want to have graphed. In this case, 1 shows the Residuals vs Fitted, and 2 shows the Normal QQ-plot. Both are needed to check the ANOVA assumptions.  Click to View Output  Click to View Output. Explanation Analysis of variance (ANOVA) is often applied to the scenario of testing for the equality of three or more means from (possibly) separate normal distributions of data. The normality assumption is required. No matter the sample size. If the distributions are skewed then a nonparametric test should be applied instead of ANOVA. One-Way ANOVA One-way ANOVA is when a completely randomized design is used with a single factor of interest. A typical mathematical model for a one-way ANOVA is of the form \\[ Y_{ik} = \\mu_i + \\epsilon_{ik} \\quad (\\text{sometimes written}\\ Y_{ik} = \\mu + \\alpha_i + \\epsilon_{ik}) \\] where \\(\\mu_i\\) is the mean of each group (or level) \\(i\\) of a factor, and \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) is the error term. The plot below demonstrates what these symbols represent. Note that the notation \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) states that we are assuming the error term \\(\\epsilon_{ik}\\) is normally distributed with a mean of 0 and a standard deviation of \\(\\sigma\\). Hypotheses The aim of ANOVA is to determine which hypothesis is more plausible, that the means of the different distributions are all equal (the null), or that at least one group mean differs (the alternative). Mathematically, \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_m = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\quad \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\}. \\] In other words, the goal is to determine if it is more plausible that each of the \\(m\\) different samples (where each sample is of size \\(n\\)) came from the same normal distribution (this is what the null hypothesis claims) or that at least one of the samples (and possibly several or all) come from different normal distributions (this is what the alternative hypothesis claims). Visualizing the Hypotheses The first figure below demonstrates what a given scenario might look like when all \\(m=3\\) samples of data are from the same normal distribution. In this case, the null hypothesis \\(H_0\\) is true. Notice that the variability of the sample means is smaller than the variability of the points. The figure below shows what a given scenario might look like for \\(m=3\\) samples of data from three different normal distributions. In this case, the alternative hypothesis \\(H_a\\) is true. Notice that the variability of the sample means, i.e., \\((\\bar{x}_1,\\bar{x}_2,\\bar{x}_3)\\), is greater than the variability of the points. Explaining the Name The above plots are useful in understanding the mathematical details behind ANOVA and why it is called analysis of variance. Recall that variance is a measure of the spread of data. When data is very spread out, the variance is large. When the data is close together, the variance is small. ANOVA utilizes two important variances, the between groups variance and the within groups variance. Between groups variance–a measure of the variability in the sample means, the \\(\\bar{x}\\)’s. Within groups variance–a combined measure of the variability of the points within each sample. The plot below combines the information from the previous plots for ease of reference. It emphasizes the fact that when the null hypothesis is true, the points should have a large variance (be really spread out) while the sample means are relatively close together. On the other hand, when the points are relative close together within each sample and the sample means have a large variance (are really spread out) then the alternative hypothesis is true. This is the theory behind analysis of variance, or ANOVA. Calculating the Test Statistic, \\(F\\) The ratio of the “between groups variation” to the “within groups variation” provides the test statistic for ANOVA. Note that the test statistic of ANOVA is an \\(F\\) statistic. \\[ F = \\frac{\\text{Between groups variation}}{\\text{Within groups variation}} \\] It would be good to take a minute and review the \\(F\\) distribution. The \\(p\\)-value for ANOVA thus comes from an \\(F\\) distribution with parameters \\(p_1 = m-1\\) and \\(p_2 = n-m\\) where \\(m\\) is the number of samples and \\(n\\) is the total number of data points. A Deeper Look at Variance It is useful to take a few minutes and explain the word variance as well as mathematically define the terms “within group variance” and “between groups variance.” Variance is a statistical measure of the variability in data. The square root of the variance is called the standard deviation and is by far the more typical measure of spread. This is because standard deviation is easier to interpret. However, mathematically speaking, the variance is the more important measurement. As mentioned previously, the variance turns out to be the key to determining which hypothesis is the most plausible, \\(H_0\\) or \\(H_a\\), when several means are under consideration. There are two variances that are important for ANOVA, the “within groups variance” and the “between groups variance.” Recall that the formula for computing a sample variance is given by \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})^2}{n-1} \\quad\\leftarrow \\frac{\\text{sum of squares}}{\\text{degrees of freedom}} \\] This formula has a couple of important pieces that are so important they have been given special names. The \\(n-1\\) in the denominator of the formula is called the “degrees of freedom.” The other important part of this formula is the \\(\\sum_{i=1}^n(x_i - \\bar{x})^2\\), which is called the “sum of squared errors” or sometimes just the “sum of squares” or “SS” for short. Thus, the sample variance is calculated by computing a “sum of squares” and dividing this by the “degrees of freedom.” It turns out that this general approach works for many different contexts. Specifically, it allows us to compute the “within groups variance” and the “between groups variance.” To introduce the mathematical definitions of these two variances, we need to introduce some new notation. Let \\(\\bar{y}_{i\\bullet}\\) represent the sample mean of group \\(i\\) for \\(i=1,\\ldots,m\\). Let \\(n_i\\) denote the sample size in group \\(i\\). Let \\(\\bar{y}_{\\bullet\\bullet}\\) represent the sample mean of all \\(n = n_1+n_2+\\cdots+n_m\\) data points. The mathematical calculations for each of these variances is given as follows. \\[ \\text{Between groups variance} = \\frac{\\sum_{i=1}^m (\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2}{m-1} \\leftarrow \\frac{\\text{Between groups sum of squares}}{\\text{Between groups degrees of freedom}} \\] \\[ \\text{Within groups variance} = \\frac{\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2}{n-m} \\leftarrow \\frac{\\text{Within groups sum of squares}}{\\text{Within groups degrees of freedom}} \\] A Fabricated Example The following table provides three samples of data: A, B, and C. These samples were randomly generated from normal distributions using a computer. The true means \\(\\mu_1, \\mu_2\\), and \\(\\mu_3\\) of the normal distributions are thus known, but withheld from you at this point of the example. A B C 13.15457 13.17463 16.66831 12.65225 12.16277 15.54719 13.73061 12.76905 16.63074 14.43471 13.38524 15.06726 13.79728 12.02690 15.57534 13.88599 13.24651 15.99915 12.77753 12.58386 15.58995 13.81536 12.64615 16.99429 13.03635 12.52055 15.47153 14.26062 14.03566 16.13330 An ANOVA will be performed with the sample data to determine which hypothesis is more plausible: \\[ H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\in \\{1,\\ldots,m\\} \\] To perform an ANOVA, we must compute the between groups variance and the within groups variance. This requires the Between groups sums of squares, within groups sums of squares, between groups degrees of freedom, and the within groups degrees of freedom. Note that to get the sums of squares, we first had to calculate \\(\\bar{y}_{1\\bullet}\\), \\(\\bar{y}_{2\\bullet}\\), \\(\\bar{y}_{3\\bullet}\\), and \\(\\bar{y}_{\\bullet\\bullet}\\) where the 1, 2, 3 corresponds to Samples A, B, and C, respectively. After some work, we find these values to be \\[ \\bar{y}_{1\\bullet} = 13.55, \\quad \\bar{y}_{2\\bullet} = 12.86 \\quad \\bar{y}_{3\\bullet} = 15.97 \\] and \\[ \\bar{y}_{\\bullet\\bullet} = \\frac{13.55+12.86+15.97}{3} = 14.13 \\] Using these values we can then compute the between groups sum of squares and the within groups sum of squares according to the formulas stated previously. This process is very tedious and will not be demonstrated. Only the results are shown in the following table which summarizes all the important information.   Degrees of Freedom Sum of Squares Variance F-value p-value Between groups 2 53.3 26.67 70.2 2e-11 Within groups 27 10.3 0.38 ANOVA Table In general, the ANOVA table is created by   Degrees of Freedom Sum of Squares Variance F-value p-value Between groups \\(m-1\\) \\(\\sum_{i=1}^m n_i(\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) \\(\\frac{\\text{Between groups variance}}{\\text{Within groups variance}}\\) \\(F\\)-distribution tail probability Within groups \\(n-m\\) \\(\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) ANOVA Assumptions{#residuals} The requirements for an analysis of variance (the assumptions of the test) are two-fold and concern only the error terms, the \\(\\epsilon_{ik}\\). The errors are normally distributed. The variance of the errors is constant. Both of these assumptions were stated in the mathematical model where we assumed that \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\). Checking ANOVA Assumptions To check that the ANOVA assumptions are satisfied, it is required to check the data in each group for normality using QQ-Plots. Also, the sample variance of each group must be relatively constant. The fastest way to check these two assumptions is by analyzing the residuals. An ANOVA residual is defined as the difference between the observed value of \\(y_{ik}\\) and the mean \\(\\bar{y}_{i\\bullet}\\). Mathematically, \\[ r_{ik} = y_{ik} - \\bar{y}_{i\\bullet} \\] One QQ-Plot of the residuals will provide the necessary evidence to decide if it is reasonable to assume that the error terms are normally distributed. Also, the constant variance can be checked visually by using what is known as a residuals versus fitted values plot. For the Fabricated Example above, the QQ-Plot and residuals versus fitted values plots show the two assumptions of ANOVA appear to be satisfied. Examples: chickwts (One-way) Block Design Repeated measures or other factors that group individuals into similar groups (blocks) are included in the study design. Overview A typical model for a block design is of the form \\[ Y_{lijk} = \\mu + B_l + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijlk} \\] where \\(\\mu\\) is the grand mean, \\(B_l\\) is the blocking factor, \\(\\alpha_i\\) is one factor with at least two levels, \\(\\beta_j\\) is another factor with at least two levels, \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors, and \\(\\epsilon_{ijlk} \\sim N(0,\\sigma^2)\\) is the error term. Only one block and one factor is required. Multiple blocks and multiple factors are allowed. It is not required to include interaction terms. The error term is always required. R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ Block+X1+X2+X1:X2, data=YourDataSet) Perform the test summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. Block is a qualitative variable that is not of direct interest, but is included in the model to account for variability in the data. It should have class(Block) equal to either factor or character. Use as.factor() if it does not. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. If it does not, use as.factor(X2). Note that factors C, D, and so on could also be added + to the model if desired. X1:X2 denotes the interaction of the factors X1 and X2. It is not required and should only be included if the interaction term is of interest. YourDataSet is the name of your data set. Examples: ChickWeight Kruskal-Wallis Rank Sum Test Allows for deciding if several samples come from the same population or if at least one sample comes from a different population. Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated. Can also be used when data is ordered (ordinal) but does not have an exact measurement. Best used when the distribution of the data is not normal. ANOVA is appropriate when the data is normal. comparing distributions Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, “is a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly. In many cases, this is practically equivalent to the mean of at least one population differing from the others.” Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable. g is a qualitative grouping variable defining which groups each value in x belongs to. It must either be a character vector or a factor vector. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. weight ‘weight’ is a numeric variable from the chickwts dataset that represents the quantatitive response variable.  ~  ‘~’ is the tilde symbol used to separate the left- and right-hand side in a model formula. feed, ‘feed’ is a qualitative grouping variable in the chickwts dataset.  data = chickwts) ‘chickwts’ is a dataset in R.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## ## Kruskal-Wallis rank sum test ## ## data: weight by feed ## Kruskal-Wallis chi-squared = 37.343, df = 5, ## p-value = 5.113e-07 Alternatively, you could use the following approach as well. kruskal.test(list(x, y, z)) x is a numeric vector of data values that represents the quantatitive response variable for the first group. y is a numeric vector of data values that represents the quantatitive response variable for the second group. z is a numeric vector of data values that represents the quantatitive response variable for the third group. Note that more than three vectors of data could be included inside of list(). Example Code Hover your mouse over the example codes to learn more. feed1 <- filter(chickwts, feed == “horsebean”) This splits out the first group of feed (horsebean) from the chickwts dataset. feed2 <- filter(chickwts, feed == “linseed”) This splits out the second group of feed (linseed) from the chickwts dataset. feed3 <- filter(chickwts, feed == “soybean”) This splits out the third group of feed (soybean) from the chickwts dataset. feed4 <- filter(chickwts, feed == “sunflower”) This splits out the fourth group of feed (sunflower) from the chickwts dataset. feed5 <- filter(chickwts, feed == “meatmeal”) This splits out the fifth group of feed (meatmeal) from the chickwts dataset. feed6 <- filter(chickwts, feed == “casein”) This splits out the sixth group of feed (casein) from the chickwts dataset. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. list(feed1$weight,feed2$weight,feed3$weight,feed4$weight,feed5$weight,feed6$weight)) A list of numeric vecotrs that represent the quantitative response variable for each group.  Click to View Output  Click to View Output. Load library(tidyverse) to run this code in R. ## ## Kruskal-Wallis rank sum test ## ## data: list(feed1$weight, feed2$weight, feed3$weight, feed4$weight, feed5$weight, feed6$weight) ## Kruskal-Wallis chi-squared = 37.343, df = 5, ## p-value = 5.113e-07 Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population. The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples. The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity. Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated. (Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\). Note that the notation \\(n_i\\) allows for each sample to be a different size. In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\). Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest. Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\). Award any tied values the average of the ranks of those values that are tied. In the bottle-cap data we have the following. Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample. Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\). In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\). In the bottle-cap data, \\(\\bar{R}_1 = \\frac{R_1}{n_1} = \\frac{24}{5} = 4.8\\) \\(\\bar{R}_2 = \\frac{R_2}{n_2} = \\frac{14}{3} = 4.667\\) \\(\\bar{R}_3 = \\frac{R_3}{n_3} = \\frac{40}{4} = 10.0\\) Step 5: If there are no ties, calculate the test statistic \\(H\\) by the formula \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] See the original article for the formula to use when there are ties present in the data. In the bottle-cap data, \\(H = 5.656\\). To understand the logic behind the test statistic \\(H\\), note that the null hypothesis assumes that each of the \\(C\\) samples are taken from the same population. Under this assumption, the ranks assigned to each sample should represent a uniform sample of the ranks \\(1,\\ldots,N\\). If the null hypothesis were true, we would expect the average of the ranks from each group, the \\(\\bar{R}_i\\), to be roughly about the same. More precisely, using the properties of the discrete uniform distribution (see wikipedia), we expect each \\(\\bar{R}_i\\) to be close to \\(\\frac{1}{2}(N+1)\\), which is the mean of the discrete uniform distribution. That \\(\\bar{R}_i\\) should be close to this value comes from the fact that the mean of a distribution of means is equal to the population mean. Further, the variance of \\(\\bar{R}_i\\) is given by the population variance, \\(\\frac{1}{12}(N^2-1)\\) in this case, divided by the sample size \\(n_i\\). Finally, the \\(\\frac{N-1}{N}\\) is a correction factor that will not be explained here. The full article explains the details about how a multivariate normal distribution is used to uncover the formula for \\(H\\). Thus, understanding \\(H\\) completely will require an advanced degree in statistical theory. However, the short story is that the distribution of the test statistic \\(H\\) is approximately a chi squared distribution with \\(C-1\\) degrees of freedom and has the form of a sum of values consisting of the (observed mean - expected mean)^2/(variance of the observed mean). Step 6: Calculate the p-value of \\(H\\) using a chi squared distribution with \\(C-1\\) degrees of freedom. \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] In the bottle-cap data, \\(p = 0.059\\), which represents the probability of \\(H\\) being as extreme or more extreme than the observed value of 5.656, assuming the null hypothesis is true. Examples: wages, pigweights Quantitative Y | Multiple Categorical X Graphics Y is a single quantitative variable of interest, like “heights” of BYU-Idaho students. X1 is a categorical (qualitative) variable like gender, with levels “boy” and “girl.” X2 is another categorical (qualitative) variable like “Classrank” with levels “Freshman”, “Sophomore”, and “Junior”. Of course, both X1 and X2 can have as many levels as you want to work with, but must have at least two levels each. Test(s) used for this category: Two-Way ANOVA (means) Used to answer THREE questions: the effects of factor 1 on the quantitative variable the effects of factor 2 on the quantitative variable the interaction between the two factors best paired with numerical summary looking at mean Analysis : Two-way ANOVA: Final Beginning Algebra Grades Question Examples: Based on your p-values and these graphs, which of the following is a correct conclusion to reach? xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1))) ANSWER : Among fourth graders, right handed boys have longer feet on average than left handed boys while the opposite is true for girls. Does the amount a student participates in the course affect the average final Beginning Algebra grades? Does the semester in which a student takes Beginning Algebra affect their average final grade? Does a student’s participation level vary depending on the semester? (Alternatively, does the semester influence a student’s participation?) BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") BAFinalGrades$Semester <- factor(BAFinalGrades$Semester, levels = c(\"Fall\",\"Winter\",\"Spring\")) ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=1)) + geom_point(color=\"firebrick1\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Spring Semester Seems to Have Higher Final Grades\", x=\"Semester\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades$Participation <- factor(BAFinalGrades$Participation, levels = c(\"Low\",\"Moderate\",\"High\")) ggplot(BAFinalGrades, aes(x=Participation, y=FinalGrade, group=1)) + geom_point(color=\"darkred\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Participation Seems to Lead to Higher Grades\", x=\"Participation\", y=\"Beginning Algebra Final Grades\") + theme_minimal() ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=Participation, color=Participation)) + geom_point() + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Beginning Algebra Seems Better during the Spring Semester\", x=\"BYU-Idaho Semesters\", y=\"Beginning Algebra Final Grades\") + theme_minimal() Tests Two-way ANOVA Overview A two-way ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses. Writing out the hypotheses can be very involved depending on whether you use the official “effects model” notation (very mathematically correct) or a simplified “means model” notation (which isn’t very mathematically correct, but gets the idea across in an acceptable way). Means Model Effects Model The first set of hypotheses are a “one-way” set of hypotheses for the first factor of the ANOVA. Factor: X1 with say, levels \\(A\\) and \\(B\\). \\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a “one-way” set of hypotheses, but for the second factor of the ANOVA. Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\mu_C = \\mu_D = \\mu_E = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one}\\ i\\in\\{1=C,2=D,3=E\\} \\] The third set of hypotheses are the most interesting hypotheses in a two-way ANOVA. These are called the interaction hypotheses. They test to see if the levels of one of the factors, say \\(X1\\), impact \\(Y\\) differently for the differing levels of the other factor, \\(X2\\). The hypotheses read formally as \\[ H_0: \\text{The effect of the first factor on Y} \\\\ \\text{is the same for all levels of the second factor.} \\] \\[ H_a: \\text{The effect of the first factor on Y is not the same} \\\\ \\text{for all levels of the second factor.} \\] A mathematically correct way to state the two-way ANOVA model is with the equation \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] In this model, \\(\\mu\\) is the grand mean (which is the average Y-value ignoring all information contained in the factors); \\(\\alpha_i\\) is the first factor \\(X1\\) with levels \\(A\\) and \\(B\\) (though there could be more levels in \\(X1\\) depending on your data); \\(\\beta_j\\) is the second factor with levels \\(C\\), \\(D\\), and \\(E\\) (though there could be fewer or more levels to this factor depending on your data); \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors which has \\(2\\times3=6\\) (may differ for your data) levels; and \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) is the normally distributed error term for each point \\(k\\) found within level \\(i\\) of \\(X1\\) and level \\(j\\) of \\(X2\\). This model allows us to more formally state the hypotheses as First factor \\(X1\\) having say, levels \\(A\\) and \\(B\\). \\[ H_0: \\alpha_A = \\alpha_B = 0 \\] \\[ H_a: \\alpha_i \\neq 0 \\ \\text{for at least one}\\ i\\in\\{1=A,2=B\\} \\] Second factor \\(X2\\) with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\beta_C = \\beta_D = \\beta_E = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j\\in\\{1=C,2=D,3=E\\} \\] Does the effect of the first factor (\\(X1\\)) change for the different levels of the second factor (\\(X2\\))? In other words, is there an interaction between the two factors \\(X1\\) and \\(X2\\)? \\[ H_0: \\alpha\\beta_{ij} = 0 \\ \\text{for all } i,j \\] \\[ H_a: \\alpha\\beta_{ij} \\neq 0 \\ \\text{for at least one } i,j \\] R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ X1+X2+X1:X2, data=YourDataSet) Perform the ANOVA summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. Note that factors X3, X4, and so on could also be added + to the model if desired, but this would create a three-way, or four-way ANOVA model, and so on. X1:X2 denotes the interaction of the factors X1 and X2. It is not required, but is usually included. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. warp.aov <-  Saves the results of the ANOVA test as an object named ‘warp.aov’. aov( ‘aov()’ is a function in R used to perform the ANOVA. breaks \\(Y\\) is ‘breaks’, which is a numeric variable from the warpbreaks dataset.  ~  ‘~’ is the tilde symbol used to separate the left- and right-hand side in a model formula. wool  The first factor \\(X1\\) is ‘wool’, which is a qualitative variable in the warpbreaks dataset. In this case, wool is a factor with two levels. Use str(warpbreaks) to see this. + tension  The second factor \\(X2\\) is ‘tension’, which is another qualitative variable in the warpbreaks dataset. In this case, tension is a factor with three levels. Use str(warpbreaks) to see this. + wool:tension, The interaction of the two factors: wool and tension.  data = warpbreaks) ‘warpbreaks’ is a dataset in R. summary( ‘summary()’ shows the results of the ANOVA. warp.aov) ‘warp.aov’ is the name of the ANOVA.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## wool 1 451 450.7 3.765 0.058213 ## tension 2 2034 1017.1 8.498 0.000693 ## wool:tension 2 1003 501.4 4.189 0.021044 ## Residuals 48 5745 119.7 ## ## wool . ## tension *** ## wool:tension * ## Residuals ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow = c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. plot( ‘plot’ is a R function for the plotting of R objects. warp.aov, ‘warp.aov’ is the name of the ANOVA.  which = 1:2) Will show the Residuals vs Fitted and the Normal QQ-plot to check the ANOVA assumptions.  Click to View Output  Click to View Output. Explanation Hypotheses in Two-way ANOVA The hypotheses that can be tested in a two-way ANOVA that includes an interaction term are three-fold. Hypotheses about \\(\\alpha\\) where \\(\\alpha\\) has \\(m\\) levels. \\[ H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_m = 0 \\] \\[ H_a: \\alpha_i \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\} \\] Hypotheses about \\(\\beta\\) where \\(\\beta\\) has \\(q\\) levels. \\[ H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_q = 0 \\] \\[ H_a: \\beta_j \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,q\\} \\] Hypotheses about the interaction term \\(\\alpha\\beta\\). \\[ H_0: \\text{the effect of one factor is the same across all levels of the other factor} \\] \\[ H_a: \\text{the effect of one factor differs for at least one level of the other factor} \\] Expanding the ANOVA Model{#expanding} It turns out that more can be done with ANOVA than simply checking to see if the means of several groups differ. Reconsider the mathematical model of two-way ANOVA. \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] This model could be expanded to include any number of new terms in the model. The power of this approach is in the several questions (hypotheses) that can be posed to data simultaneously. What happens if your ANOVA fails both requirement tests? Example: BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") ## Rows: 204 Columns: 3 ## ── Column specification ────────────────────────── ## Delimiter: \",\" ## chr (2): Semester, Participation ## dbl (1): FinalGrade ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. BAanova <- aov(FinalGrade ~ Semester + Participation + Semester:Participation, data=BAFinalGrades) par(mfrow=c(1,2)) plot(BAanova, which=1:2, pch=16) The residual plot shows that the constant (equal) variance assumption raises some concern as the points within each group have varying spread. The spread of each group seems to gets closer and closer as we go from left to right as well as some straying point from the more clustered groups. Thus, showing the variance of the residuals are not constant and do not accurately show a relationship between each variable. For the Q-Q plot, it shows that the residuals of the data is not quite normally distributed. There are visible points at the lower and upper ends of the graph deviating away form the diagonal line, which represents a normal distribution. Thus, showing us that the residuals of the data are not normal distributed. Although both graphs indicate that the data fails to meet both requirements, this primarily means our findings may be less reliable due to the abnormalities in the data. As a result, we should interpret the results cautiously as we proceed with our Two-Way ANOVA test! Examples: warpbreaks (Two-way), CO2 (three-way) Quantitative Y | Quantitative X Graphics Y is a single quantitative variable of interest, like “height”. X is another single quantitative variable of interest, like “shoe-size”. This would imply we are using “shoe-size” (X) to explain “height” (Y). Test(s) used for this category: Simple Linear Regression Analysis : Simple Linear Regression: BYU-Idaho Math Tutoring Used to find the relationship between one independent variable (predictor) and one dependent variable(outcome): Is there a relationship between the length of children’s feet and the width of their foot? Example: Does the amount of tutoring a student receives correlate with their chapter exam scores? (Simple Linear Regression) ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams) Tests Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\). Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$\\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$$ \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is “equal to” the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to “God’s Law” or “Natural Law”. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are “random” and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)… \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced “y-hat”, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what follows… \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, “the change in the average y-value for a one unit change in the x-value.” It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, “the average y-value when x is zero.” It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful. R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command.  <-  This is the “left arrow” assignment operator that stores the results of your lm() code into mylm name. lm( lm(…) is an R function that stands for “Linear Model”. It performs a linear regression analysis for Y ~ X. Y  Y is your quantitative response variable. It is the name of one of the columns in your data set. ~  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X. X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value.  data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X. ) Closing parenthesis for the lm(…) function. summary(mylm) The summary command allows you to print the results of your linear regression that were previously saved in mylm name.  Click to Show Output  Click to View Output. Example output from a regression. Hover each piece to learn more. Call: lm(formula = dist ~ speed, data = cars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -29.069 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -9.525 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -2.272 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -2.272 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-2.272) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   9.215 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   43.201 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -17.5791 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   6.7584 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -2.601 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (-17.5791) by its standard error (6.7584). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of -17.5791 is -2.601 standard errors (6.7584) from zero, which is a fairly surprising distance as shown by the p-value. 0.0123 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). speed This is always the name of your X-variable in your lm(Y ~ X, …).   3.9324 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.4155 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 9.464 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” of the slope (3.9324) by its standard error (0.4155). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from zero, which is a really surprising distance as shown by the smallness of the p-value. 1.49e-12 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  15.38 For this particular regression, the estimate of \\(\\sigma\\) is 15.38. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 48 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 50 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 50-2 = 48. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.6511, In this particular regression, 65.11% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data.  0.6438 In this case, the value of 0.6438 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is found as the ratio of the MSR/MSE where MSR = SSR/(p-1) and MSE = SSE/(n-p) where n is the sample size and p is the number of parameters in the regression model.  89.57 This is the value of the F-statistic for the lm(dist ~ speed, data=cars) regression. Note that SSE = sum( cars.lm$res^2 ) = 11353.52 with n - p = 50 - 2 = 48 degrees of freedom for this data. Further, SSR = sum( (cars.lm$fit - mean(cars$dist))^2 ) = 21185.46 with p - 1 = 1 degree of freedom. So MSR = 21185.46 and MSE = 11353.52 / 48 = 236.5317. So MSR / MSE = 21185.46 / 236.5317 = 89.56711.  on 1 and 48 DF, The 1 degree of freedom is the SSR degrees of freedom (p-1). The 48 is the SSE degrees of freedom (n-p).  p-value: 1.49e-12 The p-value for an F-statistic is found by the code pf(89.56711, 1, 48, lower.tail=FALSE), which gives the probability of being more extreme than the observed F-statistic in an F distribution with 1 and 48 degrees of freedom. Check Assumptions 1, 2, 3, and 5 When making it in an r chunk make your height 3! { r, fig.height = 3 } par( The par(…) command stands for “Graphical PARameters”. It allows you to control various aspects of graphics in Base R. mfrow= This stands for “multiple frames filled by row”, which means, put lots of plots on the same row, starting with the plot on the left, then working towards the right as more plots are created. c( The combine function c(…) is used to specify how many rows and columns of graphics should be placed together. 1, This specifies that 1 row of graphics should be produced. 3 This states that 3 columns of graphics should be produced. ) Closing parenthesis for c(…) function. ) Closing parenthesis for par(…) function. plot( This version of plot(…) will actually create several regression diagnostic plots by default. mylm, This is the name of an lm object that you created previously. which= This allows you to select “which” regression diagnostic plots should be drawn. 1 Selecting 1, would give the residuals vs. fitted values plot only. : The colon allows you to select more than just one plot. 2 Selecting 2 also gives the Q-Q Plot of residuals. If you wanted to instead you could just use which=1 to get the residuals vs fitted values plot, then you could use qqPlot(mylm$residuals) to create a fancier Q-Q Plot of the residuals. ) Closing parenthesis for plot(…) function. plot( This version of plot(…) will be used to create a time-ordered plot of the residuals. The order of the residuals is the original order of the x-values in the original data set. If the original data set doesn’t have an order, then this plot is not interesting. mylm The lm object that you created previously. $ This allows you to access various elements from the regression that was performed. residuals This grabs the residuals for each observation in the regression. ) Closing parenthesis for plot(…) function.  Click to Show Output  Click to View Output. Which regression assumption(s) does the residuals versus fitted values plot diagnose? Linear Relation Constance Variance Which regression assumption(s) does the Q-Q Plot of the residuals diagnose? Normal Errors Which regression assumption(s) does the Residuals versus Order plot diagnose? (Remember, this plot can only be created when the data was collected in a specific order.) Independent Errors Plotting the Regression Line Base R ggplot2 To add the regression line to a scatterplot use the abline(...) command: plot( The plot(…) function is used to create a scatterplot with a y-axis (the vertical axis) and an x-axis (the horizontal axis). Y  This is the “response variable” of your regression. The thing you are interested in predicting. This is the name of a “numeric” column of data from the data set called YourDataSet. ~  The tilde “~” is used to relate Y to X and can be found on the top-left key of your keyboard. X,  This is the explanatory variable of your regression. It is the name of a “numeric” column of data from YourDataSet. data= The data= statement is used to specify the name of the data set where the columns of “X” and “Y” are located. YourDataSet This is the name of your data set, like KidsFeet or cars or airquality. ) Closing parenthesis for plot(…) function. abline( This stands for “a” (intercept) “b” (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm This is the name of an lm(…) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(…) function will locate these two values from within mylm and use them to add a line to your current plot(…). ) Closing parenthesis for abline(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm) You can customize the look of the regression line with abline( This stands for “a” (intercept) “b” (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm, This is the name of an lm(…) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(…) function will locate these two values from within mylm and use them to add a line to your current plot(…). lty= The lty= stands for “line type” and allows you to select between 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. 1, This creates a solid line. Remember, other options include: 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. lwd= The lwd= allows you to specify the width of the line. The default width is 1. Using lwd=2 would double the thickness, and so on. Any positive value is allowed. 1, Default line width. To make a thicker line, us 2 or 3… To make a thinner line, try 0.5, but 1 is already pretty thin. col= This allows you to specify the color of the line using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). “someColor” Type colors() in R for options. ) Closing parenthesis for abline(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm, lty=1, lwd=1, col=\"firebrick\") You can add points to the regression with… points( This is like plot(…) but adds points to the current plot(…) instead of creating a new plot. newY  newY should be a column of values from some data set. Or, use points(newX, newY) to add a single point to a graph. ~  This links Y to X in the plot. newX,  newX should be a column of values from some data set. It should be the same length as newY. If just a single value, use points(newX, newY) instead. data=YourDataSet,  If newY and newX come from a dataset, then use data= to tell the points(…) function what data set they come from. If newY and newX are just single values, then data= is not needed. col=“skyblue”, This allows you to specify the color of the points using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). pch=16 This allows you to specify the type of plotting symbol to be used for the points. Type ?pch and scroll half way down in the help file that appears to learn about other possible symbols. ) Closing parenthesis for points(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) points(7,40, pch=16, col=\"skyblue\", cex=2) text(7,40, \"New Dot\", pos=3, cex=0.5) points(dist ~ speed, data=filter(cars, mylm$res > 2), cex=.8, col=\"red\") abline(mylm, lty=1, lwd=1, col=\"firebrick\") To add the regression line to a scatterplot using the ggplot2 approach, first ensure: library(ggplot2) or library(tidyverse) is loaded. Then, use the geom_smooth(method = lm) command: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. YourDataSet,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. X, This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet.  y =  “y= ” declares which variable will become the y-axis of the graphic. Y This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function. + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. + Here the + is used to add yet another layer to ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. There are several other methods that could be used here.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(…) to place more complicated models on the plot.  se = FALSE se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. ) Closing parenthesis for the geom_smooth() function.  Click to Show Output  Click to View Output. ggplot(cars, aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = \"lm\", formula=y~x, se=FALSE) There are a number of ways to customize the appearance of the regression line: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. speed,  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet. y =  “y= ” declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic.  + Here the + is used to add yet another layer to ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(…) to place more complicated models on the plot.  se = FALSE, se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.  size = 2, Use size = 2 to adjust the thickness of the line to size 2.  color = “orange”, Use color = “orange” to change the color of the line to orange.   linetype = “dashed” Use linetype = “dashed” to change the solid line to a dashed line. Some linetype options include “dashed”, “dotted”, “longdash”, “dotdash”, etc. ) Closing parenthesis for the geom_smooth() function.  Click to Show Output  Click to View Output. In addition to customizing the regression line, you can customize the points, add points, add lines, and much more. ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. speed,  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet. y =  “y= ” declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point( geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. size = 1.5, Use size = 1.5 to change the size of the points.  color = “skyblue” Use color = “skyblue” to change the color of the points to Brother Saunders’ favorite color.  alpha = 0.5 Use alpha = 0.5 to change the transparency of the points to 0.5. ) Closing parenthesis of geom_point() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in ways similar to lm(…) to place more complicated models on the plot.  se = FALSE, se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.  color = “navy”, Use color = “navy” to change the color of the line to navy blue.  size = 1.5 Use size = 1.5 to adjust the thickness of the line to 1.5. ) Closing parenthesis of geom_smooth() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_hline( Use geom_hline() to add a horizontal line at a specified y-intercept. You can also use geom_vline(xintercept = some_number) to add a vertical line to the graph. yintercept = Use “yintercept =” to tell geom_hline() that you are going to declare a y intercept for the horizontal line.  75 75 is the value of the y-intercept. , color = “firebrick” Use color = “firebrick” to change the color of the horizontal line to firebrick red. , size = 1, Use size = 1 to adjust the thickness of the horizontal line to size 1.              linetype = “longdash” Use linetype = “longdash” to change the solid line to a dashed line with longer dashes. Some linetype options include “dashed”, “dotted”, “longdash”, “dotdash”, etc. , alpha = 0.5 Use alpha = 0.5 to change the transparency of the horizontal line to 0.5. ) Closing parenthesis of geom_hline function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_segment( geom_segment() allows you to add a line segment to ggplot() by using specified start and end points. x = “x =” tells geom_segment() that you are going to declare the x-coordinate for the starting point of the line segment.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the starting point of the line segment.  y =“y =” tells geom_segment() that you are going to declare the y-coordinate for the starting point of the line segment.  75, 75 is a number on the y-axis of your graph. It is the y-coordinate of the starting point of the line segment.  xend = “xend =” tells geom_segment() that you are going to declare the x-coordinate for the end point of the line segment.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the end point of the line segment.  yend = “yend =” tells geom_segment() that you are going to declare the y-coordinate for the end point of the line segment.  38, 38 is a number on the y-axis of your graph. It is the y-coordinate of the end point of the line segment.                size = 1 Use size = 1 to adjust the thickness of the line segment. , color = “lightgray” Use color = “lightgray” to change the color of the line segment to light gray. , linetype = “longdash” Use *linetype = “longdash* to change the solid line segment to a dashed one. Some linetype options include”dashed”, “dotted”, “longdash”, “dotdash”, etc. ) Closing parenthesis for geom_segment() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_point( geom_point() can also be used to add individual points to the graph. Simply declare the x and y coordinates of the point you want to plot. x = “x =” tells geom_point() that you are going to declare the x-coordinate for the point.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the point.  y = “y =” tells geom_point() that you are going to declare the y-coordinate for the point.  75 75 is a number on the y-axis of your graph. It is the y-coordinate of the point. , size = 3 Use size = 3 to make the point stand out more. , color = “firebrick” Use color = “firebrick” to change the color of the point to firebrick red. ) Closing parenthesis of the geom_point() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_text( geom_text() allows you to add customized text anywhere on the graph. It is very similar to the base R equivalent, text(…). x = “x =” tells geom_text() that you are going to declare the x-coordinate for the text.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the text.  y = “y =” tells geom_text() that you are going to declare the y-coordinate for the text.  84, 84 is a number on the y-axis of your graph. It is the y-coordinate of the text.  label = “label =” tells geom_text() that you are going to give it the label.  “My Point (14, 75)”, “My Point (14, 75)” is the text that will appear on the graph.             color = “navy” Use color = “navy” to change the color of the text to navy blue. , size = 3 Use size = 3 to change the size of the text. ) Closing parenthesis of the geom_text() function.  + The + allows you to add more layers to the framework provided by ggplot().   theme_minimal() Add a minimalistic theme to the graph. There are many other themes that you can try out.  Click to Show Output  Click to View Output. ## `geom_smooth()` using formula = 'y ~ x' Accessing Parts of the Regression Finally, note that the mylm object contains the names(mylm) of mylm$coefficients Contains two values. The first is the estimated \\(y\\)-intercept. The second is the estimated slope. ## (Intercept) speed ## -17.579095 3.932409 mylm$residuals Contains the residuals from the regression in the same order as the actual dataset. ## 1 2 3 4 ## 3.849460 11.849460 -5.947766 12.052234 ## 5 6 7 8 ## 2.119825 -7.812584 -3.744993 4.255007 ## 9 10 11 12 ## 12.255007 -8.677401 2.322599 -15.609810 ## 13 14 15 16 ## -9.609810 -5.609810 -1.609810 -7.542219 ## 17 18 19 20 ## 0.457781 0.457781 12.457781 -11.474628 ## 21 22 23 24 ## -1.474628 22.525372 42.525372 -21.407036 ## 25 26 27 28 ## -15.407036 12.592964 -13.339445 -5.339445 ## 29 30 31 32 ## -17.271854 -9.271854 0.728146 -11.204263 ## 33 34 35 36 ## 2.795737 22.795737 30.795737 -21.136672 ## 37 38 39 40 ## -11.136672 10.863328 -29.069080 -13.069080 ## 41 42 43 44 ## -9.069080 -5.069080 2.930920 -2.933898 ## 45 46 47 48 ## -18.866307 -6.798715 15.201285 16.201285 ## 49 50 ## 43.201285 4.268876 mylm$fitted.values The values of \\(\\hat{Y}\\) in the same order as the original dataset. ## 1 2 3 4 5 ## -1.849460 -1.849460 9.947766 9.947766 13.880175 ## 6 7 8 9 10 ## 17.812584 21.744993 21.744993 21.744993 25.677401 ## 11 12 13 14 15 ## 25.677401 29.609810 29.609810 29.609810 29.609810 ## 16 17 18 19 20 ## 33.542219 33.542219 33.542219 33.542219 37.474628 ## 21 22 23 24 25 ## 37.474628 37.474628 37.474628 41.407036 41.407036 ## 26 27 28 29 30 ## 41.407036 45.339445 45.339445 49.271854 49.271854 ## 31 32 33 34 35 ## 49.271854 53.204263 53.204263 53.204263 53.204263 ## 36 37 38 39 40 ## 57.136672 57.136672 57.136672 61.069080 61.069080 ## 41 42 43 44 45 ## 61.069080 61.069080 61.069080 68.933898 72.866307 ## 46 47 48 49 50 ## 76.798715 76.798715 76.798715 76.798715 80.731124 mylm$… several other things that will not be explained here. Making Predictions predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ) Closing parenthesis for the data.frame(…) function. ) Closing parenthesis for the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12)) 1   29.60981 The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(…) function.  interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. “prediction” This specifies that a prediction interval will be included with the predicted value. A prediction interval gives you a 95% confidence interval that captures 95% of the data, or \\(Y_i\\) values for the specific \\(X\\)-value specified in the prediction. ) Closing parenthesis of the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"prediction\")   fit The “fit” is the predicted value.   lwr The “lwr” is the lower bound.   upr The “upr” is the upper bound. 1 29.60981 In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet. -1.749529 This is the lower bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us the stopping distance could be as quick as -1.749529 feet (or 0 because distance can’t go negative). 60.96915 This is the upper bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us that the actual stopping distance could be as high as 60.96915 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(…) function.  interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. “confidence” This specifies that a confidence interval for the prediction should be provided. This is of use whenever your interest is in just estimating the average y-value, not the actual y-values. ) Closing parenthesis of the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"confidence\")   fit The “fit” is the predicted value.   lwr The “lwr” is the lower bound.   upr The “upr” is the upper bound. 1 29.60981 In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. 24.39514 This is the lower bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is greater than this value. 34.82448 This is the upper bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is less than this value. Finding Confidence Intervals for Model Parameters confint( The R function confint(…) allows you to use an lm(…) object to compute confidence intervals for one or more parameters (like \\(\\beta_0\\) or \\(\\beta_1\\)) in your model. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  level = “level =” tells the confint(…) function that you are going to declare at what level of confidence you want the interval. The default is “level = 0.95.” If you want to find 95% confidence intervals for your parameters, then just run confint(mylm).  someConfidenceLevel someConfidenceLevel is simply a confidence level you choose when you want something other than a 95% confidence interval. Some examples of appropriate levels include 0.90 and 0.99. ) Closing parenthesis for confint(..) function. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.90)     5 % The lower bound of a 90% confidence interval occurs at the 5th percentile. This is because at 90% confidence, 10% is left in the tails, with 5% on each end. The upper bound will thus end at the 95th percentile, hence the 5% and 95% as the column names.   95 % The upper bound of a 90% confidence interval ends at the 95th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -28.914514 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -6.243676 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.235501 and 4.629317. 3.235501 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. 4.629317 This is the upper bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.95)     2.5 % The lower bound of a 95% confidence interval occurs at the 2.5th percentile. This is because at 95% confidence, 5% is left in the tails, with 2.5% on each end. The upper bound will thus end at the 97.5th percentile, hence the 2.5% and 97.5% as the column names for the lower and upper bounds, respectively.   97.5 % The upper bound of a 95% confidence interval ends at the 97.5th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -31.167850 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -3.990340 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.096964 and 4.767853. 3.096964 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.096964 and 4.767853 4.767853 This is the upper bound of the confidence interval. In this case, you can be 95% confident that the slope lies between 3.096964 and 4.767853 Explanation Linear regression has a rich mathematical theory behind it. This is because it uses a mathematical function and a random error term to describe the regression relation between a response variable \\(Y\\) and an explanatory variable called \\(X\\). Expand each element below to learn more. Regression Cheat Sheet (Expand) Term Pronunciation Meaning Math R Code \\(Y_i\\) $Y_i$ “why-eye” The data \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)\\) $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$ YourDataSet$YourYvariable \\(\\hat{Y}_i\\) $\\hat{Y}_i$ “why-hat-eye” The fitted line \\(\\hat{Y}_i = b_0 + b_1 X_i\\) $\\hat{Y}_i = b_0 + b_1 X_i$ lmObject$fitted.values \\(E\\{Y_i\\}\\) $E\\{Y_i\\}$ “expected value of why-eye” True mean y-value \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) $E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i$ <none> \\(\\beta_0\\) $\\beta_0$ “beta-zero” True y-intercept <none> <none> \\(\\beta_1\\) $\\beta_1$ “beta-one” True slope <none> <none> \\(b_0\\) $b_0$ “b-zero” Estimated y-intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) $b_0 = \\bar{Y} - b_1\\bar{X} b_0 <- mean(Y) - b_1*mean(X)$ \\(b_1\\) $b_1$ “b-one” Estimated slope \\(b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\) $b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})} {\\sum(X_i - \\bar{X})^2}$ b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) \\(\\epsilon_i\\) $\\epsilon_i$ “epsilon-eye” Distance of dot to true line \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) $\\epsilon_i = Y_i - E\\{Y_i\\}$ <none> \\(r_i\\) $r_i$ “r-eye” or “residual-eye” Distance of dot to estimated line \\(r_i = Y_i - \\hat{Y}_i\\) $r_i = Y_i - \\hat{Y}_i$ lmObject$residuals \\(\\sigma^2\\) $\\sigma^2$ “sigma-squared” Variance of the \\(\\epsilon_i\\) \\(Var\\{\\epsilon_i\\} = \\sigma^2\\)$Var\\{\\epsilon_i\\} = \\sigma^2$ <none> \\(MSE\\) $MSE$ “mean squared error” Estimate of \\(\\sigma^2\\) \\(MSE = \\frac{SSE}{n-p}\\)$MSE = \\frac{SSE}{n-p}$ sum( lmObject$res^2 ) / (n - p) \\(SSE\\) $SSE$ “sum of squared error” (residuals) Measure of dot’s total deviation from the line \\(SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\)$SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$ sum( lmObject$res^2 ) \\(SSR\\) $SSR$ “sum of squared regression error” Measure of line’s deviation from y-bar \\(SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\)$SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2$ sum( (lmObject$fit - mean(YourData$Y))^2 ) \\(SSTO\\) $SSTO$ “total sum of squares” Measure of total variation in Y \\(SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\)$SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2$ sum( (YourData$Y - mean(YourData$Y))^2 ) \\(R^2\\) $R^2$ “R-squared” Proportion of variation in Y explained by the regression \\(R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\)$R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}$ SSR/SSTO \\(r\\) $r$ “r” Correlation between X and Y. \\(r = \\sqrt{R^2}\\)$r = \\sqrt{R^2}$ sqrt(R^2) \\(\\hat{Y}_h\\) $\\hat{Y}_h$ “why-hat-aitch” Estimated mean y-value for some x-value called \\(X_h\\) \\(\\hat{Y}_h = b_0 + b_1 X_h\\)$\\hat{Y}_h = b_0 + b_1 X_h$ predict(lmObject, data.frame(XvarName=#)) \\(X_h\\) $X_h$ “ex-aitch” Some x-value, not necessarily one of the \\(X_i\\) values used in the regression \\(X_h =\\) some number$X_h = $ Xh = # Confidence Interval “confidence interval” Estimated bounds at a certain level of confidence for a parameter \\(b_0 \\pm t^* \\cdot s_{b_0}\\)b_0 \\pm t^* \\cdot s_{b_0} or \\(b_1 \\pm t^* \\cdot s_{b_1}\\)b_1 \\pm t^* \\cdot s_{b_1} confint(mylm, level = someConfidenceLevel) Parameter Estimate \\(\\beta_0\\) \\(b_0\\) \\(\\beta_1\\) \\(b_1\\) \\(\\epsilon_i\\) \\(r_i\\) \\(\\sigma^2\\) \\(MSE\\) \\(\\sigma\\) \\(\\sqrt{MSE}\\), the Residual standard error The Mathematical Model (Expand) \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)… There are three main elements to the mathematical model of regression. Each of these three elements is pictured below in the “Regression Relation Diagram.” Study both the three bullet points and their visual representations in the plot below for a clearer understanding. The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read more…) The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as “natural law” or “God’s law”. It is some true line that is unknown to us. The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line. The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\). Note: \\(E\\{Y\\}\\) is pronounced “the expected value of y” because, well… the mean is the typical, average, or “expected” value. The dots, i.e., the regression relation plus an error term: \\(Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}} + \\underbrace{\\epsilon_i}_\\text{error term} \\quad \\text{where} \\ \\epsilon_i\\sim N(0,\\sigma^2)\\) (Read more…) This is shown by the dots in the graph below. This is the data. In regression, the assumption is that the y-value for individual \\(i\\), denoted by \\(Y_i\\), was “created” by adding an error term \\(\\epsilon_i\\) to each individual’s “expected” value \\(\\beta_0 + \\beta_1 X_i\\). Note the “order of creation” would require first knowing an indivual’s x-value, \\(X_i\\), then their expected value from the regression relation \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) and then adding their \\(\\epsilon_i\\) value to the result. The \\(\\epsilon_i\\) allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance \\(\\epsilon_i\\) from the line. Note: \\(Y_i\\) is pronounced “why-eye” because it is the y-value for individual \\(i\\). Sometimes also called “why-sub-eye” because \\(i\\) is in the subscript of \\(Y\\). The estimated line, i.e., the line we get from a sample of data. \\(\\underbrace{\\hat{Y}_i}_{\\substack{\\text{estimated mean} \\\\ \\text{y-value}}} = \\underbrace{b_0 + b_1 X_i}_\\text{estimated regression equation}\\) (Read more…) The estimated line is shown by the solid line in the graph below. \\(\\hat{Y}\\) is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation \\(E\\{Y\\}\\). So \\(\\hat{Y}\\) is interpreted as the estimated average (or mean) \\(Y\\)-value for any given \\(X\\)-value. Thus, \\(b_0\\) is the estimated y-intercept and \\(b_1\\) is the estimated slope. The b’s are sample statistics, like \\(\\bar{x}\\) and the \\(\\beta\\)’s are population parameters like \\(\\mu\\). The \\(b\\)’s estimate the \\(\\beta\\)’s. Note: \\(\\hat{Y}_i\\) is pronounced “why-hat-eye” and is known as the “estimated y-value” or “fitted y-value” because it is the y-value you get from \\(b_0 + b_1 X_i\\). It is always different from \\(Y_i\\) because dots are rarely if ever exactly on the estimated regression line. This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line). Something to ponder: The true line, when coupled with the error terms, “creates” the data. The estimated (or fitted) line uses the sampled data to try to “re-create” the true line. We could loosely call this the “order of creation” as shown by the following diagram. par(mfrow=c(1,3), mai=c(.2,.2,.4,.1)) plot(y ~ x, col=\"white\", main=\"A Law is Given\", yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, main=\"Data is Created\", xaxt='n', yaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, xaxt='n', yaxt='n', main=\"The Law is Estimated\") curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) A Law is Given Data is Created The Law is Estimated \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) \\(Y_i = E\\{Y_i\\} + \\epsilon_i\\) \\(\\hat{Y}_i = b_0 + b_1 X_i\\) The true line is the “law”. The \\(Y_i\\) are created by adding \\(\\epsilon_i\\) to \\(E\\{Y_i\\}\\) where \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\). The law is estimated with \\(\\hat{Y}_i\\) which is given with lm(...). Click open the “Code” buttom below to the right to find code that runs a simulation demonstrating this “order of creation”. ## Simulating Data from a Regression Model ## This R-chunk is meant to be played in your R Console. ## It allows you to explore how the various elements ## of the regression model combine together to \"create\" ## data and then use the data to \"re-create\" the line. set.seed(101) #Allows us to always get the same \"random\" sample #Change to a new number to get a new sample n <- 3 #set the sample size X_i <- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45. beta0 <- 3 #Our choice for the y-intercept. beta1 <- .1 #Our choice for the slope. sigma <- 12.5 #Our choice for the std. deviation of the error terms. epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data View(fabData) #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it. fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData. summary(fab.lm) #Summarize your model. plot(y ~ x, data=fabData) #Plot the data. abline(fab.lm) #Add the estimated regression line to your plot. # Now for something you can't do in real life... but since we created the data... abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). legend(\"topleft\", legend=c(\"True Line\", \"Estimated Line\"), lty=c(2,1), bty=\"n\") #Add a legend to your plot specifying which line is which. Interpreting the Model Parameters (Expand) \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted as… The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model. If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\). The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\). It is often misunderstood to be the “average change in y” or just “the change in y” but it is more correctly referred to as the “change in the average y”. To better see this, consider the three graphics shown below. par(mfrow=c(1,3)) hist(mtcars$mpg, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Number of Vehicles\", xlab=\"Gas Mileage (mpg)\", col=\"skyblue\") boxplot(mpg ~ cyl, data=mtcars, border=\"skyblue\", boxwex=0.5, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Number of Cylinders of Engine (cyl)\") plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=\"skyblue\", main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Quarter Mile Time (qsec)\") abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=\"darkgray\") mtext(side=3, text=\"Automatic Transmissions Only (am==0)\", cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=\"gray\") The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09. The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg. The scatterplot on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value. Residuals and Errors (Expand) \\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true error… Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\). The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\). We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summary… Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\). \\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms. Keep in mind the idea that the errors \\(\\epsilon_i\\) “created” the data and that the residuals \\(r_i\\) are computed after using the data to “re-create” the line. Residuals have many uses in regression analysis. They allow us to diagnose the regression assumptions, See the “Assumptions” section below for more details. estimate the regression relation, See the “Estimating the Model Parameters” section below for more details. estimate the variance of the error terms, See the “Estimating the Model Variance” section below for more details. and assess the fit of the regression relation. See the “Assessing the Fit of a Regression” section below for more details. Assessing the Fit of a Regression (Expand) \\(R^2\\), SSTO, SSR, and SSE… Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line. par(mfrow=c(1,3), mai=c(.1,.1,.5,.1)) set.seed(2) x <- runif(30,0,20) y1 <- 2 + 3.5*x + rnorm(30,0,2) y2 <- 2 + 3.5*x + rnorm(30,0,8) y3 <- 2 + 3.5*x + rnorm(30,0,27) plot(y1 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Excellent Fit\") abline(lm(y1 ~ x), col=\"gray\") plot(y2 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Good Fit\") abline(lm(y2 ~ x), col=\"gray\") plot(y3 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Poor Fit\") abline(lm(y3 ~ x), col=\"gray\") A common way to measure the fit of a regression is with correlation. While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\). (If you are a Math 325 student, just stick with correlation for now and skip on to the next section of this Explanation tab. If you are a Math 425 student, it is critical that you come to understand \\(R^2\\) deeply, so read on.) Before you can understand \\(R^2\\), you must understand three important “sums of squares”. (Read more about sums…) A sum is just a fancy word for adding things together. \\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand. So we use the symbol \\(\\Sigma\\) to denote the word “sum”. Further, we use a subscript \\(\\Sigma_{i=1}\\) to state what value the sum is beginning with, and a superscript \\(\\Sigma_{i=1}^6\\) to state the value we are ending at. This gives \\[ \\sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Test your knowledge, do you see why the answer is 6 to the sum below? \\[ \\sum_{i=1}^3 i = 6 \\] Computing sums in R is fairly easy. Type the following codes in your R Console. sum(1:6) #gives the answer of 21 sum(1:3) #gives the answer of 6 However, sums really become useful when used with a data set. Each row of a data set represents an “individual’s” data. We can reference each individual with a row number. In the data below, individual 3, denoted by \\(i=3\\), has a speed of 7 and a dist of 4. pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3) Individual speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 To compute the sum of the speed column, use sum(speed). If we divided this sum by 6, we would get the mean of speed mean(speed). In fact, the two most used statistics mean(...) and sd(...) both use sums. Take a moment to review the formulas for mean and standard deviation. It is strongly recommended that you study the Explanation tab for both as well. We’ll wait. See you back here shortly. … Welcome back. Suppose we let X = speed and Y = dist. Then \\(X_3 = 7\\) and \\(Y_3 = 4\\) because we are accessing row 3 of both the \\(X\\) (or speed) column and \\(Y\\) (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, sum(speed) would be written mathematically as \\(\\sum_{i=1}^6 X_i\\) and sum(dist) would be written as \\(\\sum_{i=1}^6 Y_i\\). Sum of Squared Errors Sum of Squares Regression Total Sum of Squares \\(\\text{SSE} = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\) \\(\\text{SSR} = \\sum_{i=1}^n \\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\) \\(\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\\) Measures how much the residuals deviate from the line. Measures how much the regression line deviates from the average y-value. Measures how much the y-values deviate from the average y-value. Equals SSTO - SSR Equals SSTO - SSE Equals SSE + SSR sum( (Y - mylm$fit)^2 ) sum( (mylm$fit - mean(Y))^2 ) sum( (Y - mean(Y))^2 ) It is important to remember that SSE and SSR split up SSTO, so that \\[ \\text{SSTO} = \\text{SSE} + \\text{SSR} \\] This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works. The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called \\(R^2\\) (“r-squared”). R-Squared (\\(R^2\\)) \\[ \\underbrace{R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}}_\\text{Interpretation: Proportion of variation in Y explained by the regression.} \\] The smallest \\(R^2\\) can be is zero, and the largest it can be is 1. This is because \\(SSR\\) must be between 0 and SSTO, inclusive. Residual Plots & Regression Assumptions (Expand) Residuals vs. fitted-values, Q-Q Plot of the residuals, and residuals vs. order plots… There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate. Each assumption is labeled in the regression equation below. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions. (Read more…) As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not. Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3 The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(\\hat{Y}_i\\). The residuals are the \\(r_i\\). This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable. | Show Examples | (Read more…) The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption. The linear relation is assumed to be satisfied if there are no apparent trends in the plot. The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values. The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance. set.seed(2) X <- rnorm(30,15,3) notLin <- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8)) notLin.lm <- lm(Y~X, data=notLin) set.seed(15) Lin <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) Lin.lm <- lm(Y~X, data=Lin) par(mfrow=c(3,3), mai=c(.25,.25,.25,.25), mgp=c(1,.75,0)) plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Not Linear\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(notLin.lm$fitted.values,notLin.lm$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Good: Linear, Constant Variance\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) set.seed(6) notCon <- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5)) notCon.lm <- lm(Y~X, data=notCon) LinO <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) LinO[1] <- LinO[1]^2 LinO.lm <- lm(Y~X, data=LinO) plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Unconstant Variance\", cex.main=0.95, yaxt='n', xaxt='n', col=\"firebrick\") polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) # plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Outliers\", cex.main=0.95) # abline(h=0) tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(Girth ~ Volume, data=trees[-31,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(Height ~ Volume, data=trees) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) abline(h=0) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals, f=.4) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(weight ~ repwt, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(weight ~ repht, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) abline(h=0) Q-Q Plot of the Residuals: Checks Assumption #2 The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption. | Show Examples | (Read more…) There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot. Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) set.seed(123) tmp <- rnorm(100) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"skyblue\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Normal\", col=\"skyblue\") tmp <- Davis$weight qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Right-skewed\", breaks=15, col=\"firebrick\") par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- rbeta(100, 5,1) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Left-skewed\", breaks=seq(min(tmp),max(tmp), length.out=13), col=\"firebrick\") tmp <- rbeta(100,2,2) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Heavy-tailed\", col=\"firebrick\") Residuals versus Order Plot: Checks Assumption #5 When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated. | Show Examples | (Read more…) Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Good: No Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: General Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ qsec, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: Interesting Patterns\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),]) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Bad: Obvious Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") Problems from Failed Assumptions There are various problems that can arise when certain of the regression assupmtions are not satisfied. Lack of Linearity When the linearity assumption is violated, pretty much everything we obtain from the regression summary is no longer meaningful. The y-intercept estimate can be drastically off from its actual true value. Important model information is lost by trying to use a simple slope term \\(\\beta_1\\) to describe the model with respect to \\(X\\). The residual standard error will be much higher than it otherwise would be because of curvature patterns in the data that the line cannot capture. Thus, R-squared will be lower than it otherwise should be. P-values can become non-significant, when in fact there is a strong pattern in the data, but that pattern just cannot be captured by a simple line. *Normality of the errors is often put into question as well when a simplified line is used to try to capture a more complicated curved model. The plot below demonstrate these difficulties. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 7.5 #True slope beta_2 <- -0.25 #True bend X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors Y_i <- beta_0 + beta_1*X_i + beta_2*X_i^2 + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Non-Linear Relation\") abline(mylm, col=\"gray\") #Add fitted line to plot curve(beta_0 + beta_1*x + beta_2*x^2, col=\"gray\", lty=2, add=TRUE) #Add True line to plot #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (True value:\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (True value:\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (True value:\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Non-normal Error Terms When the normality assumption for the error terms is violated, not all is lost. In fact, the estimate of the slope and intercept are still often fairly meaningful. However, it is unwise to put too much trust in the residual standard error as an estimate of the standard deviation \\(\\sigma\\). This is because the standard deviation in skewed distributions does not carry the same meaning it has in normal distributions. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rchisq(n, 1)*3 - 1 #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Unconstant Variance When variance of the error term changes across the regression, the regression approximates the “average variance” of the errors because the regression is still assuming the variance is constant across the regression. The estimates of the slope and intercept are still typically quite good, and can be used for interpretation. The residual standard error however should not be considered to be meaningful as it will be too large on one end of the regression and too small on the other end. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma + X_i) #normally distributed errors #with increasing variance Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Variance Varies (Non-Constant)\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 13.82 Slope 3.5 3.768 Sigma 2.5 13.02 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", round(mean(sigma + X_i), 2), \", mean)\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Normality Violated As silly as it sounds, if the only problem with the regression is the lack of normality of the error terms, it isn’t all that big of a problem. Depending on how non-normal the residuals appear, there could be some skewing to the residual standard error, but otherwise, the slope and intercept are still interpretable and meaningful. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- runif(n, -sqrt(12*sigma^2)/2, sqrt(12*sigma^2)/2) #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 14.7 Slope 3.5 3.449 Sigma 2.5 2.317 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Independence Assumption Violated While the slope and intercept are often still meaningful when the independence assumption is violated, the residual standard error is unnecessarily large in this case. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, 2.5) + (1:n -n/2)*.5 #normal, but correlated errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Independence Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 16.47 Slope 3.5 3.296 Sigma 2.5 4.819 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Outliers Present While outliers do not violate any of the regression assumptions, they do pose substantial difficulties for the least squares regression estimates of the slope and intercept. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors epsilon_i[3] <- ifelse(X_i[3] < 10, runif(1,25,35), -runif(1,25,35)) #create outlier Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"An Outlier Present\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 13.18 Slope 3.5 3.447 Sigma 2.5 6.389 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) The material below this section is meant for Math 425 students only. Estimating the Model Parameters (Expand) How to get \\(b_0\\) and \\(b_1\\): least squares & maximum likelihood… There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical. The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas. Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 <- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\). When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\). Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i. \\label{exp} \\end{equation}\\] Least Squares{#leastSquares} To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\). \\[ Q = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2 \\] Then we use the function Q as if it were a function of \\(\\beta_0\\) and \\(\\beta_1\\). Ironically, the values of \\(Y\\) and \\(X\\) are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we don’t know are the values of \\(\\beta_0\\) and \\(\\beta_1\\). This least squares applet is a good way to explore how various choices of the slope and intercept yield different values of the “sum of squared residuals”. But it turns out that there is one “best” choice of the slope and intercept that yields a “smallest” value of the “sum of squared residuals.” This best choice can actually be found using calculus by taking the partial derivatives of \\(Q\\) with respect to both \\(\\beta_0\\) and \\(\\beta_1\\). \\[ \\frac{\\partial Q}{\\partial \\beta_0} = -2\\sum (Y_i - \\beta_0 - \\beta_1X_i) \\] \\[ \\frac{\\partial Q}{\\partial \\beta_1} = -2\\sum X_i(Y_i-\\beta_0-\\beta_1X_i) \\] Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize \\(Q\\) for a given set of data. After all the calculations are completed we find the values of the parameter estimators \\(b_0\\) and \\(b_1\\) (of \\(\\beta_0\\) and \\(\\beta_1\\), respectively) are as stated previously. Maximum Likelihood{#mle} The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of \\(\\beta_0\\) and \\(\\beta_1\\) which minime the least squares \\(Q\\) function, we choose the values of \\(\\beta_0\\) and \\(\\beta_1\\) which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the \\(Y_i\\) for all observations \\(i=1,\\ldots,n\\). We can do this rather simply by using the assumption that the errors, \\(\\epsilon_i\\) are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if \\(f(Y_i)\\) denotes the probability density function for \\(Y_i\\), then the joint probability density for all \\(Y_i\\), \\(f(Y_1,\\ldots,Y_n)\\) is given by \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) \\] Since each \\(Y_i\\) is assumed to be normally distributed with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\) (see model (\\(\\ref{model}\\))) we have that \\[ f(Y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{Y_i-\\beta_0-\\beta_1X_i}{\\sigma}\\right)^2\\right]} \\] which provides the joint probability as \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] The likelihood function \\(L\\) is then given by consider the \\(Y_i\\) and \\(X_i\\) fixed and the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) as the variables in the function. \\[ L(\\beta_0,\\beta_1,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] Instead of taking partial derivatives of \\(L\\) directly (with respect to all parameters) we take the partial derivatives of the \\(\\log\\) of \\(L\\), which is easier to work with. In a similar, but more difficult calculation, to that of minimizing \\(Q\\), we obtain the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) which maximize the log of \\(L\\), and which therefore maximize \\(L\\). (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate \\(\\hat{\\sigma}^2\\) of \\(\\sigma^2\\). \\[ \\hat{\\sigma}^2 = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n} \\] Estimating the Model Variance (Expand) Estimating \\(\\sigma^2\\) with MSE… As shown previously in the “Estimating Model Parameters” section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation. Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\). This estimate turns out to be a biased estimator. This means that it is consistently wrong in its estimates of \\(\\sigma^2\\). If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong. This is bad. Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\). Without going into all the details, to “fix” the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently. To find the correct degrees of freedom, we have to notice that the \\(\\hat{Y}_i\\) in the numerator of \\(\\widehat{\\sigma}^2\\) is defined by \\[\\begin{equation} \\widehat{Y}_i = b_0 + b_1X_i \\label{hatY} \\end{equation}\\] From this equation, we notice that two means, \\(\\bar{X}\\) and \\(\\bar{Y}\\), were estimated from the data in order to obtain \\(\\hat{Y}_i\\). (See the formulas for \\(b_0\\) and \\(b_1\\) above, and note how they use both \\(\\bar{X}\\) and \\(\\bar{Y}\\) in their calculation.) Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for \\(\\hat{\\sigma}^2\\) should be \\(n-2\\) instead of \\(n\\). Some incredibly long calculations will show that the “fixed” estimator \\[\\begin{equation} s^2 = MSE = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n-2} \\quad \\text{(Unbiased Estimator of $\\sigma^2$)} \\end{equation}\\] is an unbiased estimator of \\(\\sigma^2\\). Here \\(MSE\\) stands for mean squared error, which is the most obvious name for a formula that squares the errors \\(Y_i-\\hat{Y}_i\\) then adds them up and divides by their degrees of freedom. Similarly, we call the numerator \\(\\sum(Y_i-\\hat{Y}_i)^2\\) the sum of the squared errors, denoted by \\(SSE\\). It is also important to note that the errors are often denoted by \\(r_i = Y_i-\\hat{Y}_i\\), the residuals. Putting this all together we get the following equivalent statements for \\(MSE\\). \\[\\begin{equation} s^2 = MSE = \\frac{SSE}{n-2} = \\frac{\\sum(Y_i-\\widehat{Y}_i)^2}{n-2} = \\frac{\\sum r_i^2}{n-2} \\end{equation}\\] As a final note, even though the expected value \\(E\\{MSE\\} = \\sigma^2\\), which shows \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\), it unfortunately isn’t true that \\(\\sqrt{MSE}\\) is an unbiased estimator of \\(\\sigma\\). This presents a few problems later on, but these are minimal enough that we can overlook the issue and move forward. Transformations (Expand) \\(Y'\\), \\(X'\\), and returning to the original space… Y transformations are denoted by y-prime, written \\(Y'\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\). \\[ Y' = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y' = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y' = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y' = \\log(Y)\\) lm(log(Y) ~ X) 0.25 \\(Y' = \\sqrt(\\sqrt(Y))\\) lm(sqrt(sqrt(Y)) ~ X) 0.5 \\(Y' = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y' = Y\\) lm(Y ~ X) 2 \\(Y' = Y^2\\) lm(Y^2 ~ X) Using “maximum-likelihood” estimation, the Box-Cox procedure can actually automatically detect the “optimal” value of \\(\\lambda\\) to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise. Scatterplot Recognition Box-Cox Suggestion An Example Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\). set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Ya <- 1/sqrt(Y) #1/Y^2 Lam = -2 Yb <- 1/Y #1/Y Lam = -1 Yc <- exp(.02*Y) #log(Y) Lam = 0 Yd <- Y^2 #sqrt(Y) Lam = 0.5 Ye <- Y #Y Lam = 1 Yf <- sqrt(Y) #Y^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Ya ~ X, main=expression(paste(\"Use \", lambda == -2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ya^-2 ~ X)) curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yb ~ X, main=expression(paste(\"Use \", lambda == -1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yb^-1 ~ X)) curve(1/(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yc ~ X, main=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(log(Yc) ~ X)) curve(exp(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yd ~ X, main=expression(paste(\"Use \", lambda == 0.5)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(sqrt(Yd) ~ X)) curve((b[1] + b[2]*x)^2, add=TRUE, col=\"green\", lwd=2) plot(Ye ~ X, main=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ye ~ X)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yf ~ X, main=expression(paste(\"Use \", lambda == 2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yf^2 ~ X)) curve(sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try. par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0)) boxCox(lm(Ya ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -2)), line=.5) boxCox(lm(Yb ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -1)), line=.5) boxCox(lm(Yc ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), line=.5) boxCox(lm(Yd ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0.5)), line=.5) boxCox(lm(Ye ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), line=.5) boxCox(lm(Yf ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 2)), line=.5) An Example Suppose we were running a simple linear regression on the cars dataset. This would be done with the code cars.lm <- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesn’t quite fit the data as well as we would hope. Instead, the data looks a little curved. cars.lm <-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") abline(cars.lm, col=\"gray\") Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful. library(car) boxCox(cars.lm) The output from the boxCox(...) function looks as follows. This plot tells use to use the \\(\\lambda = 0.5\\) transformation, so that \\(Y' = Y^0.5 = \\sqrt{Y}\\). (To see this yourself, click on the “Box-Cox Suggestion” tab above, as well as on the “Scatterplot Recognition” tab.) Now, a transformation regression is performed using sqrt(Y) in place of Y as follows: cars.lm.t <- lm(sqrt(dist) ~ speed, data=cars) summary(cars.lm.t)   Estimate Std. Error t value Pr(> (Intercept) 1.277 0.4844 2.636 0.01126 speed 0.3224 0.02978 10.83 1.773e-14 Then, \\[ \\widehat{Y}_i' = 1.277 + 0.3224 X_i \\] And replacing \\(\\hat{Y}_i' = \\sqrt{\\hat{Y}_i}\\) we have \\[ \\sqrt{\\widehat{Y}_i} = 1.277 + 0.3224 X_i \\] Solving for \\(\\hat{Y}_i\\) gives \\[ \\widehat{Y}_i = (1.277 + 0.3224 X_i)^2 \\] Which, using curve((1.277 + 0.3224*x)^2, add=TRUE) (see code for details) looks like this: plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") curve( (1.277 + 0.3224*x)^2, add=TRUE, col=\"firebrick\") X-Transformations X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them. The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated. The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation. set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Xa <- 1/sqrt(X) #1/X^2 Lam = -2 Xb <- 1/X #1/X Lam = -1 Xc <- exp(.02*X) #log(X) Lam = 0 Xd <- X^2 #sqrt(X) Lam = 0.5 Xe <- X #X Lam = 1 Xf <- sqrt(X) #X^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Y ~ Xa, main=expression(paste(\"Use \", X*minute == X^-2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xa^-2))) curve(b[1] + b[2]*x^-2, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xb, main=expression(paste(\"Use \", X*minute == X^-1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xb^-1))) curve(b[1] + b[2]*x^-1, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xc, main=expression(paste(\"Use \", X*minute == log(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ log(Xc))) curve(b[1] + b[2]*log(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xd, main=expression(paste(\"Use \", X*minute == sqrt(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ sqrt(Xd))) curve(b[1] + b[2]*sqrt(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xe, main=expression(paste(\"Use \", X*minute == X, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ Xe)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xf, main=expression(paste(\"Use \", X*minute == X^2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xf^2))) curve(b[1] + b[2]*x^2, add=TRUE, col=\"green\", lwd=2) Inference for the Model Parameters (Expand) t test formulas, sampling distributions, confidence intervals, and F tests… When fitting the regression model given by the equation \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] to a sample of data, we typically test hypotheses about the parameters \\(\\beta_0\\), \\(\\beta_1\\), or both. Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_0 - \\overbrace{0}^\\text{a number}}{s_{b_0}}\\] This is the formula for the test statistic. It measures how far the estimated y-intercept \\(b_0\\) is from the null hypothesis for \\(\\beta_0\\) in units of “standard errors of \\(b_0\\)”. Thus the division by \\(s_{b_0}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0, it could be any number. \\(H_0: \\beta_1 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_1\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_1 - \\overbrace{0}^\\text{a number}}{s_{b_1}}\\] This is the formula for the test statistic. It measures how far the estimated slope \\(b_1\\) is from the null hypothesis for \\(\\beta_1\\) in units of “standard errors of \\(b_1\\)”. Thus the division by \\(s_{b_1}\\). Though the hypothesized value of \\(\\beta_1\\) is typically 0, it could be any number. Left-tailed p-value = pt(-abs(tvalue), degrees of freedom). Double it to get the two-sided p-value. In R, these values correspond to the output summary of an lm as follows. (Show Example) Consider the cars data in R. Suppose we used the regression model given by \\[ \\underbrace{Y_i}_\\text{Feet to Stop} = \\beta_0 + \\beta_1 \\underbrace{X_{i}}_\\text{mph} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\quad \\sim N(0,\\sigma^2) \\] to model the feet a vehicle (from the 1920’s) takes to stop when traveling at a certain speed (in miles per hour, mph) prior to stopping. When the regression is performed and summarized in R, it is always testing the following two hypotheses: \\[ H_0: \\beta_0 = 0 \\quad\\quad H_0: \\beta_1 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\quad\\quad H_a: \\beta_1 \\neq 0 \\] To perform the test of these hypotheses for the regression stated above, we would run the following codes in R. cars.lm <- lm(dist ~ speed, data=cars) pander(summary(cars.lm)$coefficients) These would produce summary output like the following, but the following output has been labeled with the math notation corresponding to each value. Let’s emphasize what is happening in this summary output table. First, here is how the “t value” is calculated for the “(Intercept)” in the summary table above. \\[ t = \\frac{b_0-0}{s_{b_0}} = \\frac{-17.58 - 0}{6.758} = -2.601 \\] Second, here is a visual representation of how the P-value, the “Pr(>|t|)” as it is called in the summary table above, is calculated for this test statistic. (Click the graph to view an interactive applet showing this calculation.) Notice both ends of the t-distribution are being shaded to compute the P-value because the alternative hypothesis was \\(H_a: \\beta_0 \\neq 0\\). To compute the P-value in R, we use the “percentile function for the t-distribution” called pt( ). This function requires two things, the t-value and the degrees of freedom, in our case pt(-2.601, 48). Note the degrees of freedom (df) are 48 because the sample size is \\(n=50\\) and there are two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) in our regression model. Running this code in R gives: pt(-2.601, 48) = 0.00616 However, note that this value is only half of the actual P-value of 0.0123. To get the “two-sided” P-value (note that our alternative hypothesis used a \\(\\neq\\) symbol) we need to double this left-tailed P-value. 2*pt(-2.601, 48)) = 0.0123 Finally, note that the same procedure can be used to test hypotheses that use a value other than 0 in the null and alternative. For example, to test the hypotheses: \\[ H_0: \\beta_1 = 3 \\\\ H_a: \\beta_1 \\neq 3 \\] Use the t-formula \\[ t = \\frac{b_1 - 3}{s_{b_1}} = \\frac{\\overbrace{3.932}^{b_1} - \\overbrace{3}^{H_0}}{\\underbrace{0.4155}_{s_{b_1}}} = 2.243 \\] then the P-value is calculated in R by 2*pt(-abs(2.243), 48) = 0.0295495 To obtain confidence intervals in R use confint(mylm). Confidence Interval Formula Standard Error \\(\\beta_0\\) \\(b_0 \\pm\\) \\(t^*\\) This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\). Use qt(0.975, df) to get \\(t*\\) in R. \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_0\\). \\(s_{b_0}\\) The standard error of \\(b_0\\), denoted by \\(s_{b_0}\\) is provided in the regression summary output under the column header called “Std. Error” for the “(Intercept)” row of the output. It is calculated using the formula shown below. \\[s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\] This is called the “estimated variance of \\(b_0\\)”. Taking the square root of this number gives the “standard error of \\(b_0\\)”. \\(\\beta_1\\) \\(b_1 \\pm\\) \\(t^*\\) This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\) (sample size - number of parameters in the regression model). Use qt(0.975, df) to get \\(t*\\) in R \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_1\\). \\(s_{b_1}\\) The standard error of \\(b_1\\), denoted by \\(s_{b_1}\\) is provided in the regression summary output under the column header called “Std. It is calculated using the formula shown below. \\[s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\] This is called the “estimated variance of \\(b_1\\)”. Taking the square root of this number gives the “standard error of \\(b_1\\)”. To be more exact, the types of inference we are interested in are the following. Determine if there is evidence of a meaningful linear relationship in the data. If \\(\\beta_1 = 0\\), then there is no relation between \\(X\\) and \\(E\\{Y\\}\\). Hence we might be interested in testing the hypotheses \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) is some hypothesized number. To provide a confidence interval for the true value of \\(\\beta_1\\). Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the sampling distribution of the estimate \\(b_1\\) of the parameter \\(\\beta_1\\). And, while we are at it, we may as well come to understand the sampling distribution of the estimate \\(b_0\\) of the parameter \\(\\beta_0\\). Review sampling distributions from Math 221. Since \\(b_1\\) is an estimate, it will vary from sample to sample, even though the truth, \\(\\beta_1\\), remains fixed. (The same holds for \\(b_0\\) and \\(\\beta_0\\).) It turns out that the sampling distribution of \\(b_1\\) (where the \\(X\\) values remain fixed from study to study) is normal with mean and variance: \\[ \\mu_{b_1} = \\beta_1 \\] \\[ \\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} \\] ## Simulation to Show relationship between Standard Errors ##----------------------------------------------- ## Edit anything in this area... n <- 100 #sample size Xstart <- 30 #lower-bound for x-axis Xstop <- 100 #upper-bound for x-axis beta_0 <- 2 #choice of true y-intercept beta_1 <- 3.5 #choice of true slope sigma <- 13.8 #choice of st. deviation of error terms ## End of Editable area. ##----------------------------------------------- # Create X, which will be used in the next R-chunk. X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) ## After playing this chunk, play the next chunk as well. To see that this is true, consider the regression model with values specified for each parameter as follows. \\[ Y_i = \\overbrace{\\beta_0}^{2} + \\overbrace{\\beta_1}^{3.5} X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\overbrace{\\sigma^2}^{\\sigma=13.8}) \\] Using the equations above for \\(\\mu_{b_1}\\) and \\(\\sigma^2_{b_1}\\) we obtain that the mean of the sampling distribution of \\(b_1\\) will be \\(\\mu_{b_1} = \\beta_1 = 3.5\\) Further, we see that the variance of the sampling distribution of \\(b_1\\) will be \\(\\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} = \\frac{13.8^2}{4.25\\times 10^{4}}\\) Taking the square root of the variance, the standard deviation of the sampling distribution of \\(b_1\\) will be \\(\\sigma_{b_1} = 0.067\\). That’s very nice. But to really believe it, let’s run a simulation ourselves. The “Code” below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of \\(b_1\\) and \\(b_0\\) for that regression. After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of \\(b_0\\) is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of \\(b_1\\) is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of \\(\\beta_0\\) or \\(\\beta_1\\), respectively. Also, the “Std. Error” of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of \\(\\sigma_{b_0}\\) and \\(\\sigma_{b_1}\\), respectively. N <- 5000 #number of times to pull a random sample storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N) for (i in 1:N){ Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model mylm <- lm(Y ~ X) storage_b0[i] <- coef(mylm)[1] storage_b1[i] <- coef(mylm)[2] storage_rmse[i] <- summary(mylm)$sigma } layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3)) Ystart <- 0 #min(0,min(Y)) Ystop <- 500 #max(max(Y), 0) Yrange <- Ystop - Ystart plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), ylim=c(Ystart, Ystop), pch=16, col=\"gray\", main=\"Regression Lines from many Samples (gray lines) \\n Plus Residual Standard Deviation Lines (green lines)\") text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1) text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1) text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1) for (i in 1:N){ abline(storage_b0[i], storage_b1[i], col=\"darkgray\") } abline(beta_0, beta_1, col=\"green\", lwd=3) abline(beta_0+sigma, beta_1, col=\"green\", lwd=2) abline(beta_0-sigma, beta_1, col=\"green\", lwd=2) abline(beta_0+2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0-2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0+3*sigma, beta_1, col=\"green\", lwd=.5) abline(beta_0-3*sigma, beta_1, col=\"green\", lwd=.5) par(mai=c(1,.6,.5,.01)) addnorm <- function(m,s, col=\"firebrick\"){ curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2) lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col) lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col) lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col) lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col) lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col) lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col) lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col) legend(\"topleft\", legend=paste(\"Std. Error = \", round(s,3)), cex=0.7, bty=\"n\") } h0 <- hist(storage_b0, col=\"skyblue3\", main=\"Sampling Distribution\\n Y-intercept\", xlab=expression(paste(\"Estimates of \", beta[0], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m0 <- mean(storage_b0) s0 <- sd(storage_b0) addnorm(m0,s0, col=\"green\") h1 <- hist(storage_b1, col=\"skyblue3\", main=\"Sampling Distribution\\n Slope\", xlab=expression(paste(\"Estimates of \", beta[1], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m1 <- mean(storage_b1) s1 <- sd(storage_b1) addnorm(m1,s1, col=\"green\") t Tests{#tTests} Using the information above about the sampling distributions of \\(b_1\\) and \\(b_0\\), an immediate choice of statistical test to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) can be zero, or any other value, is a t test given by \\[ t = \\frac{b_1 - \\beta_{10}}{s_{b_1}} \\] where \\(s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\). (You may want to review the section “Estimating the Model Variance” of this file to know where MSE came from.) With quite a bit of work it has been shown that \\(t\\) is distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. The nearly identical test statistic for testing \\[ H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_0 \\neq \\beta_{00} \\] is given by \\[ t = \\frac{b_0 - \\beta_{00}}{s_{b_0}} \\] where \\(s^2_{b_0} = MSE\\left[\\frac{1}{n}+\\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\). This version of \\(t\\) has also been shown to be distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. Confidence Intervals Creating a confidence interval for either \\(\\beta_1\\) or \\(\\beta_0\\) follows immediately from these results using the formulas \\[ b_1 \\pm t^*_{n-2}\\cdot s_{b_1} \\] \\[ b_0 \\pm t^*_{n-2}\\cdot s_{b_0} \\] where \\(t^*_{n-2}\\) is the critical value from a t distribution with \\(n-2\\) degrees of freedom corresponding to the chosen confidence level. F tests{#Ftests} Another way to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\quad\\quad \\text{or} \\quad\\quad H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\quad\\quad \\ \\ \\quad \\quad H_a: \\beta_0 \\neq \\beta_{00} \\] is with an \\(F\\) Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an \\(F\\) test is very general and can be used in many places that a t test cannot. In its most general form, the \\(F\\) test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that \\(H_0:\\beta_1=0\\) against the alternative that \\(H_a: \\beta_1\\neq 0\\), we are essentially comparing two models against each other. If \\(\\beta_1=0\\), then the corresponding model would be \\(E\\{Y_i\\} = \\beta_0\\). If \\(\\beta_1\\neq0\\), then the model remains \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the \\(F\\) Test, that the null model (reduced model) have fewer “free” parameters than the alternative model (full model). To demonstrate what we mean by “free” parameters, consider the following example. Say we wanted to test the hypothesis that \\(H_0:\\beta_1 = 2.5\\) against the alternative that \\(\\beta_1\\neq2.5\\). Then the null, or reduced model, would be \\(E\\{Y_i\\}=\\beta_0+2.5X_i\\). The alternative, or full model, would be \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). Thus, the null (reduced) model contains only one “free” parameter because \\(\\beta_1\\) has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two “free” parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model. Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing \\(H_0: \\beta_1=0\\) against \\(H_a:\\beta_1\\neq0\\) we have the partition \\[ \\underbrace{Y_i-\\bar{Y}}_{Total} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{Regression} + \\underbrace{Y_i-\\hat{Y}_i}_{Error} \\] The reason we use \\(\\bar{Y}\\) for the null model is that \\(\\bar{Y}\\) is the unbiased estimator of \\(\\beta_0\\) for the null model, \\(E\\{Y_i\\} = \\beta_0\\). Thus we would compute the following sums of squares: \\[ SSTO = \\sum(Y_i-\\bar{Y})^2 \\] \\[ SSR = \\sum(\\hat{Y}_i-\\bar{Y})^2 \\] \\[ SSE = \\sum(Y_i-\\hat{Y}_i)^2 \\] and note that \\(SSTO = SSR + SSE\\). Important to note is that \\(SSTO\\) uses the difference between the observations \\(Y_i\\) and the null (reduced) model. The \\(SSR\\) uses the diffences between the alternative (full) and null (reduced) model. The \\(SSE\\) uses the differences between the observations \\(Y_i\\) and the alternative (full) model. From these we could set up a General \\(F\\) table of the form   Sum Sq Df Mean Sq F Value Model Error \\(SSR\\) \\(df_R-df_F\\) \\(\\frac{SSR}{df_R-df_F}\\) \\(\\frac{SSR}{df_R-df_F}\\cdot\\frac{df_F}{SSE}\\) Residual Error \\(SSE\\) \\(df_F\\) \\(\\frac{SSE}{df_F}\\) Total Error \\(SSTO\\) \\(df_R\\) Prediction and Confidence Intervals for \\(\\hat{Y}_h\\) (Expand) predict(…, interval=“prediction”)… It is a common mistake to assume that averages (means) describe individuals. They do not. So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line. Interval R Code Math Equation When to Use Prediction predict(..., interval=\"prediction\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\text{Pred}\\ Y}\\) Predict an individual’s value. Confidence predict(..., interval=\"confidence\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\hat{Y}}\\) Estimate location of the mean y-value. predict(mylm, data.frame(XvarName = number), interval=...) For example, consider this graph. Then click here to read about the graph. Notice the three dots above 15 mph in the graph. Each of these dots show a car that was going 15 mph when it applied the brakes. However, stopping distances of the three individual cars differ with one at 20 feet, one at 26 feet and one at 54 feet. The regression line represents the average stopping distance of cars. In this case, cars going 15 mph are estimated to have an average stopping distance of about 40 feet, as shown by the line. But individual vehicles, all going the same speed of 15 mph, varied from stopping distances of 20 feet up to 54 feet! So, to predict that a car going 15 mph will take 41.4 feet to stop, doesn’t tell the whole story. Far more revealing is the complete statement, “Cars going 15 mph are predicted to take anywhere from 10.2 to 72.6 feet to stop, with an average stopping distance of 41.4 feet.” This is called the “prediction interval” and is shown in the graph in blue. It is obtained in R with the codes: cars.lm <- lm(dist ~ speed, data=cars) predict(cars.lm, data.frame(speed=15), interval=\"prediction\") cars.lm <- lm(dist ~ speed, data=cars) pander(predict(cars.lm, data.frame(speed=15), interval=\"prediction\")) fit lwr upr 41.41 10.17 72.64 plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=\"firebrick2\", cex=1.5) cars.lm <- lm(dist ~ speed, data=cars) abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3)) abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2)) abline(v=15, lty=2, col=\"firebrick\") preds <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12) lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8)) lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8)) Now, for the details behind prediction intervals and confidence intervals. Let’s begin by recalling some details (from the section “Inference for the Model Parameters”) about the standard error of the y-intercept, \\(b_0\\). Recall that the y-intercept is the average y-value for the given x-value of \\(x=0\\). Recall further that the formula for the standard error of \\(b_0\\) is given by the formula \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] If we wanted to be more exact with this formula, we would write it as \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{(0-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice the addition of \\((0 - \\bar{X})^2\\) instead of just \\(\\bar{X}^2\\) in the numerator of the right-most part of the equation? This more complete statement obviously would reduce to just \\(\\bar{X}^2\\), but that is only because \\(X=0\\) when we are working with the y-intercept, \\(b_0\\). We could be working with other values of \\(X\\) than just zero. Let’s take a quick detour and talk notation for a second. Typically, \\(X_i\\) and \\(Y_i\\) are used to denote the x-value and y-value of points that are contained in our data set. When we want to reference a point that wasn’t within our original data set, we use the notation \\(X_h\\) and \\(Y_h\\). (The letter h is close to i, but different from i, so why not. There is really no other reason to use h.) Thus, \\(Y_h\\) is the y-value for the \\(X_h\\) x-value, neither of which were included in our original regression of \\(X_i\\)’s and \\(Y_i\\)’s. Now, back to the previous discussion. If \\(X_h = 0\\), then \\(\\hat{Y}_h\\) is the y-intercept, so \\(\\hat{Y}_h = b_0\\) when \\(X_h=0\\). So, we could write, \\[ s^2_{\\hat{Y}_h} = MSE\\left[\\frac{1}{n} + \\frac{(X_h-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice how the \\(b_0\\) in \\(s_{b_0}\\) was replaced with \\(\\hat{Y}_h\\) to get \\(s_{\\hat{Y}_h}\\) and the 0 in \\((0 - \\bar{X})^2\\) was replaced with \\(X_h\\) to get \\((X_h - \\bar{X})^2\\)? Interesting. We now have a formula that would give us the standard error of \\(\\hat{Y}_h\\) for any \\(X_h\\) value, not just \\(X_h = 0\\), or the y-intercept, \\(b_0\\). That is fantastic. It would look like this if plotted. Notice how the gray region is showing the standard error for each \\(\\hat{Y}_h\\) value? (It is technically showing the confidence interval for \\(E\\{Y_h\\}\\) at every possible \\(X_h\\) value, but that is just \\(\\hat{Y}_h \\pm t^* \\cdot s_{\\hat{Y}_h}\\).) ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + theme_bw() ## `geom_smooth()` using formula = 'y ~ x' Confidence Interval for \\(\\hat{Y}_h\\) \\[ \\hat{Y}_h \\pm t^* s_{\\hat{Y}_h} \\quad \\text{where} \\ s_{\\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] The confidence interval is a wonderful tool for estimating \\(E\\{Y_h\\}\\), the “true” average y-value for a given x-value of \\(X_h\\). However, it is not valuable for predicting an individual dot, or \\(Y_h\\) value. Notice how few of the dots of the regression are actually contained within the confidence interval band in the plot? The confidence interval does not really predict where the dots will land, just where the average y-value is located for each x-value. Remember the 68-95-99.7 Rule of the normal distribution? If not, here is a link back to that concept in the Math 221 textbook. This rule states that roughly 95% of data, when normally distributed, will be between \\(z=-2\\) and \\(z=2\\) standard deviations from the mean. So, is going two “residual standard errors” to both sides of the regression line enough to capture 95% of the data? The answer is, not quite. The reason for this is because our knowledge of where the true mean lies is uncertain. (Notice the confidence interval band shown in the plot.) However, adding two standard errors to the edges of the confidence band would get us in the right place. In other words, there are two sources of variability at play here, (1) our uncertaintity in where the regression line is sitting, and (2) the natural variability of the data points around the line. Thus, the “prediction interval” requires accounting for both of these sources of variability to produce the following equation. Prediction Interval for \\(Y_h\\) \\[ \\hat{Y}_h \\pm t^* s_{Pred \\hat{Y}_h} \\quad \\text{where} \\ s_{Pred \\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + 1 + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] This formula provides a useful band for identifying a region where we are 95% confident that a new observation for \\(Y_h\\) will land, given the value of \\(X_h\\). It looks as follows. Notice the prediction interval is much wider than the confidence interval. This is because data varies far more than do means. Prediction is for where the individual data points will land, confidence is for where the mean will land. cars.lm <- lm(dist ~ speed, data=cars) predy <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + geom_segment(aes(x=15, xend=15, y=predy[2], yend=predy[3]), lwd=4, color=rgb(.5,.7,.5,.01)) + geom_point(aes(x=15, y=predy[1]), cex=2, color=\"skyblue\", pch=15) + theme_bw() Lowess (and Loess) Curves (Expand) A non-parametric approach to estimating \\(E\\{Y_i\\}\\)… Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value. Using Base R # Select the relevant columns and remove NA values air2 <- airquality %>% dplyr::select(Temp, Ozone) %>% # Use dplyr's select function explicitly drop_na() # Drop rows with NA values in Temp or Ozone plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") lines(lowess(air2$Ozone, air2$Temp), col=\"firebrick\") Using ggplot2 # Your dataset (air2) should have been created earlier, ensuring no NAs air2 <- na.omit(airquality[c(\"Temp\", \"Ozone\")]) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=\"darkgray\") + # Add points with dark gray color geom_smooth(se=FALSE, method=\"loess\", method.args = list(degree=1)) + # Lowess curve, no standard error shading theme_bw() # Black and white theme ## `geom_smooth()` using formula = 'y ~ x' # Optionally, allow for predictions as well as the graph: # Uncomment the following lines if you want predictions as well: # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fitted, x=Ozone)) # Use fitted values from loess Advantages Disadvantages Quick. Good at ignoring outliers. Good at capturing the general pattern in the data. Good for making predictions within the scope of the data. No mathematical model. Not interpretable. No p-values. No adjusted R-squared. How it Works The Lowess curve localizes the regression model to a “neighborhood” of points, and then joins these localized regressions together into a smooth line. It minimizes the effect of outliers, and let’s the data “speak for itself”. As a downside, it is not interpretable, and has no final way to write the model mathematically. All the same, it is a very powerful tool for identifying an appropriate model, or verifying the fit of a model, or making predictions when no reasonable model does an adequate job. Study this graphic and the explanations below to learn how it works. Recommendation: run the code in this “Code” chunk to the right in your Console, and flip through the resulting graphics. X <- cars$speed Y <- cars$dist X <- X[!is.na(X) & !is.na(Y)] Y <- Y[!is.na(X) & !is.na(Y)] f <- 1/2 n <- length(X) lfit <- rep(NA,n) for (xh in 1:n){ xdists <- X - X[xh] nn <- floor(n*f) r <- sort(abs(xdists))[nn] xdists.nbrhd <- which(abs(xdists) < r) w <- rep(0, length(xdists)) w[xdists.nbrhd] <- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w), col=rgb(.2,.2,.2,.3), cex=1.5, yaxt='n', xaxt='n', xlab=\"\", ylab=\"\") points(Y[xh] ~ X[xh], pch=16, col=\"orange\") lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2)) cat(\"\\n\\n\") readline(prompt=paste0(\"Center point is point #\", xh, \"... Press [enter] to continue...\")) MADnotThereYet <- TRUE count <- 0 while(MADnotThereYet){ readline(prompt=paste0(\"\\n Adjusting line to account for outliers in the y-direction... Press [enter] to continue...\")) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"wheat\", add=TRUE) MAD <- median(abs(lmc$res)) resm <- lmc$res/(6*MAD) resm[resm>1] <- 1 bisq <- (1-resm^2)^2 w <- w*bisq obs <- coef(lmc) lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) count <- count + 1 if ( (sum(abs(obs-lmc$coef))<.1) | (count > 3)) MADnotThereYet <- FALSE } curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"green\", add=TRUE) points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=\"green\") readline(prompt=paste0(\"\\n Use final line to get fitted value for this point... Press [enter] to continue to next point...\")) lfit[xh] <- predict(lmc, data.frame(X=X[xh])) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") if (xh == n){ readline(prompt=paste0(\"\\n Press [enter] to see actual Lowess curve...\")) lines(lowess(X,Y, f=f), col=\"firebrick\") legend(\"topleft\", bty=\"n\", legend=\"Actual lowess Curve using lowess(...)\", col=\"firebrick\", lty=1) } } Select a fraction of the data to use for the “neighborhood” of points (shown in blue in the graph above). The lowess function in R uses “f=2/3” and the loess function uses “span=0.75” for this value, which selects the nearest two-thirds or 75% of the data, respectively, depending on which function you use. For this example, we set the fraction of points at 50%. Both functions can be set to whatever you want. Pick any point in the regression, eventually selecting all points one at a time. The selected point becomes the “center” of a “neighborhood” of points surrounding it. In this example, the center point is in orange, and the neighboring points are in blue. Use the points within the neighborhood to fit a regression line. However, make the regression depend most on points closest to “center” and least on points furthest from “center.” This is called a weighted regression. Weights are decided according to what is called the tricubic weight function, so that the weight \\(w\\) given to point \\(j\\) of the neighborhood of points is defined by \\[ w_j = \\left(1- \\left( \\frac{|X_c - X_j|}{\\max_k |X_c - X_k|}\\right)^3\\right)^3 \\] where \\(X_c\\) is the x-value of the “center” dot and \\(X_j\\) is the x-value of any other dot in the neighborhood. The fitted-value of \\(\\hat{Y}_c\\) is obtained for the center point \\(X_c\\) of the current regression. This point is used as the Lowess (or Loess) curve’s value at that particular x-value. Well, almost. It’s a first guess at where this value will end up, but there’s a little more to the algorithm before we are done. Initial guesses for each of these fitted values are obtained for each point in the regression. Now each local regression for each neighborhood is re-run a few times in such a way the the effect of outliers is minimized. The final line for each neighborhood is obtained by the following steps. Compute all residuals for points in the neighborhood of the current regression, denoted by \\(r_i\\). Then compute the MAD, median absolute deviation, of the residuals \\(MAD = \\text{median} (|r_1|, |r_2|, \\ldots)\\). Divide all residuals by 6 times the MAD: \\(u_i = r_i/(6\\cdot MAD)\\) (If \\(r_i > 6\\cdot MAD\\) then set \\(u_i = 0\\).) Compute what are called bisquare weights using the formula: \\(b_i = (1 - u_i^2)^2\\) Perform a regression using the weights \\(w_i = w_i b_i\\) Repeat the above process with the new weights \\(w_i\\) until the weights stop changing very much. The final fitted values for each \\(X\\)-value in the regression are obtained from the final regression line for each neighborhood. These fitted values make up the Lowess (or loess) curve. Note that the default of the loess function in R is to use quadratic regressions in each neighborhood instead of linear regressions. This can be controlled with the loess option of “degree=2” (quadratic fits) or “degree = 1”. In the lowess function only a linear regression in each neighborhood is allowed. Examples: bodyweight, cars Quantitative Y | Multiple X Graphics Y is a single quantitative variable of interest, like height. While we could use an X1 of “shoe-size” to explain height, we might also want to use a second x-variable, X2, like “gender” to help explain height. Further x-variables could also be used. Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a student’s final exam grade? How do factors like age, weight, exercise level, and diet impact a person’s blood pressure? Examples: Is one Navy base more prone to heatwaves than the other? The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH) Tests Multiple Linear Regression Multiple regression is a way to understand how different factors affect something we’re interested in. It’s like having more than one clue to solve a mystery. Here’s what you need to know: We can use several different pieces of information (called variables) to predict an outcome. There are many ways to combine these pieces of information. We start with simple combinations and build up to more complex ones, kind of like building with Lego blocks. This helps us make better predictions and understand complex situations more clearly. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model is a bit like using the same ingredient twice in a recipe, but in different ways. It uses the same factor (\\(X_i^2\\)) in two different forms: Once in its regular form (\\(\\beta_1 X_i\\)) Once squared (\\(\\beta_2 X_i^2\\)) This squared part is what we call the “quadratic” term. It helps us describe relationships that aren’t just straight lines, but have curves to them. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and “steepness” of the Model: negative values face down, positive values face up; large values imply “steeper” parabolas and low values imply “flatter” parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)’s explanation. An Example Using the airquality data set, we run the following “quadratic” regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our “quadratic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. Temp Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These “Estimates” can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(Month^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of Month from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, )   Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will “open down” (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be “month zero.” Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the “cubic” term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the “base slope coefficient” and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following “cubic” regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our “cubic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. uptake Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^3) \\(X_{i}^3\\), where the function I(…) protects the cubing of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will “open up”. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The “two-lines” model is a way to compare two different groups in statistics using numbers. Here’s how it works: It uses two types of numbers: A regular number that can be any value (\\(X_{1i}\\)) A special number that’s either 0 or 1 (called an “indicator variable”, \\(X_{2i}\\)) The special number (\\(X_{2i}\\)) helps us turn group information (like “Group A” or “Group B”) into numbers we can use in calculations. This model creates two separate lines on a graph: One line for Group A One line for Group B By using this method, we can easily see and compare how the two groups are different from each other. Think of it like having two recipes for the same dish, but with slightly different ingredients. This model helps us see how those small changes affect the final result. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the “base-line” of the model, the “Group 0” line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the “base-line” line. \\(\\beta_3\\) Called the “interaction” term. Controls the change in the slope for the second line in the model as compared to the slope of the “base-line” line. An Example Using the mtcars data set, we run the following “two-lines” regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our “two-lines” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. mpg Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) (beta zero): This is like the starting point. It’s estimated by something called the “Intercept”. \\(\\beta_1\\) (beta one): This shows how one thing changes when another thing (called “qsec”) changes. It’s estimated to be 1.439. \\(\\beta_2\\) (beta two): This shows how something else (called “am”) affects the overall result. It’s estimated to be -14.51. \\(\\beta_3\\) (beta three): This shows how “qsec” and “am” work together to affect the result. It’s estimated to be 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called “3D” regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to “bend”. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the “Temp” direction is estimated to be 2.659 and the slope in the “Month” direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction term’s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the “slopes” in each direction to change, creating a “curved” surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called “HD”, or “High Dimensional”, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isn’t really even possible to “mentally connect” with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)’s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the “Wind” direction is estimated to be -3.334. The slope in the “Temp” direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the “Solar.R” direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here. R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the “Overview” section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the “column (c) bind” function and it joins together things as columns. Res =  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals.  panel=panel.smooth,  This places a lowess smoothing line on each scatterplot. col =  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(“color1”,“color2”, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for “linear model.” Y Y must be a “numeric” vector of the quantitative response variable.  ~  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats “numeric” variables as quantitative and “character” or “factor” variables as qualitative. R will automatcially recode qualitative variables to become “numeric” variables using a 0,1 encoding. See the Explanation tab for details.  + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details.  + …, * ... emphasizes that as many explanatory variables as are desired can be included in the model.  data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(…) function displays the results of an lm(…) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(…) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -4.3818 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -2.2696 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.1344 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   1.7058 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   5.8752 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero.   2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (26.6248479) by its standard error (2.1829432). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a “star”. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + …).   -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model.   0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” by its standard error. It gives the “number of standard errors” away from zero that the “estimate” has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + …).   5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\).   2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot “.” implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like.   0.0004029 This is the estimate of the coefficient of the interaction term.   0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data.  0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that “at least one is not.”  34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom.  on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero.  p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that “at least one” of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the “Overview” sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =… Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “prediction”) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “confidence”) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BIC… There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (“R-squared”). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The “simplest” but “best” model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it “an information criterion (AIC)” when he published the method and other people later began calling it the “Akaike Information Criterion.”) Model Selection (Expand) pairs plots, added variable plots, and pattern recognition… Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots {.tabset .tabset-pills} A useful visualization tool for model selection is the “pairs plot.” This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice that… the y-axis of each plot is found by locating the variable name (like “mpg”) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like “disp”) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model {.tabset .tabset-pills} Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldn’t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Let’s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a model’s ability to generalize to new data… The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesn’t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, let’s remind ourselves why we use regression models in the first place. The main goal is to capture the “essence” of the data. In other words, the general pattern is what we are after. We want a model that tells us how “all such” data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the “complicated model” is overfitting the original data. It does not capture the “essence” of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-value… The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the model… The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions{#check} The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read more…) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) Outlier Analysis (Expand) Cook’s Distances and Leverage Values… The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs. fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cook’s Distances The idea behind Cook’s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article “Detection of Influential Observation in Linear Regression” (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, let’s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2))   1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the “differences” in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cook’s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cook’s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cook’s Distances using the code cooks.distance(lmObject). Also, a graph of Cook’s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of “leverage” and is “pulling” the regression toward itself. A value near 0 implies the point is just “one of many” and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that “minimize” the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)’s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the “hat matrix” \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the “leverage values” and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a “lot of pull,” and values close to 0 represent “little pull.” In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with “lots of leverage” and a large “Cook’s Distance” are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regression… Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class. Examples: Civic Vs Corolla cadillacs Binomial Y | Quantitative X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t. We might use an explanatory variable X of “height” to see if taller students are more likely to get an A in Math 325 than shorter students. (They aren’t, if you were wondering.) Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1’s and 0’s (this does happen or this doesn’t happen): Does the width of a child’s foot predict whether or not they are right-handed? What is the probability the cat adopted is a male based on how many days it took them to get adopted? Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted? (Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\") Tests Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The “P” stands for “Probability that…” \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1… This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)… in other words, the “|” says “given” and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the “natural constant” number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14…) Part What it means \\(P(Y_i = 1| x_i)\\) - The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s) - The symbol used to represent the notation in logisitc regression is \\(\\pi_i\\) \\(Y_i\\) (Responsive Variable) Describes the probability of $Y_i = $ 1 or 0 for a given scenario. \\(x_i\\) (Explanatory variable) Describes the observed value and how it effects the \\(Y_i\\) \\(\\frac{e^{\\text{stuff}}}{1+e^{\\text{stuff}}}\\) The content for “stuff” being simple linear regression of the form \\(\\beta_0 + \\beta_{1x_i}\\) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -1.5651 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.6648 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -0.2460 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   0.7276 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   2.2691 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The test statistic is a regular old z-score. It is most reliable when the sample size is “large.” It is a measurement of the number of standard errors the estimate is from 0.   Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds.   1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, …).   -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds.   0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a “star”. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about.   Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\).  43.230  on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates.  29.732 This can be calculated by sum(log(myglm$res^2)).  on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, “A version of Akaike’s An Information Criterion…” The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model.  33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 ## disp -0.014604 0.005168 -2.826 0.00471 ## ## (Intercept) * ## disp ** ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0’s or 1’s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == “B” or height < 68. In other words, Y needs to be a collection of 0’s and 1’s.  ~  The tilde is read “Y on X.” X, X is some quantitative variable.  data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(…) function. curve( A function in R that draws a “curve” on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the “Intercept” estimate from your logistic regression summary output.  \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case “x” in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula.  exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator.  add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(…) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set.  aes( The aes(…) function stands for “aesthetics” and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The plus sign adds a new layer to the ggplot.   geom_point( Tells the plot to add the physical geometry of “points” to the plot. ) Closing parenthesis for the geom_point() function.  + Add another layer to the plot.   geom_smooth( Add a smoothing line to the graph. method=“glm”, This adds a general linear model to the graph. method.args = list(family=“binomial”), This tells the method=“glm” to choose specifically the “binomial” model, otherwise known as the “logistic regression” model.  se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function.  + Add another layer to the ggplot.   theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),  The xVariableName is the same one you used in your glm(y ~ x, …) statement for “x”. Input any desired value for the “someNumericValue” spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016 Explanation Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal). The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\). Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page. Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful. Checking Model Assumptions {#diagnostics} The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit. Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease. Binomial Y | Multiple X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t. We might use an explanatory variable X1 of “height” and a second explanatory variable X2 of “gender” to try to predict whether or not a student will get an A in Math 325. Test(s) used for this category: Multiple Logistic Regression Model Tests Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two. Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. …, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -2.9446 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.2364 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.0000 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q   0.2776 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   1.9968 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The Z-score testing the hypothesis that \\(\\beta_j=0\\)   Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, …).   0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\).   1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\).   4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\).   3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\).   0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\).   0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a “star”. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\).   471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option.   Null Deviance: The deviance of the null model.  800.44 In this case, the null deviance is 800.44  on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression.  256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit.  on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model.  272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities ## numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) -4.97603 0.65316 -7.618 ## Time 0.39111 0.04979 7.854 ## Diet2 0.58283 1.04061 0.560 ## Diet3 -8.44476 4.32324 -1.953 ## Diet4 -67.41995 3768.10023 -0.018 ## Time:Diet2 0.02391 0.08679 0.275 ## Time:Diet3 1.20955 0.51249 2.360 ## Time:Diet4 8.83168 471.01259 0.019 ## Pr(>|z|) ## (Intercept) 2.57e-14 *** ## Time 4.02e-15 *** ## Diet2 0.5754 ## Diet3 0.0508 . ## Diet4 0.9857 ## Time:Diet2 0.7830 ## Time:Diet3 0.0183 * ## Time:Diet4 0.9850 ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, …), X2 = c(Value 1, Value 2, …), …) command. You should see the GSS example file for an example of how to use this function. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the “b” vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(“SomeColor”,“DifferentColor”,…) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(…) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==“B”.  ~  The formula operator in R. X1, The first x-variable in your glm code.  data = YourDataSet, Specify the name of your data set.  pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(…) function computes e^(stuff) in R and stands for the “exponential function.” (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(…) function requires that you call the x-variable “x” although you can change this behavior using xname=“SomeOtherName” if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)).  col = palette()[1], Pull the first color from the color palette for the first curve.  add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters.  col = palette()[2], Set the color of this second curve to the second color in the palette.  add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. “topright”, Typically the legend is placed in the top-right corner of the graph. But it could be placed “top”, “topleft”, “left”, “bottomleft”, “bottom”, “center”, “right”, or “bottomright”.  legend = Why you have to write legend again, no one knows, but you do. c(“Lable 1”, “Label 2”), These are the values that will appear in the legend, one entry on each line of the legend.  col = palette(), This specifies the colors of the symbols in the legend. First color goes with “Label 1”, and second goes with “Label 2” and so on.  lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist.  bty = Specifies the “box type” (bty) that should be drawn around the legend. ‘n’) By placing a “no” option (‘n’) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot.  aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + Add a layer to the ggplot.   geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function.  + Add a layer to the ggplot.   geom_smooth(method=“glm”, method.args=list(family=“binomial”), se=FALSE, Add the logistic regression curve to the plot.   aes(color=“X2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function.  + Add a layer to the plot.   theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x' Explanation Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative. The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations. Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant. Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression. Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\). Categorical Y | Categorical X Graphics Y is a single categorical variable of interest, like gender. X is another categorical variable of interest, like “hair color”. This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender. Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year? Whats the probability that you have brown hair and blue eyes? Tests Add your own notes about appropriate inferential procedures for this type of data here.",
    "sentences": ["One Sample t Test A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable.", "Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average?", "Is human body temperature really 98.6&deg; F on average?", "Do I spend less than $3 a day,on average, purchasing snacks?", "Requirements This test is only appropriate when both of the following are satisfied.", "The sample is representative of the population.", "(Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal.", "This is a safe assumption when either: the population data can be assumed to be normally distributed using a Q-Q Plot OR the size of the sample (n) that was taken from the population is large (at least n > 30, but “large” really depends on how badly the data is skewed).", "If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population.", "If the requirements are not met, then that doesn’t mean the results of the test are necessarily bad, but there is no guarantee that they are good."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 85,
    "title": "📍 Quantitative Y | Categorical X (2\r\nGroups)",
    "url": "index.html#quantitativeycategoricalx2groups",
    "content": "Quantitative Y | Categorical X (2\r\nGroups) Graphics Y is a single quantitative variable of interest. This would be like “heights” of BYU-Idaho students. X is a qualitative (categorical) variable of interest like “gender” that has just two groups “A” and “B”. So this logo represents situtations where we would want to compare heights of male (group A) and female (group B) students. Test(s) used for this category: Independent Samples T-test Analysis : High School Seniors t Test Wilcoxon Rank Sum (Mann- Whitney Test) Analysis : The Benefits of Word Recall Strategies Used for the comparison of two groups, Quantitative Y (ex. height, weight, money, distance) and Categorical X (ex. gender, birth month, phone number) Questions that this answers: How long are boys feet and how long are girls feet? Do boys (B) or girls (G) have longer feet, on average, in the KidsFeet dataset? These are the best graphics to use: Examples: How long are boys feet and how long are girls feet? (Independent Sample) Can use box or dot plot! plot_ly(KidsFeet, y=~length, x=~as.factor(sex),type=\"box\",color=~sex, colors=c(\"lightblue\",\"pink\")) %>% layout( title=\"4th Grader's Average Foot Size\", xaxis=list(title=\"Gender\"),yaxis=list(title=\"Length (cm)\")) ggplot(KidsFeet, aes(x=factor(sex),y=length, fill=sex)) + coord_flip() + geom_dotplot(binaxis=\"y\", stackdir=\"up\", position=\"dodge\", dotsize=0.45, binwidth=0.5) + scale_fill_manual(values=c(\"B\"=\"lightblue\",\"G\"=\"pink\")) + labs(title= \"Kennedy 4th Grader's Foot Size\", x=\"Length (cm)\", y=\"Gender\") Does the Meshed or Before approach have any positive benefits on memory recall when it comes to remembering content? (Wilcoxon Rank Sum Test) Dot plot over box plot shows the distribution and the individuals within each distribution",
    "sentences": ["Graphics Y is a single quantitative variable of interest.", "This would be like “heights” of BYU-Idaho students.", "X is a qualitative (categorical) variable of interest like “gender” that has just two groups “A” and “B”.", "So this logo represents situtations where we would want to compare heights of male (group A) and female (group B) students.", "Test(s) used for this category: Independent Samples T-test Analysis : High School Seniors t Test Wilcoxon Rank Sum (Mann- Whitney Test) Analysis : The Benefits of Word Recall Strategies Used for the comparison of two groups, Quantitative Y (ex.", "height, weight, money, distance) and Categorical X (ex.", "gender, birth month, phone number) Questions that this answers: How long are boys feet and how long are girls feet?", "Do boys (B) or girls (G) have longer feet, on average, in the KidsFeet dataset?", "These are the best graphics to use: Examples: How long are boys feet and how long are girls feet?", "(Independent Sample) Can use box or dot plot!"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Quantitative Y | Categorical X (2\r\nGroups)"
  },
  {
    "id": 86,
    "title": "📍 Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single quantitative variable of interest. This would be like “heights” of BYU-Idaho students. X is a qualitative (categorical) variable of interest like “gender” that has just two groups “A” and “B”. So this logo represents situtations where we would want to compare heights of male (group A) and female (group B) students. Test(s) used for this category: Independent Samples T-test Analysis : High School Seniors t Test Wilcoxon Rank Sum (Mann- Whitney Test) Analysis : The Benefits of Word Recall Strategies Used for the comparison of two groups, Quantitative Y (ex. height, weight, money, distance) and Categorical X (ex. gender, birth month, phone number) Questions that this answers: How long are boys feet and how long are girls feet? Do boys (B) or girls (G) have longer feet, on average, in the KidsFeet dataset? These are the best graphics to use: Examples: How long are boys feet and how long are girls feet? (Independent Sample) Can use box or dot plot! plot_ly(KidsFeet, y=~length, x=~as.factor(sex),type=\"box\",color=~sex, colors=c(\"lightblue\",\"pink\")) %>% layout( title=\"4th Grader's Average Foot Size\", xaxis=list(title=\"Gender\"),yaxis=list(title=\"Length (cm)\")) ggplot(KidsFeet, aes(x=factor(sex),y=length, fill=sex)) + coord_flip() + geom_dotplot(binaxis=\"y\", stackdir=\"up\", position=\"dodge\", dotsize=0.45, binwidth=0.5) + scale_fill_manual(values=c(\"B\"=\"lightblue\",\"G\"=\"pink\")) + labs(title= \"Kennedy 4th Grader's Foot Size\", x=\"Length (cm)\", y=\"Gender\") Does the Meshed or Before approach have any positive benefits on memory recall when it comes to remembering content? (Wilcoxon Rank Sum Test) Dot plot over box plot shows the distribution and the individuals within each distribution",
    "sentences": ["Y is a single quantitative variable of interest.", "This would be like “heights” of BYU-Idaho students.", "X is a qualitative (categorical) variable of interest like “gender” that has just two groups “A” and “B”.", "So this logo represents situtations where we would want to compare heights of male (group A) and female (group B) students.", "Test(s) used for this category: Independent Samples T-test Analysis : High School Seniors t Test Wilcoxon Rank Sum (Mann- Whitney Test) Analysis : The Benefits of Word Recall Strategies Used for the comparison of two groups, Quantitative Y (ex.", "height, weight, money, distance) and Categorical X (ex.", "gender, birth month, phone number) Questions that this answers: How long are boys feet and how long are girls feet?", "Do boys (B) or girls (G) have longer feet, on average, in the KidsFeet dataset?", "These are the best graphics to use: Examples: How long are boys feet and how long are girls feet?", "(Independent Sample) Can use box or dot plot!"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 87,
    "title": "📍 Tests",
    "url": "index.html#tests",
    "content": "Tests Independent Samples t Test The independent samples t test is used when a value is hypothesized for the difference between two (possibly) different population means, \\(\\mu_1 - \\mu_2\\). finds the mean of each data set, then subtracts those means for their difference two groups don’t depend on each other’s results Overview Questions The Independent Samples t Test can be used to answer questions like: Are boys taller than girls on average? Do students who show up to class everyday get higher scores on average than those who don’t? Do you take more steps on average on weekdays or on weekends? Requirements The test is only appropriate when both of the following are satisfied. Both samples are representative of the population. (Simple random samples are the best way to do this.) The sampling distribution of the difference of the sample means \\((\\bar{x}_1 - \\bar{x}_2)\\) can be assumed to be normal. This is a safe assumption when the sample size of each group is \\(30\\) or greater or when the population data from each group can be assumed to be normal with a Q-Q Plot. Hypotheses Math Code $$ H_0: \\mu_\\text{label 1} - \\mu_\\text{label 2} = 13.2 $$ $$ H_a: \\mu_\\text{label 1} - \\mu_\\text{label 2} \\neq 13.2 $$ \\(H_0: \\mu_1 - \\mu_2 = \\text{some number, but typically 0}\\) \\(H_a: \\mu_1 - \\mu_2 \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepInd student1 student2 R Instructions Console Help Command: ?t.test() There are two ways to perform the test. Option 1: t.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a “numeric” vector from YourData that represents the data for both samples. X must be a “factor” or “character” vector from YourData that represents the group assignment for each observation. There can only be two groups specified in this column of data. YourNull is the numeric value from your null hypothesis for \\(\\mu_1-\\mu_2\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y ~ X, data=YourData) Example Code Hover your mouse over the example codes to learn more. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. length  ‘length’ is a quantitative variable (numeric vector). ~  ‘~’ is the tilde symbol. sex,  ‘sex’ is a ‘factor’ or ‘character’ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet,  ‘KidsFeet’ is a dataset in library(mosaic). Type View(KidsFeet) to view it. mu = 0,  The numeric value from the null hypothesis for μ1-μ2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = “two.sided”,  The alternative is “two-sided” meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\)     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. Welch Two Sample t-test EXPLANATION. data: length by sex EXPLANATION. t = 1.9174, EXPLANATION.  df = 36.275, EXPLANATION.  p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  -0.04502067 EXPLANATION.  1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean in group B mean in group G EXPLANATION.   25.10500 EXPLANATION.   24.32105 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. length  ‘length’ is a quantitative variable (numeric vector). ~  ‘~’ is the tilde symbol. sex,  ‘sex’ is a “factor” or “character” vector that represents the group assignment for each observation. There are two groups. data=KidsFeet) ‘KidsFeet’ is a dataset in library(mosaic). Type View(KidsFeet) to view it.  Click to Show Output  Click to View Output. Option 2: t.test(NameOfYourData$Y1, NameOfYourData$Y2, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a “numeric” vector that represents the quantitative data from the first sample. Y2 must be a “numeric” vector that represents the quantitative data from the second sample. YourNull is the numeric value from your null hypothesis for the difference of \\(\\mu_1-\\mu_2\\). This is typically zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) par(mfrow=c(1,2)) qqPlot(NameOfYourData$Y1) qqPlot(NameOfYourData$Y2) Example Code Hover your mouse over the example codes to learn more. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. KidsFeet$length[KidsFeet$sex == “B”],  A numeric vector that represents the quantitative data or the foot length for the first sample of data which in this case is the boys. KidsFeet$length[KidsFeet$sex == “G”],  A numeric vector that represents the quantitative data or the foot length for the second sample of data which in this case is the girls. mu = 0,  The numeric value from the null hypothesis for μ1-μ2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = “two.sided”,  The alternative is “two-sided” meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\)     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Welch Two Sample t-test EXPLANATION. data: KidsFeet$length[KidsFeet$sex == “B”] and KidsFeet$length[KidsFeet$sex == “G”] EXPLANATION. t = 1.9174, EXPLANATION.  df = 36.275, EXPLANATION.  p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  -0.04502067 EXPLANATION.  1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean of x mean of y EXPLANATION.   25.10500 EXPLANATION.   24.32105 EXPLANATION. par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow=c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. KidsFeet$length[KidsFeet$sex == “B”]) A numeric vector that represents the quantitative data or the foot length for the first sample of data which in this case is the boys. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. KidsFeet$length[KidsFeet$sex == “G”]) A numeric vector that represents the quantitative data or the foot length for the second sample of data which in this case is the girls.  …  Click to View Output. ## [1] 9 8 ## [1] 18 7 Explanation The first figure below depicts the scenario where the difference in means of two separate normal distributions is non-zero. - In other words, the two distributions have different means, \\(\\mu_1\\) and \\(\\mu_2\\), respectively. - It is worth emphasizing that the values of \\(\\mu_1\\) and \\(\\mu_2\\) are unknown to the researcher. The only thing observed are two separate samples of data (blue dots) of sizes \\(n_1\\) and \\(n_2\\), respectively. For the scenario depicted, the null hypothesis that \\(H_0: \\mu_1 - \\mu_2 = 0\\) (i.e., that \\(\\mu_1=\\mu_2\\)) is rejected in favor of the alternative that \\(H_a: \\mu_1 - \\mu_2 \\neq 0\\) based on the sample data observed. - This dicision would be correct as the true difference in means, \\(\\mu_1-\\mu_2\\) is non-zero in this case. When the null hypothesis is true, that \\(H_0: \\mu_1 - \\mu_2 = 0\\) : - the test statistic \\(t\\) that is obtained by measuring the distance between the two sample means, \\(\\bar{x}_1-\\bar{x}_2\\) - appropriately standardizing the result follows a \\(t\\) distribution with degrees of freedom less than or equal to \\(n_1+n_2-2\\). - Thus, the \\(p\\)-value of the independent samples \\(t\\) test is obtained by using this \\(t\\) distribution to calculate the probability of a test statistic \\(t\\) being as extreme or more extreme than the one observed assuming the null hypothesis is true. \\[ t = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{s_1/n_1+s_2/n_2 }} \\] Wilcoxon Rank Sum (Mann-Whitney) Test For testing the equality of the medians of two (possibly different) distributions of a quantitative variable. Box plots + Dot plots are great to show off this kind of data (because it shows the median!) Median line closer to the bottom of the box plot = Right Skewed Median line is closer to the top of the box plot = Left Skewed Overview The nonparametric equivalent of the Independent Samples t Test. Can also be used when data is ordered (ordinal) but does not have an exact measurement. For example, first place, second place, and so on. The Independent Samples t Test is more appropriate when the distributions are normal, or when the sample size for each sample is large. The test is negatively affected when there are ties (repeated values) present in the data, but the results are still useful if there are relatively few ties. Hypotheses Originally designed to test for the equality of medians from two identically shaped distributions. Math Code $$ H_0: \\text{difference in medians} = 0 $$ $$ H_a: \\text{difference in medians} \\neq 0 $$ \\(H_0: \\text{difference in medians} = 0\\) \\(H_a: \\text{difference in medians} \\neq 0\\) However, the test also allows for the more general hypotheses that one distribution is stochastically greater than the other. \\(H_0: \\text{the distributions are stochastically equal}\\) \\(H_a: \\text{one distribution is stochastically greater than the other}\\) If these hypotheses are used, then the distributions do not have to be identically distributed. (Note: Men’s heights are stochastically greater than women’s heights because men are generally taller than women, but not all men are taller than all women.) Examples: BugSpray, MoralIntegration R Instructions Console Help Command: ?wilcox.test() There are two ways to perform the test. Option 1: wilcox.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a “numeric” vector from YourData that represents the data for both samples. X must be a “factor” or “character” vector from YourData that represents the group assignment for each observation. There can only be two groups specified in this column of data. YourNull is the numeric value from your null hypothesis for the difference in medians from the two groups. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests. length  ‘length’ is a quantitative variable (numeric vector). ~  ‘~’ is the tilde symbol. sex,  ‘sex’ is a ‘factor’ or ‘character’ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet,  ‘KidsFeet’ is a dataset in library(mosaic). Type View(KidsFeet) to view it. mu = 0,  The numeric value from the null hypothesis for the difference in medians from the two groups is 0 meaning the null hypothesis is \\(\\text{difference in medians} = 0\\) alternative = “two.sided”,  The alternative is “two-sided” meaning the alternative hypothesis is \\(\\text{difference in medians} \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\)     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Wilcoxon rank sum test with continuity correction This states the test that was performed and that a normal approximation to the test statistic was used instead of the exact distribution. data: length by sex This states that the length column was split into the two groups found in the “sex” column. Unfortunately, it forgets to remind us that the test used the KidsFeet data set. V = 252, The test statistic of the test. In this case, the sum of the ranks from the alphabetically first group minus the minimum sum of ranks possible.  p-value = 0.0836 The p-value of the test. alternative hypothesis: true location shift is not equal to 0 This reports that the test used an alternative hypothesis of “not equal” (a two-sided test). Further, the phrase “true location shift” emphasizes that the Wilcoxon Rank Sum Test is testing to see if one distribution is shifted higher or lower than the other. Option 2: wilcox.test(object1, object2, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object1 must be a “numeric” vector that represents the first sample of data. obejct2 must be a “numeric” vector that represents the second sample of data. YourNull is the numeric value from your null hypothesis for the difference in medians from the two groups. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests. KidsFeet$length[KidsFeet$sex == “B”],  A numeric vector of foot length for the first sample of data or for the group of boys. KidsFeet$length[KidsFeet$sex == “G”],  A numeric vector of foot length for the second sample of data or for the group of girls. mu = 0,  The numeric value from the null hypothesis for the difference in medians from the two groups is 0 meaning the null hypothesis is \\(\\text{difference in medians} = 0\\) alternative = “two.sided”,  The alternative is “two-sided” meaning the alternative hypothesis is \\(\\text{difference in medians} \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\)     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Wilcoxon rank sum test with continuity correction This states the type of test performed. data: KidsFeet$length[KidsFeet$sex == “B”] and KidsFeet$length[KidsFeet$sex == “G”] Reminds you what data was used for the test and points out that the first set of data listed is “Group 1” and the second set of data listed is “Group 2.” V = 252, The test statistic of the test.  p-value = 0.0836 The p-value of the test. alternative hypothesis: true location shift is not equal to 0 The alternative hypothesis of the test. The phrase “not equal” implies a “two-sided” alternative was used. Explanation In many cases it is of interest to perform a hypothesis test concerning the equality of the centers of two (possibly different) distributions. In other words, an independent samples test. The Wilcoxon Rank Sum Test allows a nonparametric approach to doing this. It is often considered the nonparametric equivalent of the independent samples t test. The method is most easily explained through an example. The theory behind it is very similar to the theory behind the Wilcoxon Signed-Rank Test. Independent Samples Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon. Background The percent of flies (bugs) killed from two different concentrations of a certain spray were recorded from 16 different trials, 8 trials per treatment concentration. The experiment hypothesized that the center of the distributions of the percent killed by either concentration were the same. In other words, that both treatments were equally effective. The alternative hypothesis was that the treatments differed in their effectiveness. Spray Concentration Percent Killed A 68, 68, 59, 72, 64, 67, 70, 74 B 60, 67, 61, 62, 67, 63, 56, 58 Step 1 The first step of the Wilcoxon Rank Sum Test is to order all the data from smallest magnitude to largest magnitude, while keeping track of the group. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Step 2 The next step is to rank the ordered values. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=16\\). Any ranks that are tied need to have the average rank assigned to each of those that are tied. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Rank 1 2 3 4 5 6 7 8 10 10 10 12.5 12.5 14 15 16 Step 3 The ranks are then returned to their original groups. Ranks of Spray A Ranks of Spray B 3, 8, 10, 12.5, 12.5, 14, 15, 16 1, 2, 4, 5, 6, 7, 10, 10 Step 4 The ranks are summed for one of the groups. (It does not matter which group.) Sum of Ranks for Spray A: \\[ 3+8+10+12.5+12.5+14+15+16 = 91 \\] Step 5 The \\(p\\)-value of the test is then obtained by computing the probabilities of all possible sums that one group can achieve using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=16\\) ranks, with just \\(8\\) of the ranks assigned to one group, the possible sums of ranks range from \\(36\\) to \\(100\\) and include every integer in between, i.e., \\(36, 37, 38, \\ldots, 100\\). Note that the smallest sum would be obtained if the ranks 1-8 were in the group. The largest sum would be obtained if the ranks 9-16 were in the group. The probability of each possible sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 12,870 total different sets of ranks possible when there are \\(n=16\\) ranks and \\(8\\) are assigned to one group.) The distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(91\\) (or its opposite of \\(136-91=45\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of negative ranks as extreme or more extreme than \\(91\\) is \\(p=0.01476\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would reject the null hypothesis that the difference in the center of the distributions is zero. We conclude that the Spray Concentration A is more effective at killing flies (bugs). Comment The hypotheses for the Wilcoxon Rank Sum Test are difficult to write out in simple mathematical statements. The test is often referred to as a test of medians, but goes deeper than this. Technically, it allows us to determine if one distribution is stochastically larger than another. In other words, if one distribution typically gives larger values than does another distribution. If the distributions are identically shaped and have the same spread, then this implies the medians (and means) are different. Thus, the hypotheses for the test can be written mathematically as \\[ H_0: \\text{difference in medians} = 0 \\] \\[ H_a: \\text{difference in medians} \\neq 0 \\] or even as \\[ H_0: \\mu_1-\\mu_2 = 0 \\] \\[ H_a: \\mu_1-\\mu_2 \\neq 0 \\] However, it is important to remember that the estimated difference in location parameters that results from the test does not provide a measurement on either of these things. However, the \\(p\\)-value can lead us to determine whether to reject, or fail to reject the null, whichever of the above hypotheses is used. As stated in the R help file for this test ?wilcox.test() “the estimator for the difference in location parameters does not estimate the difference in medians (a common misconception) but rather the median of the difference between a sample from [the first population] and a sample from [the second population].” These are technical details that most people ignore without encountering too much difficulty. However, it does remind us that the t test is more easily interpreted whenever it is appropriate to use that test. Final Comment Notice that when the sample size is large the distribution of the test statistic can be approximated by a normal distribution. Most software applications use this approximation when the sample size is over \\(50\\), i.e., for \\(n\\geq50\\) because computing all the possible sums of ranks becomes incredibly time consuming. Thus, for large sample sizes the results will be almost identical whether the Wilcoxon Tests or a t Test is used.",
    "sentences": ["Independent Samples t Test The independent samples t test is used when a value is hypothesized for the difference between two (possibly) different population means, \\(\\mu_1 - \\mu_2\\).", "finds the mean of each data set, then subtracts those means for their difference two groups don’t depend on each other’s results Overview Questions The Independent Samples t Test can be used to answer questions like: Are boys taller than girls on average?", "Do students who show up to class everyday get higher scores on average than those who don’t?", "Do you take more steps on average on weekdays or on weekends?", "Requirements The test is only appropriate when both of the following are satisfied.", "Both samples are representative of the population.", "(Simple random samples are the best way to do this.) The sampling distribution of the difference of the sample means \\((\\bar{x}_1 - \\bar{x}_2)\\) can be assumed to be normal.", "This is a safe assumption when the sample size of each group is \\(30\\) or greater or when the population data from each group can be assumed to be normal with a Q-Q Plot.", "Hypotheses Math Code $$ H_0: \\mu_\\text{label 1} - \\mu_\\text{label 2} = 13.2 $$ $$ H_a: \\mu_\\text{label 1} - \\mu_\\text{label 2} \\neq 13.2 $$ \\(H_0: \\mu_1 - \\mu_2 = \\text{some number, but typically 0}\\) \\(H_a: \\mu_1 - \\mu_2 \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepInd student1 student2 R Instructions Console Help Command: ?t.test() There are two ways to perform the test.", "Option 1: t.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a “numeric” vector from YourData that represents the data for both samples."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 88,
    "title": "📍 Quantitative Y | Categorical X (3+\r\nGroups)",
    "url": "index.html#quantitativeycategoricalx3groups",
    "content": "Quantitative Y | Categorical X (3+\r\nGroups) Graphics Y is a single quantitative variable of interest, like “heights” of BYU-Idaho students. X is a categorical (qualitative) variable like which Math 221 you took, 221A, 221B, or 221C. In other words, X has three or more groups. So “Classrank” could be X, with groups “Freshman”, “Sophomore”, “Junior”, and “Senior”. Test(s) used for this category: One-Way ANOVA (means) Block Design Kruskal-Wallis Rank Sum Test (distributions) Analysis : College Student’s Food Perception Used comparing several groups based on means or distributions: How long are their feet based of width and gender? Are there certain months of the year that are associated with children having longer feet, on average, than others? Does a college student’s perception of food result in a change in their weight? (Kruskal-Wallis Rank Sum Test) # Read data food <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv\") # Data transformation and cleaning food_cleaned <- food %>% # Convert weight to numeric right away mutate(weight = as.numeric(weight)) %>% # Remove rows with NA weight or missing drink values filter(!is.na(weight) & !is.na(drink)) %>% # Create a new column `Food Perception Score` mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) # Create plots boxplot(weight ~ `Food Perception Score`, data=food_cleaned, col=\"lightgreen\", xlab=\"Food Perception Score\", main=\"Weight of College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=food_cleaned, pch=16, vertical=TRUE, add=TRUE, col=\"palegreen4\") Tests One-way ANOVA Each experimental unit is assigned to exactly one factor-level combination. Another way to say this is “one measurement per individual” (no repeated measures) and “equal numbers of individuals per group”. Overview An ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA can be assumed to be normally distributed when seen in a Q-Q Plot.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out in a Residuals versus fitted-values plot.) comparing means Hypotheses For a One-way ANOVA \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] Mathematical Model A typical model for a one-way ANOVA is of the form \\[ Y_{ij} = \\mu_i + \\epsilon_{ij} \\] where \\(\\mu_i\\) is the mean for level (group) \\(i\\), and \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) is the error term for each point \\(j\\) within level (group) \\(i\\). R Instructions Console Help Command: ?aov() myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. X is a qualitative variable (should have class(X) equal to factor or character. If it does not, use as.factor(X) inside the aov(Y ~ as.factor(X),...) command. YourDataSet is the name of your data set. Perform the ANOVA myaov <- aov(Y ~ X, data=YourDataSet) summary(myaov) Diagnose ANOVA Assumptions par(mfrow=c(1,2)) plot(myaov, which=1:2) Example Code Hover your mouse over the example codes to learn more. Perform the ANOVA chick.aov <-  Saves the results of the ANOVA test as an object named ‘chick.aov’. aov( ‘aov()’ is a function in R used to perform the ANOVA. weight Y is ‘weight’, which is a numeric variable from the chickwts dataset.  ~  ‘~’ is the tilde symbol used to separate the Y and X in a model formula. feed, X is ‘feed’, which is a qualitative variable in the chickwts dataset, or more specifically, a factor with six levels: “casein”, “horsebean”, and so on… Use str(chickwts) to see this.  data = chickwts) ‘chickwts’ is a dataset in R. summary( ‘summary()’ shows the results of the ANOVA. chick.aov) ‘chick.aov’ is the name of the ANOVA.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## feed 5 231129 46226 15.37 5.94e-10 ## Residuals 65 195556 3009 ## ## feed *** ## Residuals ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Diagnose the ANOVA par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow = c(1,2)) The mfrow parameter controls “multiple frames on a row”. In this case, the c(1,2) specifies 1 row of 2 plots. This will cause the two diagnostic plots to be placed side-by-side. plot( ‘plot’ is a R function for the plotting of R objects. chick.aov, ‘chick.aov’ is the name of the ANOVA.  which = 1:2) The which=1:2 selects “which” of 6 available plots we want to have graphed. In this case, 1 shows the Residuals vs Fitted, and 2 shows the Normal QQ-plot. Both are needed to check the ANOVA assumptions.  Click to View Output  Click to View Output. Explanation Analysis of variance (ANOVA) is often applied to the scenario of testing for the equality of three or more means from (possibly) separate normal distributions of data. The normality assumption is required. No matter the sample size. If the distributions are skewed then a nonparametric test should be applied instead of ANOVA. One-Way ANOVA One-way ANOVA is when a completely randomized design is used with a single factor of interest. A typical mathematical model for a one-way ANOVA is of the form \\[ Y_{ik} = \\mu_i + \\epsilon_{ik} \\quad (\\text{sometimes written}\\ Y_{ik} = \\mu + \\alpha_i + \\epsilon_{ik}) \\] where \\(\\mu_i\\) is the mean of each group (or level) \\(i\\) of a factor, and \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) is the error term. The plot below demonstrates what these symbols represent. Note that the notation \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) states that we are assuming the error term \\(\\epsilon_{ik}\\) is normally distributed with a mean of 0 and a standard deviation of \\(\\sigma\\). Hypotheses The aim of ANOVA is to determine which hypothesis is more plausible, that the means of the different distributions are all equal (the null), or that at least one group mean differs (the alternative). Mathematically, \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_m = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\quad \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\}. \\] In other words, the goal is to determine if it is more plausible that each of the \\(m\\) different samples (where each sample is of size \\(n\\)) came from the same normal distribution (this is what the null hypothesis claims) or that at least one of the samples (and possibly several or all) come from different normal distributions (this is what the alternative hypothesis claims). Visualizing the Hypotheses The first figure below demonstrates what a given scenario might look like when all \\(m=3\\) samples of data are from the same normal distribution. In this case, the null hypothesis \\(H_0\\) is true. Notice that the variability of the sample means is smaller than the variability of the points. The figure below shows what a given scenario might look like for \\(m=3\\) samples of data from three different normal distributions. In this case, the alternative hypothesis \\(H_a\\) is true. Notice that the variability of the sample means, i.e., \\((\\bar{x}_1,\\bar{x}_2,\\bar{x}_3)\\), is greater than the variability of the points. Explaining the Name The above plots are useful in understanding the mathematical details behind ANOVA and why it is called analysis of variance. Recall that variance is a measure of the spread of data. When data is very spread out, the variance is large. When the data is close together, the variance is small. ANOVA utilizes two important variances, the between groups variance and the within groups variance. Between groups variance–a measure of the variability in the sample means, the \\(\\bar{x}\\)’s. Within groups variance–a combined measure of the variability of the points within each sample. The plot below combines the information from the previous plots for ease of reference. It emphasizes the fact that when the null hypothesis is true, the points should have a large variance (be really spread out) while the sample means are relatively close together. On the other hand, when the points are relative close together within each sample and the sample means have a large variance (are really spread out) then the alternative hypothesis is true. This is the theory behind analysis of variance, or ANOVA. Calculating the Test Statistic, \\(F\\) The ratio of the “between groups variation” to the “within groups variation” provides the test statistic for ANOVA. Note that the test statistic of ANOVA is an \\(F\\) statistic. \\[ F = \\frac{\\text{Between groups variation}}{\\text{Within groups variation}} \\] It would be good to take a minute and review the \\(F\\) distribution. The \\(p\\)-value for ANOVA thus comes from an \\(F\\) distribution with parameters \\(p_1 = m-1\\) and \\(p_2 = n-m\\) where \\(m\\) is the number of samples and \\(n\\) is the total number of data points. A Deeper Look at Variance It is useful to take a few minutes and explain the word variance as well as mathematically define the terms “within group variance” and “between groups variance.” Variance is a statistical measure of the variability in data. The square root of the variance is called the standard deviation and is by far the more typical measure of spread. This is because standard deviation is easier to interpret. However, mathematically speaking, the variance is the more important measurement. As mentioned previously, the variance turns out to be the key to determining which hypothesis is the most plausible, \\(H_0\\) or \\(H_a\\), when several means are under consideration. There are two variances that are important for ANOVA, the “within groups variance” and the “between groups variance.” Recall that the formula for computing a sample variance is given by \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})^2}{n-1} \\quad\\leftarrow \\frac{\\text{sum of squares}}{\\text{degrees of freedom}} \\] This formula has a couple of important pieces that are so important they have been given special names. The \\(n-1\\) in the denominator of the formula is called the “degrees of freedom.” The other important part of this formula is the \\(\\sum_{i=1}^n(x_i - \\bar{x})^2\\), which is called the “sum of squared errors” or sometimes just the “sum of squares” or “SS” for short. Thus, the sample variance is calculated by computing a “sum of squares” and dividing this by the “degrees of freedom.” It turns out that this general approach works for many different contexts. Specifically, it allows us to compute the “within groups variance” and the “between groups variance.” To introduce the mathematical definitions of these two variances, we need to introduce some new notation. Let \\(\\bar{y}_{i\\bullet}\\) represent the sample mean of group \\(i\\) for \\(i=1,\\ldots,m\\). Let \\(n_i\\) denote the sample size in group \\(i\\). Let \\(\\bar{y}_{\\bullet\\bullet}\\) represent the sample mean of all \\(n = n_1+n_2+\\cdots+n_m\\) data points. The mathematical calculations for each of these variances is given as follows. \\[ \\text{Between groups variance} = \\frac{\\sum_{i=1}^m (\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2}{m-1} \\leftarrow \\frac{\\text{Between groups sum of squares}}{\\text{Between groups degrees of freedom}} \\] \\[ \\text{Within groups variance} = \\frac{\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2}{n-m} \\leftarrow \\frac{\\text{Within groups sum of squares}}{\\text{Within groups degrees of freedom}} \\] A Fabricated Example The following table provides three samples of data: A, B, and C. These samples were randomly generated from normal distributions using a computer. The true means \\(\\mu_1, \\mu_2\\), and \\(\\mu_3\\) of the normal distributions are thus known, but withheld from you at this point of the example. A B C 13.15457 13.17463 16.66831 12.65225 12.16277 15.54719 13.73061 12.76905 16.63074 14.43471 13.38524 15.06726 13.79728 12.02690 15.57534 13.88599 13.24651 15.99915 12.77753 12.58386 15.58995 13.81536 12.64615 16.99429 13.03635 12.52055 15.47153 14.26062 14.03566 16.13330 An ANOVA will be performed with the sample data to determine which hypothesis is more plausible: \\[ H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\in \\{1,\\ldots,m\\} \\] To perform an ANOVA, we must compute the between groups variance and the within groups variance. This requires the Between groups sums of squares, within groups sums of squares, between groups degrees of freedom, and the within groups degrees of freedom. Note that to get the sums of squares, we first had to calculate \\(\\bar{y}_{1\\bullet}\\), \\(\\bar{y}_{2\\bullet}\\), \\(\\bar{y}_{3\\bullet}\\), and \\(\\bar{y}_{\\bullet\\bullet}\\) where the 1, 2, 3 corresponds to Samples A, B, and C, respectively. After some work, we find these values to be \\[ \\bar{y}_{1\\bullet} = 13.55, \\quad \\bar{y}_{2\\bullet} = 12.86 \\quad \\bar{y}_{3\\bullet} = 15.97 \\] and \\[ \\bar{y}_{\\bullet\\bullet} = \\frac{13.55+12.86+15.97}{3} = 14.13 \\] Using these values we can then compute the between groups sum of squares and the within groups sum of squares according to the formulas stated previously. This process is very tedious and will not be demonstrated. Only the results are shown in the following table which summarizes all the important information.   Degrees of Freedom Sum of Squares Variance F-value p-value Between groups 2 53.3 26.67 70.2 2e-11 Within groups 27 10.3 0.38 ANOVA Table In general, the ANOVA table is created by   Degrees of Freedom Sum of Squares Variance F-value p-value Between groups \\(m-1\\) \\(\\sum_{i=1}^m n_i(\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) \\(\\frac{\\text{Between groups variance}}{\\text{Within groups variance}}\\) \\(F\\)-distribution tail probability Within groups \\(n-m\\) \\(\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) ANOVA Assumptions{#residuals} The requirements for an analysis of variance (the assumptions of the test) are two-fold and concern only the error terms, the \\(\\epsilon_{ik}\\). The errors are normally distributed. The variance of the errors is constant. Both of these assumptions were stated in the mathematical model where we assumed that \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\). Checking ANOVA Assumptions To check that the ANOVA assumptions are satisfied, it is required to check the data in each group for normality using QQ-Plots. Also, the sample variance of each group must be relatively constant. The fastest way to check these two assumptions is by analyzing the residuals. An ANOVA residual is defined as the difference between the observed value of \\(y_{ik}\\) and the mean \\(\\bar{y}_{i\\bullet}\\). Mathematically, \\[ r_{ik} = y_{ik} - \\bar{y}_{i\\bullet} \\] One QQ-Plot of the residuals will provide the necessary evidence to decide if it is reasonable to assume that the error terms are normally distributed. Also, the constant variance can be checked visually by using what is known as a residuals versus fitted values plot. For the Fabricated Example above, the QQ-Plot and residuals versus fitted values plots show the two assumptions of ANOVA appear to be satisfied. Examples: chickwts (One-way) Block Design Repeated measures or other factors that group individuals into similar groups (blocks) are included in the study design. Overview A typical model for a block design is of the form \\[ Y_{lijk} = \\mu + B_l + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijlk} \\] where \\(\\mu\\) is the grand mean, \\(B_l\\) is the blocking factor, \\(\\alpha_i\\) is one factor with at least two levels, \\(\\beta_j\\) is another factor with at least two levels, \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors, and \\(\\epsilon_{ijlk} \\sim N(0,\\sigma^2)\\) is the error term. Only one block and one factor is required. Multiple blocks and multiple factors are allowed. It is not required to include interaction terms. The error term is always required. R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ Block+X1+X2+X1:X2, data=YourDataSet) Perform the test summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. Block is a qualitative variable that is not of direct interest, but is included in the model to account for variability in the data. It should have class(Block) equal to either factor or character. Use as.factor() if it does not. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. If it does not, use as.factor(X2). Note that factors C, D, and so on could also be added + to the model if desired. X1:X2 denotes the interaction of the factors X1 and X2. It is not required and should only be included if the interaction term is of interest. YourDataSet is the name of your data set. Examples: ChickWeight Kruskal-Wallis Rank Sum Test Allows for deciding if several samples come from the same population or if at least one sample comes from a different population. Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated. Can also be used when data is ordered (ordinal) but does not have an exact measurement. Best used when the distribution of the data is not normal. ANOVA is appropriate when the data is normal. comparing distributions Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, “is a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly. In many cases, this is practically equivalent to the mean of at least one population differing from the others.” Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable. g is a qualitative grouping variable defining which groups each value in x belongs to. It must either be a character vector or a factor vector. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. weight ‘weight’ is a numeric variable from the chickwts dataset that represents the quantatitive response variable.  ~  ‘~’ is the tilde symbol used to separate the left- and right-hand side in a model formula. feed, ‘feed’ is a qualitative grouping variable in the chickwts dataset.  data = chickwts) ‘chickwts’ is a dataset in R.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## ## Kruskal-Wallis rank sum test ## ## data: weight by feed ## Kruskal-Wallis chi-squared = 37.343, df = 5, ## p-value = 5.113e-07 Alternatively, you could use the following approach as well. kruskal.test(list(x, y, z)) x is a numeric vector of data values that represents the quantatitive response variable for the first group. y is a numeric vector of data values that represents the quantatitive response variable for the second group. z is a numeric vector of data values that represents the quantatitive response variable for the third group. Note that more than three vectors of data could be included inside of list(). Example Code Hover your mouse over the example codes to learn more. feed1 <- filter(chickwts, feed == “horsebean”) This splits out the first group of feed (horsebean) from the chickwts dataset. feed2 <- filter(chickwts, feed == “linseed”) This splits out the second group of feed (linseed) from the chickwts dataset. feed3 <- filter(chickwts, feed == “soybean”) This splits out the third group of feed (soybean) from the chickwts dataset. feed4 <- filter(chickwts, feed == “sunflower”) This splits out the fourth group of feed (sunflower) from the chickwts dataset. feed5 <- filter(chickwts, feed == “meatmeal”) This splits out the fifth group of feed (meatmeal) from the chickwts dataset. feed6 <- filter(chickwts, feed == “casein”) This splits out the sixth group of feed (casein) from the chickwts dataset. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. list(feed1$weight,feed2$weight,feed3$weight,feed4$weight,feed5$weight,feed6$weight)) A list of numeric vecotrs that represent the quantitative response variable for each group.  Click to View Output  Click to View Output. Load library(tidyverse) to run this code in R. ## ## Kruskal-Wallis rank sum test ## ## data: list(feed1$weight, feed2$weight, feed3$weight, feed4$weight, feed5$weight, feed6$weight) ## Kruskal-Wallis chi-squared = 37.343, df = 5, ## p-value = 5.113e-07 Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population. The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples. The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity. Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated. (Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\). Note that the notation \\(n_i\\) allows for each sample to be a different size. In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\). Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest. Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\). Award any tied values the average of the ranks of those values that are tied. In the bottle-cap data we have the following. Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample. Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\). In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\). In the bottle-cap data, \\(\\bar{R}_1 = \\frac{R_1}{n_1} = \\frac{24}{5} = 4.8\\) \\(\\bar{R}_2 = \\frac{R_2}{n_2} = \\frac{14}{3} = 4.667\\) \\(\\bar{R}_3 = \\frac{R_3}{n_3} = \\frac{40}{4} = 10.0\\) Step 5: If there are no ties, calculate the test statistic \\(H\\) by the formula \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] See the original article for the formula to use when there are ties present in the data. In the bottle-cap data, \\(H = 5.656\\). To understand the logic behind the test statistic \\(H\\), note that the null hypothesis assumes that each of the \\(C\\) samples are taken from the same population. Under this assumption, the ranks assigned to each sample should represent a uniform sample of the ranks \\(1,\\ldots,N\\). If the null hypothesis were true, we would expect the average of the ranks from each group, the \\(\\bar{R}_i\\), to be roughly about the same. More precisely, using the properties of the discrete uniform distribution (see wikipedia), we expect each \\(\\bar{R}_i\\) to be close to \\(\\frac{1}{2}(N+1)\\), which is the mean of the discrete uniform distribution. That \\(\\bar{R}_i\\) should be close to this value comes from the fact that the mean of a distribution of means is equal to the population mean. Further, the variance of \\(\\bar{R}_i\\) is given by the population variance, \\(\\frac{1}{12}(N^2-1)\\) in this case, divided by the sample size \\(n_i\\). Finally, the \\(\\frac{N-1}{N}\\) is a correction factor that will not be explained here. The full article explains the details about how a multivariate normal distribution is used to uncover the formula for \\(H\\). Thus, understanding \\(H\\) completely will require an advanced degree in statistical theory. However, the short story is that the distribution of the test statistic \\(H\\) is approximately a chi squared distribution with \\(C-1\\) degrees of freedom and has the form of a sum of values consisting of the (observed mean - expected mean)^2/(variance of the observed mean). Step 6: Calculate the p-value of \\(H\\) using a chi squared distribution with \\(C-1\\) degrees of freedom. \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] In the bottle-cap data, \\(p = 0.059\\), which represents the probability of \\(H\\) being as extreme or more extreme than the observed value of 5.656, assuming the null hypothesis is true. Examples: wages, pigweights Quantitative Y | Multiple Categorical X Graphics Y is a single quantitative variable of interest, like “heights” of BYU-Idaho students. X1 is a categorical (qualitative) variable like gender, with levels “boy” and “girl.” X2 is another categorical (qualitative) variable like “Classrank” with levels “Freshman”, “Sophomore”, and “Junior”. Of course, both X1 and X2 can have as many levels as you want to work with, but must have at least two levels each. Test(s) used for this category: Two-Way ANOVA (means) Used to answer THREE questions: the effects of factor 1 on the quantitative variable the effects of factor 2 on the quantitative variable the interaction between the two factors best paired with numerical summary looking at mean Analysis : Two-way ANOVA: Final Beginning Algebra Grades Question Examples: Based on your p-values and these graphs, which of the following is a correct conclusion to reach? xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1))) ANSWER : Among fourth graders, right handed boys have longer feet on average than left handed boys while the opposite is true for girls. Does the amount a student participates in the course affect the average final Beginning Algebra grades? Does the semester in which a student takes Beginning Algebra affect their average final grade? Does a student’s participation level vary depending on the semester? (Alternatively, does the semester influence a student’s participation?) BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") BAFinalGrades$Semester <- factor(BAFinalGrades$Semester, levels = c(\"Fall\",\"Winter\",\"Spring\")) ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=1)) + geom_point(color=\"firebrick1\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Spring Semester Seems to Have Higher Final Grades\", x=\"Semester\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades$Participation <- factor(BAFinalGrades$Participation, levels = c(\"Low\",\"Moderate\",\"High\")) ggplot(BAFinalGrades, aes(x=Participation, y=FinalGrade, group=1)) + geom_point(color=\"darkred\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Participation Seems to Lead to Higher Grades\", x=\"Participation\", y=\"Beginning Algebra Final Grades\") + theme_minimal() ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=Participation, color=Participation)) + geom_point() + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Beginning Algebra Seems Better during the Spring Semester\", x=\"BYU-Idaho Semesters\", y=\"Beginning Algebra Final Grades\") + theme_minimal() Tests Two-way ANOVA Overview A two-way ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses. Writing out the hypotheses can be very involved depending on whether you use the official “effects model” notation (very mathematically correct) or a simplified “means model” notation (which isn’t very mathematically correct, but gets the idea across in an acceptable way). Means Model Effects Model The first set of hypotheses are a “one-way” set of hypotheses for the first factor of the ANOVA. Factor: X1 with say, levels \\(A\\) and \\(B\\). \\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a “one-way” set of hypotheses, but for the second factor of the ANOVA. Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\mu_C = \\mu_D = \\mu_E = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one}\\ i\\in\\{1=C,2=D,3=E\\} \\] The third set of hypotheses are the most interesting hypotheses in a two-way ANOVA. These are called the interaction hypotheses. They test to see if the levels of one of the factors, say \\(X1\\), impact \\(Y\\) differently for the differing levels of the other factor, \\(X2\\). The hypotheses read formally as \\[ H_0: \\text{The effect of the first factor on Y} \\\\ \\text{is the same for all levels of the second factor.} \\] \\[ H_a: \\text{The effect of the first factor on Y is not the same} \\\\ \\text{for all levels of the second factor.} \\] A mathematically correct way to state the two-way ANOVA model is with the equation \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] In this model, \\(\\mu\\) is the grand mean (which is the average Y-value ignoring all information contained in the factors); \\(\\alpha_i\\) is the first factor \\(X1\\) with levels \\(A\\) and \\(B\\) (though there could be more levels in \\(X1\\) depending on your data); \\(\\beta_j\\) is the second factor with levels \\(C\\), \\(D\\), and \\(E\\) (though there could be fewer or more levels to this factor depending on your data); \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors which has \\(2\\times3=6\\) (may differ for your data) levels; and \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) is the normally distributed error term for each point \\(k\\) found within level \\(i\\) of \\(X1\\) and level \\(j\\) of \\(X2\\). This model allows us to more formally state the hypotheses as First factor \\(X1\\) having say, levels \\(A\\) and \\(B\\). \\[ H_0: \\alpha_A = \\alpha_B = 0 \\] \\[ H_a: \\alpha_i \\neq 0 \\ \\text{for at least one}\\ i\\in\\{1=A,2=B\\} \\] Second factor \\(X2\\) with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\beta_C = \\beta_D = \\beta_E = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j\\in\\{1=C,2=D,3=E\\} \\] Does the effect of the first factor (\\(X1\\)) change for the different levels of the second factor (\\(X2\\))? In other words, is there an interaction between the two factors \\(X1\\) and \\(X2\\)? \\[ H_0: \\alpha\\beta_{ij} = 0 \\ \\text{for all } i,j \\] \\[ H_a: \\alpha\\beta_{ij} \\neq 0 \\ \\text{for at least one } i,j \\] R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ X1+X2+X1:X2, data=YourDataSet) Perform the ANOVA summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. Note that factors X3, X4, and so on could also be added + to the model if desired, but this would create a three-way, or four-way ANOVA model, and so on. X1:X2 denotes the interaction of the factors X1 and X2. It is not required, but is usually included. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. warp.aov <-  Saves the results of the ANOVA test as an object named ‘warp.aov’. aov( ‘aov()’ is a function in R used to perform the ANOVA. breaks \\(Y\\) is ‘breaks’, which is a numeric variable from the warpbreaks dataset.  ~  ‘~’ is the tilde symbol used to separate the left- and right-hand side in a model formula. wool  The first factor \\(X1\\) is ‘wool’, which is a qualitative variable in the warpbreaks dataset. In this case, wool is a factor with two levels. Use str(warpbreaks) to see this. + tension  The second factor \\(X2\\) is ‘tension’, which is another qualitative variable in the warpbreaks dataset. In this case, tension is a factor with three levels. Use str(warpbreaks) to see this. + wool:tension, The interaction of the two factors: wool and tension.  data = warpbreaks) ‘warpbreaks’ is a dataset in R. summary( ‘summary()’ shows the results of the ANOVA. warp.aov) ‘warp.aov’ is the name of the ANOVA.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## wool 1 451 450.7 3.765 0.058213 ## tension 2 2034 1017.1 8.498 0.000693 ## wool:tension 2 1003 501.4 4.189 0.021044 ## Residuals 48 5745 119.7 ## ## wool . ## tension *** ## wool:tension * ## Residuals ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow = c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. plot( ‘plot’ is a R function for the plotting of R objects. warp.aov, ‘warp.aov’ is the name of the ANOVA.  which = 1:2) Will show the Residuals vs Fitted and the Normal QQ-plot to check the ANOVA assumptions.  Click to View Output  Click to View Output. Explanation Hypotheses in Two-way ANOVA The hypotheses that can be tested in a two-way ANOVA that includes an interaction term are three-fold. Hypotheses about \\(\\alpha\\) where \\(\\alpha\\) has \\(m\\) levels. \\[ H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_m = 0 \\] \\[ H_a: \\alpha_i \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\} \\] Hypotheses about \\(\\beta\\) where \\(\\beta\\) has \\(q\\) levels. \\[ H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_q = 0 \\] \\[ H_a: \\beta_j \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,q\\} \\] Hypotheses about the interaction term \\(\\alpha\\beta\\). \\[ H_0: \\text{the effect of one factor is the same across all levels of the other factor} \\] \\[ H_a: \\text{the effect of one factor differs for at least one level of the other factor} \\] Expanding the ANOVA Model{#expanding} It turns out that more can be done with ANOVA than simply checking to see if the means of several groups differ. Reconsider the mathematical model of two-way ANOVA. \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] This model could be expanded to include any number of new terms in the model. The power of this approach is in the several questions (hypotheses) that can be posed to data simultaneously. What happens if your ANOVA fails both requirement tests? Example: BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") ## Rows: 204 Columns: 3 ## ── Column specification ────────────────────────── ## Delimiter: \",\" ## chr (2): Semester, Participation ## dbl (1): FinalGrade ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. BAanova <- aov(FinalGrade ~ Semester + Participation + Semester:Participation, data=BAFinalGrades) par(mfrow=c(1,2)) plot(BAanova, which=1:2, pch=16) The residual plot shows that the constant (equal) variance assumption raises some concern as the points within each group have varying spread. The spread of each group seems to gets closer and closer as we go from left to right as well as some straying point from the more clustered groups. Thus, showing the variance of the residuals are not constant and do not accurately show a relationship between each variable. For the Q-Q plot, it shows that the residuals of the data is not quite normally distributed. There are visible points at the lower and upper ends of the graph deviating away form the diagonal line, which represents a normal distribution. Thus, showing us that the residuals of the data are not normal distributed. Although both graphs indicate that the data fails to meet both requirements, this primarily means our findings may be less reliable due to the abnormalities in the data. As a result, we should interpret the results cautiously as we proceed with our Two-Way ANOVA test! Examples: warpbreaks (Two-way), CO2 (three-way) Quantitative Y | Quantitative X Graphics Y is a single quantitative variable of interest, like “height”. X is another single quantitative variable of interest, like “shoe-size”. This would imply we are using “shoe-size” (X) to explain “height” (Y). Test(s) used for this category: Simple Linear Regression Analysis : Simple Linear Regression: BYU-Idaho Math Tutoring Used to find the relationship between one independent variable (predictor) and one dependent variable(outcome): Is there a relationship between the length of children’s feet and the width of their foot? Example: Does the amount of tutoring a student receives correlate with their chapter exam scores? (Simple Linear Regression) ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams) Tests Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\). Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$\\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$$ \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is “equal to” the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to “God’s Law” or “Natural Law”. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are “random” and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)… \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced “y-hat”, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what follows… \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, “the change in the average y-value for a one unit change in the x-value.” It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, “the average y-value when x is zero.” It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful. R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command.  <-  This is the “left arrow” assignment operator that stores the results of your lm() code into mylm name. lm( lm(…) is an R function that stands for “Linear Model”. It performs a linear regression analysis for Y ~ X. Y  Y is your quantitative response variable. It is the name of one of the columns in your data set. ~  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X. X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value.  data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X. ) Closing parenthesis for the lm(…) function. summary(mylm) The summary command allows you to print the results of your linear regression that were previously saved in mylm name.  Click to Show Output  Click to View Output. Example output from a regression. Hover each piece to learn more. Call: lm(formula = dist ~ speed, data = cars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -29.069 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -9.525 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -2.272 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -2.272 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-2.272) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   9.215 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   43.201 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -17.5791 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   6.7584 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -2.601 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (-17.5791) by its standard error (6.7584). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of -17.5791 is -2.601 standard errors (6.7584) from zero, which is a fairly surprising distance as shown by the p-value. 0.0123 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). speed This is always the name of your X-variable in your lm(Y ~ X, …).   3.9324 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.4155 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 9.464 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” of the slope (3.9324) by its standard error (0.4155). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from zero, which is a really surprising distance as shown by the smallness of the p-value. 1.49e-12 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  15.38 For this particular regression, the estimate of \\(\\sigma\\) is 15.38. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 48 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 50 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 50-2 = 48. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.6511, In this particular regression, 65.11% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data.  0.6438 In this case, the value of 0.6438 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is found as the ratio of the MSR/MSE where MSR = SSR/(p-1) and MSE = SSE/(n-p) where n is the sample size and p is the number of parameters in the regression model.  89.57 This is the value of the F-statistic for the lm(dist ~ speed, data=cars) regression. Note that SSE = sum( cars.lm$res^2 ) = 11353.52 with n - p = 50 - 2 = 48 degrees of freedom for this data. Further, SSR = sum( (cars.lm$fit - mean(cars$dist))^2 ) = 21185.46 with p - 1 = 1 degree of freedom. So MSR = 21185.46 and MSE = 11353.52 / 48 = 236.5317. So MSR / MSE = 21185.46 / 236.5317 = 89.56711.  on 1 and 48 DF, The 1 degree of freedom is the SSR degrees of freedom (p-1). The 48 is the SSE degrees of freedom (n-p).  p-value: 1.49e-12 The p-value for an F-statistic is found by the code pf(89.56711, 1, 48, lower.tail=FALSE), which gives the probability of being more extreme than the observed F-statistic in an F distribution with 1 and 48 degrees of freedom. Check Assumptions 1, 2, 3, and 5 When making it in an r chunk make your height 3! { r, fig.height = 3 } par( The par(…) command stands for “Graphical PARameters”. It allows you to control various aspects of graphics in Base R. mfrow= This stands for “multiple frames filled by row”, which means, put lots of plots on the same row, starting with the plot on the left, then working towards the right as more plots are created. c( The combine function c(…) is used to specify how many rows and columns of graphics should be placed together. 1, This specifies that 1 row of graphics should be produced. 3 This states that 3 columns of graphics should be produced. ) Closing parenthesis for c(…) function. ) Closing parenthesis for par(…) function. plot( This version of plot(…) will actually create several regression diagnostic plots by default. mylm, This is the name of an lm object that you created previously. which= This allows you to select “which” regression diagnostic plots should be drawn. 1 Selecting 1, would give the residuals vs. fitted values plot only. : The colon allows you to select more than just one plot. 2 Selecting 2 also gives the Q-Q Plot of residuals. If you wanted to instead you could just use which=1 to get the residuals vs fitted values plot, then you could use qqPlot(mylm$residuals) to create a fancier Q-Q Plot of the residuals. ) Closing parenthesis for plot(…) function. plot( This version of plot(…) will be used to create a time-ordered plot of the residuals. The order of the residuals is the original order of the x-values in the original data set. If the original data set doesn’t have an order, then this plot is not interesting. mylm The lm object that you created previously. $ This allows you to access various elements from the regression that was performed. residuals This grabs the residuals for each observation in the regression. ) Closing parenthesis for plot(…) function.  Click to Show Output  Click to View Output. Which regression assumption(s) does the residuals versus fitted values plot diagnose? Linear Relation Constance Variance Which regression assumption(s) does the Q-Q Plot of the residuals diagnose? Normal Errors Which regression assumption(s) does the Residuals versus Order plot diagnose? (Remember, this plot can only be created when the data was collected in a specific order.) Independent Errors Plotting the Regression Line Base R ggplot2 To add the regression line to a scatterplot use the abline(...) command: plot( The plot(…) function is used to create a scatterplot with a y-axis (the vertical axis) and an x-axis (the horizontal axis). Y  This is the “response variable” of your regression. The thing you are interested in predicting. This is the name of a “numeric” column of data from the data set called YourDataSet. ~  The tilde “~” is used to relate Y to X and can be found on the top-left key of your keyboard. X,  This is the explanatory variable of your regression. It is the name of a “numeric” column of data from YourDataSet. data= The data= statement is used to specify the name of the data set where the columns of “X” and “Y” are located. YourDataSet This is the name of your data set, like KidsFeet or cars or airquality. ) Closing parenthesis for plot(…) function. abline( This stands for “a” (intercept) “b” (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm This is the name of an lm(…) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(…) function will locate these two values from within mylm and use them to add a line to your current plot(…). ) Closing parenthesis for abline(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm) You can customize the look of the regression line with abline( This stands for “a” (intercept) “b” (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm, This is the name of an lm(…) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(…) function will locate these two values from within mylm and use them to add a line to your current plot(…). lty= The lty= stands for “line type” and allows you to select between 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. 1, This creates a solid line. Remember, other options include: 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. lwd= The lwd= allows you to specify the width of the line. The default width is 1. Using lwd=2 would double the thickness, and so on. Any positive value is allowed. 1, Default line width. To make a thicker line, us 2 or 3… To make a thinner line, try 0.5, but 1 is already pretty thin. col= This allows you to specify the color of the line using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). “someColor” Type colors() in R for options. ) Closing parenthesis for abline(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm, lty=1, lwd=1, col=\"firebrick\") You can add points to the regression with… points( This is like plot(…) but adds points to the current plot(…) instead of creating a new plot. newY  newY should be a column of values from some data set. Or, use points(newX, newY) to add a single point to a graph. ~  This links Y to X in the plot. newX,  newX should be a column of values from some data set. It should be the same length as newY. If just a single value, use points(newX, newY) instead. data=YourDataSet,  If newY and newX come from a dataset, then use data= to tell the points(…) function what data set they come from. If newY and newX are just single values, then data= is not needed. col=“skyblue”, This allows you to specify the color of the points using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). pch=16 This allows you to specify the type of plotting symbol to be used for the points. Type ?pch and scroll half way down in the help file that appears to learn about other possible symbols. ) Closing parenthesis for points(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) points(7,40, pch=16, col=\"skyblue\", cex=2) text(7,40, \"New Dot\", pos=3, cex=0.5) points(dist ~ speed, data=filter(cars, mylm$res > 2), cex=.8, col=\"red\") abline(mylm, lty=1, lwd=1, col=\"firebrick\") To add the regression line to a scatterplot using the ggplot2 approach, first ensure: library(ggplot2) or library(tidyverse) is loaded. Then, use the geom_smooth(method = lm) command: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. YourDataSet,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. X, This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet.  y =  “y= ” declares which variable will become the y-axis of the graphic. Y This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function. + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. + Here the + is used to add yet another layer to ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. There are several other methods that could be used here.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(…) to place more complicated models on the plot.  se = FALSE se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. ) Closing parenthesis for the geom_smooth() function.  Click to Show Output  Click to View Output. ggplot(cars, aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = \"lm\", formula=y~x, se=FALSE) There are a number of ways to customize the appearance of the regression line: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. speed,  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet. y =  “y= ” declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic.  + Here the + is used to add yet another layer to ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(…) to place more complicated models on the plot.  se = FALSE, se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.  size = 2, Use size = 2 to adjust the thickness of the line to size 2.  color = “orange”, Use color = “orange” to change the color of the line to orange.   linetype = “dashed” Use linetype = “dashed” to change the solid line to a dashed line. Some linetype options include “dashed”, “dotted”, “longdash”, “dotdash”, etc. ) Closing parenthesis for the geom_smooth() function.  Click to Show Output  Click to View Output. In addition to customizing the regression line, you can customize the points, add points, add lines, and much more. ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. speed,  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet. y =  “y= ” declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point( geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. size = 1.5, Use size = 1.5 to change the size of the points.  color = “skyblue” Use color = “skyblue” to change the color of the points to Brother Saunders’ favorite color.  alpha = 0.5 Use alpha = 0.5 to change the transparency of the points to 0.5. ) Closing parenthesis of geom_point() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in ways similar to lm(…) to place more complicated models on the plot.  se = FALSE, se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.  color = “navy”, Use color = “navy” to change the color of the line to navy blue.  size = 1.5 Use size = 1.5 to adjust the thickness of the line to 1.5. ) Closing parenthesis of geom_smooth() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_hline( Use geom_hline() to add a horizontal line at a specified y-intercept. You can also use geom_vline(xintercept = some_number) to add a vertical line to the graph. yintercept = Use “yintercept =” to tell geom_hline() that you are going to declare a y intercept for the horizontal line.  75 75 is the value of the y-intercept. , color = “firebrick” Use color = “firebrick” to change the color of the horizontal line to firebrick red. , size = 1, Use size = 1 to adjust the thickness of the horizontal line to size 1.              linetype = “longdash” Use linetype = “longdash” to change the solid line to a dashed line with longer dashes. Some linetype options include “dashed”, “dotted”, “longdash”, “dotdash”, etc. , alpha = 0.5 Use alpha = 0.5 to change the transparency of the horizontal line to 0.5. ) Closing parenthesis of geom_hline function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_segment( geom_segment() allows you to add a line segment to ggplot() by using specified start and end points. x = “x =” tells geom_segment() that you are going to declare the x-coordinate for the starting point of the line segment.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the starting point of the line segment.  y =“y =” tells geom_segment() that you are going to declare the y-coordinate for the starting point of the line segment.  75, 75 is a number on the y-axis of your graph. It is the y-coordinate of the starting point of the line segment.  xend = “xend =” tells geom_segment() that you are going to declare the x-coordinate for the end point of the line segment.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the end point of the line segment.  yend = “yend =” tells geom_segment() that you are going to declare the y-coordinate for the end point of the line segment.  38, 38 is a number on the y-axis of your graph. It is the y-coordinate of the end point of the line segment.                size = 1 Use size = 1 to adjust the thickness of the line segment. , color = “lightgray” Use color = “lightgray” to change the color of the line segment to light gray. , linetype = “longdash” Use *linetype = “longdash* to change the solid line segment to a dashed one. Some linetype options include”dashed”, “dotted”, “longdash”, “dotdash”, etc. ) Closing parenthesis for geom_segment() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_point( geom_point() can also be used to add individual points to the graph. Simply declare the x and y coordinates of the point you want to plot. x = “x =” tells geom_point() that you are going to declare the x-coordinate for the point.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the point.  y = “y =” tells geom_point() that you are going to declare the y-coordinate for the point.  75 75 is a number on the y-axis of your graph. It is the y-coordinate of the point. , size = 3 Use size = 3 to make the point stand out more. , color = “firebrick” Use color = “firebrick” to change the color of the point to firebrick red. ) Closing parenthesis of the geom_point() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_text( geom_text() allows you to add customized text anywhere on the graph. It is very similar to the base R equivalent, text(…). x = “x =” tells geom_text() that you are going to declare the x-coordinate for the text.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the text.  y = “y =” tells geom_text() that you are going to declare the y-coordinate for the text.  84, 84 is a number on the y-axis of your graph. It is the y-coordinate of the text.  label = “label =” tells geom_text() that you are going to give it the label.  “My Point (14, 75)”, “My Point (14, 75)” is the text that will appear on the graph.             color = “navy” Use color = “navy” to change the color of the text to navy blue. , size = 3 Use size = 3 to change the size of the text. ) Closing parenthesis of the geom_text() function.  + The + allows you to add more layers to the framework provided by ggplot().   theme_minimal() Add a minimalistic theme to the graph. There are many other themes that you can try out.  Click to Show Output  Click to View Output. ## `geom_smooth()` using formula = 'y ~ x' Accessing Parts of the Regression Finally, note that the mylm object contains the names(mylm) of mylm$coefficients Contains two values. The first is the estimated \\(y\\)-intercept. The second is the estimated slope. ## (Intercept) speed ## -17.579095 3.932409 mylm$residuals Contains the residuals from the regression in the same order as the actual dataset. ## 1 2 3 4 ## 3.849460 11.849460 -5.947766 12.052234 ## 5 6 7 8 ## 2.119825 -7.812584 -3.744993 4.255007 ## 9 10 11 12 ## 12.255007 -8.677401 2.322599 -15.609810 ## 13 14 15 16 ## -9.609810 -5.609810 -1.609810 -7.542219 ## 17 18 19 20 ## 0.457781 0.457781 12.457781 -11.474628 ## 21 22 23 24 ## -1.474628 22.525372 42.525372 -21.407036 ## 25 26 27 28 ## -15.407036 12.592964 -13.339445 -5.339445 ## 29 30 31 32 ## -17.271854 -9.271854 0.728146 -11.204263 ## 33 34 35 36 ## 2.795737 22.795737 30.795737 -21.136672 ## 37 38 39 40 ## -11.136672 10.863328 -29.069080 -13.069080 ## 41 42 43 44 ## -9.069080 -5.069080 2.930920 -2.933898 ## 45 46 47 48 ## -18.866307 -6.798715 15.201285 16.201285 ## 49 50 ## 43.201285 4.268876 mylm$fitted.values The values of \\(\\hat{Y}\\) in the same order as the original dataset. ## 1 2 3 4 5 ## -1.849460 -1.849460 9.947766 9.947766 13.880175 ## 6 7 8 9 10 ## 17.812584 21.744993 21.744993 21.744993 25.677401 ## 11 12 13 14 15 ## 25.677401 29.609810 29.609810 29.609810 29.609810 ## 16 17 18 19 20 ## 33.542219 33.542219 33.542219 33.542219 37.474628 ## 21 22 23 24 25 ## 37.474628 37.474628 37.474628 41.407036 41.407036 ## 26 27 28 29 30 ## 41.407036 45.339445 45.339445 49.271854 49.271854 ## 31 32 33 34 35 ## 49.271854 53.204263 53.204263 53.204263 53.204263 ## 36 37 38 39 40 ## 57.136672 57.136672 57.136672 61.069080 61.069080 ## 41 42 43 44 45 ## 61.069080 61.069080 61.069080 68.933898 72.866307 ## 46 47 48 49 50 ## 76.798715 76.798715 76.798715 76.798715 80.731124 mylm$… several other things that will not be explained here. Making Predictions predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ) Closing parenthesis for the data.frame(…) function. ) Closing parenthesis for the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12)) 1   29.60981 The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(…) function.  interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. “prediction” This specifies that a prediction interval will be included with the predicted value. A prediction interval gives you a 95% confidence interval that captures 95% of the data, or \\(Y_i\\) values for the specific \\(X\\)-value specified in the prediction. ) Closing parenthesis of the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"prediction\")   fit The “fit” is the predicted value.   lwr The “lwr” is the lower bound.   upr The “upr” is the upper bound. 1 29.60981 In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet. -1.749529 This is the lower bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us the stopping distance could be as quick as -1.749529 feet (or 0 because distance can’t go negative). 60.96915 This is the upper bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us that the actual stopping distance could be as high as 60.96915 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(…) function.  interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. “confidence” This specifies that a confidence interval for the prediction should be provided. This is of use whenever your interest is in just estimating the average y-value, not the actual y-values. ) Closing parenthesis of the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"confidence\")   fit The “fit” is the predicted value.   lwr The “lwr” is the lower bound.   upr The “upr” is the upper bound. 1 29.60981 In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. 24.39514 This is the lower bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is greater than this value. 34.82448 This is the upper bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is less than this value. Finding Confidence Intervals for Model Parameters confint( The R function confint(…) allows you to use an lm(…) object to compute confidence intervals for one or more parameters (like \\(\\beta_0\\) or \\(\\beta_1\\)) in your model. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  level = “level =” tells the confint(…) function that you are going to declare at what level of confidence you want the interval. The default is “level = 0.95.” If you want to find 95% confidence intervals for your parameters, then just run confint(mylm).  someConfidenceLevel someConfidenceLevel is simply a confidence level you choose when you want something other than a 95% confidence interval. Some examples of appropriate levels include 0.90 and 0.99. ) Closing parenthesis for confint(..) function. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.90)     5 % The lower bound of a 90% confidence interval occurs at the 5th percentile. This is because at 90% confidence, 10% is left in the tails, with 5% on each end. The upper bound will thus end at the 95th percentile, hence the 5% and 95% as the column names.   95 % The upper bound of a 90% confidence interval ends at the 95th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -28.914514 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -6.243676 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.235501 and 4.629317. 3.235501 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. 4.629317 This is the upper bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.95)     2.5 % The lower bound of a 95% confidence interval occurs at the 2.5th percentile. This is because at 95% confidence, 5% is left in the tails, with 2.5% on each end. The upper bound will thus end at the 97.5th percentile, hence the 2.5% and 97.5% as the column names for the lower and upper bounds, respectively.   97.5 % The upper bound of a 95% confidence interval ends at the 97.5th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -31.167850 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -3.990340 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.096964 and 4.767853. 3.096964 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.096964 and 4.767853 4.767853 This is the upper bound of the confidence interval. In this case, you can be 95% confident that the slope lies between 3.096964 and 4.767853 Explanation Linear regression has a rich mathematical theory behind it. This is because it uses a mathematical function and a random error term to describe the regression relation between a response variable \\(Y\\) and an explanatory variable called \\(X\\). Expand each element below to learn more. Regression Cheat Sheet (Expand) Term Pronunciation Meaning Math R Code \\(Y_i\\) $Y_i$ “why-eye” The data \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)\\) $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$ YourDataSet$YourYvariable \\(\\hat{Y}_i\\) $\\hat{Y}_i$ “why-hat-eye” The fitted line \\(\\hat{Y}_i = b_0 + b_1 X_i\\) $\\hat{Y}_i = b_0 + b_1 X_i$ lmObject$fitted.values \\(E\\{Y_i\\}\\) $E\\{Y_i\\}$ “expected value of why-eye” True mean y-value \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) $E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i$ <none> \\(\\beta_0\\) $\\beta_0$ “beta-zero” True y-intercept <none> <none> \\(\\beta_1\\) $\\beta_1$ “beta-one” True slope <none> <none> \\(b_0\\) $b_0$ “b-zero” Estimated y-intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) $b_0 = \\bar{Y} - b_1\\bar{X} b_0 <- mean(Y) - b_1*mean(X)$ \\(b_1\\) $b_1$ “b-one” Estimated slope \\(b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\) $b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})} {\\sum(X_i - \\bar{X})^2}$ b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) \\(\\epsilon_i\\) $\\epsilon_i$ “epsilon-eye” Distance of dot to true line \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) $\\epsilon_i = Y_i - E\\{Y_i\\}$ <none> \\(r_i\\) $r_i$ “r-eye” or “residual-eye” Distance of dot to estimated line \\(r_i = Y_i - \\hat{Y}_i\\) $r_i = Y_i - \\hat{Y}_i$ lmObject$residuals \\(\\sigma^2\\) $\\sigma^2$ “sigma-squared” Variance of the \\(\\epsilon_i\\) \\(Var\\{\\epsilon_i\\} = \\sigma^2\\)$Var\\{\\epsilon_i\\} = \\sigma^2$ <none> \\(MSE\\) $MSE$ “mean squared error” Estimate of \\(\\sigma^2\\) \\(MSE = \\frac{SSE}{n-p}\\)$MSE = \\frac{SSE}{n-p}$ sum( lmObject$res^2 ) / (n - p) \\(SSE\\) $SSE$ “sum of squared error” (residuals) Measure of dot’s total deviation from the line \\(SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\)$SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$ sum( lmObject$res^2 ) \\(SSR\\) $SSR$ “sum of squared regression error” Measure of line’s deviation from y-bar \\(SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\)$SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2$ sum( (lmObject$fit - mean(YourData$Y))^2 ) \\(SSTO\\) $SSTO$ “total sum of squares” Measure of total variation in Y \\(SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\)$SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2$ sum( (YourData$Y - mean(YourData$Y))^2 ) \\(R^2\\) $R^2$ “R-squared” Proportion of variation in Y explained by the regression \\(R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\)$R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}$ SSR/SSTO \\(r\\) $r$ “r” Correlation between X and Y. \\(r = \\sqrt{R^2}\\)$r = \\sqrt{R^2}$ sqrt(R^2) \\(\\hat{Y}_h\\) $\\hat{Y}_h$ “why-hat-aitch” Estimated mean y-value for some x-value called \\(X_h\\) \\(\\hat{Y}_h = b_0 + b_1 X_h\\)$\\hat{Y}_h = b_0 + b_1 X_h$ predict(lmObject, data.frame(XvarName=#)) \\(X_h\\) $X_h$ “ex-aitch” Some x-value, not necessarily one of the \\(X_i\\) values used in the regression \\(X_h =\\) some number$X_h = $ Xh = # Confidence Interval “confidence interval” Estimated bounds at a certain level of confidence for a parameter \\(b_0 \\pm t^* \\cdot s_{b_0}\\)b_0 \\pm t^* \\cdot s_{b_0} or \\(b_1 \\pm t^* \\cdot s_{b_1}\\)b_1 \\pm t^* \\cdot s_{b_1} confint(mylm, level = someConfidenceLevel) Parameter Estimate \\(\\beta_0\\) \\(b_0\\) \\(\\beta_1\\) \\(b_1\\) \\(\\epsilon_i\\) \\(r_i\\) \\(\\sigma^2\\) \\(MSE\\) \\(\\sigma\\) \\(\\sqrt{MSE}\\), the Residual standard error The Mathematical Model (Expand) \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)… There are three main elements to the mathematical model of regression. Each of these three elements is pictured below in the “Regression Relation Diagram.” Study both the three bullet points and their visual representations in the plot below for a clearer understanding. The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read more…) The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as “natural law” or “God’s law”. It is some true line that is unknown to us. The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line. The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\). Note: \\(E\\{Y\\}\\) is pronounced “the expected value of y” because, well… the mean is the typical, average, or “expected” value. The dots, i.e., the regression relation plus an error term: \\(Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}} + \\underbrace{\\epsilon_i}_\\text{error term} \\quad \\text{where} \\ \\epsilon_i\\sim N(0,\\sigma^2)\\) (Read more…) This is shown by the dots in the graph below. This is the data. In regression, the assumption is that the y-value for individual \\(i\\), denoted by \\(Y_i\\), was “created” by adding an error term \\(\\epsilon_i\\) to each individual’s “expected” value \\(\\beta_0 + \\beta_1 X_i\\). Note the “order of creation” would require first knowing an indivual’s x-value, \\(X_i\\), then their expected value from the regression relation \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) and then adding their \\(\\epsilon_i\\) value to the result. The \\(\\epsilon_i\\) allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance \\(\\epsilon_i\\) from the line. Note: \\(Y_i\\) is pronounced “why-eye” because it is the y-value for individual \\(i\\). Sometimes also called “why-sub-eye” because \\(i\\) is in the subscript of \\(Y\\). The estimated line, i.e., the line we get from a sample of data. \\(\\underbrace{\\hat{Y}_i}_{\\substack{\\text{estimated mean} \\\\ \\text{y-value}}} = \\underbrace{b_0 + b_1 X_i}_\\text{estimated regression equation}\\) (Read more…) The estimated line is shown by the solid line in the graph below. \\(\\hat{Y}\\) is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation \\(E\\{Y\\}\\). So \\(\\hat{Y}\\) is interpreted as the estimated average (or mean) \\(Y\\)-value for any given \\(X\\)-value. Thus, \\(b_0\\) is the estimated y-intercept and \\(b_1\\) is the estimated slope. The b’s are sample statistics, like \\(\\bar{x}\\) and the \\(\\beta\\)’s are population parameters like \\(\\mu\\). The \\(b\\)’s estimate the \\(\\beta\\)’s. Note: \\(\\hat{Y}_i\\) is pronounced “why-hat-eye” and is known as the “estimated y-value” or “fitted y-value” because it is the y-value you get from \\(b_0 + b_1 X_i\\). It is always different from \\(Y_i\\) because dots are rarely if ever exactly on the estimated regression line. This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line). Something to ponder: The true line, when coupled with the error terms, “creates” the data. The estimated (or fitted) line uses the sampled data to try to “re-create” the true line. We could loosely call this the “order of creation” as shown by the following diagram. par(mfrow=c(1,3), mai=c(.2,.2,.4,.1)) plot(y ~ x, col=\"white\", main=\"A Law is Given\", yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, main=\"Data is Created\", xaxt='n', yaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, xaxt='n', yaxt='n', main=\"The Law is Estimated\") curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) A Law is Given Data is Created The Law is Estimated \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) \\(Y_i = E\\{Y_i\\} + \\epsilon_i\\) \\(\\hat{Y}_i = b_0 + b_1 X_i\\) The true line is the “law”. The \\(Y_i\\) are created by adding \\(\\epsilon_i\\) to \\(E\\{Y_i\\}\\) where \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\). The law is estimated with \\(\\hat{Y}_i\\) which is given with lm(...). Click open the “Code” buttom below to the right to find code that runs a simulation demonstrating this “order of creation”. ## Simulating Data from a Regression Model ## This R-chunk is meant to be played in your R Console. ## It allows you to explore how the various elements ## of the regression model combine together to \"create\" ## data and then use the data to \"re-create\" the line. set.seed(101) #Allows us to always get the same \"random\" sample #Change to a new number to get a new sample n <- 3 #set the sample size X_i <- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45. beta0 <- 3 #Our choice for the y-intercept. beta1 <- .1 #Our choice for the slope. sigma <- 12.5 #Our choice for the std. deviation of the error terms. epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data View(fabData) #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it. fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData. summary(fab.lm) #Summarize your model. plot(y ~ x, data=fabData) #Plot the data. abline(fab.lm) #Add the estimated regression line to your plot. # Now for something you can't do in real life... but since we created the data... abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). legend(\"topleft\", legend=c(\"True Line\", \"Estimated Line\"), lty=c(2,1), bty=\"n\") #Add a legend to your plot specifying which line is which. Interpreting the Model Parameters (Expand) \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted as… The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model. If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\). The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\). It is often misunderstood to be the “average change in y” or just “the change in y” but it is more correctly referred to as the “change in the average y”. To better see this, consider the three graphics shown below. par(mfrow=c(1,3)) hist(mtcars$mpg, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Number of Vehicles\", xlab=\"Gas Mileage (mpg)\", col=\"skyblue\") boxplot(mpg ~ cyl, data=mtcars, border=\"skyblue\", boxwex=0.5, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Number of Cylinders of Engine (cyl)\") plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=\"skyblue\", main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Quarter Mile Time (qsec)\") abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=\"darkgray\") mtext(side=3, text=\"Automatic Transmissions Only (am==0)\", cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=\"gray\") The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09. The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg. The scatterplot on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value. Residuals and Errors (Expand) \\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true error… Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\). The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\). We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summary… Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\). \\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms. Keep in mind the idea that the errors \\(\\epsilon_i\\) “created” the data and that the residuals \\(r_i\\) are computed after using the data to “re-create” the line. Residuals have many uses in regression analysis. They allow us to diagnose the regression assumptions, See the “Assumptions” section below for more details. estimate the regression relation, See the “Estimating the Model Parameters” section below for more details. estimate the variance of the error terms, See the “Estimating the Model Variance” section below for more details. and assess the fit of the regression relation. See the “Assessing the Fit of a Regression” section below for more details. Assessing the Fit of a Regression (Expand) \\(R^2\\), SSTO, SSR, and SSE… Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line. par(mfrow=c(1,3), mai=c(.1,.1,.5,.1)) set.seed(2) x <- runif(30,0,20) y1 <- 2 + 3.5*x + rnorm(30,0,2) y2 <- 2 + 3.5*x + rnorm(30,0,8) y3 <- 2 + 3.5*x + rnorm(30,0,27) plot(y1 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Excellent Fit\") abline(lm(y1 ~ x), col=\"gray\") plot(y2 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Good Fit\") abline(lm(y2 ~ x), col=\"gray\") plot(y3 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Poor Fit\") abline(lm(y3 ~ x), col=\"gray\") A common way to measure the fit of a regression is with correlation. While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\). (If you are a Math 325 student, just stick with correlation for now and skip on to the next section of this Explanation tab. If you are a Math 425 student, it is critical that you come to understand \\(R^2\\) deeply, so read on.) Before you can understand \\(R^2\\), you must understand three important “sums of squares”. (Read more about sums…) A sum is just a fancy word for adding things together. \\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand. So we use the symbol \\(\\Sigma\\) to denote the word “sum”. Further, we use a subscript \\(\\Sigma_{i=1}\\) to state what value the sum is beginning with, and a superscript \\(\\Sigma_{i=1}^6\\) to state the value we are ending at. This gives \\[ \\sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Test your knowledge, do you see why the answer is 6 to the sum below? \\[ \\sum_{i=1}^3 i = 6 \\] Computing sums in R is fairly easy. Type the following codes in your R Console. sum(1:6) #gives the answer of 21 sum(1:3) #gives the answer of 6 However, sums really become useful when used with a data set. Each row of a data set represents an “individual’s” data. We can reference each individual with a row number. In the data below, individual 3, denoted by \\(i=3\\), has a speed of 7 and a dist of 4. pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3) Individual speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 To compute the sum of the speed column, use sum(speed). If we divided this sum by 6, we would get the mean of speed mean(speed). In fact, the two most used statistics mean(...) and sd(...) both use sums. Take a moment to review the formulas for mean and standard deviation. It is strongly recommended that you study the Explanation tab for both as well. We’ll wait. See you back here shortly. … Welcome back. Suppose we let X = speed and Y = dist. Then \\(X_3 = 7\\) and \\(Y_3 = 4\\) because we are accessing row 3 of both the \\(X\\) (or speed) column and \\(Y\\) (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, sum(speed) would be written mathematically as \\(\\sum_{i=1}^6 X_i\\) and sum(dist) would be written as \\(\\sum_{i=1}^6 Y_i\\). Sum of Squared Errors Sum of Squares Regression Total Sum of Squares \\(\\text{SSE} = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\) \\(\\text{SSR} = \\sum_{i=1}^n \\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\) \\(\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\\) Measures how much the residuals deviate from the line. Measures how much the regression line deviates from the average y-value. Measures how much the y-values deviate from the average y-value. Equals SSTO - SSR Equals SSTO - SSE Equals SSE + SSR sum( (Y - mylm$fit)^2 ) sum( (mylm$fit - mean(Y))^2 ) sum( (Y - mean(Y))^2 ) It is important to remember that SSE and SSR split up SSTO, so that \\[ \\text{SSTO} = \\text{SSE} + \\text{SSR} \\] This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works. The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called \\(R^2\\) (“r-squared”). R-Squared (\\(R^2\\)) \\[ \\underbrace{R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}}_\\text{Interpretation: Proportion of variation in Y explained by the regression.} \\] The smallest \\(R^2\\) can be is zero, and the largest it can be is 1. This is because \\(SSR\\) must be between 0 and SSTO, inclusive. Residual Plots & Regression Assumptions (Expand) Residuals vs. fitted-values, Q-Q Plot of the residuals, and residuals vs. order plots… There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate. Each assumption is labeled in the regression equation below. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions. (Read more…) As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not. Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3 The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(\\hat{Y}_i\\). The residuals are the \\(r_i\\). This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable. | Show Examples | (Read more…) The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption. The linear relation is assumed to be satisfied if there are no apparent trends in the plot. The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values. The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance. set.seed(2) X <- rnorm(30,15,3) notLin <- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8)) notLin.lm <- lm(Y~X, data=notLin) set.seed(15) Lin <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) Lin.lm <- lm(Y~X, data=Lin) par(mfrow=c(3,3), mai=c(.25,.25,.25,.25), mgp=c(1,.75,0)) plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Not Linear\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(notLin.lm$fitted.values,notLin.lm$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Good: Linear, Constant Variance\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) set.seed(6) notCon <- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5)) notCon.lm <- lm(Y~X, data=notCon) LinO <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) LinO[1] <- LinO[1]^2 LinO.lm <- lm(Y~X, data=LinO) plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Unconstant Variance\", cex.main=0.95, yaxt='n', xaxt='n', col=\"firebrick\") polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) # plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Outliers\", cex.main=0.95) # abline(h=0) tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(Girth ~ Volume, data=trees[-31,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(Height ~ Volume, data=trees) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) abline(h=0) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals, f=.4) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(weight ~ repwt, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(weight ~ repht, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) abline(h=0) Q-Q Plot of the Residuals: Checks Assumption #2 The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption. | Show Examples | (Read more…) There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot. Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) set.seed(123) tmp <- rnorm(100) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"skyblue\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Normal\", col=\"skyblue\") tmp <- Davis$weight qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Right-skewed\", breaks=15, col=\"firebrick\") par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- rbeta(100, 5,1) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Left-skewed\", breaks=seq(min(tmp),max(tmp), length.out=13), col=\"firebrick\") tmp <- rbeta(100,2,2) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Heavy-tailed\", col=\"firebrick\") Residuals versus Order Plot: Checks Assumption #5 When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated. | Show Examples | (Read more…) Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Good: No Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: General Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ qsec, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: Interesting Patterns\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),]) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Bad: Obvious Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") Problems from Failed Assumptions There are various problems that can arise when certain of the regression assupmtions are not satisfied. Lack of Linearity When the linearity assumption is violated, pretty much everything we obtain from the regression summary is no longer meaningful. The y-intercept estimate can be drastically off from its actual true value. Important model information is lost by trying to use a simple slope term \\(\\beta_1\\) to describe the model with respect to \\(X\\). The residual standard error will be much higher than it otherwise would be because of curvature patterns in the data that the line cannot capture. Thus, R-squared will be lower than it otherwise should be. P-values can become non-significant, when in fact there is a strong pattern in the data, but that pattern just cannot be captured by a simple line. *Normality of the errors is often put into question as well when a simplified line is used to try to capture a more complicated curved model. The plot below demonstrate these difficulties. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 7.5 #True slope beta_2 <- -0.25 #True bend X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors Y_i <- beta_0 + beta_1*X_i + beta_2*X_i^2 + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Non-Linear Relation\") abline(mylm, col=\"gray\") #Add fitted line to plot curve(beta_0 + beta_1*x + beta_2*x^2, col=\"gray\", lty=2, add=TRUE) #Add True line to plot #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (True value:\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (True value:\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (True value:\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Non-normal Error Terms When the normality assumption for the error terms is violated, not all is lost. In fact, the estimate of the slope and intercept are still often fairly meaningful. However, it is unwise to put too much trust in the residual standard error as an estimate of the standard deviation \\(\\sigma\\). This is because the standard deviation in skewed distributions does not carry the same meaning it has in normal distributions. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rchisq(n, 1)*3 - 1 #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Unconstant Variance When variance of the error term changes across the regression, the regression approximates the “average variance” of the errors because the regression is still assuming the variance is constant across the regression. The estimates of the slope and intercept are still typically quite good, and can be used for interpretation. The residual standard error however should not be considered to be meaningful as it will be too large on one end of the regression and too small on the other end. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma + X_i) #normally distributed errors #with increasing variance Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Variance Varies (Non-Constant)\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 13.82 Slope 3.5 3.768 Sigma 2.5 13.02 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", round(mean(sigma + X_i), 2), \", mean)\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Normality Violated As silly as it sounds, if the only problem with the regression is the lack of normality of the error terms, it isn’t all that big of a problem. Depending on how non-normal the residuals appear, there could be some skewing to the residual standard error, but otherwise, the slope and intercept are still interpretable and meaningful. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- runif(n, -sqrt(12*sigma^2)/2, sqrt(12*sigma^2)/2) #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 14.7 Slope 3.5 3.449 Sigma 2.5 2.317 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Independence Assumption Violated While the slope and intercept are often still meaningful when the independence assumption is violated, the residual standard error is unnecessarily large in this case. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, 2.5) + (1:n -n/2)*.5 #normal, but correlated errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Independence Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 16.47 Slope 3.5 3.296 Sigma 2.5 4.819 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Outliers Present While outliers do not violate any of the regression assumptions, they do pose substantial difficulties for the least squares regression estimates of the slope and intercept. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors epsilon_i[3] <- ifelse(X_i[3] < 10, runif(1,25,35), -runif(1,25,35)) #create outlier Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"An Outlier Present\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 13.18 Slope 3.5 3.447 Sigma 2.5 6.389 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) The material below this section is meant for Math 425 students only. Estimating the Model Parameters (Expand) How to get \\(b_0\\) and \\(b_1\\): least squares & maximum likelihood… There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical. The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas. Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 <- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\). When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\). Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i. \\label{exp} \\end{equation}\\] Least Squares{#leastSquares} To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\). \\[ Q = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2 \\] Then we use the function Q as if it were a function of \\(\\beta_0\\) and \\(\\beta_1\\). Ironically, the values of \\(Y\\) and \\(X\\) are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we don’t know are the values of \\(\\beta_0\\) and \\(\\beta_1\\). This least squares applet is a good way to explore how various choices of the slope and intercept yield different values of the “sum of squared residuals”. But it turns out that there is one “best” choice of the slope and intercept that yields a “smallest” value of the “sum of squared residuals.” This best choice can actually be found using calculus by taking the partial derivatives of \\(Q\\) with respect to both \\(\\beta_0\\) and \\(\\beta_1\\). \\[ \\frac{\\partial Q}{\\partial \\beta_0} = -2\\sum (Y_i - \\beta_0 - \\beta_1X_i) \\] \\[ \\frac{\\partial Q}{\\partial \\beta_1} = -2\\sum X_i(Y_i-\\beta_0-\\beta_1X_i) \\] Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize \\(Q\\) for a given set of data. After all the calculations are completed we find the values of the parameter estimators \\(b_0\\) and \\(b_1\\) (of \\(\\beta_0\\) and \\(\\beta_1\\), respectively) are as stated previously. Maximum Likelihood{#mle} The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of \\(\\beta_0\\) and \\(\\beta_1\\) which minime the least squares \\(Q\\) function, we choose the values of \\(\\beta_0\\) and \\(\\beta_1\\) which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the \\(Y_i\\) for all observations \\(i=1,\\ldots,n\\). We can do this rather simply by using the assumption that the errors, \\(\\epsilon_i\\) are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if \\(f(Y_i)\\) denotes the probability density function for \\(Y_i\\), then the joint probability density for all \\(Y_i\\), \\(f(Y_1,\\ldots,Y_n)\\) is given by \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) \\] Since each \\(Y_i\\) is assumed to be normally distributed with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\) (see model (\\(\\ref{model}\\))) we have that \\[ f(Y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{Y_i-\\beta_0-\\beta_1X_i}{\\sigma}\\right)^2\\right]} \\] which provides the joint probability as \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] The likelihood function \\(L\\) is then given by consider the \\(Y_i\\) and \\(X_i\\) fixed and the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) as the variables in the function. \\[ L(\\beta_0,\\beta_1,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] Instead of taking partial derivatives of \\(L\\) directly (with respect to all parameters) we take the partial derivatives of the \\(\\log\\) of \\(L\\), which is easier to work with. In a similar, but more difficult calculation, to that of minimizing \\(Q\\), we obtain the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) which maximize the log of \\(L\\), and which therefore maximize \\(L\\). (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate \\(\\hat{\\sigma}^2\\) of \\(\\sigma^2\\). \\[ \\hat{\\sigma}^2 = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n} \\] Estimating the Model Variance (Expand) Estimating \\(\\sigma^2\\) with MSE… As shown previously in the “Estimating Model Parameters” section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation. Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\). This estimate turns out to be a biased estimator. This means that it is consistently wrong in its estimates of \\(\\sigma^2\\). If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong. This is bad. Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\). Without going into all the details, to “fix” the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently. To find the correct degrees of freedom, we have to notice that the \\(\\hat{Y}_i\\) in the numerator of \\(\\widehat{\\sigma}^2\\) is defined by \\[\\begin{equation} \\widehat{Y}_i = b_0 + b_1X_i \\label{hatY} \\end{equation}\\] From this equation, we notice that two means, \\(\\bar{X}\\) and \\(\\bar{Y}\\), were estimated from the data in order to obtain \\(\\hat{Y}_i\\). (See the formulas for \\(b_0\\) and \\(b_1\\) above, and note how they use both \\(\\bar{X}\\) and \\(\\bar{Y}\\) in their calculation.) Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for \\(\\hat{\\sigma}^2\\) should be \\(n-2\\) instead of \\(n\\). Some incredibly long calculations will show that the “fixed” estimator \\[\\begin{equation} s^2 = MSE = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n-2} \\quad \\text{(Unbiased Estimator of $\\sigma^2$)} \\end{equation}\\] is an unbiased estimator of \\(\\sigma^2\\). Here \\(MSE\\) stands for mean squared error, which is the most obvious name for a formula that squares the errors \\(Y_i-\\hat{Y}_i\\) then adds them up and divides by their degrees of freedom. Similarly, we call the numerator \\(\\sum(Y_i-\\hat{Y}_i)^2\\) the sum of the squared errors, denoted by \\(SSE\\). It is also important to note that the errors are often denoted by \\(r_i = Y_i-\\hat{Y}_i\\), the residuals. Putting this all together we get the following equivalent statements for \\(MSE\\). \\[\\begin{equation} s^2 = MSE = \\frac{SSE}{n-2} = \\frac{\\sum(Y_i-\\widehat{Y}_i)^2}{n-2} = \\frac{\\sum r_i^2}{n-2} \\end{equation}\\] As a final note, even though the expected value \\(E\\{MSE\\} = \\sigma^2\\), which shows \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\), it unfortunately isn’t true that \\(\\sqrt{MSE}\\) is an unbiased estimator of \\(\\sigma\\). This presents a few problems later on, but these are minimal enough that we can overlook the issue and move forward. Transformations (Expand) \\(Y'\\), \\(X'\\), and returning to the original space… Y transformations are denoted by y-prime, written \\(Y'\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\). \\[ Y' = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y' = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y' = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y' = \\log(Y)\\) lm(log(Y) ~ X) 0.25 \\(Y' = \\sqrt(\\sqrt(Y))\\) lm(sqrt(sqrt(Y)) ~ X) 0.5 \\(Y' = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y' = Y\\) lm(Y ~ X) 2 \\(Y' = Y^2\\) lm(Y^2 ~ X) Using “maximum-likelihood” estimation, the Box-Cox procedure can actually automatically detect the “optimal” value of \\(\\lambda\\) to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise. Scatterplot Recognition Box-Cox Suggestion An Example Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\). set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Ya <- 1/sqrt(Y) #1/Y^2 Lam = -2 Yb <- 1/Y #1/Y Lam = -1 Yc <- exp(.02*Y) #log(Y) Lam = 0 Yd <- Y^2 #sqrt(Y) Lam = 0.5 Ye <- Y #Y Lam = 1 Yf <- sqrt(Y) #Y^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Ya ~ X, main=expression(paste(\"Use \", lambda == -2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ya^-2 ~ X)) curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yb ~ X, main=expression(paste(\"Use \", lambda == -1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yb^-1 ~ X)) curve(1/(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yc ~ X, main=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(log(Yc) ~ X)) curve(exp(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yd ~ X, main=expression(paste(\"Use \", lambda == 0.5)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(sqrt(Yd) ~ X)) curve((b[1] + b[2]*x)^2, add=TRUE, col=\"green\", lwd=2) plot(Ye ~ X, main=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ye ~ X)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yf ~ X, main=expression(paste(\"Use \", lambda == 2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yf^2 ~ X)) curve(sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try. par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0)) boxCox(lm(Ya ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -2)), line=.5) boxCox(lm(Yb ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -1)), line=.5) boxCox(lm(Yc ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), line=.5) boxCox(lm(Yd ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0.5)), line=.5) boxCox(lm(Ye ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), line=.5) boxCox(lm(Yf ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 2)), line=.5) An Example Suppose we were running a simple linear regression on the cars dataset. This would be done with the code cars.lm <- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesn’t quite fit the data as well as we would hope. Instead, the data looks a little curved. cars.lm <-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") abline(cars.lm, col=\"gray\") Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful. library(car) boxCox(cars.lm) The output from the boxCox(...) function looks as follows. This plot tells use to use the \\(\\lambda = 0.5\\) transformation, so that \\(Y' = Y^0.5 = \\sqrt{Y}\\). (To see this yourself, click on the “Box-Cox Suggestion” tab above, as well as on the “Scatterplot Recognition” tab.) Now, a transformation regression is performed using sqrt(Y) in place of Y as follows: cars.lm.t <- lm(sqrt(dist) ~ speed, data=cars) summary(cars.lm.t)   Estimate Std. Error t value Pr(> (Intercept) 1.277 0.4844 2.636 0.01126 speed 0.3224 0.02978 10.83 1.773e-14 Then, \\[ \\widehat{Y}_i' = 1.277 + 0.3224 X_i \\] And replacing \\(\\hat{Y}_i' = \\sqrt{\\hat{Y}_i}\\) we have \\[ \\sqrt{\\widehat{Y}_i} = 1.277 + 0.3224 X_i \\] Solving for \\(\\hat{Y}_i\\) gives \\[ \\widehat{Y}_i = (1.277 + 0.3224 X_i)^2 \\] Which, using curve((1.277 + 0.3224*x)^2, add=TRUE) (see code for details) looks like this: plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") curve( (1.277 + 0.3224*x)^2, add=TRUE, col=\"firebrick\") X-Transformations X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them. The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated. The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation. set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Xa <- 1/sqrt(X) #1/X^2 Lam = -2 Xb <- 1/X #1/X Lam = -1 Xc <- exp(.02*X) #log(X) Lam = 0 Xd <- X^2 #sqrt(X) Lam = 0.5 Xe <- X #X Lam = 1 Xf <- sqrt(X) #X^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Y ~ Xa, main=expression(paste(\"Use \", X*minute == X^-2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xa^-2))) curve(b[1] + b[2]*x^-2, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xb, main=expression(paste(\"Use \", X*minute == X^-1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xb^-1))) curve(b[1] + b[2]*x^-1, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xc, main=expression(paste(\"Use \", X*minute == log(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ log(Xc))) curve(b[1] + b[2]*log(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xd, main=expression(paste(\"Use \", X*minute == sqrt(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ sqrt(Xd))) curve(b[1] + b[2]*sqrt(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xe, main=expression(paste(\"Use \", X*minute == X, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ Xe)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xf, main=expression(paste(\"Use \", X*minute == X^2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xf^2))) curve(b[1] + b[2]*x^2, add=TRUE, col=\"green\", lwd=2) Inference for the Model Parameters (Expand) t test formulas, sampling distributions, confidence intervals, and F tests… When fitting the regression model given by the equation \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] to a sample of data, we typically test hypotheses about the parameters \\(\\beta_0\\), \\(\\beta_1\\), or both. Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_0 - \\overbrace{0}^\\text{a number}}{s_{b_0}}\\] This is the formula for the test statistic. It measures how far the estimated y-intercept \\(b_0\\) is from the null hypothesis for \\(\\beta_0\\) in units of “standard errors of \\(b_0\\)”. Thus the division by \\(s_{b_0}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0, it could be any number. \\(H_0: \\beta_1 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_1\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_1 - \\overbrace{0}^\\text{a number}}{s_{b_1}}\\] This is the formula for the test statistic. It measures how far the estimated slope \\(b_1\\) is from the null hypothesis for \\(\\beta_1\\) in units of “standard errors of \\(b_1\\)”. Thus the division by \\(s_{b_1}\\). Though the hypothesized value of \\(\\beta_1\\) is typically 0, it could be any number. Left-tailed p-value = pt(-abs(tvalue), degrees of freedom). Double it to get the two-sided p-value. In R, these values correspond to the output summary of an lm as follows. (Show Example) Consider the cars data in R. Suppose we used the regression model given by \\[ \\underbrace{Y_i}_\\text{Feet to Stop} = \\beta_0 + \\beta_1 \\underbrace{X_{i}}_\\text{mph} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\quad \\sim N(0,\\sigma^2) \\] to model the feet a vehicle (from the 1920’s) takes to stop when traveling at a certain speed (in miles per hour, mph) prior to stopping. When the regression is performed and summarized in R, it is always testing the following two hypotheses: \\[ H_0: \\beta_0 = 0 \\quad\\quad H_0: \\beta_1 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\quad\\quad H_a: \\beta_1 \\neq 0 \\] To perform the test of these hypotheses for the regression stated above, we would run the following codes in R. cars.lm <- lm(dist ~ speed, data=cars) pander(summary(cars.lm)$coefficients) These would produce summary output like the following, but the following output has been labeled with the math notation corresponding to each value. Let’s emphasize what is happening in this summary output table. First, here is how the “t value” is calculated for the “(Intercept)” in the summary table above. \\[ t = \\frac{b_0-0}{s_{b_0}} = \\frac{-17.58 - 0}{6.758} = -2.601 \\] Second, here is a visual representation of how the P-value, the “Pr(>|t|)” as it is called in the summary table above, is calculated for this test statistic. (Click the graph to view an interactive applet showing this calculation.) Notice both ends of the t-distribution are being shaded to compute the P-value because the alternative hypothesis was \\(H_a: \\beta_0 \\neq 0\\). To compute the P-value in R, we use the “percentile function for the t-distribution” called pt( ). This function requires two things, the t-value and the degrees of freedom, in our case pt(-2.601, 48). Note the degrees of freedom (df) are 48 because the sample size is \\(n=50\\) and there are two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) in our regression model. Running this code in R gives: pt(-2.601, 48) = 0.00616 However, note that this value is only half of the actual P-value of 0.0123. To get the “two-sided” P-value (note that our alternative hypothesis used a \\(\\neq\\) symbol) we need to double this left-tailed P-value. 2*pt(-2.601, 48)) = 0.0123 Finally, note that the same procedure can be used to test hypotheses that use a value other than 0 in the null and alternative. For example, to test the hypotheses: \\[ H_0: \\beta_1 = 3 \\\\ H_a: \\beta_1 \\neq 3 \\] Use the t-formula \\[ t = \\frac{b_1 - 3}{s_{b_1}} = \\frac{\\overbrace{3.932}^{b_1} - \\overbrace{3}^{H_0}}{\\underbrace{0.4155}_{s_{b_1}}} = 2.243 \\] then the P-value is calculated in R by 2*pt(-abs(2.243), 48) = 0.0295495 To obtain confidence intervals in R use confint(mylm). Confidence Interval Formula Standard Error \\(\\beta_0\\) \\(b_0 \\pm\\) \\(t^*\\) This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\). Use qt(0.975, df) to get \\(t*\\) in R. \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_0\\). \\(s_{b_0}\\) The standard error of \\(b_0\\), denoted by \\(s_{b_0}\\) is provided in the regression summary output under the column header called “Std. Error” for the “(Intercept)” row of the output. It is calculated using the formula shown below. \\[s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\] This is called the “estimated variance of \\(b_0\\)”. Taking the square root of this number gives the “standard error of \\(b_0\\)”. \\(\\beta_1\\) \\(b_1 \\pm\\) \\(t^*\\) This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\) (sample size - number of parameters in the regression model). Use qt(0.975, df) to get \\(t*\\) in R \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_1\\). \\(s_{b_1}\\) The standard error of \\(b_1\\), denoted by \\(s_{b_1}\\) is provided in the regression summary output under the column header called “Std. It is calculated using the formula shown below. \\[s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\] This is called the “estimated variance of \\(b_1\\)”. Taking the square root of this number gives the “standard error of \\(b_1\\)”. To be more exact, the types of inference we are interested in are the following. Determine if there is evidence of a meaningful linear relationship in the data. If \\(\\beta_1 = 0\\), then there is no relation between \\(X\\) and \\(E\\{Y\\}\\). Hence we might be interested in testing the hypotheses \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) is some hypothesized number. To provide a confidence interval for the true value of \\(\\beta_1\\). Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the sampling distribution of the estimate \\(b_1\\) of the parameter \\(\\beta_1\\). And, while we are at it, we may as well come to understand the sampling distribution of the estimate \\(b_0\\) of the parameter \\(\\beta_0\\). Review sampling distributions from Math 221. Since \\(b_1\\) is an estimate, it will vary from sample to sample, even though the truth, \\(\\beta_1\\), remains fixed. (The same holds for \\(b_0\\) and \\(\\beta_0\\).) It turns out that the sampling distribution of \\(b_1\\) (where the \\(X\\) values remain fixed from study to study) is normal with mean and variance: \\[ \\mu_{b_1} = \\beta_1 \\] \\[ \\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} \\] ## Simulation to Show relationship between Standard Errors ##----------------------------------------------- ## Edit anything in this area... n <- 100 #sample size Xstart <- 30 #lower-bound for x-axis Xstop <- 100 #upper-bound for x-axis beta_0 <- 2 #choice of true y-intercept beta_1 <- 3.5 #choice of true slope sigma <- 13.8 #choice of st. deviation of error terms ## End of Editable area. ##----------------------------------------------- # Create X, which will be used in the next R-chunk. X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) ## After playing this chunk, play the next chunk as well. To see that this is true, consider the regression model with values specified for each parameter as follows. \\[ Y_i = \\overbrace{\\beta_0}^{2} + \\overbrace{\\beta_1}^{3.5} X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\overbrace{\\sigma^2}^{\\sigma=13.8}) \\] Using the equations above for \\(\\mu_{b_1}\\) and \\(\\sigma^2_{b_1}\\) we obtain that the mean of the sampling distribution of \\(b_1\\) will be \\(\\mu_{b_1} = \\beta_1 = 3.5\\) Further, we see that the variance of the sampling distribution of \\(b_1\\) will be \\(\\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} = \\frac{13.8^2}{4.25\\times 10^{4}}\\) Taking the square root of the variance, the standard deviation of the sampling distribution of \\(b_1\\) will be \\(\\sigma_{b_1} = 0.067\\). That’s very nice. But to really believe it, let’s run a simulation ourselves. The “Code” below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of \\(b_1\\) and \\(b_0\\) for that regression. After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of \\(b_0\\) is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of \\(b_1\\) is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of \\(\\beta_0\\) or \\(\\beta_1\\), respectively. Also, the “Std. Error” of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of \\(\\sigma_{b_0}\\) and \\(\\sigma_{b_1}\\), respectively. N <- 5000 #number of times to pull a random sample storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N) for (i in 1:N){ Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model mylm <- lm(Y ~ X) storage_b0[i] <- coef(mylm)[1] storage_b1[i] <- coef(mylm)[2] storage_rmse[i] <- summary(mylm)$sigma } layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3)) Ystart <- 0 #min(0,min(Y)) Ystop <- 500 #max(max(Y), 0) Yrange <- Ystop - Ystart plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), ylim=c(Ystart, Ystop), pch=16, col=\"gray\", main=\"Regression Lines from many Samples (gray lines) \\n Plus Residual Standard Deviation Lines (green lines)\") text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1) text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1) text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1) for (i in 1:N){ abline(storage_b0[i], storage_b1[i], col=\"darkgray\") } abline(beta_0, beta_1, col=\"green\", lwd=3) abline(beta_0+sigma, beta_1, col=\"green\", lwd=2) abline(beta_0-sigma, beta_1, col=\"green\", lwd=2) abline(beta_0+2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0-2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0+3*sigma, beta_1, col=\"green\", lwd=.5) abline(beta_0-3*sigma, beta_1, col=\"green\", lwd=.5) par(mai=c(1,.6,.5,.01)) addnorm <- function(m,s, col=\"firebrick\"){ curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2) lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col) lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col) lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col) lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col) lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col) lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col) lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col) legend(\"topleft\", legend=paste(\"Std. Error = \", round(s,3)), cex=0.7, bty=\"n\") } h0 <- hist(storage_b0, col=\"skyblue3\", main=\"Sampling Distribution\\n Y-intercept\", xlab=expression(paste(\"Estimates of \", beta[0], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m0 <- mean(storage_b0) s0 <- sd(storage_b0) addnorm(m0,s0, col=\"green\") h1 <- hist(storage_b1, col=\"skyblue3\", main=\"Sampling Distribution\\n Slope\", xlab=expression(paste(\"Estimates of \", beta[1], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m1 <- mean(storage_b1) s1 <- sd(storage_b1) addnorm(m1,s1, col=\"green\") t Tests{#tTests} Using the information above about the sampling distributions of \\(b_1\\) and \\(b_0\\), an immediate choice of statistical test to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) can be zero, or any other value, is a t test given by \\[ t = \\frac{b_1 - \\beta_{10}}{s_{b_1}} \\] where \\(s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\). (You may want to review the section “Estimating the Model Variance” of this file to know where MSE came from.) With quite a bit of work it has been shown that \\(t\\) is distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. The nearly identical test statistic for testing \\[ H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_0 \\neq \\beta_{00} \\] is given by \\[ t = \\frac{b_0 - \\beta_{00}}{s_{b_0}} \\] where \\(s^2_{b_0} = MSE\\left[\\frac{1}{n}+\\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\). This version of \\(t\\) has also been shown to be distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. Confidence Intervals Creating a confidence interval for either \\(\\beta_1\\) or \\(\\beta_0\\) follows immediately from these results using the formulas \\[ b_1 \\pm t^*_{n-2}\\cdot s_{b_1} \\] \\[ b_0 \\pm t^*_{n-2}\\cdot s_{b_0} \\] where \\(t^*_{n-2}\\) is the critical value from a t distribution with \\(n-2\\) degrees of freedom corresponding to the chosen confidence level. F tests{#Ftests} Another way to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\quad\\quad \\text{or} \\quad\\quad H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\quad\\quad \\ \\ \\quad \\quad H_a: \\beta_0 \\neq \\beta_{00} \\] is with an \\(F\\) Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an \\(F\\) test is very general and can be used in many places that a t test cannot. In its most general form, the \\(F\\) test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that \\(H_0:\\beta_1=0\\) against the alternative that \\(H_a: \\beta_1\\neq 0\\), we are essentially comparing two models against each other. If \\(\\beta_1=0\\), then the corresponding model would be \\(E\\{Y_i\\} = \\beta_0\\). If \\(\\beta_1\\neq0\\), then the model remains \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the \\(F\\) Test, that the null model (reduced model) have fewer “free” parameters than the alternative model (full model). To demonstrate what we mean by “free” parameters, consider the following example. Say we wanted to test the hypothesis that \\(H_0:\\beta_1 = 2.5\\) against the alternative that \\(\\beta_1\\neq2.5\\). Then the null, or reduced model, would be \\(E\\{Y_i\\}=\\beta_0+2.5X_i\\). The alternative, or full model, would be \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). Thus, the null (reduced) model contains only one “free” parameter because \\(\\beta_1\\) has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two “free” parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model. Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing \\(H_0: \\beta_1=0\\) against \\(H_a:\\beta_1\\neq0\\) we have the partition \\[ \\underbrace{Y_i-\\bar{Y}}_{Total} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{Regression} + \\underbrace{Y_i-\\hat{Y}_i}_{Error} \\] The reason we use \\(\\bar{Y}\\) for the null model is that \\(\\bar{Y}\\) is the unbiased estimator of \\(\\beta_0\\) for the null model, \\(E\\{Y_i\\} = \\beta_0\\). Thus we would compute the following sums of squares: \\[ SSTO = \\sum(Y_i-\\bar{Y})^2 \\] \\[ SSR = \\sum(\\hat{Y}_i-\\bar{Y})^2 \\] \\[ SSE = \\sum(Y_i-\\hat{Y}_i)^2 \\] and note that \\(SSTO = SSR + SSE\\). Important to note is that \\(SSTO\\) uses the difference between the observations \\(Y_i\\) and the null (reduced) model. The \\(SSR\\) uses the diffences between the alternative (full) and null (reduced) model. The \\(SSE\\) uses the differences between the observations \\(Y_i\\) and the alternative (full) model. From these we could set up a General \\(F\\) table of the form   Sum Sq Df Mean Sq F Value Model Error \\(SSR\\) \\(df_R-df_F\\) \\(\\frac{SSR}{df_R-df_F}\\) \\(\\frac{SSR}{df_R-df_F}\\cdot\\frac{df_F}{SSE}\\) Residual Error \\(SSE\\) \\(df_F\\) \\(\\frac{SSE}{df_F}\\) Total Error \\(SSTO\\) \\(df_R\\) Prediction and Confidence Intervals for \\(\\hat{Y}_h\\) (Expand) predict(…, interval=“prediction”)… It is a common mistake to assume that averages (means) describe individuals. They do not. So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line. Interval R Code Math Equation When to Use Prediction predict(..., interval=\"prediction\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\text{Pred}\\ Y}\\) Predict an individual’s value. Confidence predict(..., interval=\"confidence\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\hat{Y}}\\) Estimate location of the mean y-value. predict(mylm, data.frame(XvarName = number), interval=...) For example, consider this graph. Then click here to read about the graph. Notice the three dots above 15 mph in the graph. Each of these dots show a car that was going 15 mph when it applied the brakes. However, stopping distances of the three individual cars differ with one at 20 feet, one at 26 feet and one at 54 feet. The regression line represents the average stopping distance of cars. In this case, cars going 15 mph are estimated to have an average stopping distance of about 40 feet, as shown by the line. But individual vehicles, all going the same speed of 15 mph, varied from stopping distances of 20 feet up to 54 feet! So, to predict that a car going 15 mph will take 41.4 feet to stop, doesn’t tell the whole story. Far more revealing is the complete statement, “Cars going 15 mph are predicted to take anywhere from 10.2 to 72.6 feet to stop, with an average stopping distance of 41.4 feet.” This is called the “prediction interval” and is shown in the graph in blue. It is obtained in R with the codes: cars.lm <- lm(dist ~ speed, data=cars) predict(cars.lm, data.frame(speed=15), interval=\"prediction\") cars.lm <- lm(dist ~ speed, data=cars) pander(predict(cars.lm, data.frame(speed=15), interval=\"prediction\")) fit lwr upr 41.41 10.17 72.64 plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=\"firebrick2\", cex=1.5) cars.lm <- lm(dist ~ speed, data=cars) abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3)) abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2)) abline(v=15, lty=2, col=\"firebrick\") preds <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12) lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8)) lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8)) Now, for the details behind prediction intervals and confidence intervals. Let’s begin by recalling some details (from the section “Inference for the Model Parameters”) about the standard error of the y-intercept, \\(b_0\\). Recall that the y-intercept is the average y-value for the given x-value of \\(x=0\\). Recall further that the formula for the standard error of \\(b_0\\) is given by the formula \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] If we wanted to be more exact with this formula, we would write it as \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{(0-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice the addition of \\((0 - \\bar{X})^2\\) instead of just \\(\\bar{X}^2\\) in the numerator of the right-most part of the equation? This more complete statement obviously would reduce to just \\(\\bar{X}^2\\), but that is only because \\(X=0\\) when we are working with the y-intercept, \\(b_0\\). We could be working with other values of \\(X\\) than just zero. Let’s take a quick detour and talk notation for a second. Typically, \\(X_i\\) and \\(Y_i\\) are used to denote the x-value and y-value of points that are contained in our data set. When we want to reference a point that wasn’t within our original data set, we use the notation \\(X_h\\) and \\(Y_h\\). (The letter h is close to i, but different from i, so why not. There is really no other reason to use h.) Thus, \\(Y_h\\) is the y-value for the \\(X_h\\) x-value, neither of which were included in our original regression of \\(X_i\\)’s and \\(Y_i\\)’s. Now, back to the previous discussion. If \\(X_h = 0\\), then \\(\\hat{Y}_h\\) is the y-intercept, so \\(\\hat{Y}_h = b_0\\) when \\(X_h=0\\). So, we could write, \\[ s^2_{\\hat{Y}_h} = MSE\\left[\\frac{1}{n} + \\frac{(X_h-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice how the \\(b_0\\) in \\(s_{b_0}\\) was replaced with \\(\\hat{Y}_h\\) to get \\(s_{\\hat{Y}_h}\\) and the 0 in \\((0 - \\bar{X})^2\\) was replaced with \\(X_h\\) to get \\((X_h - \\bar{X})^2\\)? Interesting. We now have a formula that would give us the standard error of \\(\\hat{Y}_h\\) for any \\(X_h\\) value, not just \\(X_h = 0\\), or the y-intercept, \\(b_0\\). That is fantastic. It would look like this if plotted. Notice how the gray region is showing the standard error for each \\(\\hat{Y}_h\\) value? (It is technically showing the confidence interval for \\(E\\{Y_h\\}\\) at every possible \\(X_h\\) value, but that is just \\(\\hat{Y}_h \\pm t^* \\cdot s_{\\hat{Y}_h}\\).) ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + theme_bw() ## `geom_smooth()` using formula = 'y ~ x' Confidence Interval for \\(\\hat{Y}_h\\) \\[ \\hat{Y}_h \\pm t^* s_{\\hat{Y}_h} \\quad \\text{where} \\ s_{\\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] The confidence interval is a wonderful tool for estimating \\(E\\{Y_h\\}\\), the “true” average y-value for a given x-value of \\(X_h\\). However, it is not valuable for predicting an individual dot, or \\(Y_h\\) value. Notice how few of the dots of the regression are actually contained within the confidence interval band in the plot? The confidence interval does not really predict where the dots will land, just where the average y-value is located for each x-value. Remember the 68-95-99.7 Rule of the normal distribution? If not, here is a link back to that concept in the Math 221 textbook. This rule states that roughly 95% of data, when normally distributed, will be between \\(z=-2\\) and \\(z=2\\) standard deviations from the mean. So, is going two “residual standard errors” to both sides of the regression line enough to capture 95% of the data? The answer is, not quite. The reason for this is because our knowledge of where the true mean lies is uncertain. (Notice the confidence interval band shown in the plot.) However, adding two standard errors to the edges of the confidence band would get us in the right place. In other words, there are two sources of variability at play here, (1) our uncertaintity in where the regression line is sitting, and (2) the natural variability of the data points around the line. Thus, the “prediction interval” requires accounting for both of these sources of variability to produce the following equation. Prediction Interval for \\(Y_h\\) \\[ \\hat{Y}_h \\pm t^* s_{Pred \\hat{Y}_h} \\quad \\text{where} \\ s_{Pred \\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + 1 + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] This formula provides a useful band for identifying a region where we are 95% confident that a new observation for \\(Y_h\\) will land, given the value of \\(X_h\\). It looks as follows. Notice the prediction interval is much wider than the confidence interval. This is because data varies far more than do means. Prediction is for where the individual data points will land, confidence is for where the mean will land. cars.lm <- lm(dist ~ speed, data=cars) predy <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + geom_segment(aes(x=15, xend=15, y=predy[2], yend=predy[3]), lwd=4, color=rgb(.5,.7,.5,.01)) + geom_point(aes(x=15, y=predy[1]), cex=2, color=\"skyblue\", pch=15) + theme_bw() Lowess (and Loess) Curves (Expand) A non-parametric approach to estimating \\(E\\{Y_i\\}\\)… Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value. Using Base R # Select the relevant columns and remove NA values air2 <- airquality %>% dplyr::select(Temp, Ozone) %>% # Use dplyr's select function explicitly drop_na() # Drop rows with NA values in Temp or Ozone plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") lines(lowess(air2$Ozone, air2$Temp), col=\"firebrick\") Using ggplot2 # Your dataset (air2) should have been created earlier, ensuring no NAs air2 <- na.omit(airquality[c(\"Temp\", \"Ozone\")]) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=\"darkgray\") + # Add points with dark gray color geom_smooth(se=FALSE, method=\"loess\", method.args = list(degree=1)) + # Lowess curve, no standard error shading theme_bw() # Black and white theme ## `geom_smooth()` using formula = 'y ~ x' # Optionally, allow for predictions as well as the graph: # Uncomment the following lines if you want predictions as well: # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fitted, x=Ozone)) # Use fitted values from loess Advantages Disadvantages Quick. Good at ignoring outliers. Good at capturing the general pattern in the data. Good for making predictions within the scope of the data. No mathematical model. Not interpretable. No p-values. No adjusted R-squared. How it Works The Lowess curve localizes the regression model to a “neighborhood” of points, and then joins these localized regressions together into a smooth line. It minimizes the effect of outliers, and let’s the data “speak for itself”. As a downside, it is not interpretable, and has no final way to write the model mathematically. All the same, it is a very powerful tool for identifying an appropriate model, or verifying the fit of a model, or making predictions when no reasonable model does an adequate job. Study this graphic and the explanations below to learn how it works. Recommendation: run the code in this “Code” chunk to the right in your Console, and flip through the resulting graphics. X <- cars$speed Y <- cars$dist X <- X[!is.na(X) & !is.na(Y)] Y <- Y[!is.na(X) & !is.na(Y)] f <- 1/2 n <- length(X) lfit <- rep(NA,n) for (xh in 1:n){ xdists <- X - X[xh] nn <- floor(n*f) r <- sort(abs(xdists))[nn] xdists.nbrhd <- which(abs(xdists) < r) w <- rep(0, length(xdists)) w[xdists.nbrhd] <- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w), col=rgb(.2,.2,.2,.3), cex=1.5, yaxt='n', xaxt='n', xlab=\"\", ylab=\"\") points(Y[xh] ~ X[xh], pch=16, col=\"orange\") lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2)) cat(\"\\n\\n\") readline(prompt=paste0(\"Center point is point #\", xh, \"... Press [enter] to continue...\")) MADnotThereYet <- TRUE count <- 0 while(MADnotThereYet){ readline(prompt=paste0(\"\\n Adjusting line to account for outliers in the y-direction... Press [enter] to continue...\")) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"wheat\", add=TRUE) MAD <- median(abs(lmc$res)) resm <- lmc$res/(6*MAD) resm[resm>1] <- 1 bisq <- (1-resm^2)^2 w <- w*bisq obs <- coef(lmc) lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) count <- count + 1 if ( (sum(abs(obs-lmc$coef))<.1) | (count > 3)) MADnotThereYet <- FALSE } curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"green\", add=TRUE) points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=\"green\") readline(prompt=paste0(\"\\n Use final line to get fitted value for this point... Press [enter] to continue to next point...\")) lfit[xh] <- predict(lmc, data.frame(X=X[xh])) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") if (xh == n){ readline(prompt=paste0(\"\\n Press [enter] to see actual Lowess curve...\")) lines(lowess(X,Y, f=f), col=\"firebrick\") legend(\"topleft\", bty=\"n\", legend=\"Actual lowess Curve using lowess(...)\", col=\"firebrick\", lty=1) } } Select a fraction of the data to use for the “neighborhood” of points (shown in blue in the graph above). The lowess function in R uses “f=2/3” and the loess function uses “span=0.75” for this value, which selects the nearest two-thirds or 75% of the data, respectively, depending on which function you use. For this example, we set the fraction of points at 50%. Both functions can be set to whatever you want. Pick any point in the regression, eventually selecting all points one at a time. The selected point becomes the “center” of a “neighborhood” of points surrounding it. In this example, the center point is in orange, and the neighboring points are in blue. Use the points within the neighborhood to fit a regression line. However, make the regression depend most on points closest to “center” and least on points furthest from “center.” This is called a weighted regression. Weights are decided according to what is called the tricubic weight function, so that the weight \\(w\\) given to point \\(j\\) of the neighborhood of points is defined by \\[ w_j = \\left(1- \\left( \\frac{|X_c - X_j|}{\\max_k |X_c - X_k|}\\right)^3\\right)^3 \\] where \\(X_c\\) is the x-value of the “center” dot and \\(X_j\\) is the x-value of any other dot in the neighborhood. The fitted-value of \\(\\hat{Y}_c\\) is obtained for the center point \\(X_c\\) of the current regression. This point is used as the Lowess (or Loess) curve’s value at that particular x-value. Well, almost. It’s a first guess at where this value will end up, but there’s a little more to the algorithm before we are done. Initial guesses for each of these fitted values are obtained for each point in the regression. Now each local regression for each neighborhood is re-run a few times in such a way the the effect of outliers is minimized. The final line for each neighborhood is obtained by the following steps. Compute all residuals for points in the neighborhood of the current regression, denoted by \\(r_i\\). Then compute the MAD, median absolute deviation, of the residuals \\(MAD = \\text{median} (|r_1|, |r_2|, \\ldots)\\). Divide all residuals by 6 times the MAD: \\(u_i = r_i/(6\\cdot MAD)\\) (If \\(r_i > 6\\cdot MAD\\) then set \\(u_i = 0\\).) Compute what are called bisquare weights using the formula: \\(b_i = (1 - u_i^2)^2\\) Perform a regression using the weights \\(w_i = w_i b_i\\) Repeat the above process with the new weights \\(w_i\\) until the weights stop changing very much. The final fitted values for each \\(X\\)-value in the regression are obtained from the final regression line for each neighborhood. These fitted values make up the Lowess (or loess) curve. Note that the default of the loess function in R is to use quadratic regressions in each neighborhood instead of linear regressions. This can be controlled with the loess option of “degree=2” (quadratic fits) or “degree = 1”. In the lowess function only a linear regression in each neighborhood is allowed. Examples: bodyweight, cars Quantitative Y | Multiple X Graphics Y is a single quantitative variable of interest, like height. While we could use an X1 of “shoe-size” to explain height, we might also want to use a second x-variable, X2, like “gender” to help explain height. Further x-variables could also be used. Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a student’s final exam grade? How do factors like age, weight, exercise level, and diet impact a person’s blood pressure? Examples: Is one Navy base more prone to heatwaves than the other? The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH) Tests Multiple Linear Regression Multiple regression is a way to understand how different factors affect something we’re interested in. It’s like having more than one clue to solve a mystery. Here’s what you need to know: We can use several different pieces of information (called variables) to predict an outcome. There are many ways to combine these pieces of information. We start with simple combinations and build up to more complex ones, kind of like building with Lego blocks. This helps us make better predictions and understand complex situations more clearly. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model is a bit like using the same ingredient twice in a recipe, but in different ways. It uses the same factor (\\(X_i^2\\)) in two different forms: Once in its regular form (\\(\\beta_1 X_i\\)) Once squared (\\(\\beta_2 X_i^2\\)) This squared part is what we call the “quadratic” term. It helps us describe relationships that aren’t just straight lines, but have curves to them. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and “steepness” of the Model: negative values face down, positive values face up; large values imply “steeper” parabolas and low values imply “flatter” parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)’s explanation. An Example Using the airquality data set, we run the following “quadratic” regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our “quadratic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. Temp Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These “Estimates” can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(Month^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of Month from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, )   Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will “open down” (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be “month zero.” Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the “cubic” term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the “base slope coefficient” and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following “cubic” regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our “cubic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. uptake Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^3) \\(X_{i}^3\\), where the function I(…) protects the cubing of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will “open up”. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The “two-lines” model is a way to compare two different groups in statistics using numbers. Here’s how it works: It uses two types of numbers: A regular number that can be any value (\\(X_{1i}\\)) A special number that’s either 0 or 1 (called an “indicator variable”, \\(X_{2i}\\)) The special number (\\(X_{2i}\\)) helps us turn group information (like “Group A” or “Group B”) into numbers we can use in calculations. This model creates two separate lines on a graph: One line for Group A One line for Group B By using this method, we can easily see and compare how the two groups are different from each other. Think of it like having two recipes for the same dish, but with slightly different ingredients. This model helps us see how those small changes affect the final result. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the “base-line” of the model, the “Group 0” line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the “base-line” line. \\(\\beta_3\\) Called the “interaction” term. Controls the change in the slope for the second line in the model as compared to the slope of the “base-line” line. An Example Using the mtcars data set, we run the following “two-lines” regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our “two-lines” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. mpg Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) (beta zero): This is like the starting point. It’s estimated by something called the “Intercept”. \\(\\beta_1\\) (beta one): This shows how one thing changes when another thing (called “qsec”) changes. It’s estimated to be 1.439. \\(\\beta_2\\) (beta two): This shows how something else (called “am”) affects the overall result. It’s estimated to be -14.51. \\(\\beta_3\\) (beta three): This shows how “qsec” and “am” work together to affect the result. It’s estimated to be 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called “3D” regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to “bend”. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the “Temp” direction is estimated to be 2.659 and the slope in the “Month” direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction term’s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the “slopes” in each direction to change, creating a “curved” surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called “HD”, or “High Dimensional”, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isn’t really even possible to “mentally connect” with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)’s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the “Wind” direction is estimated to be -3.334. The slope in the “Temp” direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the “Solar.R” direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here. R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the “Overview” section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the “column (c) bind” function and it joins together things as columns. Res =  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals.  panel=panel.smooth,  This places a lowess smoothing line on each scatterplot. col =  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(“color1”,“color2”, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for “linear model.” Y Y must be a “numeric” vector of the quantitative response variable.  ~  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats “numeric” variables as quantitative and “character” or “factor” variables as qualitative. R will automatcially recode qualitative variables to become “numeric” variables using a 0,1 encoding. See the Explanation tab for details.  + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details.  + …, * ... emphasizes that as many explanatory variables as are desired can be included in the model.  data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(…) function displays the results of an lm(…) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(…) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -4.3818 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -2.2696 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.1344 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   1.7058 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   5.8752 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero.   2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (26.6248479) by its standard error (2.1829432). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a “star”. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + …).   -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model.   0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” by its standard error. It gives the “number of standard errors” away from zero that the “estimate” has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + …).   5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\).   2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot “.” implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like.   0.0004029 This is the estimate of the coefficient of the interaction term.   0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data.  0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that “at least one is not.”  34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom.  on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero.  p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that “at least one” of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the “Overview” sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =… Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “prediction”) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “confidence”) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BIC… There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (“R-squared”). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The “simplest” but “best” model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it “an information criterion (AIC)” when he published the method and other people later began calling it the “Akaike Information Criterion.”) Model Selection (Expand) pairs plots, added variable plots, and pattern recognition… Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots {.tabset .tabset-pills} A useful visualization tool for model selection is the “pairs plot.” This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice that… the y-axis of each plot is found by locating the variable name (like “mpg”) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like “disp”) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model {.tabset .tabset-pills} Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldn’t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Let’s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a model’s ability to generalize to new data… The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesn’t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, let’s remind ourselves why we use regression models in the first place. The main goal is to capture the “essence” of the data. In other words, the general pattern is what we are after. We want a model that tells us how “all such” data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the “complicated model” is overfitting the original data. It does not capture the “essence” of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-value… The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the model… The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions{#check} The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read more…) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) Outlier Analysis (Expand) Cook’s Distances and Leverage Values… The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs. fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cook’s Distances The idea behind Cook’s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article “Detection of Influential Observation in Linear Regression” (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, let’s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2))   1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the “differences” in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cook’s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cook’s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cook’s Distances using the code cooks.distance(lmObject). Also, a graph of Cook’s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of “leverage” and is “pulling” the regression toward itself. A value near 0 implies the point is just “one of many” and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that “minimize” the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)’s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the “hat matrix” \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the “leverage values” and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a “lot of pull,” and values close to 0 represent “little pull.” In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with “lots of leverage” and a large “Cook’s Distance” are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regression… Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class. Examples: Civic Vs Corolla cadillacs Binomial Y | Quantitative X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t. We might use an explanatory variable X of “height” to see if taller students are more likely to get an A in Math 325 than shorter students. (They aren’t, if you were wondering.) Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1’s and 0’s (this does happen or this doesn’t happen): Does the width of a child’s foot predict whether or not they are right-handed? What is the probability the cat adopted is a male based on how many days it took them to get adopted? Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted? (Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\") Tests Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The “P” stands for “Probability that…” \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1… This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)… in other words, the “|” says “given” and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the “natural constant” number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14…) Part What it means \\(P(Y_i = 1| x_i)\\) - The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s) - The symbol used to represent the notation in logisitc regression is \\(\\pi_i\\) \\(Y_i\\) (Responsive Variable) Describes the probability of $Y_i = $ 1 or 0 for a given scenario. \\(x_i\\) (Explanatory variable) Describes the observed value and how it effects the \\(Y_i\\) \\(\\frac{e^{\\text{stuff}}}{1+e^{\\text{stuff}}}\\) The content for “stuff” being simple linear regression of the form \\(\\beta_0 + \\beta_{1x_i}\\) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -1.5651 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.6648 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -0.2460 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   0.7276 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   2.2691 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The test statistic is a regular old z-score. It is most reliable when the sample size is “large.” It is a measurement of the number of standard errors the estimate is from 0.   Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds.   1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, …).   -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds.   0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a “star”. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about.   Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\).  43.230  on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates.  29.732 This can be calculated by sum(log(myglm$res^2)).  on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, “A version of Akaike’s An Information Criterion…” The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model.  33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 ## disp -0.014604 0.005168 -2.826 0.00471 ## ## (Intercept) * ## disp ** ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0’s or 1’s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == “B” or height < 68. In other words, Y needs to be a collection of 0’s and 1’s.  ~  The tilde is read “Y on X.” X, X is some quantitative variable.  data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(…) function. curve( A function in R that draws a “curve” on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the “Intercept” estimate from your logistic regression summary output.  \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case “x” in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula.  exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator.  add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(…) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set.  aes( The aes(…) function stands for “aesthetics” and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The plus sign adds a new layer to the ggplot.   geom_point( Tells the plot to add the physical geometry of “points” to the plot. ) Closing parenthesis for the geom_point() function.  + Add another layer to the plot.   geom_smooth( Add a smoothing line to the graph. method=“glm”, This adds a general linear model to the graph. method.args = list(family=“binomial”), This tells the method=“glm” to choose specifically the “binomial” model, otherwise known as the “logistic regression” model.  se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function.  + Add another layer to the ggplot.   theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),  The xVariableName is the same one you used in your glm(y ~ x, …) statement for “x”. Input any desired value for the “someNumericValue” spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016 Explanation Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal). The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\). Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page. Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful. Checking Model Assumptions {#diagnostics} The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit. Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease. Binomial Y | Multiple X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t. We might use an explanatory variable X1 of “height” and a second explanatory variable X2 of “gender” to try to predict whether or not a student will get an A in Math 325. Test(s) used for this category: Multiple Logistic Regression Model Tests Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two. Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. …, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -2.9446 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.2364 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.0000 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q   0.2776 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   1.9968 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The Z-score testing the hypothesis that \\(\\beta_j=0\\)   Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, …).   0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\).   1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\).   4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\).   3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\).   0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\).   0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a “star”. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\).   471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option.   Null Deviance: The deviance of the null model.  800.44 In this case, the null deviance is 800.44  on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression.  256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit.  on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model.  272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities ## numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) -4.97603 0.65316 -7.618 ## Time 0.39111 0.04979 7.854 ## Diet2 0.58283 1.04061 0.560 ## Diet3 -8.44476 4.32324 -1.953 ## Diet4 -67.41995 3768.10023 -0.018 ## Time:Diet2 0.02391 0.08679 0.275 ## Time:Diet3 1.20955 0.51249 2.360 ## Time:Diet4 8.83168 471.01259 0.019 ## Pr(>|z|) ## (Intercept) 2.57e-14 *** ## Time 4.02e-15 *** ## Diet2 0.5754 ## Diet3 0.0508 . ## Diet4 0.9857 ## Time:Diet2 0.7830 ## Time:Diet3 0.0183 * ## Time:Diet4 0.9850 ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, …), X2 = c(Value 1, Value 2, …), …) command. You should see the GSS example file for an example of how to use this function. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the “b” vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(“SomeColor”,“DifferentColor”,…) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(…) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==“B”.  ~  The formula operator in R. X1, The first x-variable in your glm code.  data = YourDataSet, Specify the name of your data set.  pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(…) function computes e^(stuff) in R and stands for the “exponential function.” (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(…) function requires that you call the x-variable “x” although you can change this behavior using xname=“SomeOtherName” if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)).  col = palette()[1], Pull the first color from the color palette for the first curve.  add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters.  col = palette()[2], Set the color of this second curve to the second color in the palette.  add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. “topright”, Typically the legend is placed in the top-right corner of the graph. But it could be placed “top”, “topleft”, “left”, “bottomleft”, “bottom”, “center”, “right”, or “bottomright”.  legend = Why you have to write legend again, no one knows, but you do. c(“Lable 1”, “Label 2”), These are the values that will appear in the legend, one entry on each line of the legend.  col = palette(), This specifies the colors of the symbols in the legend. First color goes with “Label 1”, and second goes with “Label 2” and so on.  lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist.  bty = Specifies the “box type” (bty) that should be drawn around the legend. ‘n’) By placing a “no” option (‘n’) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot.  aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + Add a layer to the ggplot.   geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function.  + Add a layer to the ggplot.   geom_smooth(method=“glm”, method.args=list(family=“binomial”), se=FALSE, Add the logistic regression curve to the plot.   aes(color=“X2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function.  + Add a layer to the plot.   theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x' Explanation Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative. The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations. Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant. Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression. Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\). Categorical Y | Categorical X Graphics Y is a single categorical variable of interest, like gender. X is another categorical variable of interest, like “hair color”. This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender. Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year? Whats the probability that you have brown hair and blue eyes? Tests Add your own notes about appropriate inferential procedures for this type of data here.",
    "sentences": ["Graphics Y is a single quantitative variable of interest, like “heights” of BYU-Idaho students.", "X is a categorical (qualitative) variable like which Math 221 you took, 221A, 221B, or 221C.", "In other words, X has three or more groups.", "So “Classrank” could be X, with groups “Freshman”, “Sophomore”, “Junior”, and “Senior”.", "Test(s) used for this category: One-Way ANOVA (means) Block Design Kruskal-Wallis Rank Sum Test (distributions) Analysis : College Student’s Food Perception Used comparing several groups based on means or distributions: How long are their feet based of width and gender?", "Are there certain months of the year that are associated with children having longer feet, on average, than others?", "Does a college student’s perception of food result in a change in their weight?", "(Kruskal-Wallis Rank Sum Test) # Read data food <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv\") # Data transformation and cleaning food_cleaned <- food %>% # Convert weight to numeric right away mutate(weight = as.numeric(weight)) %>% # Remove rows with NA weight or missing drink values filter(!is.na(weight) & !is.na(drink)) %>% # Create a new column `Food Perception Score` mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) # Create plots boxplot(weight ~ `Food Perception Score`, data=food_cleaned, col=\"lightgreen\", xlab=\"Food Perception Score\", main=\"Weight of College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=food_cleaned, pch=16, vertical=TRUE, add=TRUE, col=\"palegreen4\")", "Tests One-way ANOVA Each experimental unit is assigned to exactly one factor-level combination.", "Another way to say this is “one measurement per individual” (no repeated measures) and “equal numbers of individuals per group”."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Quantitative Y | Categorical X (3+\r\nGroups)"
  },
  {
    "id": 89,
    "title": "📍 Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single quantitative variable of interest, like “heights” of BYU-Idaho students. X is a categorical (qualitative) variable like which Math 221 you took, 221A, 221B, or 221C. In other words, X has three or more groups. So “Classrank” could be X, with groups “Freshman”, “Sophomore”, “Junior”, and “Senior”. Test(s) used for this category: One-Way ANOVA (means) Block Design Kruskal-Wallis Rank Sum Test (distributions) Analysis : College Student’s Food Perception Used comparing several groups based on means or distributions: How long are their feet based of width and gender? Are there certain months of the year that are associated with children having longer feet, on average, than others? Does a college student’s perception of food result in a change in their weight? (Kruskal-Wallis Rank Sum Test) # Read data food <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv\") # Data transformation and cleaning food_cleaned <- food %>% # Convert weight to numeric right away mutate(weight = as.numeric(weight)) %>% # Remove rows with NA weight or missing drink values filter(!is.na(weight) & !is.na(drink)) %>% # Create a new column `Food Perception Score` mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) # Create plots boxplot(weight ~ `Food Perception Score`, data=food_cleaned, col=\"lightgreen\", xlab=\"Food Perception Score\", main=\"Weight of College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=food_cleaned, pch=16, vertical=TRUE, add=TRUE, col=\"palegreen4\")",
    "sentences": ["Y is a single quantitative variable of interest, like “heights” of BYU-Idaho students.", "X is a categorical (qualitative) variable like which Math 221 you took, 221A, 221B, or 221C.", "In other words, X has three or more groups.", "So “Classrank” could be X, with groups “Freshman”, “Sophomore”, “Junior”, and “Senior”.", "Test(s) used for this category: One-Way ANOVA (means) Block Design Kruskal-Wallis Rank Sum Test (distributions) Analysis : College Student’s Food Perception Used comparing several groups based on means or distributions: How long are their feet based of width and gender?", "Are there certain months of the year that are associated with children having longer feet, on average, than others?", "Does a college student’s perception of food result in a change in their weight?", "(Kruskal-Wallis Rank Sum Test) # Read data food <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv\") # Data transformation and cleaning food_cleaned <- food %>% # Convert weight to numeric right away mutate(weight = as.numeric(weight)) %>% # Remove rows with NA weight or missing drink values filter(!is.na(weight) & !is.na(drink)) %>% # Create a new column `Food Perception Score` mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) # Create plots boxplot(weight ~ `Food Perception Score`, data=food_cleaned, col=\"lightgreen\", xlab=\"Food Perception Score\", main=\"Weight of College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=food_cleaned, pch=16, vertical=TRUE, add=TRUE, col=\"palegreen4\")"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 90,
    "title": "📍 Tests",
    "url": "index.html#tests",
    "content": "Tests One-way ANOVA Each experimental unit is assigned to exactly one factor-level combination. Another way to say this is “one measurement per individual” (no repeated measures) and “equal numbers of individuals per group”. Overview An ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA can be assumed to be normally distributed when seen in a Q-Q Plot.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out in a Residuals versus fitted-values plot.) comparing means Hypotheses For a One-way ANOVA \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] Mathematical Model A typical model for a one-way ANOVA is of the form \\[ Y_{ij} = \\mu_i + \\epsilon_{ij} \\] where \\(\\mu_i\\) is the mean for level (group) \\(i\\), and \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) is the error term for each point \\(j\\) within level (group) \\(i\\). R Instructions Console Help Command: ?aov() myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. X is a qualitative variable (should have class(X) equal to factor or character. If it does not, use as.factor(X) inside the aov(Y ~ as.factor(X),...) command. YourDataSet is the name of your data set. Perform the ANOVA myaov <- aov(Y ~ X, data=YourDataSet) summary(myaov) Diagnose ANOVA Assumptions par(mfrow=c(1,2)) plot(myaov, which=1:2) Example Code Hover your mouse over the example codes to learn more. Perform the ANOVA chick.aov <-  Saves the results of the ANOVA test as an object named ‘chick.aov’. aov( ‘aov()’ is a function in R used to perform the ANOVA. weight Y is ‘weight’, which is a numeric variable from the chickwts dataset.  ~  ‘~’ is the tilde symbol used to separate the Y and X in a model formula. feed, X is ‘feed’, which is a qualitative variable in the chickwts dataset, or more specifically, a factor with six levels: “casein”, “horsebean”, and so on… Use str(chickwts) to see this.  data = chickwts) ‘chickwts’ is a dataset in R. summary( ‘summary()’ shows the results of the ANOVA. chick.aov) ‘chick.aov’ is the name of the ANOVA.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## feed 5 231129 46226 15.37 5.94e-10 ## Residuals 65 195556 3009 ## ## feed *** ## Residuals ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Diagnose the ANOVA par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow = c(1,2)) The mfrow parameter controls “multiple frames on a row”. In this case, the c(1,2) specifies 1 row of 2 plots. This will cause the two diagnostic plots to be placed side-by-side. plot( ‘plot’ is a R function for the plotting of R objects. chick.aov, ‘chick.aov’ is the name of the ANOVA.  which = 1:2) The which=1:2 selects “which” of 6 available plots we want to have graphed. In this case, 1 shows the Residuals vs Fitted, and 2 shows the Normal QQ-plot. Both are needed to check the ANOVA assumptions.  Click to View Output  Click to View Output. Explanation Analysis of variance (ANOVA) is often applied to the scenario of testing for the equality of three or more means from (possibly) separate normal distributions of data. The normality assumption is required. No matter the sample size. If the distributions are skewed then a nonparametric test should be applied instead of ANOVA. One-Way ANOVA One-way ANOVA is when a completely randomized design is used with a single factor of interest. A typical mathematical model for a one-way ANOVA is of the form \\[ Y_{ik} = \\mu_i + \\epsilon_{ik} \\quad (\\text{sometimes written}\\ Y_{ik} = \\mu + \\alpha_i + \\epsilon_{ik}) \\] where \\(\\mu_i\\) is the mean of each group (or level) \\(i\\) of a factor, and \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) is the error term. The plot below demonstrates what these symbols represent. Note that the notation \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) states that we are assuming the error term \\(\\epsilon_{ik}\\) is normally distributed with a mean of 0 and a standard deviation of \\(\\sigma\\). Hypotheses The aim of ANOVA is to determine which hypothesis is more plausible, that the means of the different distributions are all equal (the null), or that at least one group mean differs (the alternative). Mathematically, \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_m = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\quad \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\}. \\] In other words, the goal is to determine if it is more plausible that each of the \\(m\\) different samples (where each sample is of size \\(n\\)) came from the same normal distribution (this is what the null hypothesis claims) or that at least one of the samples (and possibly several or all) come from different normal distributions (this is what the alternative hypothesis claims). Visualizing the Hypotheses The first figure below demonstrates what a given scenario might look like when all \\(m=3\\) samples of data are from the same normal distribution. In this case, the null hypothesis \\(H_0\\) is true. Notice that the variability of the sample means is smaller than the variability of the points. The figure below shows what a given scenario might look like for \\(m=3\\) samples of data from three different normal distributions. In this case, the alternative hypothesis \\(H_a\\) is true. Notice that the variability of the sample means, i.e., \\((\\bar{x}_1,\\bar{x}_2,\\bar{x}_3)\\), is greater than the variability of the points. Explaining the Name The above plots are useful in understanding the mathematical details behind ANOVA and why it is called analysis of variance. Recall that variance is a measure of the spread of data. When data is very spread out, the variance is large. When the data is close together, the variance is small. ANOVA utilizes two important variances, the between groups variance and the within groups variance. Between groups variance–a measure of the variability in the sample means, the \\(\\bar{x}\\)’s. Within groups variance–a combined measure of the variability of the points within each sample. The plot below combines the information from the previous plots for ease of reference. It emphasizes the fact that when the null hypothesis is true, the points should have a large variance (be really spread out) while the sample means are relatively close together. On the other hand, when the points are relative close together within each sample and the sample means have a large variance (are really spread out) then the alternative hypothesis is true. This is the theory behind analysis of variance, or ANOVA. Calculating the Test Statistic, \\(F\\) The ratio of the “between groups variation” to the “within groups variation” provides the test statistic for ANOVA. Note that the test statistic of ANOVA is an \\(F\\) statistic. \\[ F = \\frac{\\text{Between groups variation}}{\\text{Within groups variation}} \\] It would be good to take a minute and review the \\(F\\) distribution. The \\(p\\)-value for ANOVA thus comes from an \\(F\\) distribution with parameters \\(p_1 = m-1\\) and \\(p_2 = n-m\\) where \\(m\\) is the number of samples and \\(n\\) is the total number of data points. A Deeper Look at Variance It is useful to take a few minutes and explain the word variance as well as mathematically define the terms “within group variance” and “between groups variance.” Variance is a statistical measure of the variability in data. The square root of the variance is called the standard deviation and is by far the more typical measure of spread. This is because standard deviation is easier to interpret. However, mathematically speaking, the variance is the more important measurement. As mentioned previously, the variance turns out to be the key to determining which hypothesis is the most plausible, \\(H_0\\) or \\(H_a\\), when several means are under consideration. There are two variances that are important for ANOVA, the “within groups variance” and the “between groups variance.” Recall that the formula for computing a sample variance is given by \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})^2}{n-1} \\quad\\leftarrow \\frac{\\text{sum of squares}}{\\text{degrees of freedom}} \\] This formula has a couple of important pieces that are so important they have been given special names. The \\(n-1\\) in the denominator of the formula is called the “degrees of freedom.” The other important part of this formula is the \\(\\sum_{i=1}^n(x_i - \\bar{x})^2\\), which is called the “sum of squared errors” or sometimes just the “sum of squares” or “SS” for short. Thus, the sample variance is calculated by computing a “sum of squares” and dividing this by the “degrees of freedom.” It turns out that this general approach works for many different contexts. Specifically, it allows us to compute the “within groups variance” and the “between groups variance.” To introduce the mathematical definitions of these two variances, we need to introduce some new notation. Let \\(\\bar{y}_{i\\bullet}\\) represent the sample mean of group \\(i\\) for \\(i=1,\\ldots,m\\). Let \\(n_i\\) denote the sample size in group \\(i\\). Let \\(\\bar{y}_{\\bullet\\bullet}\\) represent the sample mean of all \\(n = n_1+n_2+\\cdots+n_m\\) data points. The mathematical calculations for each of these variances is given as follows. \\[ \\text{Between groups variance} = \\frac{\\sum_{i=1}^m (\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2}{m-1} \\leftarrow \\frac{\\text{Between groups sum of squares}}{\\text{Between groups degrees of freedom}} \\] \\[ \\text{Within groups variance} = \\frac{\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2}{n-m} \\leftarrow \\frac{\\text{Within groups sum of squares}}{\\text{Within groups degrees of freedom}} \\] A Fabricated Example The following table provides three samples of data: A, B, and C. These samples were randomly generated from normal distributions using a computer. The true means \\(\\mu_1, \\mu_2\\), and \\(\\mu_3\\) of the normal distributions are thus known, but withheld from you at this point of the example. A B C 13.15457 13.17463 16.66831 12.65225 12.16277 15.54719 13.73061 12.76905 16.63074 14.43471 13.38524 15.06726 13.79728 12.02690 15.57534 13.88599 13.24651 15.99915 12.77753 12.58386 15.58995 13.81536 12.64615 16.99429 13.03635 12.52055 15.47153 14.26062 14.03566 16.13330 An ANOVA will be performed with the sample data to determine which hypothesis is more plausible: \\[ H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\in \\{1,\\ldots,m\\} \\] To perform an ANOVA, we must compute the between groups variance and the within groups variance. This requires the Between groups sums of squares, within groups sums of squares, between groups degrees of freedom, and the within groups degrees of freedom. Note that to get the sums of squares, we first had to calculate \\(\\bar{y}_{1\\bullet}\\), \\(\\bar{y}_{2\\bullet}\\), \\(\\bar{y}_{3\\bullet}\\), and \\(\\bar{y}_{\\bullet\\bullet}\\) where the 1, 2, 3 corresponds to Samples A, B, and C, respectively. After some work, we find these values to be \\[ \\bar{y}_{1\\bullet} = 13.55, \\quad \\bar{y}_{2\\bullet} = 12.86 \\quad \\bar{y}_{3\\bullet} = 15.97 \\] and \\[ \\bar{y}_{\\bullet\\bullet} = \\frac{13.55+12.86+15.97}{3} = 14.13 \\] Using these values we can then compute the between groups sum of squares and the within groups sum of squares according to the formulas stated previously. This process is very tedious and will not be demonstrated. Only the results are shown in the following table which summarizes all the important information.   Degrees of Freedom Sum of Squares Variance F-value p-value Between groups 2 53.3 26.67 70.2 2e-11 Within groups 27 10.3 0.38 ANOVA Table In general, the ANOVA table is created by   Degrees of Freedom Sum of Squares Variance F-value p-value Between groups \\(m-1\\) \\(\\sum_{i=1}^m n_i(\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) \\(\\frac{\\text{Between groups variance}}{\\text{Within groups variance}}\\) \\(F\\)-distribution tail probability Within groups \\(n-m\\) \\(\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) ANOVA Assumptions{#residuals} The requirements for an analysis of variance (the assumptions of the test) are two-fold and concern only the error terms, the \\(\\epsilon_{ik}\\). The errors are normally distributed. The variance of the errors is constant. Both of these assumptions were stated in the mathematical model where we assumed that \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\). Checking ANOVA Assumptions To check that the ANOVA assumptions are satisfied, it is required to check the data in each group for normality using QQ-Plots. Also, the sample variance of each group must be relatively constant. The fastest way to check these two assumptions is by analyzing the residuals. An ANOVA residual is defined as the difference between the observed value of \\(y_{ik}\\) and the mean \\(\\bar{y}_{i\\bullet}\\). Mathematically, \\[ r_{ik} = y_{ik} - \\bar{y}_{i\\bullet} \\] One QQ-Plot of the residuals will provide the necessary evidence to decide if it is reasonable to assume that the error terms are normally distributed. Also, the constant variance can be checked visually by using what is known as a residuals versus fitted values plot. For the Fabricated Example above, the QQ-Plot and residuals versus fitted values plots show the two assumptions of ANOVA appear to be satisfied. Examples: chickwts (One-way) Block Design Repeated measures or other factors that group individuals into similar groups (blocks) are included in the study design. Overview A typical model for a block design is of the form \\[ Y_{lijk} = \\mu + B_l + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijlk} \\] where \\(\\mu\\) is the grand mean, \\(B_l\\) is the blocking factor, \\(\\alpha_i\\) is one factor with at least two levels, \\(\\beta_j\\) is another factor with at least two levels, \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors, and \\(\\epsilon_{ijlk} \\sim N(0,\\sigma^2)\\) is the error term. Only one block and one factor is required. Multiple blocks and multiple factors are allowed. It is not required to include interaction terms. The error term is always required. R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ Block+X1+X2+X1:X2, data=YourDataSet) Perform the test summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. Block is a qualitative variable that is not of direct interest, but is included in the model to account for variability in the data. It should have class(Block) equal to either factor or character. Use as.factor() if it does not. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. If it does not, use as.factor(X2). Note that factors C, D, and so on could also be added + to the model if desired. X1:X2 denotes the interaction of the factors X1 and X2. It is not required and should only be included if the interaction term is of interest. YourDataSet is the name of your data set. Examples: ChickWeight Kruskal-Wallis Rank Sum Test Allows for deciding if several samples come from the same population or if at least one sample comes from a different population. Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated. Can also be used when data is ordered (ordinal) but does not have an exact measurement. Best used when the distribution of the data is not normal. ANOVA is appropriate when the data is normal. comparing distributions Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, “is a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly. In many cases, this is practically equivalent to the mean of at least one population differing from the others.” Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable. g is a qualitative grouping variable defining which groups each value in x belongs to. It must either be a character vector or a factor vector. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. weight ‘weight’ is a numeric variable from the chickwts dataset that represents the quantatitive response variable.  ~  ‘~’ is the tilde symbol used to separate the left- and right-hand side in a model formula. feed, ‘feed’ is a qualitative grouping variable in the chickwts dataset.  data = chickwts) ‘chickwts’ is a dataset in R.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## ## Kruskal-Wallis rank sum test ## ## data: weight by feed ## Kruskal-Wallis chi-squared = 37.343, df = 5, ## p-value = 5.113e-07 Alternatively, you could use the following approach as well. kruskal.test(list(x, y, z)) x is a numeric vector of data values that represents the quantatitive response variable for the first group. y is a numeric vector of data values that represents the quantatitive response variable for the second group. z is a numeric vector of data values that represents the quantatitive response variable for the third group. Note that more than three vectors of data could be included inside of list(). Example Code Hover your mouse over the example codes to learn more. feed1 <- filter(chickwts, feed == “horsebean”) This splits out the first group of feed (horsebean) from the chickwts dataset. feed2 <- filter(chickwts, feed == “linseed”) This splits out the second group of feed (linseed) from the chickwts dataset. feed3 <- filter(chickwts, feed == “soybean”) This splits out the third group of feed (soybean) from the chickwts dataset. feed4 <- filter(chickwts, feed == “sunflower”) This splits out the fourth group of feed (sunflower) from the chickwts dataset. feed5 <- filter(chickwts, feed == “meatmeal”) This splits out the fifth group of feed (meatmeal) from the chickwts dataset. feed6 <- filter(chickwts, feed == “casein”) This splits out the sixth group of feed (casein) from the chickwts dataset. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. list(feed1$weight,feed2$weight,feed3$weight,feed4$weight,feed5$weight,feed6$weight)) A list of numeric vecotrs that represent the quantitative response variable for each group.  Click to View Output  Click to View Output. Load library(tidyverse) to run this code in R. ## ## Kruskal-Wallis rank sum test ## ## data: list(feed1$weight, feed2$weight, feed3$weight, feed4$weight, feed5$weight, feed6$weight) ## Kruskal-Wallis chi-squared = 37.343, df = 5, ## p-value = 5.113e-07 Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population. The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples. The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity. Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated. (Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\). Note that the notation \\(n_i\\) allows for each sample to be a different size. In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\). Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest. Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\). Award any tied values the average of the ranks of those values that are tied. In the bottle-cap data we have the following. Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample. Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\). In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\). In the bottle-cap data, \\(\\bar{R}_1 = \\frac{R_1}{n_1} = \\frac{24}{5} = 4.8\\) \\(\\bar{R}_2 = \\frac{R_2}{n_2} = \\frac{14}{3} = 4.667\\) \\(\\bar{R}_3 = \\frac{R_3}{n_3} = \\frac{40}{4} = 10.0\\) Step 5: If there are no ties, calculate the test statistic \\(H\\) by the formula \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] See the original article for the formula to use when there are ties present in the data. In the bottle-cap data, \\(H = 5.656\\). To understand the logic behind the test statistic \\(H\\), note that the null hypothesis assumes that each of the \\(C\\) samples are taken from the same population. Under this assumption, the ranks assigned to each sample should represent a uniform sample of the ranks \\(1,\\ldots,N\\). If the null hypothesis were true, we would expect the average of the ranks from each group, the \\(\\bar{R}_i\\), to be roughly about the same. More precisely, using the properties of the discrete uniform distribution (see wikipedia), we expect each \\(\\bar{R}_i\\) to be close to \\(\\frac{1}{2}(N+1)\\), which is the mean of the discrete uniform distribution. That \\(\\bar{R}_i\\) should be close to this value comes from the fact that the mean of a distribution of means is equal to the population mean. Further, the variance of \\(\\bar{R}_i\\) is given by the population variance, \\(\\frac{1}{12}(N^2-1)\\) in this case, divided by the sample size \\(n_i\\). Finally, the \\(\\frac{N-1}{N}\\) is a correction factor that will not be explained here. The full article explains the details about how a multivariate normal distribution is used to uncover the formula for \\(H\\). Thus, understanding \\(H\\) completely will require an advanced degree in statistical theory. However, the short story is that the distribution of the test statistic \\(H\\) is approximately a chi squared distribution with \\(C-1\\) degrees of freedom and has the form of a sum of values consisting of the (observed mean - expected mean)^2/(variance of the observed mean). Step 6: Calculate the p-value of \\(H\\) using a chi squared distribution with \\(C-1\\) degrees of freedom. \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] In the bottle-cap data, \\(p = 0.059\\), which represents the probability of \\(H\\) being as extreme or more extreme than the observed value of 5.656, assuming the null hypothesis is true. Examples: wages, pigweights Quantitative Y | Multiple Categorical X Graphics Y is a single quantitative variable of interest, like “heights” of BYU-Idaho students. X1 is a categorical (qualitative) variable like gender, with levels “boy” and “girl.” X2 is another categorical (qualitative) variable like “Classrank” with levels “Freshman”, “Sophomore”, and “Junior”. Of course, both X1 and X2 can have as many levels as you want to work with, but must have at least two levels each. Test(s) used for this category: Two-Way ANOVA (means) Used to answer THREE questions: the effects of factor 1 on the quantitative variable the effects of factor 2 on the quantitative variable the interaction between the two factors best paired with numerical summary looking at mean Analysis : Two-way ANOVA: Final Beginning Algebra Grades Question Examples: Based on your p-values and these graphs, which of the following is a correct conclusion to reach? xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1))) ANSWER : Among fourth graders, right handed boys have longer feet on average than left handed boys while the opposite is true for girls. Does the amount a student participates in the course affect the average final Beginning Algebra grades? Does the semester in which a student takes Beginning Algebra affect their average final grade? Does a student’s participation level vary depending on the semester? (Alternatively, does the semester influence a student’s participation?) BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") BAFinalGrades$Semester <- factor(BAFinalGrades$Semester, levels = c(\"Fall\",\"Winter\",\"Spring\")) ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=1)) + geom_point(color=\"firebrick1\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Spring Semester Seems to Have Higher Final Grades\", x=\"Semester\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades$Participation <- factor(BAFinalGrades$Participation, levels = c(\"Low\",\"Moderate\",\"High\")) ggplot(BAFinalGrades, aes(x=Participation, y=FinalGrade, group=1)) + geom_point(color=\"darkred\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Participation Seems to Lead to Higher Grades\", x=\"Participation\", y=\"Beginning Algebra Final Grades\") + theme_minimal() ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=Participation, color=Participation)) + geom_point() + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Beginning Algebra Seems Better during the Spring Semester\", x=\"BYU-Idaho Semesters\", y=\"Beginning Algebra Final Grades\") + theme_minimal() Tests Two-way ANOVA Overview A two-way ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses. Writing out the hypotheses can be very involved depending on whether you use the official “effects model” notation (very mathematically correct) or a simplified “means model” notation (which isn’t very mathematically correct, but gets the idea across in an acceptable way). Means Model Effects Model The first set of hypotheses are a “one-way” set of hypotheses for the first factor of the ANOVA. Factor: X1 with say, levels \\(A\\) and \\(B\\). \\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a “one-way” set of hypotheses, but for the second factor of the ANOVA. Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\mu_C = \\mu_D = \\mu_E = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one}\\ i\\in\\{1=C,2=D,3=E\\} \\] The third set of hypotheses are the most interesting hypotheses in a two-way ANOVA. These are called the interaction hypotheses. They test to see if the levels of one of the factors, say \\(X1\\), impact \\(Y\\) differently for the differing levels of the other factor, \\(X2\\). The hypotheses read formally as \\[ H_0: \\text{The effect of the first factor on Y} \\\\ \\text{is the same for all levels of the second factor.} \\] \\[ H_a: \\text{The effect of the first factor on Y is not the same} \\\\ \\text{for all levels of the second factor.} \\] A mathematically correct way to state the two-way ANOVA model is with the equation \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] In this model, \\(\\mu\\) is the grand mean (which is the average Y-value ignoring all information contained in the factors); \\(\\alpha_i\\) is the first factor \\(X1\\) with levels \\(A\\) and \\(B\\) (though there could be more levels in \\(X1\\) depending on your data); \\(\\beta_j\\) is the second factor with levels \\(C\\), \\(D\\), and \\(E\\) (though there could be fewer or more levels to this factor depending on your data); \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors which has \\(2\\times3=6\\) (may differ for your data) levels; and \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) is the normally distributed error term for each point \\(k\\) found within level \\(i\\) of \\(X1\\) and level \\(j\\) of \\(X2\\). This model allows us to more formally state the hypotheses as First factor \\(X1\\) having say, levels \\(A\\) and \\(B\\). \\[ H_0: \\alpha_A = \\alpha_B = 0 \\] \\[ H_a: \\alpha_i \\neq 0 \\ \\text{for at least one}\\ i\\in\\{1=A,2=B\\} \\] Second factor \\(X2\\) with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\beta_C = \\beta_D = \\beta_E = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j\\in\\{1=C,2=D,3=E\\} \\] Does the effect of the first factor (\\(X1\\)) change for the different levels of the second factor (\\(X2\\))? In other words, is there an interaction between the two factors \\(X1\\) and \\(X2\\)? \\[ H_0: \\alpha\\beta_{ij} = 0 \\ \\text{for all } i,j \\] \\[ H_a: \\alpha\\beta_{ij} \\neq 0 \\ \\text{for at least one } i,j \\] R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ X1+X2+X1:X2, data=YourDataSet) Perform the ANOVA summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. Note that factors X3, X4, and so on could also be added + to the model if desired, but this would create a three-way, or four-way ANOVA model, and so on. X1:X2 denotes the interaction of the factors X1 and X2. It is not required, but is usually included. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. warp.aov <-  Saves the results of the ANOVA test as an object named ‘warp.aov’. aov( ‘aov()’ is a function in R used to perform the ANOVA. breaks \\(Y\\) is ‘breaks’, which is a numeric variable from the warpbreaks dataset.  ~  ‘~’ is the tilde symbol used to separate the left- and right-hand side in a model formula. wool  The first factor \\(X1\\) is ‘wool’, which is a qualitative variable in the warpbreaks dataset. In this case, wool is a factor with two levels. Use str(warpbreaks) to see this. + tension  The second factor \\(X2\\) is ‘tension’, which is another qualitative variable in the warpbreaks dataset. In this case, tension is a factor with three levels. Use str(warpbreaks) to see this. + wool:tension, The interaction of the two factors: wool and tension.  data = warpbreaks) ‘warpbreaks’ is a dataset in R. summary( ‘summary()’ shows the results of the ANOVA. warp.aov) ‘warp.aov’ is the name of the ANOVA.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## wool 1 451 450.7 3.765 0.058213 ## tension 2 2034 1017.1 8.498 0.000693 ## wool:tension 2 1003 501.4 4.189 0.021044 ## Residuals 48 5745 119.7 ## ## wool . ## tension *** ## wool:tension * ## Residuals ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow = c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. plot( ‘plot’ is a R function for the plotting of R objects. warp.aov, ‘warp.aov’ is the name of the ANOVA.  which = 1:2) Will show the Residuals vs Fitted and the Normal QQ-plot to check the ANOVA assumptions.  Click to View Output  Click to View Output. Explanation Hypotheses in Two-way ANOVA The hypotheses that can be tested in a two-way ANOVA that includes an interaction term are three-fold. Hypotheses about \\(\\alpha\\) where \\(\\alpha\\) has \\(m\\) levels. \\[ H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_m = 0 \\] \\[ H_a: \\alpha_i \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\} \\] Hypotheses about \\(\\beta\\) where \\(\\beta\\) has \\(q\\) levels. \\[ H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_q = 0 \\] \\[ H_a: \\beta_j \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,q\\} \\] Hypotheses about the interaction term \\(\\alpha\\beta\\). \\[ H_0: \\text{the effect of one factor is the same across all levels of the other factor} \\] \\[ H_a: \\text{the effect of one factor differs for at least one level of the other factor} \\] Expanding the ANOVA Model{#expanding} It turns out that more can be done with ANOVA than simply checking to see if the means of several groups differ. Reconsider the mathematical model of two-way ANOVA. \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] This model could be expanded to include any number of new terms in the model. The power of this approach is in the several questions (hypotheses) that can be posed to data simultaneously. What happens if your ANOVA fails both requirement tests? Example: BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") ## Rows: 204 Columns: 3 ## ── Column specification ────────────────────────── ## Delimiter: \",\" ## chr (2): Semester, Participation ## dbl (1): FinalGrade ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. BAanova <- aov(FinalGrade ~ Semester + Participation + Semester:Participation, data=BAFinalGrades) par(mfrow=c(1,2)) plot(BAanova, which=1:2, pch=16) The residual plot shows that the constant (equal) variance assumption raises some concern as the points within each group have varying spread. The spread of each group seems to gets closer and closer as we go from left to right as well as some straying point from the more clustered groups. Thus, showing the variance of the residuals are not constant and do not accurately show a relationship between each variable. For the Q-Q plot, it shows that the residuals of the data is not quite normally distributed. There are visible points at the lower and upper ends of the graph deviating away form the diagonal line, which represents a normal distribution. Thus, showing us that the residuals of the data are not normal distributed. Although both graphs indicate that the data fails to meet both requirements, this primarily means our findings may be less reliable due to the abnormalities in the data. As a result, we should interpret the results cautiously as we proceed with our Two-Way ANOVA test! Examples: warpbreaks (Two-way), CO2 (three-way) Quantitative Y | Quantitative X Graphics Y is a single quantitative variable of interest, like “height”. X is another single quantitative variable of interest, like “shoe-size”. This would imply we are using “shoe-size” (X) to explain “height” (Y). Test(s) used for this category: Simple Linear Regression Analysis : Simple Linear Regression: BYU-Idaho Math Tutoring Used to find the relationship between one independent variable (predictor) and one dependent variable(outcome): Is there a relationship between the length of children’s feet and the width of their foot? Example: Does the amount of tutoring a student receives correlate with their chapter exam scores? (Simple Linear Regression) ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams) Tests Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\). Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$\\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$$ \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is “equal to” the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to “God’s Law” or “Natural Law”. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are “random” and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)… \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced “y-hat”, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what follows… \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, “the change in the average y-value for a one unit change in the x-value.” It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, “the average y-value when x is zero.” It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful. R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command.  <-  This is the “left arrow” assignment operator that stores the results of your lm() code into mylm name. lm( lm(…) is an R function that stands for “Linear Model”. It performs a linear regression analysis for Y ~ X. Y  Y is your quantitative response variable. It is the name of one of the columns in your data set. ~  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X. X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value.  data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X. ) Closing parenthesis for the lm(…) function. summary(mylm) The summary command allows you to print the results of your linear regression that were previously saved in mylm name.  Click to Show Output  Click to View Output. Example output from a regression. Hover each piece to learn more. Call: lm(formula = dist ~ speed, data = cars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -29.069 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -9.525 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -2.272 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -2.272 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-2.272) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   9.215 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   43.201 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -17.5791 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   6.7584 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -2.601 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (-17.5791) by its standard error (6.7584). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of -17.5791 is -2.601 standard errors (6.7584) from zero, which is a fairly surprising distance as shown by the p-value. 0.0123 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). speed This is always the name of your X-variable in your lm(Y ~ X, …).   3.9324 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.4155 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 9.464 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” of the slope (3.9324) by its standard error (0.4155). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from zero, which is a really surprising distance as shown by the smallness of the p-value. 1.49e-12 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  15.38 For this particular regression, the estimate of \\(\\sigma\\) is 15.38. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 48 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 50 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 50-2 = 48. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.6511, In this particular regression, 65.11% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data.  0.6438 In this case, the value of 0.6438 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is found as the ratio of the MSR/MSE where MSR = SSR/(p-1) and MSE = SSE/(n-p) where n is the sample size and p is the number of parameters in the regression model.  89.57 This is the value of the F-statistic for the lm(dist ~ speed, data=cars) regression. Note that SSE = sum( cars.lm$res^2 ) = 11353.52 with n - p = 50 - 2 = 48 degrees of freedom for this data. Further, SSR = sum( (cars.lm$fit - mean(cars$dist))^2 ) = 21185.46 with p - 1 = 1 degree of freedom. So MSR = 21185.46 and MSE = 11353.52 / 48 = 236.5317. So MSR / MSE = 21185.46 / 236.5317 = 89.56711.  on 1 and 48 DF, The 1 degree of freedom is the SSR degrees of freedom (p-1). The 48 is the SSE degrees of freedom (n-p).  p-value: 1.49e-12 The p-value for an F-statistic is found by the code pf(89.56711, 1, 48, lower.tail=FALSE), which gives the probability of being more extreme than the observed F-statistic in an F distribution with 1 and 48 degrees of freedom. Check Assumptions 1, 2, 3, and 5 When making it in an r chunk make your height 3! { r, fig.height = 3 } par( The par(…) command stands for “Graphical PARameters”. It allows you to control various aspects of graphics in Base R. mfrow= This stands for “multiple frames filled by row”, which means, put lots of plots on the same row, starting with the plot on the left, then working towards the right as more plots are created. c( The combine function c(…) is used to specify how many rows and columns of graphics should be placed together. 1, This specifies that 1 row of graphics should be produced. 3 This states that 3 columns of graphics should be produced. ) Closing parenthesis for c(…) function. ) Closing parenthesis for par(…) function. plot( This version of plot(…) will actually create several regression diagnostic plots by default. mylm, This is the name of an lm object that you created previously. which= This allows you to select “which” regression diagnostic plots should be drawn. 1 Selecting 1, would give the residuals vs. fitted values plot only. : The colon allows you to select more than just one plot. 2 Selecting 2 also gives the Q-Q Plot of residuals. If you wanted to instead you could just use which=1 to get the residuals vs fitted values plot, then you could use qqPlot(mylm$residuals) to create a fancier Q-Q Plot of the residuals. ) Closing parenthesis for plot(…) function. plot( This version of plot(…) will be used to create a time-ordered plot of the residuals. The order of the residuals is the original order of the x-values in the original data set. If the original data set doesn’t have an order, then this plot is not interesting. mylm The lm object that you created previously. $ This allows you to access various elements from the regression that was performed. residuals This grabs the residuals for each observation in the regression. ) Closing parenthesis for plot(…) function.  Click to Show Output  Click to View Output. Which regression assumption(s) does the residuals versus fitted values plot diagnose? Linear Relation Constance Variance Which regression assumption(s) does the Q-Q Plot of the residuals diagnose? Normal Errors Which regression assumption(s) does the Residuals versus Order plot diagnose? (Remember, this plot can only be created when the data was collected in a specific order.) Independent Errors Plotting the Regression Line Base R ggplot2 To add the regression line to a scatterplot use the abline(...) command: plot( The plot(…) function is used to create a scatterplot with a y-axis (the vertical axis) and an x-axis (the horizontal axis). Y  This is the “response variable” of your regression. The thing you are interested in predicting. This is the name of a “numeric” column of data from the data set called YourDataSet. ~  The tilde “~” is used to relate Y to X and can be found on the top-left key of your keyboard. X,  This is the explanatory variable of your regression. It is the name of a “numeric” column of data from YourDataSet. data= The data= statement is used to specify the name of the data set where the columns of “X” and “Y” are located. YourDataSet This is the name of your data set, like KidsFeet or cars or airquality. ) Closing parenthesis for plot(…) function. abline( This stands for “a” (intercept) “b” (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm This is the name of an lm(…) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(…) function will locate these two values from within mylm and use them to add a line to your current plot(…). ) Closing parenthesis for abline(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm) You can customize the look of the regression line with abline( This stands for “a” (intercept) “b” (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm, This is the name of an lm(…) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(…) function will locate these two values from within mylm and use them to add a line to your current plot(…). lty= The lty= stands for “line type” and allows you to select between 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. 1, This creates a solid line. Remember, other options include: 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. lwd= The lwd= allows you to specify the width of the line. The default width is 1. Using lwd=2 would double the thickness, and so on. Any positive value is allowed. 1, Default line width. To make a thicker line, us 2 or 3… To make a thinner line, try 0.5, but 1 is already pretty thin. col= This allows you to specify the color of the line using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). “someColor” Type colors() in R for options. ) Closing parenthesis for abline(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm, lty=1, lwd=1, col=\"firebrick\") You can add points to the regression with… points( This is like plot(…) but adds points to the current plot(…) instead of creating a new plot. newY  newY should be a column of values from some data set. Or, use points(newX, newY) to add a single point to a graph. ~  This links Y to X in the plot. newX,  newX should be a column of values from some data set. It should be the same length as newY. If just a single value, use points(newX, newY) instead. data=YourDataSet,  If newY and newX come from a dataset, then use data= to tell the points(…) function what data set they come from. If newY and newX are just single values, then data= is not needed. col=“skyblue”, This allows you to specify the color of the points using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). pch=16 This allows you to specify the type of plotting symbol to be used for the points. Type ?pch and scroll half way down in the help file that appears to learn about other possible symbols. ) Closing parenthesis for points(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) points(7,40, pch=16, col=\"skyblue\", cex=2) text(7,40, \"New Dot\", pos=3, cex=0.5) points(dist ~ speed, data=filter(cars, mylm$res > 2), cex=.8, col=\"red\") abline(mylm, lty=1, lwd=1, col=\"firebrick\") To add the regression line to a scatterplot using the ggplot2 approach, first ensure: library(ggplot2) or library(tidyverse) is loaded. Then, use the geom_smooth(method = lm) command: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. YourDataSet,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. X, This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet.  y =  “y= ” declares which variable will become the y-axis of the graphic. Y This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function. + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. + Here the + is used to add yet another layer to ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. There are several other methods that could be used here.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(…) to place more complicated models on the plot.  se = FALSE se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. ) Closing parenthesis for the geom_smooth() function.  Click to Show Output  Click to View Output. ggplot(cars, aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = \"lm\", formula=y~x, se=FALSE) There are a number of ways to customize the appearance of the regression line: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. speed,  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet. y =  “y= ” declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic.  + Here the + is used to add yet another layer to ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(…) to place more complicated models on the plot.  se = FALSE, se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.  size = 2, Use size = 2 to adjust the thickness of the line to size 2.  color = “orange”, Use color = “orange” to change the color of the line to orange.   linetype = “dashed” Use linetype = “dashed” to change the solid line to a dashed line. Some linetype options include “dashed”, “dotted”, “longdash”, “dotdash”, etc. ) Closing parenthesis for the geom_smooth() function.  Click to Show Output  Click to View Output. In addition to customizing the regression line, you can customize the points, add points, add lines, and much more. ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. speed,  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet. y =  “y= ” declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point( geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. size = 1.5, Use size = 1.5 to change the size of the points.  color = “skyblue” Use color = “skyblue” to change the color of the points to Brother Saunders’ favorite color.  alpha = 0.5 Use alpha = 0.5 to change the transparency of the points to 0.5. ) Closing parenthesis of geom_point() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in ways similar to lm(…) to place more complicated models on the plot.  se = FALSE, se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.  color = “navy”, Use color = “navy” to change the color of the line to navy blue.  size = 1.5 Use size = 1.5 to adjust the thickness of the line to 1.5. ) Closing parenthesis of geom_smooth() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_hline( Use geom_hline() to add a horizontal line at a specified y-intercept. You can also use geom_vline(xintercept = some_number) to add a vertical line to the graph. yintercept = Use “yintercept =” to tell geom_hline() that you are going to declare a y intercept for the horizontal line.  75 75 is the value of the y-intercept. , color = “firebrick” Use color = “firebrick” to change the color of the horizontal line to firebrick red. , size = 1, Use size = 1 to adjust the thickness of the horizontal line to size 1.              linetype = “longdash” Use linetype = “longdash” to change the solid line to a dashed line with longer dashes. Some linetype options include “dashed”, “dotted”, “longdash”, “dotdash”, etc. , alpha = 0.5 Use alpha = 0.5 to change the transparency of the horizontal line to 0.5. ) Closing parenthesis of geom_hline function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_segment( geom_segment() allows you to add a line segment to ggplot() by using specified start and end points. x = “x =” tells geom_segment() that you are going to declare the x-coordinate for the starting point of the line segment.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the starting point of the line segment.  y =“y =” tells geom_segment() that you are going to declare the y-coordinate for the starting point of the line segment.  75, 75 is a number on the y-axis of your graph. It is the y-coordinate of the starting point of the line segment.  xend = “xend =” tells geom_segment() that you are going to declare the x-coordinate for the end point of the line segment.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the end point of the line segment.  yend = “yend =” tells geom_segment() that you are going to declare the y-coordinate for the end point of the line segment.  38, 38 is a number on the y-axis of your graph. It is the y-coordinate of the end point of the line segment.                size = 1 Use size = 1 to adjust the thickness of the line segment. , color = “lightgray” Use color = “lightgray” to change the color of the line segment to light gray. , linetype = “longdash” Use *linetype = “longdash* to change the solid line segment to a dashed one. Some linetype options include”dashed”, “dotted”, “longdash”, “dotdash”, etc. ) Closing parenthesis for geom_segment() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_point( geom_point() can also be used to add individual points to the graph. Simply declare the x and y coordinates of the point you want to plot. x = “x =” tells geom_point() that you are going to declare the x-coordinate for the point.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the point.  y = “y =” tells geom_point() that you are going to declare the y-coordinate for the point.  75 75 is a number on the y-axis of your graph. It is the y-coordinate of the point. , size = 3 Use size = 3 to make the point stand out more. , color = “firebrick” Use color = “firebrick” to change the color of the point to firebrick red. ) Closing parenthesis of the geom_point() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_text( geom_text() allows you to add customized text anywhere on the graph. It is very similar to the base R equivalent, text(…). x = “x =” tells geom_text() that you are going to declare the x-coordinate for the text.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the text.  y = “y =” tells geom_text() that you are going to declare the y-coordinate for the text.  84, 84 is a number on the y-axis of your graph. It is the y-coordinate of the text.  label = “label =” tells geom_text() that you are going to give it the label.  “My Point (14, 75)”, “My Point (14, 75)” is the text that will appear on the graph.             color = “navy” Use color = “navy” to change the color of the text to navy blue. , size = 3 Use size = 3 to change the size of the text. ) Closing parenthesis of the geom_text() function.  + The + allows you to add more layers to the framework provided by ggplot().   theme_minimal() Add a minimalistic theme to the graph. There are many other themes that you can try out.  Click to Show Output  Click to View Output. ## `geom_smooth()` using formula = 'y ~ x' Accessing Parts of the Regression Finally, note that the mylm object contains the names(mylm) of mylm$coefficients Contains two values. The first is the estimated \\(y\\)-intercept. The second is the estimated slope. ## (Intercept) speed ## -17.579095 3.932409 mylm$residuals Contains the residuals from the regression in the same order as the actual dataset. ## 1 2 3 4 ## 3.849460 11.849460 -5.947766 12.052234 ## 5 6 7 8 ## 2.119825 -7.812584 -3.744993 4.255007 ## 9 10 11 12 ## 12.255007 -8.677401 2.322599 -15.609810 ## 13 14 15 16 ## -9.609810 -5.609810 -1.609810 -7.542219 ## 17 18 19 20 ## 0.457781 0.457781 12.457781 -11.474628 ## 21 22 23 24 ## -1.474628 22.525372 42.525372 -21.407036 ## 25 26 27 28 ## -15.407036 12.592964 -13.339445 -5.339445 ## 29 30 31 32 ## -17.271854 -9.271854 0.728146 -11.204263 ## 33 34 35 36 ## 2.795737 22.795737 30.795737 -21.136672 ## 37 38 39 40 ## -11.136672 10.863328 -29.069080 -13.069080 ## 41 42 43 44 ## -9.069080 -5.069080 2.930920 -2.933898 ## 45 46 47 48 ## -18.866307 -6.798715 15.201285 16.201285 ## 49 50 ## 43.201285 4.268876 mylm$fitted.values The values of \\(\\hat{Y}\\) in the same order as the original dataset. ## 1 2 3 4 5 ## -1.849460 -1.849460 9.947766 9.947766 13.880175 ## 6 7 8 9 10 ## 17.812584 21.744993 21.744993 21.744993 25.677401 ## 11 12 13 14 15 ## 25.677401 29.609810 29.609810 29.609810 29.609810 ## 16 17 18 19 20 ## 33.542219 33.542219 33.542219 33.542219 37.474628 ## 21 22 23 24 25 ## 37.474628 37.474628 37.474628 41.407036 41.407036 ## 26 27 28 29 30 ## 41.407036 45.339445 45.339445 49.271854 49.271854 ## 31 32 33 34 35 ## 49.271854 53.204263 53.204263 53.204263 53.204263 ## 36 37 38 39 40 ## 57.136672 57.136672 57.136672 61.069080 61.069080 ## 41 42 43 44 45 ## 61.069080 61.069080 61.069080 68.933898 72.866307 ## 46 47 48 49 50 ## 76.798715 76.798715 76.798715 76.798715 80.731124 mylm$… several other things that will not be explained here. Making Predictions predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ) Closing parenthesis for the data.frame(…) function. ) Closing parenthesis for the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12)) 1   29.60981 The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(…) function.  interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. “prediction” This specifies that a prediction interval will be included with the predicted value. A prediction interval gives you a 95% confidence interval that captures 95% of the data, or \\(Y_i\\) values for the specific \\(X\\)-value specified in the prediction. ) Closing parenthesis of the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"prediction\")   fit The “fit” is the predicted value.   lwr The “lwr” is the lower bound.   upr The “upr” is the upper bound. 1 29.60981 In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet. -1.749529 This is the lower bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us the stopping distance could be as quick as -1.749529 feet (or 0 because distance can’t go negative). 60.96915 This is the upper bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us that the actual stopping distance could be as high as 60.96915 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(…) function.  interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. “confidence” This specifies that a confidence interval for the prediction should be provided. This is of use whenever your interest is in just estimating the average y-value, not the actual y-values. ) Closing parenthesis of the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"confidence\")   fit The “fit” is the predicted value.   lwr The “lwr” is the lower bound.   upr The “upr” is the upper bound. 1 29.60981 In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. 24.39514 This is the lower bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is greater than this value. 34.82448 This is the upper bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is less than this value. Finding Confidence Intervals for Model Parameters confint( The R function confint(…) allows you to use an lm(…) object to compute confidence intervals for one or more parameters (like \\(\\beta_0\\) or \\(\\beta_1\\)) in your model. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  level = “level =” tells the confint(…) function that you are going to declare at what level of confidence you want the interval. The default is “level = 0.95.” If you want to find 95% confidence intervals for your parameters, then just run confint(mylm).  someConfidenceLevel someConfidenceLevel is simply a confidence level you choose when you want something other than a 95% confidence interval. Some examples of appropriate levels include 0.90 and 0.99. ) Closing parenthesis for confint(..) function. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.90)     5 % The lower bound of a 90% confidence interval occurs at the 5th percentile. This is because at 90% confidence, 10% is left in the tails, with 5% on each end. The upper bound will thus end at the 95th percentile, hence the 5% and 95% as the column names.   95 % The upper bound of a 90% confidence interval ends at the 95th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -28.914514 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -6.243676 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.235501 and 4.629317. 3.235501 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. 4.629317 This is the upper bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.95)     2.5 % The lower bound of a 95% confidence interval occurs at the 2.5th percentile. This is because at 95% confidence, 5% is left in the tails, with 2.5% on each end. The upper bound will thus end at the 97.5th percentile, hence the 2.5% and 97.5% as the column names for the lower and upper bounds, respectively.   97.5 % The upper bound of a 95% confidence interval ends at the 97.5th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -31.167850 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -3.990340 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.096964 and 4.767853. 3.096964 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.096964 and 4.767853 4.767853 This is the upper bound of the confidence interval. In this case, you can be 95% confident that the slope lies between 3.096964 and 4.767853 Explanation Linear regression has a rich mathematical theory behind it. This is because it uses a mathematical function and a random error term to describe the regression relation between a response variable \\(Y\\) and an explanatory variable called \\(X\\). Expand each element below to learn more. Regression Cheat Sheet (Expand) Term Pronunciation Meaning Math R Code \\(Y_i\\) $Y_i$ “why-eye” The data \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)\\) $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$ YourDataSet$YourYvariable \\(\\hat{Y}_i\\) $\\hat{Y}_i$ “why-hat-eye” The fitted line \\(\\hat{Y}_i = b_0 + b_1 X_i\\) $\\hat{Y}_i = b_0 + b_1 X_i$ lmObject$fitted.values \\(E\\{Y_i\\}\\) $E\\{Y_i\\}$ “expected value of why-eye” True mean y-value \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) $E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i$ <none> \\(\\beta_0\\) $\\beta_0$ “beta-zero” True y-intercept <none> <none> \\(\\beta_1\\) $\\beta_1$ “beta-one” True slope <none> <none> \\(b_0\\) $b_0$ “b-zero” Estimated y-intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) $b_0 = \\bar{Y} - b_1\\bar{X} b_0 <- mean(Y) - b_1*mean(X)$ \\(b_1\\) $b_1$ “b-one” Estimated slope \\(b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\) $b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})} {\\sum(X_i - \\bar{X})^2}$ b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) \\(\\epsilon_i\\) $\\epsilon_i$ “epsilon-eye” Distance of dot to true line \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) $\\epsilon_i = Y_i - E\\{Y_i\\}$ <none> \\(r_i\\) $r_i$ “r-eye” or “residual-eye” Distance of dot to estimated line \\(r_i = Y_i - \\hat{Y}_i\\) $r_i = Y_i - \\hat{Y}_i$ lmObject$residuals \\(\\sigma^2\\) $\\sigma^2$ “sigma-squared” Variance of the \\(\\epsilon_i\\) \\(Var\\{\\epsilon_i\\} = \\sigma^2\\)$Var\\{\\epsilon_i\\} = \\sigma^2$ <none> \\(MSE\\) $MSE$ “mean squared error” Estimate of \\(\\sigma^2\\) \\(MSE = \\frac{SSE}{n-p}\\)$MSE = \\frac{SSE}{n-p}$ sum( lmObject$res^2 ) / (n - p) \\(SSE\\) $SSE$ “sum of squared error” (residuals) Measure of dot’s total deviation from the line \\(SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\)$SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$ sum( lmObject$res^2 ) \\(SSR\\) $SSR$ “sum of squared regression error” Measure of line’s deviation from y-bar \\(SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\)$SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2$ sum( (lmObject$fit - mean(YourData$Y))^2 ) \\(SSTO\\) $SSTO$ “total sum of squares” Measure of total variation in Y \\(SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\)$SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2$ sum( (YourData$Y - mean(YourData$Y))^2 ) \\(R^2\\) $R^2$ “R-squared” Proportion of variation in Y explained by the regression \\(R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\)$R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}$ SSR/SSTO \\(r\\) $r$ “r” Correlation between X and Y. \\(r = \\sqrt{R^2}\\)$r = \\sqrt{R^2}$ sqrt(R^2) \\(\\hat{Y}_h\\) $\\hat{Y}_h$ “why-hat-aitch” Estimated mean y-value for some x-value called \\(X_h\\) \\(\\hat{Y}_h = b_0 + b_1 X_h\\)$\\hat{Y}_h = b_0 + b_1 X_h$ predict(lmObject, data.frame(XvarName=#)) \\(X_h\\) $X_h$ “ex-aitch” Some x-value, not necessarily one of the \\(X_i\\) values used in the regression \\(X_h =\\) some number$X_h = $ Xh = # Confidence Interval “confidence interval” Estimated bounds at a certain level of confidence for a parameter \\(b_0 \\pm t^* \\cdot s_{b_0}\\)b_0 \\pm t^* \\cdot s_{b_0} or \\(b_1 \\pm t^* \\cdot s_{b_1}\\)b_1 \\pm t^* \\cdot s_{b_1} confint(mylm, level = someConfidenceLevel) Parameter Estimate \\(\\beta_0\\) \\(b_0\\) \\(\\beta_1\\) \\(b_1\\) \\(\\epsilon_i\\) \\(r_i\\) \\(\\sigma^2\\) \\(MSE\\) \\(\\sigma\\) \\(\\sqrt{MSE}\\), the Residual standard error The Mathematical Model (Expand) \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)… There are three main elements to the mathematical model of regression. Each of these three elements is pictured below in the “Regression Relation Diagram.” Study both the three bullet points and their visual representations in the plot below for a clearer understanding. The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read more…) The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as “natural law” or “God’s law”. It is some true line that is unknown to us. The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line. The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\). Note: \\(E\\{Y\\}\\) is pronounced “the expected value of y” because, well… the mean is the typical, average, or “expected” value. The dots, i.e., the regression relation plus an error term: \\(Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}} + \\underbrace{\\epsilon_i}_\\text{error term} \\quad \\text{where} \\ \\epsilon_i\\sim N(0,\\sigma^2)\\) (Read more…) This is shown by the dots in the graph below. This is the data. In regression, the assumption is that the y-value for individual \\(i\\), denoted by \\(Y_i\\), was “created” by adding an error term \\(\\epsilon_i\\) to each individual’s “expected” value \\(\\beta_0 + \\beta_1 X_i\\). Note the “order of creation” would require first knowing an indivual’s x-value, \\(X_i\\), then their expected value from the regression relation \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) and then adding their \\(\\epsilon_i\\) value to the result. The \\(\\epsilon_i\\) allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance \\(\\epsilon_i\\) from the line. Note: \\(Y_i\\) is pronounced “why-eye” because it is the y-value for individual \\(i\\). Sometimes also called “why-sub-eye” because \\(i\\) is in the subscript of \\(Y\\). The estimated line, i.e., the line we get from a sample of data. \\(\\underbrace{\\hat{Y}_i}_{\\substack{\\text{estimated mean} \\\\ \\text{y-value}}} = \\underbrace{b_0 + b_1 X_i}_\\text{estimated regression equation}\\) (Read more…) The estimated line is shown by the solid line in the graph below. \\(\\hat{Y}\\) is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation \\(E\\{Y\\}\\). So \\(\\hat{Y}\\) is interpreted as the estimated average (or mean) \\(Y\\)-value for any given \\(X\\)-value. Thus, \\(b_0\\) is the estimated y-intercept and \\(b_1\\) is the estimated slope. The b’s are sample statistics, like \\(\\bar{x}\\) and the \\(\\beta\\)’s are population parameters like \\(\\mu\\). The \\(b\\)’s estimate the \\(\\beta\\)’s. Note: \\(\\hat{Y}_i\\) is pronounced “why-hat-eye” and is known as the “estimated y-value” or “fitted y-value” because it is the y-value you get from \\(b_0 + b_1 X_i\\). It is always different from \\(Y_i\\) because dots are rarely if ever exactly on the estimated regression line. This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line). Something to ponder: The true line, when coupled with the error terms, “creates” the data. The estimated (or fitted) line uses the sampled data to try to “re-create” the true line. We could loosely call this the “order of creation” as shown by the following diagram. par(mfrow=c(1,3), mai=c(.2,.2,.4,.1)) plot(y ~ x, col=\"white\", main=\"A Law is Given\", yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, main=\"Data is Created\", xaxt='n', yaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, xaxt='n', yaxt='n', main=\"The Law is Estimated\") curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) A Law is Given Data is Created The Law is Estimated \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) \\(Y_i = E\\{Y_i\\} + \\epsilon_i\\) \\(\\hat{Y}_i = b_0 + b_1 X_i\\) The true line is the “law”. The \\(Y_i\\) are created by adding \\(\\epsilon_i\\) to \\(E\\{Y_i\\}\\) where \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\). The law is estimated with \\(\\hat{Y}_i\\) which is given with lm(...). Click open the “Code” buttom below to the right to find code that runs a simulation demonstrating this “order of creation”. ## Simulating Data from a Regression Model ## This R-chunk is meant to be played in your R Console. ## It allows you to explore how the various elements ## of the regression model combine together to \"create\" ## data and then use the data to \"re-create\" the line. set.seed(101) #Allows us to always get the same \"random\" sample #Change to a new number to get a new sample n <- 3 #set the sample size X_i <- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45. beta0 <- 3 #Our choice for the y-intercept. beta1 <- .1 #Our choice for the slope. sigma <- 12.5 #Our choice for the std. deviation of the error terms. epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data View(fabData) #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it. fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData. summary(fab.lm) #Summarize your model. plot(y ~ x, data=fabData) #Plot the data. abline(fab.lm) #Add the estimated regression line to your plot. # Now for something you can't do in real life... but since we created the data... abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). legend(\"topleft\", legend=c(\"True Line\", \"Estimated Line\"), lty=c(2,1), bty=\"n\") #Add a legend to your plot specifying which line is which. Interpreting the Model Parameters (Expand) \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted as… The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model. If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\). The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\). It is often misunderstood to be the “average change in y” or just “the change in y” but it is more correctly referred to as the “change in the average y”. To better see this, consider the three graphics shown below. par(mfrow=c(1,3)) hist(mtcars$mpg, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Number of Vehicles\", xlab=\"Gas Mileage (mpg)\", col=\"skyblue\") boxplot(mpg ~ cyl, data=mtcars, border=\"skyblue\", boxwex=0.5, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Number of Cylinders of Engine (cyl)\") plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=\"skyblue\", main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Quarter Mile Time (qsec)\") abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=\"darkgray\") mtext(side=3, text=\"Automatic Transmissions Only (am==0)\", cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=\"gray\") The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09. The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg. The scatterplot on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value. Residuals and Errors (Expand) \\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true error… Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\). The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\). We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summary… Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\). \\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms. Keep in mind the idea that the errors \\(\\epsilon_i\\) “created” the data and that the residuals \\(r_i\\) are computed after using the data to “re-create” the line. Residuals have many uses in regression analysis. They allow us to diagnose the regression assumptions, See the “Assumptions” section below for more details. estimate the regression relation, See the “Estimating the Model Parameters” section below for more details. estimate the variance of the error terms, See the “Estimating the Model Variance” section below for more details. and assess the fit of the regression relation. See the “Assessing the Fit of a Regression” section below for more details. Assessing the Fit of a Regression (Expand) \\(R^2\\), SSTO, SSR, and SSE… Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line. par(mfrow=c(1,3), mai=c(.1,.1,.5,.1)) set.seed(2) x <- runif(30,0,20) y1 <- 2 + 3.5*x + rnorm(30,0,2) y2 <- 2 + 3.5*x + rnorm(30,0,8) y3 <- 2 + 3.5*x + rnorm(30,0,27) plot(y1 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Excellent Fit\") abline(lm(y1 ~ x), col=\"gray\") plot(y2 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Good Fit\") abline(lm(y2 ~ x), col=\"gray\") plot(y3 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Poor Fit\") abline(lm(y3 ~ x), col=\"gray\") A common way to measure the fit of a regression is with correlation. While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\). (If you are a Math 325 student, just stick with correlation for now and skip on to the next section of this Explanation tab. If you are a Math 425 student, it is critical that you come to understand \\(R^2\\) deeply, so read on.) Before you can understand \\(R^2\\), you must understand three important “sums of squares”. (Read more about sums…) A sum is just a fancy word for adding things together. \\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand. So we use the symbol \\(\\Sigma\\) to denote the word “sum”. Further, we use a subscript \\(\\Sigma_{i=1}\\) to state what value the sum is beginning with, and a superscript \\(\\Sigma_{i=1}^6\\) to state the value we are ending at. This gives \\[ \\sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Test your knowledge, do you see why the answer is 6 to the sum below? \\[ \\sum_{i=1}^3 i = 6 \\] Computing sums in R is fairly easy. Type the following codes in your R Console. sum(1:6) #gives the answer of 21 sum(1:3) #gives the answer of 6 However, sums really become useful when used with a data set. Each row of a data set represents an “individual’s” data. We can reference each individual with a row number. In the data below, individual 3, denoted by \\(i=3\\), has a speed of 7 and a dist of 4. pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3) Individual speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 To compute the sum of the speed column, use sum(speed). If we divided this sum by 6, we would get the mean of speed mean(speed). In fact, the two most used statistics mean(...) and sd(...) both use sums. Take a moment to review the formulas for mean and standard deviation. It is strongly recommended that you study the Explanation tab for both as well. We’ll wait. See you back here shortly. … Welcome back. Suppose we let X = speed and Y = dist. Then \\(X_3 = 7\\) and \\(Y_3 = 4\\) because we are accessing row 3 of both the \\(X\\) (or speed) column and \\(Y\\) (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, sum(speed) would be written mathematically as \\(\\sum_{i=1}^6 X_i\\) and sum(dist) would be written as \\(\\sum_{i=1}^6 Y_i\\). Sum of Squared Errors Sum of Squares Regression Total Sum of Squares \\(\\text{SSE} = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\) \\(\\text{SSR} = \\sum_{i=1}^n \\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\) \\(\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\\) Measures how much the residuals deviate from the line. Measures how much the regression line deviates from the average y-value. Measures how much the y-values deviate from the average y-value. Equals SSTO - SSR Equals SSTO - SSE Equals SSE + SSR sum( (Y - mylm$fit)^2 ) sum( (mylm$fit - mean(Y))^2 ) sum( (Y - mean(Y))^2 ) It is important to remember that SSE and SSR split up SSTO, so that \\[ \\text{SSTO} = \\text{SSE} + \\text{SSR} \\] This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works. The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called \\(R^2\\) (“r-squared”). R-Squared (\\(R^2\\)) \\[ \\underbrace{R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}}_\\text{Interpretation: Proportion of variation in Y explained by the regression.} \\] The smallest \\(R^2\\) can be is zero, and the largest it can be is 1. This is because \\(SSR\\) must be between 0 and SSTO, inclusive. Residual Plots & Regression Assumptions (Expand) Residuals vs. fitted-values, Q-Q Plot of the residuals, and residuals vs. order plots… There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate. Each assumption is labeled in the regression equation below. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions. (Read more…) As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not. Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3 The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(\\hat{Y}_i\\). The residuals are the \\(r_i\\). This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable. | Show Examples | (Read more…) The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption. The linear relation is assumed to be satisfied if there are no apparent trends in the plot. The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values. The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance. set.seed(2) X <- rnorm(30,15,3) notLin <- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8)) notLin.lm <- lm(Y~X, data=notLin) set.seed(15) Lin <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) Lin.lm <- lm(Y~X, data=Lin) par(mfrow=c(3,3), mai=c(.25,.25,.25,.25), mgp=c(1,.75,0)) plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Not Linear\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(notLin.lm$fitted.values,notLin.lm$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Good: Linear, Constant Variance\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) set.seed(6) notCon <- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5)) notCon.lm <- lm(Y~X, data=notCon) LinO <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) LinO[1] <- LinO[1]^2 LinO.lm <- lm(Y~X, data=LinO) plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Unconstant Variance\", cex.main=0.95, yaxt='n', xaxt='n', col=\"firebrick\") polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) # plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Outliers\", cex.main=0.95) # abline(h=0) tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(Girth ~ Volume, data=trees[-31,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(Height ~ Volume, data=trees) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) abline(h=0) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals, f=.4) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(weight ~ repwt, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(weight ~ repht, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) abline(h=0) Q-Q Plot of the Residuals: Checks Assumption #2 The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption. | Show Examples | (Read more…) There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot. Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) set.seed(123) tmp <- rnorm(100) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"skyblue\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Normal\", col=\"skyblue\") tmp <- Davis$weight qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Right-skewed\", breaks=15, col=\"firebrick\") par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- rbeta(100, 5,1) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Left-skewed\", breaks=seq(min(tmp),max(tmp), length.out=13), col=\"firebrick\") tmp <- rbeta(100,2,2) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Heavy-tailed\", col=\"firebrick\") Residuals versus Order Plot: Checks Assumption #5 When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated. | Show Examples | (Read more…) Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Good: No Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: General Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ qsec, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: Interesting Patterns\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),]) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Bad: Obvious Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") Problems from Failed Assumptions There are various problems that can arise when certain of the regression assupmtions are not satisfied. Lack of Linearity When the linearity assumption is violated, pretty much everything we obtain from the regression summary is no longer meaningful. The y-intercept estimate can be drastically off from its actual true value. Important model information is lost by trying to use a simple slope term \\(\\beta_1\\) to describe the model with respect to \\(X\\). The residual standard error will be much higher than it otherwise would be because of curvature patterns in the data that the line cannot capture. Thus, R-squared will be lower than it otherwise should be. P-values can become non-significant, when in fact there is a strong pattern in the data, but that pattern just cannot be captured by a simple line. *Normality of the errors is often put into question as well when a simplified line is used to try to capture a more complicated curved model. The plot below demonstrate these difficulties. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 7.5 #True slope beta_2 <- -0.25 #True bend X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors Y_i <- beta_0 + beta_1*X_i + beta_2*X_i^2 + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Non-Linear Relation\") abline(mylm, col=\"gray\") #Add fitted line to plot curve(beta_0 + beta_1*x + beta_2*x^2, col=\"gray\", lty=2, add=TRUE) #Add True line to plot #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (True value:\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (True value:\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (True value:\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Non-normal Error Terms When the normality assumption for the error terms is violated, not all is lost. In fact, the estimate of the slope and intercept are still often fairly meaningful. However, it is unwise to put too much trust in the residual standard error as an estimate of the standard deviation \\(\\sigma\\). This is because the standard deviation in skewed distributions does not carry the same meaning it has in normal distributions. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rchisq(n, 1)*3 - 1 #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Unconstant Variance When variance of the error term changes across the regression, the regression approximates the “average variance” of the errors because the regression is still assuming the variance is constant across the regression. The estimates of the slope and intercept are still typically quite good, and can be used for interpretation. The residual standard error however should not be considered to be meaningful as it will be too large on one end of the regression and too small on the other end. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma + X_i) #normally distributed errors #with increasing variance Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Variance Varies (Non-Constant)\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 13.82 Slope 3.5 3.768 Sigma 2.5 13.02 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", round(mean(sigma + X_i), 2), \", mean)\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Normality Violated As silly as it sounds, if the only problem with the regression is the lack of normality of the error terms, it isn’t all that big of a problem. Depending on how non-normal the residuals appear, there could be some skewing to the residual standard error, but otherwise, the slope and intercept are still interpretable and meaningful. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- runif(n, -sqrt(12*sigma^2)/2, sqrt(12*sigma^2)/2) #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 14.7 Slope 3.5 3.449 Sigma 2.5 2.317 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Independence Assumption Violated While the slope and intercept are often still meaningful when the independence assumption is violated, the residual standard error is unnecessarily large in this case. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, 2.5) + (1:n -n/2)*.5 #normal, but correlated errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Independence Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 16.47 Slope 3.5 3.296 Sigma 2.5 4.819 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Outliers Present While outliers do not violate any of the regression assumptions, they do pose substantial difficulties for the least squares regression estimates of the slope and intercept. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors epsilon_i[3] <- ifelse(X_i[3] < 10, runif(1,25,35), -runif(1,25,35)) #create outlier Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"An Outlier Present\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 13.18 Slope 3.5 3.447 Sigma 2.5 6.389 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) The material below this section is meant for Math 425 students only. Estimating the Model Parameters (Expand) How to get \\(b_0\\) and \\(b_1\\): least squares & maximum likelihood… There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical. The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas. Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 <- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\). When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\). Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i. \\label{exp} \\end{equation}\\] Least Squares{#leastSquares} To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\). \\[ Q = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2 \\] Then we use the function Q as if it were a function of \\(\\beta_0\\) and \\(\\beta_1\\). Ironically, the values of \\(Y\\) and \\(X\\) are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we don’t know are the values of \\(\\beta_0\\) and \\(\\beta_1\\). This least squares applet is a good way to explore how various choices of the slope and intercept yield different values of the “sum of squared residuals”. But it turns out that there is one “best” choice of the slope and intercept that yields a “smallest” value of the “sum of squared residuals.” This best choice can actually be found using calculus by taking the partial derivatives of \\(Q\\) with respect to both \\(\\beta_0\\) and \\(\\beta_1\\). \\[ \\frac{\\partial Q}{\\partial \\beta_0} = -2\\sum (Y_i - \\beta_0 - \\beta_1X_i) \\] \\[ \\frac{\\partial Q}{\\partial \\beta_1} = -2\\sum X_i(Y_i-\\beta_0-\\beta_1X_i) \\] Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize \\(Q\\) for a given set of data. After all the calculations are completed we find the values of the parameter estimators \\(b_0\\) and \\(b_1\\) (of \\(\\beta_0\\) and \\(\\beta_1\\), respectively) are as stated previously. Maximum Likelihood{#mle} The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of \\(\\beta_0\\) and \\(\\beta_1\\) which minime the least squares \\(Q\\) function, we choose the values of \\(\\beta_0\\) and \\(\\beta_1\\) which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the \\(Y_i\\) for all observations \\(i=1,\\ldots,n\\). We can do this rather simply by using the assumption that the errors, \\(\\epsilon_i\\) are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if \\(f(Y_i)\\) denotes the probability density function for \\(Y_i\\), then the joint probability density for all \\(Y_i\\), \\(f(Y_1,\\ldots,Y_n)\\) is given by \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) \\] Since each \\(Y_i\\) is assumed to be normally distributed with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\) (see model (\\(\\ref{model}\\))) we have that \\[ f(Y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{Y_i-\\beta_0-\\beta_1X_i}{\\sigma}\\right)^2\\right]} \\] which provides the joint probability as \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] The likelihood function \\(L\\) is then given by consider the \\(Y_i\\) and \\(X_i\\) fixed and the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) as the variables in the function. \\[ L(\\beta_0,\\beta_1,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] Instead of taking partial derivatives of \\(L\\) directly (with respect to all parameters) we take the partial derivatives of the \\(\\log\\) of \\(L\\), which is easier to work with. In a similar, but more difficult calculation, to that of minimizing \\(Q\\), we obtain the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) which maximize the log of \\(L\\), and which therefore maximize \\(L\\). (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate \\(\\hat{\\sigma}^2\\) of \\(\\sigma^2\\). \\[ \\hat{\\sigma}^2 = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n} \\] Estimating the Model Variance (Expand) Estimating \\(\\sigma^2\\) with MSE… As shown previously in the “Estimating Model Parameters” section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation. Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\). This estimate turns out to be a biased estimator. This means that it is consistently wrong in its estimates of \\(\\sigma^2\\). If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong. This is bad. Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\). Without going into all the details, to “fix” the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently. To find the correct degrees of freedom, we have to notice that the \\(\\hat{Y}_i\\) in the numerator of \\(\\widehat{\\sigma}^2\\) is defined by \\[\\begin{equation} \\widehat{Y}_i = b_0 + b_1X_i \\label{hatY} \\end{equation}\\] From this equation, we notice that two means, \\(\\bar{X}\\) and \\(\\bar{Y}\\), were estimated from the data in order to obtain \\(\\hat{Y}_i\\). (See the formulas for \\(b_0\\) and \\(b_1\\) above, and note how they use both \\(\\bar{X}\\) and \\(\\bar{Y}\\) in their calculation.) Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for \\(\\hat{\\sigma}^2\\) should be \\(n-2\\) instead of \\(n\\). Some incredibly long calculations will show that the “fixed” estimator \\[\\begin{equation} s^2 = MSE = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n-2} \\quad \\text{(Unbiased Estimator of $\\sigma^2$)} \\end{equation}\\] is an unbiased estimator of \\(\\sigma^2\\). Here \\(MSE\\) stands for mean squared error, which is the most obvious name for a formula that squares the errors \\(Y_i-\\hat{Y}_i\\) then adds them up and divides by their degrees of freedom. Similarly, we call the numerator \\(\\sum(Y_i-\\hat{Y}_i)^2\\) the sum of the squared errors, denoted by \\(SSE\\). It is also important to note that the errors are often denoted by \\(r_i = Y_i-\\hat{Y}_i\\), the residuals. Putting this all together we get the following equivalent statements for \\(MSE\\). \\[\\begin{equation} s^2 = MSE = \\frac{SSE}{n-2} = \\frac{\\sum(Y_i-\\widehat{Y}_i)^2}{n-2} = \\frac{\\sum r_i^2}{n-2} \\end{equation}\\] As a final note, even though the expected value \\(E\\{MSE\\} = \\sigma^2\\), which shows \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\), it unfortunately isn’t true that \\(\\sqrt{MSE}\\) is an unbiased estimator of \\(\\sigma\\). This presents a few problems later on, but these are minimal enough that we can overlook the issue and move forward. Transformations (Expand) \\(Y'\\), \\(X'\\), and returning to the original space… Y transformations are denoted by y-prime, written \\(Y'\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\). \\[ Y' = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y' = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y' = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y' = \\log(Y)\\) lm(log(Y) ~ X) 0.25 \\(Y' = \\sqrt(\\sqrt(Y))\\) lm(sqrt(sqrt(Y)) ~ X) 0.5 \\(Y' = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y' = Y\\) lm(Y ~ X) 2 \\(Y' = Y^2\\) lm(Y^2 ~ X) Using “maximum-likelihood” estimation, the Box-Cox procedure can actually automatically detect the “optimal” value of \\(\\lambda\\) to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise. Scatterplot Recognition Box-Cox Suggestion An Example Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\). set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Ya <- 1/sqrt(Y) #1/Y^2 Lam = -2 Yb <- 1/Y #1/Y Lam = -1 Yc <- exp(.02*Y) #log(Y) Lam = 0 Yd <- Y^2 #sqrt(Y) Lam = 0.5 Ye <- Y #Y Lam = 1 Yf <- sqrt(Y) #Y^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Ya ~ X, main=expression(paste(\"Use \", lambda == -2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ya^-2 ~ X)) curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yb ~ X, main=expression(paste(\"Use \", lambda == -1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yb^-1 ~ X)) curve(1/(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yc ~ X, main=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(log(Yc) ~ X)) curve(exp(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yd ~ X, main=expression(paste(\"Use \", lambda == 0.5)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(sqrt(Yd) ~ X)) curve((b[1] + b[2]*x)^2, add=TRUE, col=\"green\", lwd=2) plot(Ye ~ X, main=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ye ~ X)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yf ~ X, main=expression(paste(\"Use \", lambda == 2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yf^2 ~ X)) curve(sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try. par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0)) boxCox(lm(Ya ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -2)), line=.5) boxCox(lm(Yb ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -1)), line=.5) boxCox(lm(Yc ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), line=.5) boxCox(lm(Yd ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0.5)), line=.5) boxCox(lm(Ye ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), line=.5) boxCox(lm(Yf ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 2)), line=.5) An Example Suppose we were running a simple linear regression on the cars dataset. This would be done with the code cars.lm <- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesn’t quite fit the data as well as we would hope. Instead, the data looks a little curved. cars.lm <-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") abline(cars.lm, col=\"gray\") Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful. library(car) boxCox(cars.lm) The output from the boxCox(...) function looks as follows. This plot tells use to use the \\(\\lambda = 0.5\\) transformation, so that \\(Y' = Y^0.5 = \\sqrt{Y}\\). (To see this yourself, click on the “Box-Cox Suggestion” tab above, as well as on the “Scatterplot Recognition” tab.) Now, a transformation regression is performed using sqrt(Y) in place of Y as follows: cars.lm.t <- lm(sqrt(dist) ~ speed, data=cars) summary(cars.lm.t)   Estimate Std. Error t value Pr(> (Intercept) 1.277 0.4844 2.636 0.01126 speed 0.3224 0.02978 10.83 1.773e-14 Then, \\[ \\widehat{Y}_i' = 1.277 + 0.3224 X_i \\] And replacing \\(\\hat{Y}_i' = \\sqrt{\\hat{Y}_i}\\) we have \\[ \\sqrt{\\widehat{Y}_i} = 1.277 + 0.3224 X_i \\] Solving for \\(\\hat{Y}_i\\) gives \\[ \\widehat{Y}_i = (1.277 + 0.3224 X_i)^2 \\] Which, using curve((1.277 + 0.3224*x)^2, add=TRUE) (see code for details) looks like this: plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") curve( (1.277 + 0.3224*x)^2, add=TRUE, col=\"firebrick\") X-Transformations X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them. The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated. The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation. set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Xa <- 1/sqrt(X) #1/X^2 Lam = -2 Xb <- 1/X #1/X Lam = -1 Xc <- exp(.02*X) #log(X) Lam = 0 Xd <- X^2 #sqrt(X) Lam = 0.5 Xe <- X #X Lam = 1 Xf <- sqrt(X) #X^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Y ~ Xa, main=expression(paste(\"Use \", X*minute == X^-2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xa^-2))) curve(b[1] + b[2]*x^-2, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xb, main=expression(paste(\"Use \", X*minute == X^-1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xb^-1))) curve(b[1] + b[2]*x^-1, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xc, main=expression(paste(\"Use \", X*minute == log(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ log(Xc))) curve(b[1] + b[2]*log(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xd, main=expression(paste(\"Use \", X*minute == sqrt(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ sqrt(Xd))) curve(b[1] + b[2]*sqrt(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xe, main=expression(paste(\"Use \", X*minute == X, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ Xe)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xf, main=expression(paste(\"Use \", X*minute == X^2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xf^2))) curve(b[1] + b[2]*x^2, add=TRUE, col=\"green\", lwd=2) Inference for the Model Parameters (Expand) t test formulas, sampling distributions, confidence intervals, and F tests… When fitting the regression model given by the equation \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] to a sample of data, we typically test hypotheses about the parameters \\(\\beta_0\\), \\(\\beta_1\\), or both. Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_0 - \\overbrace{0}^\\text{a number}}{s_{b_0}}\\] This is the formula for the test statistic. It measures how far the estimated y-intercept \\(b_0\\) is from the null hypothesis for \\(\\beta_0\\) in units of “standard errors of \\(b_0\\)”. Thus the division by \\(s_{b_0}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0, it could be any number. \\(H_0: \\beta_1 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_1\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_1 - \\overbrace{0}^\\text{a number}}{s_{b_1}}\\] This is the formula for the test statistic. It measures how far the estimated slope \\(b_1\\) is from the null hypothesis for \\(\\beta_1\\) in units of “standard errors of \\(b_1\\)”. Thus the division by \\(s_{b_1}\\). Though the hypothesized value of \\(\\beta_1\\) is typically 0, it could be any number. Left-tailed p-value = pt(-abs(tvalue), degrees of freedom). Double it to get the two-sided p-value. In R, these values correspond to the output summary of an lm as follows. (Show Example) Consider the cars data in R. Suppose we used the regression model given by \\[ \\underbrace{Y_i}_\\text{Feet to Stop} = \\beta_0 + \\beta_1 \\underbrace{X_{i}}_\\text{mph} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\quad \\sim N(0,\\sigma^2) \\] to model the feet a vehicle (from the 1920’s) takes to stop when traveling at a certain speed (in miles per hour, mph) prior to stopping. When the regression is performed and summarized in R, it is always testing the following two hypotheses: \\[ H_0: \\beta_0 = 0 \\quad\\quad H_0: \\beta_1 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\quad\\quad H_a: \\beta_1 \\neq 0 \\] To perform the test of these hypotheses for the regression stated above, we would run the following codes in R. cars.lm <- lm(dist ~ speed, data=cars) pander(summary(cars.lm)$coefficients) These would produce summary output like the following, but the following output has been labeled with the math notation corresponding to each value. Let’s emphasize what is happening in this summary output table. First, here is how the “t value” is calculated for the “(Intercept)” in the summary table above. \\[ t = \\frac{b_0-0}{s_{b_0}} = \\frac{-17.58 - 0}{6.758} = -2.601 \\] Second, here is a visual representation of how the P-value, the “Pr(>|t|)” as it is called in the summary table above, is calculated for this test statistic. (Click the graph to view an interactive applet showing this calculation.) Notice both ends of the t-distribution are being shaded to compute the P-value because the alternative hypothesis was \\(H_a: \\beta_0 \\neq 0\\). To compute the P-value in R, we use the “percentile function for the t-distribution” called pt( ). This function requires two things, the t-value and the degrees of freedom, in our case pt(-2.601, 48). Note the degrees of freedom (df) are 48 because the sample size is \\(n=50\\) and there are two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) in our regression model. Running this code in R gives: pt(-2.601, 48) = 0.00616 However, note that this value is only half of the actual P-value of 0.0123. To get the “two-sided” P-value (note that our alternative hypothesis used a \\(\\neq\\) symbol) we need to double this left-tailed P-value. 2*pt(-2.601, 48)) = 0.0123 Finally, note that the same procedure can be used to test hypotheses that use a value other than 0 in the null and alternative. For example, to test the hypotheses: \\[ H_0: \\beta_1 = 3 \\\\ H_a: \\beta_1 \\neq 3 \\] Use the t-formula \\[ t = \\frac{b_1 - 3}{s_{b_1}} = \\frac{\\overbrace{3.932}^{b_1} - \\overbrace{3}^{H_0}}{\\underbrace{0.4155}_{s_{b_1}}} = 2.243 \\] then the P-value is calculated in R by 2*pt(-abs(2.243), 48) = 0.0295495 To obtain confidence intervals in R use confint(mylm). Confidence Interval Formula Standard Error \\(\\beta_0\\) \\(b_0 \\pm\\) \\(t^*\\) This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\). Use qt(0.975, df) to get \\(t*\\) in R. \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_0\\). \\(s_{b_0}\\) The standard error of \\(b_0\\), denoted by \\(s_{b_0}\\) is provided in the regression summary output under the column header called “Std. Error” for the “(Intercept)” row of the output. It is calculated using the formula shown below. \\[s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\] This is called the “estimated variance of \\(b_0\\)”. Taking the square root of this number gives the “standard error of \\(b_0\\)”. \\(\\beta_1\\) \\(b_1 \\pm\\) \\(t^*\\) This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\) (sample size - number of parameters in the regression model). Use qt(0.975, df) to get \\(t*\\) in R \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_1\\). \\(s_{b_1}\\) The standard error of \\(b_1\\), denoted by \\(s_{b_1}\\) is provided in the regression summary output under the column header called “Std. It is calculated using the formula shown below. \\[s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\] This is called the “estimated variance of \\(b_1\\)”. Taking the square root of this number gives the “standard error of \\(b_1\\)”. To be more exact, the types of inference we are interested in are the following. Determine if there is evidence of a meaningful linear relationship in the data. If \\(\\beta_1 = 0\\), then there is no relation between \\(X\\) and \\(E\\{Y\\}\\). Hence we might be interested in testing the hypotheses \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) is some hypothesized number. To provide a confidence interval for the true value of \\(\\beta_1\\). Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the sampling distribution of the estimate \\(b_1\\) of the parameter \\(\\beta_1\\). And, while we are at it, we may as well come to understand the sampling distribution of the estimate \\(b_0\\) of the parameter \\(\\beta_0\\). Review sampling distributions from Math 221. Since \\(b_1\\) is an estimate, it will vary from sample to sample, even though the truth, \\(\\beta_1\\), remains fixed. (The same holds for \\(b_0\\) and \\(\\beta_0\\).) It turns out that the sampling distribution of \\(b_1\\) (where the \\(X\\) values remain fixed from study to study) is normal with mean and variance: \\[ \\mu_{b_1} = \\beta_1 \\] \\[ \\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} \\] ## Simulation to Show relationship between Standard Errors ##----------------------------------------------- ## Edit anything in this area... n <- 100 #sample size Xstart <- 30 #lower-bound for x-axis Xstop <- 100 #upper-bound for x-axis beta_0 <- 2 #choice of true y-intercept beta_1 <- 3.5 #choice of true slope sigma <- 13.8 #choice of st. deviation of error terms ## End of Editable area. ##----------------------------------------------- # Create X, which will be used in the next R-chunk. X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) ## After playing this chunk, play the next chunk as well. To see that this is true, consider the regression model with values specified for each parameter as follows. \\[ Y_i = \\overbrace{\\beta_0}^{2} + \\overbrace{\\beta_1}^{3.5} X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\overbrace{\\sigma^2}^{\\sigma=13.8}) \\] Using the equations above for \\(\\mu_{b_1}\\) and \\(\\sigma^2_{b_1}\\) we obtain that the mean of the sampling distribution of \\(b_1\\) will be \\(\\mu_{b_1} = \\beta_1 = 3.5\\) Further, we see that the variance of the sampling distribution of \\(b_1\\) will be \\(\\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} = \\frac{13.8^2}{4.25\\times 10^{4}}\\) Taking the square root of the variance, the standard deviation of the sampling distribution of \\(b_1\\) will be \\(\\sigma_{b_1} = 0.067\\). That’s very nice. But to really believe it, let’s run a simulation ourselves. The “Code” below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of \\(b_1\\) and \\(b_0\\) for that regression. After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of \\(b_0\\) is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of \\(b_1\\) is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of \\(\\beta_0\\) or \\(\\beta_1\\), respectively. Also, the “Std. Error” of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of \\(\\sigma_{b_0}\\) and \\(\\sigma_{b_1}\\), respectively. N <- 5000 #number of times to pull a random sample storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N) for (i in 1:N){ Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model mylm <- lm(Y ~ X) storage_b0[i] <- coef(mylm)[1] storage_b1[i] <- coef(mylm)[2] storage_rmse[i] <- summary(mylm)$sigma } layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3)) Ystart <- 0 #min(0,min(Y)) Ystop <- 500 #max(max(Y), 0) Yrange <- Ystop - Ystart plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), ylim=c(Ystart, Ystop), pch=16, col=\"gray\", main=\"Regression Lines from many Samples (gray lines) \\n Plus Residual Standard Deviation Lines (green lines)\") text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1) text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1) text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1) for (i in 1:N){ abline(storage_b0[i], storage_b1[i], col=\"darkgray\") } abline(beta_0, beta_1, col=\"green\", lwd=3) abline(beta_0+sigma, beta_1, col=\"green\", lwd=2) abline(beta_0-sigma, beta_1, col=\"green\", lwd=2) abline(beta_0+2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0-2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0+3*sigma, beta_1, col=\"green\", lwd=.5) abline(beta_0-3*sigma, beta_1, col=\"green\", lwd=.5) par(mai=c(1,.6,.5,.01)) addnorm <- function(m,s, col=\"firebrick\"){ curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2) lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col) lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col) lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col) lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col) lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col) lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col) lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col) legend(\"topleft\", legend=paste(\"Std. Error = \", round(s,3)), cex=0.7, bty=\"n\") } h0 <- hist(storage_b0, col=\"skyblue3\", main=\"Sampling Distribution\\n Y-intercept\", xlab=expression(paste(\"Estimates of \", beta[0], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m0 <- mean(storage_b0) s0 <- sd(storage_b0) addnorm(m0,s0, col=\"green\") h1 <- hist(storage_b1, col=\"skyblue3\", main=\"Sampling Distribution\\n Slope\", xlab=expression(paste(\"Estimates of \", beta[1], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m1 <- mean(storage_b1) s1 <- sd(storage_b1) addnorm(m1,s1, col=\"green\") t Tests{#tTests} Using the information above about the sampling distributions of \\(b_1\\) and \\(b_0\\), an immediate choice of statistical test to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) can be zero, or any other value, is a t test given by \\[ t = \\frac{b_1 - \\beta_{10}}{s_{b_1}} \\] where \\(s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\). (You may want to review the section “Estimating the Model Variance” of this file to know where MSE came from.) With quite a bit of work it has been shown that \\(t\\) is distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. The nearly identical test statistic for testing \\[ H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_0 \\neq \\beta_{00} \\] is given by \\[ t = \\frac{b_0 - \\beta_{00}}{s_{b_0}} \\] where \\(s^2_{b_0} = MSE\\left[\\frac{1}{n}+\\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\). This version of \\(t\\) has also been shown to be distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. Confidence Intervals Creating a confidence interval for either \\(\\beta_1\\) or \\(\\beta_0\\) follows immediately from these results using the formulas \\[ b_1 \\pm t^*_{n-2}\\cdot s_{b_1} \\] \\[ b_0 \\pm t^*_{n-2}\\cdot s_{b_0} \\] where \\(t^*_{n-2}\\) is the critical value from a t distribution with \\(n-2\\) degrees of freedom corresponding to the chosen confidence level. F tests{#Ftests} Another way to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\quad\\quad \\text{or} \\quad\\quad H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\quad\\quad \\ \\ \\quad \\quad H_a: \\beta_0 \\neq \\beta_{00} \\] is with an \\(F\\) Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an \\(F\\) test is very general and can be used in many places that a t test cannot. In its most general form, the \\(F\\) test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that \\(H_0:\\beta_1=0\\) against the alternative that \\(H_a: \\beta_1\\neq 0\\), we are essentially comparing two models against each other. If \\(\\beta_1=0\\), then the corresponding model would be \\(E\\{Y_i\\} = \\beta_0\\). If \\(\\beta_1\\neq0\\), then the model remains \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the \\(F\\) Test, that the null model (reduced model) have fewer “free” parameters than the alternative model (full model). To demonstrate what we mean by “free” parameters, consider the following example. Say we wanted to test the hypothesis that \\(H_0:\\beta_1 = 2.5\\) against the alternative that \\(\\beta_1\\neq2.5\\). Then the null, or reduced model, would be \\(E\\{Y_i\\}=\\beta_0+2.5X_i\\). The alternative, or full model, would be \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). Thus, the null (reduced) model contains only one “free” parameter because \\(\\beta_1\\) has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two “free” parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model. Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing \\(H_0: \\beta_1=0\\) against \\(H_a:\\beta_1\\neq0\\) we have the partition \\[ \\underbrace{Y_i-\\bar{Y}}_{Total} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{Regression} + \\underbrace{Y_i-\\hat{Y}_i}_{Error} \\] The reason we use \\(\\bar{Y}\\) for the null model is that \\(\\bar{Y}\\) is the unbiased estimator of \\(\\beta_0\\) for the null model, \\(E\\{Y_i\\} = \\beta_0\\). Thus we would compute the following sums of squares: \\[ SSTO = \\sum(Y_i-\\bar{Y})^2 \\] \\[ SSR = \\sum(\\hat{Y}_i-\\bar{Y})^2 \\] \\[ SSE = \\sum(Y_i-\\hat{Y}_i)^2 \\] and note that \\(SSTO = SSR + SSE\\). Important to note is that \\(SSTO\\) uses the difference between the observations \\(Y_i\\) and the null (reduced) model. The \\(SSR\\) uses the diffences between the alternative (full) and null (reduced) model. The \\(SSE\\) uses the differences between the observations \\(Y_i\\) and the alternative (full) model. From these we could set up a General \\(F\\) table of the form   Sum Sq Df Mean Sq F Value Model Error \\(SSR\\) \\(df_R-df_F\\) \\(\\frac{SSR}{df_R-df_F}\\) \\(\\frac{SSR}{df_R-df_F}\\cdot\\frac{df_F}{SSE}\\) Residual Error \\(SSE\\) \\(df_F\\) \\(\\frac{SSE}{df_F}\\) Total Error \\(SSTO\\) \\(df_R\\) Prediction and Confidence Intervals for \\(\\hat{Y}_h\\) (Expand) predict(…, interval=“prediction”)… It is a common mistake to assume that averages (means) describe individuals. They do not. So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line. Interval R Code Math Equation When to Use Prediction predict(..., interval=\"prediction\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\text{Pred}\\ Y}\\) Predict an individual’s value. Confidence predict(..., interval=\"confidence\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\hat{Y}}\\) Estimate location of the mean y-value. predict(mylm, data.frame(XvarName = number), interval=...) For example, consider this graph. Then click here to read about the graph. Notice the three dots above 15 mph in the graph. Each of these dots show a car that was going 15 mph when it applied the brakes. However, stopping distances of the three individual cars differ with one at 20 feet, one at 26 feet and one at 54 feet. The regression line represents the average stopping distance of cars. In this case, cars going 15 mph are estimated to have an average stopping distance of about 40 feet, as shown by the line. But individual vehicles, all going the same speed of 15 mph, varied from stopping distances of 20 feet up to 54 feet! So, to predict that a car going 15 mph will take 41.4 feet to stop, doesn’t tell the whole story. Far more revealing is the complete statement, “Cars going 15 mph are predicted to take anywhere from 10.2 to 72.6 feet to stop, with an average stopping distance of 41.4 feet.” This is called the “prediction interval” and is shown in the graph in blue. It is obtained in R with the codes: cars.lm <- lm(dist ~ speed, data=cars) predict(cars.lm, data.frame(speed=15), interval=\"prediction\") cars.lm <- lm(dist ~ speed, data=cars) pander(predict(cars.lm, data.frame(speed=15), interval=\"prediction\")) fit lwr upr 41.41 10.17 72.64 plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=\"firebrick2\", cex=1.5) cars.lm <- lm(dist ~ speed, data=cars) abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3)) abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2)) abline(v=15, lty=2, col=\"firebrick\") preds <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12) lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8)) lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8)) Now, for the details behind prediction intervals and confidence intervals. Let’s begin by recalling some details (from the section “Inference for the Model Parameters”) about the standard error of the y-intercept, \\(b_0\\). Recall that the y-intercept is the average y-value for the given x-value of \\(x=0\\). Recall further that the formula for the standard error of \\(b_0\\) is given by the formula \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] If we wanted to be more exact with this formula, we would write it as \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{(0-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice the addition of \\((0 - \\bar{X})^2\\) instead of just \\(\\bar{X}^2\\) in the numerator of the right-most part of the equation? This more complete statement obviously would reduce to just \\(\\bar{X}^2\\), but that is only because \\(X=0\\) when we are working with the y-intercept, \\(b_0\\). We could be working with other values of \\(X\\) than just zero. Let’s take a quick detour and talk notation for a second. Typically, \\(X_i\\) and \\(Y_i\\) are used to denote the x-value and y-value of points that are contained in our data set. When we want to reference a point that wasn’t within our original data set, we use the notation \\(X_h\\) and \\(Y_h\\). (The letter h is close to i, but different from i, so why not. There is really no other reason to use h.) Thus, \\(Y_h\\) is the y-value for the \\(X_h\\) x-value, neither of which were included in our original regression of \\(X_i\\)’s and \\(Y_i\\)’s. Now, back to the previous discussion. If \\(X_h = 0\\), then \\(\\hat{Y}_h\\) is the y-intercept, so \\(\\hat{Y}_h = b_0\\) when \\(X_h=0\\). So, we could write, \\[ s^2_{\\hat{Y}_h} = MSE\\left[\\frac{1}{n} + \\frac{(X_h-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice how the \\(b_0\\) in \\(s_{b_0}\\) was replaced with \\(\\hat{Y}_h\\) to get \\(s_{\\hat{Y}_h}\\) and the 0 in \\((0 - \\bar{X})^2\\) was replaced with \\(X_h\\) to get \\((X_h - \\bar{X})^2\\)? Interesting. We now have a formula that would give us the standard error of \\(\\hat{Y}_h\\) for any \\(X_h\\) value, not just \\(X_h = 0\\), or the y-intercept, \\(b_0\\). That is fantastic. It would look like this if plotted. Notice how the gray region is showing the standard error for each \\(\\hat{Y}_h\\) value? (It is technically showing the confidence interval for \\(E\\{Y_h\\}\\) at every possible \\(X_h\\) value, but that is just \\(\\hat{Y}_h \\pm t^* \\cdot s_{\\hat{Y}_h}\\).) ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + theme_bw() ## `geom_smooth()` using formula = 'y ~ x' Confidence Interval for \\(\\hat{Y}_h\\) \\[ \\hat{Y}_h \\pm t^* s_{\\hat{Y}_h} \\quad \\text{where} \\ s_{\\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] The confidence interval is a wonderful tool for estimating \\(E\\{Y_h\\}\\), the “true” average y-value for a given x-value of \\(X_h\\). However, it is not valuable for predicting an individual dot, or \\(Y_h\\) value. Notice how few of the dots of the regression are actually contained within the confidence interval band in the plot? The confidence interval does not really predict where the dots will land, just where the average y-value is located for each x-value. Remember the 68-95-99.7 Rule of the normal distribution? If not, here is a link back to that concept in the Math 221 textbook. This rule states that roughly 95% of data, when normally distributed, will be between \\(z=-2\\) and \\(z=2\\) standard deviations from the mean. So, is going two “residual standard errors” to both sides of the regression line enough to capture 95% of the data? The answer is, not quite. The reason for this is because our knowledge of where the true mean lies is uncertain. (Notice the confidence interval band shown in the plot.) However, adding two standard errors to the edges of the confidence band would get us in the right place. In other words, there are two sources of variability at play here, (1) our uncertaintity in where the regression line is sitting, and (2) the natural variability of the data points around the line. Thus, the “prediction interval” requires accounting for both of these sources of variability to produce the following equation. Prediction Interval for \\(Y_h\\) \\[ \\hat{Y}_h \\pm t^* s_{Pred \\hat{Y}_h} \\quad \\text{where} \\ s_{Pred \\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + 1 + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] This formula provides a useful band for identifying a region where we are 95% confident that a new observation for \\(Y_h\\) will land, given the value of \\(X_h\\). It looks as follows. Notice the prediction interval is much wider than the confidence interval. This is because data varies far more than do means. Prediction is for where the individual data points will land, confidence is for where the mean will land. cars.lm <- lm(dist ~ speed, data=cars) predy <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + geom_segment(aes(x=15, xend=15, y=predy[2], yend=predy[3]), lwd=4, color=rgb(.5,.7,.5,.01)) + geom_point(aes(x=15, y=predy[1]), cex=2, color=\"skyblue\", pch=15) + theme_bw() Lowess (and Loess) Curves (Expand) A non-parametric approach to estimating \\(E\\{Y_i\\}\\)… Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value. Using Base R # Select the relevant columns and remove NA values air2 <- airquality %>% dplyr::select(Temp, Ozone) %>% # Use dplyr's select function explicitly drop_na() # Drop rows with NA values in Temp or Ozone plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") lines(lowess(air2$Ozone, air2$Temp), col=\"firebrick\") Using ggplot2 # Your dataset (air2) should have been created earlier, ensuring no NAs air2 <- na.omit(airquality[c(\"Temp\", \"Ozone\")]) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=\"darkgray\") + # Add points with dark gray color geom_smooth(se=FALSE, method=\"loess\", method.args = list(degree=1)) + # Lowess curve, no standard error shading theme_bw() # Black and white theme ## `geom_smooth()` using formula = 'y ~ x' # Optionally, allow for predictions as well as the graph: # Uncomment the following lines if you want predictions as well: # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fitted, x=Ozone)) # Use fitted values from loess Advantages Disadvantages Quick. Good at ignoring outliers. Good at capturing the general pattern in the data. Good for making predictions within the scope of the data. No mathematical model. Not interpretable. No p-values. No adjusted R-squared. How it Works The Lowess curve localizes the regression model to a “neighborhood” of points, and then joins these localized regressions together into a smooth line. It minimizes the effect of outliers, and let’s the data “speak for itself”. As a downside, it is not interpretable, and has no final way to write the model mathematically. All the same, it is a very powerful tool for identifying an appropriate model, or verifying the fit of a model, or making predictions when no reasonable model does an adequate job. Study this graphic and the explanations below to learn how it works. Recommendation: run the code in this “Code” chunk to the right in your Console, and flip through the resulting graphics. X <- cars$speed Y <- cars$dist X <- X[!is.na(X) & !is.na(Y)] Y <- Y[!is.na(X) & !is.na(Y)] f <- 1/2 n <- length(X) lfit <- rep(NA,n) for (xh in 1:n){ xdists <- X - X[xh] nn <- floor(n*f) r <- sort(abs(xdists))[nn] xdists.nbrhd <- which(abs(xdists) < r) w <- rep(0, length(xdists)) w[xdists.nbrhd] <- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w), col=rgb(.2,.2,.2,.3), cex=1.5, yaxt='n', xaxt='n', xlab=\"\", ylab=\"\") points(Y[xh] ~ X[xh], pch=16, col=\"orange\") lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2)) cat(\"\\n\\n\") readline(prompt=paste0(\"Center point is point #\", xh, \"... Press [enter] to continue...\")) MADnotThereYet <- TRUE count <- 0 while(MADnotThereYet){ readline(prompt=paste0(\"\\n Adjusting line to account for outliers in the y-direction... Press [enter] to continue...\")) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"wheat\", add=TRUE) MAD <- median(abs(lmc$res)) resm <- lmc$res/(6*MAD) resm[resm>1] <- 1 bisq <- (1-resm^2)^2 w <- w*bisq obs <- coef(lmc) lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) count <- count + 1 if ( (sum(abs(obs-lmc$coef))<.1) | (count > 3)) MADnotThereYet <- FALSE } curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"green\", add=TRUE) points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=\"green\") readline(prompt=paste0(\"\\n Use final line to get fitted value for this point... Press [enter] to continue to next point...\")) lfit[xh] <- predict(lmc, data.frame(X=X[xh])) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") if (xh == n){ readline(prompt=paste0(\"\\n Press [enter] to see actual Lowess curve...\")) lines(lowess(X,Y, f=f), col=\"firebrick\") legend(\"topleft\", bty=\"n\", legend=\"Actual lowess Curve using lowess(...)\", col=\"firebrick\", lty=1) } } Select a fraction of the data to use for the “neighborhood” of points (shown in blue in the graph above). The lowess function in R uses “f=2/3” and the loess function uses “span=0.75” for this value, which selects the nearest two-thirds or 75% of the data, respectively, depending on which function you use. For this example, we set the fraction of points at 50%. Both functions can be set to whatever you want. Pick any point in the regression, eventually selecting all points one at a time. The selected point becomes the “center” of a “neighborhood” of points surrounding it. In this example, the center point is in orange, and the neighboring points are in blue. Use the points within the neighborhood to fit a regression line. However, make the regression depend most on points closest to “center” and least on points furthest from “center.” This is called a weighted regression. Weights are decided according to what is called the tricubic weight function, so that the weight \\(w\\) given to point \\(j\\) of the neighborhood of points is defined by \\[ w_j = \\left(1- \\left( \\frac{|X_c - X_j|}{\\max_k |X_c - X_k|}\\right)^3\\right)^3 \\] where \\(X_c\\) is the x-value of the “center” dot and \\(X_j\\) is the x-value of any other dot in the neighborhood. The fitted-value of \\(\\hat{Y}_c\\) is obtained for the center point \\(X_c\\) of the current regression. This point is used as the Lowess (or Loess) curve’s value at that particular x-value. Well, almost. It’s a first guess at where this value will end up, but there’s a little more to the algorithm before we are done. Initial guesses for each of these fitted values are obtained for each point in the regression. Now each local regression for each neighborhood is re-run a few times in such a way the the effect of outliers is minimized. The final line for each neighborhood is obtained by the following steps. Compute all residuals for points in the neighborhood of the current regression, denoted by \\(r_i\\). Then compute the MAD, median absolute deviation, of the residuals \\(MAD = \\text{median} (|r_1|, |r_2|, \\ldots)\\). Divide all residuals by 6 times the MAD: \\(u_i = r_i/(6\\cdot MAD)\\) (If \\(r_i > 6\\cdot MAD\\) then set \\(u_i = 0\\).) Compute what are called bisquare weights using the formula: \\(b_i = (1 - u_i^2)^2\\) Perform a regression using the weights \\(w_i = w_i b_i\\) Repeat the above process with the new weights \\(w_i\\) until the weights stop changing very much. The final fitted values for each \\(X\\)-value in the regression are obtained from the final regression line for each neighborhood. These fitted values make up the Lowess (or loess) curve. Note that the default of the loess function in R is to use quadratic regressions in each neighborhood instead of linear regressions. This can be controlled with the loess option of “degree=2” (quadratic fits) or “degree = 1”. In the lowess function only a linear regression in each neighborhood is allowed. Examples: bodyweight, cars Quantitative Y | Multiple X Graphics Y is a single quantitative variable of interest, like height. While we could use an X1 of “shoe-size” to explain height, we might also want to use a second x-variable, X2, like “gender” to help explain height. Further x-variables could also be used. Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a student’s final exam grade? How do factors like age, weight, exercise level, and diet impact a person’s blood pressure? Examples: Is one Navy base more prone to heatwaves than the other? The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH) Tests Multiple Linear Regression Multiple regression is a way to understand how different factors affect something we’re interested in. It’s like having more than one clue to solve a mystery. Here’s what you need to know: We can use several different pieces of information (called variables) to predict an outcome. There are many ways to combine these pieces of information. We start with simple combinations and build up to more complex ones, kind of like building with Lego blocks. This helps us make better predictions and understand complex situations more clearly. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model is a bit like using the same ingredient twice in a recipe, but in different ways. It uses the same factor (\\(X_i^2\\)) in two different forms: Once in its regular form (\\(\\beta_1 X_i\\)) Once squared (\\(\\beta_2 X_i^2\\)) This squared part is what we call the “quadratic” term. It helps us describe relationships that aren’t just straight lines, but have curves to them. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and “steepness” of the Model: negative values face down, positive values face up; large values imply “steeper” parabolas and low values imply “flatter” parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)’s explanation. An Example Using the airquality data set, we run the following “quadratic” regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our “quadratic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. Temp Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These “Estimates” can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(Month^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of Month from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, )   Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will “open down” (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be “month zero.” Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the “cubic” term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the “base slope coefficient” and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following “cubic” regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our “cubic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. uptake Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^3) \\(X_{i}^3\\), where the function I(…) protects the cubing of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will “open up”. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The “two-lines” model is a way to compare two different groups in statistics using numbers. Here’s how it works: It uses two types of numbers: A regular number that can be any value (\\(X_{1i}\\)) A special number that’s either 0 or 1 (called an “indicator variable”, \\(X_{2i}\\)) The special number (\\(X_{2i}\\)) helps us turn group information (like “Group A” or “Group B”) into numbers we can use in calculations. This model creates two separate lines on a graph: One line for Group A One line for Group B By using this method, we can easily see and compare how the two groups are different from each other. Think of it like having two recipes for the same dish, but with slightly different ingredients. This model helps us see how those small changes affect the final result. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the “base-line” of the model, the “Group 0” line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the “base-line” line. \\(\\beta_3\\) Called the “interaction” term. Controls the change in the slope for the second line in the model as compared to the slope of the “base-line” line. An Example Using the mtcars data set, we run the following “two-lines” regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our “two-lines” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. mpg Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) (beta zero): This is like the starting point. It’s estimated by something called the “Intercept”. \\(\\beta_1\\) (beta one): This shows how one thing changes when another thing (called “qsec”) changes. It’s estimated to be 1.439. \\(\\beta_2\\) (beta two): This shows how something else (called “am”) affects the overall result. It’s estimated to be -14.51. \\(\\beta_3\\) (beta three): This shows how “qsec” and “am” work together to affect the result. It’s estimated to be 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called “3D” regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to “bend”. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the “Temp” direction is estimated to be 2.659 and the slope in the “Month” direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction term’s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the “slopes” in each direction to change, creating a “curved” surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called “HD”, or “High Dimensional”, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isn’t really even possible to “mentally connect” with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)’s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the “Wind” direction is estimated to be -3.334. The slope in the “Temp” direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the “Solar.R” direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here. R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the “Overview” section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the “column (c) bind” function and it joins together things as columns. Res =  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals.  panel=panel.smooth,  This places a lowess smoothing line on each scatterplot. col =  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(“color1”,“color2”, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for “linear model.” Y Y must be a “numeric” vector of the quantitative response variable.  ~  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats “numeric” variables as quantitative and “character” or “factor” variables as qualitative. R will automatcially recode qualitative variables to become “numeric” variables using a 0,1 encoding. See the Explanation tab for details.  + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details.  + …, * ... emphasizes that as many explanatory variables as are desired can be included in the model.  data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(…) function displays the results of an lm(…) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(…) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -4.3818 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -2.2696 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.1344 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   1.7058 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   5.8752 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero.   2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (26.6248479) by its standard error (2.1829432). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a “star”. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + …).   -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model.   0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” by its standard error. It gives the “number of standard errors” away from zero that the “estimate” has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + …).   5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\).   2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot “.” implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like.   0.0004029 This is the estimate of the coefficient of the interaction term.   0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data.  0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that “at least one is not.”  34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom.  on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero.  p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that “at least one” of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the “Overview” sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =… Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “prediction”) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “confidence”) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BIC… There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (“R-squared”). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The “simplest” but “best” model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it “an information criterion (AIC)” when he published the method and other people later began calling it the “Akaike Information Criterion.”) Model Selection (Expand) pairs plots, added variable plots, and pattern recognition… Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots {.tabset .tabset-pills} A useful visualization tool for model selection is the “pairs plot.” This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice that… the y-axis of each plot is found by locating the variable name (like “mpg”) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like “disp”) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model {.tabset .tabset-pills} Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldn’t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Let’s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a model’s ability to generalize to new data… The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesn’t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, let’s remind ourselves why we use regression models in the first place. The main goal is to capture the “essence” of the data. In other words, the general pattern is what we are after. We want a model that tells us how “all such” data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the “complicated model” is overfitting the original data. It does not capture the “essence” of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-value… The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the model… The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions{#check} The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read more…) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) Outlier Analysis (Expand) Cook’s Distances and Leverage Values… The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs. fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cook’s Distances The idea behind Cook’s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article “Detection of Influential Observation in Linear Regression” (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, let’s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2))   1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the “differences” in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cook’s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cook’s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cook’s Distances using the code cooks.distance(lmObject). Also, a graph of Cook’s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of “leverage” and is “pulling” the regression toward itself. A value near 0 implies the point is just “one of many” and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that “minimize” the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)’s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the “hat matrix” \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the “leverage values” and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a “lot of pull,” and values close to 0 represent “little pull.” In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with “lots of leverage” and a large “Cook’s Distance” are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regression… Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class. Examples: Civic Vs Corolla cadillacs Binomial Y | Quantitative X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t. We might use an explanatory variable X of “height” to see if taller students are more likely to get an A in Math 325 than shorter students. (They aren’t, if you were wondering.) Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1’s and 0’s (this does happen or this doesn’t happen): Does the width of a child’s foot predict whether or not they are right-handed? What is the probability the cat adopted is a male based on how many days it took them to get adopted? Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted? (Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\") Tests Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The “P” stands for “Probability that…” \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1… This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)… in other words, the “|” says “given” and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the “natural constant” number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14…) Part What it means \\(P(Y_i = 1| x_i)\\) - The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s) - The symbol used to represent the notation in logisitc regression is \\(\\pi_i\\) \\(Y_i\\) (Responsive Variable) Describes the probability of $Y_i = $ 1 or 0 for a given scenario. \\(x_i\\) (Explanatory variable) Describes the observed value and how it effects the \\(Y_i\\) \\(\\frac{e^{\\text{stuff}}}{1+e^{\\text{stuff}}}\\) The content for “stuff” being simple linear regression of the form \\(\\beta_0 + \\beta_{1x_i}\\) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -1.5651 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.6648 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -0.2460 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   0.7276 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   2.2691 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The test statistic is a regular old z-score. It is most reliable when the sample size is “large.” It is a measurement of the number of standard errors the estimate is from 0.   Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds.   1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, …).   -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds.   0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a “star”. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about.   Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\).  43.230  on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates.  29.732 This can be calculated by sum(log(myglm$res^2)).  on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, “A version of Akaike’s An Information Criterion…” The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model.  33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 ## disp -0.014604 0.005168 -2.826 0.00471 ## ## (Intercept) * ## disp ** ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0’s or 1’s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == “B” or height < 68. In other words, Y needs to be a collection of 0’s and 1’s.  ~  The tilde is read “Y on X.” X, X is some quantitative variable.  data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(…) function. curve( A function in R that draws a “curve” on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the “Intercept” estimate from your logistic regression summary output.  \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case “x” in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula.  exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator.  add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(…) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set.  aes( The aes(…) function stands for “aesthetics” and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The plus sign adds a new layer to the ggplot.   geom_point( Tells the plot to add the physical geometry of “points” to the plot. ) Closing parenthesis for the geom_point() function.  + Add another layer to the plot.   geom_smooth( Add a smoothing line to the graph. method=“glm”, This adds a general linear model to the graph. method.args = list(family=“binomial”), This tells the method=“glm” to choose specifically the “binomial” model, otherwise known as the “logistic regression” model.  se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function.  + Add another layer to the ggplot.   theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),  The xVariableName is the same one you used in your glm(y ~ x, …) statement for “x”. Input any desired value for the “someNumericValue” spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016 Explanation Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal). The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\). Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page. Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful. Checking Model Assumptions {#diagnostics} The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit. Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease. Binomial Y | Multiple X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t. We might use an explanatory variable X1 of “height” and a second explanatory variable X2 of “gender” to try to predict whether or not a student will get an A in Math 325. Test(s) used for this category: Multiple Logistic Regression Model Tests Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two. Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. …, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -2.9446 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.2364 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.0000 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q   0.2776 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   1.9968 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The Z-score testing the hypothesis that \\(\\beta_j=0\\)   Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, …).   0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\).   1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\).   4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\).   3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\).   0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\).   0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a “star”. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\).   471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option.   Null Deviance: The deviance of the null model.  800.44 In this case, the null deviance is 800.44  on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression.  256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit.  on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model.  272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities ## numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) -4.97603 0.65316 -7.618 ## Time 0.39111 0.04979 7.854 ## Diet2 0.58283 1.04061 0.560 ## Diet3 -8.44476 4.32324 -1.953 ## Diet4 -67.41995 3768.10023 -0.018 ## Time:Diet2 0.02391 0.08679 0.275 ## Time:Diet3 1.20955 0.51249 2.360 ## Time:Diet4 8.83168 471.01259 0.019 ## Pr(>|z|) ## (Intercept) 2.57e-14 *** ## Time 4.02e-15 *** ## Diet2 0.5754 ## Diet3 0.0508 . ## Diet4 0.9857 ## Time:Diet2 0.7830 ## Time:Diet3 0.0183 * ## Time:Diet4 0.9850 ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, …), X2 = c(Value 1, Value 2, …), …) command. You should see the GSS example file for an example of how to use this function. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the “b” vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(“SomeColor”,“DifferentColor”,…) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(…) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==“B”.  ~  The formula operator in R. X1, The first x-variable in your glm code.  data = YourDataSet, Specify the name of your data set.  pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(…) function computes e^(stuff) in R and stands for the “exponential function.” (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(…) function requires that you call the x-variable “x” although you can change this behavior using xname=“SomeOtherName” if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)).  col = palette()[1], Pull the first color from the color palette for the first curve.  add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters.  col = palette()[2], Set the color of this second curve to the second color in the palette.  add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. “topright”, Typically the legend is placed in the top-right corner of the graph. But it could be placed “top”, “topleft”, “left”, “bottomleft”, “bottom”, “center”, “right”, or “bottomright”.  legend = Why you have to write legend again, no one knows, but you do. c(“Lable 1”, “Label 2”), These are the values that will appear in the legend, one entry on each line of the legend.  col = palette(), This specifies the colors of the symbols in the legend. First color goes with “Label 1”, and second goes with “Label 2” and so on.  lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist.  bty = Specifies the “box type” (bty) that should be drawn around the legend. ‘n’) By placing a “no” option (‘n’) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot.  aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + Add a layer to the ggplot.   geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function.  + Add a layer to the ggplot.   geom_smooth(method=“glm”, method.args=list(family=“binomial”), se=FALSE, Add the logistic regression curve to the plot.   aes(color=“X2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function.  + Add a layer to the plot.   theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x' Explanation Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative. The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations. Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant. Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression. Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\). Categorical Y | Categorical X Graphics Y is a single categorical variable of interest, like gender. X is another categorical variable of interest, like “hair color”. This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender. Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year? Whats the probability that you have brown hair and blue eyes? Tests Add your own notes about appropriate inferential procedures for this type of data here.",
    "sentences": ["One-way ANOVA Each experimental unit is assigned to exactly one factor-level combination.", "Another way to say this is “one measurement per individual” (no repeated measures) and “equal numbers of individuals per group”.", "Overview An ANOVA is only appropriate when all of the following are satisfied.", "The sample(s) of data can be considered to be representative of their population(s).", "The data is normally distributed in each group.", "(This can safely be assumed to be satisfied when the residuals from the ANOVA can be assumed to be normally distributed when seen in a Q-Q Plot.) The population variance of each group can be assumed to be the same.", "(This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out in a Residuals versus fitted-values plot.) comparing means Hypotheses For a One-way ANOVA \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] Mathematical Model A typical model for a one-way ANOVA is of the form \\[ Y_{ij} = \\mu_i + \\epsilon_{ij} \\] where \\(\\mu_i\\) is the mean for level (group) \\(i\\), and \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) is the error term for each point \\(j\\) within level (group) \\(i\\).", "R Instructions Console Help Command: ?aov() myaov is some name you come up with to store the results of the aov() test.", "Y must be a “numeric” vector of the quantitative response variable.", "X is a qualitative variable (should have class(X) equal to factor or character."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 91,
    "title": "📍 Quantitative Y | Multiple Categorical\r\nX",
    "url": "index.html#quantitativeymultiplecategoricalx",
    "content": "Quantitative Y | Multiple Categorical\r\nX Graphics Y is a single quantitative variable of interest, like “heights” of BYU-Idaho students. X1 is a categorical (qualitative) variable like gender, with levels “boy” and “girl.” X2 is another categorical (qualitative) variable like “Classrank” with levels “Freshman”, “Sophomore”, and “Junior”. Of course, both X1 and X2 can have as many levels as you want to work with, but must have at least two levels each. Test(s) used for this category: Two-Way ANOVA (means) Used to answer THREE questions: the effects of factor 1 on the quantitative variable the effects of factor 2 on the quantitative variable the interaction between the two factors best paired with numerical summary looking at mean Analysis : Two-way ANOVA: Final Beginning Algebra Grades Question Examples: Based on your p-values and these graphs, which of the following is a correct conclusion to reach? xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1))) ANSWER : Among fourth graders, right handed boys have longer feet on average than left handed boys while the opposite is true for girls. Does the amount a student participates in the course affect the average final Beginning Algebra grades? Does the semester in which a student takes Beginning Algebra affect their average final grade? Does a student’s participation level vary depending on the semester? (Alternatively, does the semester influence a student’s participation?) BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") BAFinalGrades$Semester <- factor(BAFinalGrades$Semester, levels = c(\"Fall\",\"Winter\",\"Spring\")) ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=1)) + geom_point(color=\"firebrick1\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Spring Semester Seems to Have Higher Final Grades\", x=\"Semester\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades$Participation <- factor(BAFinalGrades$Participation, levels = c(\"Low\",\"Moderate\",\"High\")) ggplot(BAFinalGrades, aes(x=Participation, y=FinalGrade, group=1)) + geom_point(color=\"darkred\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Participation Seems to Lead to Higher Grades\", x=\"Participation\", y=\"Beginning Algebra Final Grades\") + theme_minimal() ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=Participation, color=Participation)) + geom_point() + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Beginning Algebra Seems Better during the Spring Semester\", x=\"BYU-Idaho Semesters\", y=\"Beginning Algebra Final Grades\") + theme_minimal() Tests Two-way ANOVA Overview A two-way ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses. Writing out the hypotheses can be very involved depending on whether you use the official “effects model” notation (very mathematically correct) or a simplified “means model” notation (which isn’t very mathematically correct, but gets the idea across in an acceptable way). Means Model Effects Model The first set of hypotheses are a “one-way” set of hypotheses for the first factor of the ANOVA. Factor: X1 with say, levels \\(A\\) and \\(B\\). \\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a “one-way” set of hypotheses, but for the second factor of the ANOVA. Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\mu_C = \\mu_D = \\mu_E = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one}\\ i\\in\\{1=C,2=D,3=E\\} \\] The third set of hypotheses are the most interesting hypotheses in a two-way ANOVA. These are called the interaction hypotheses. They test to see if the levels of one of the factors, say \\(X1\\), impact \\(Y\\) differently for the differing levels of the other factor, \\(X2\\). The hypotheses read formally as \\[ H_0: \\text{The effect of the first factor on Y} \\\\ \\text{is the same for all levels of the second factor.} \\] \\[ H_a: \\text{The effect of the first factor on Y is not the same} \\\\ \\text{for all levels of the second factor.} \\] A mathematically correct way to state the two-way ANOVA model is with the equation \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] In this model, \\(\\mu\\) is the grand mean (which is the average Y-value ignoring all information contained in the factors); \\(\\alpha_i\\) is the first factor \\(X1\\) with levels \\(A\\) and \\(B\\) (though there could be more levels in \\(X1\\) depending on your data); \\(\\beta_j\\) is the second factor with levels \\(C\\), \\(D\\), and \\(E\\) (though there could be fewer or more levels to this factor depending on your data); \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors which has \\(2\\times3=6\\) (may differ for your data) levels; and \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) is the normally distributed error term for each point \\(k\\) found within level \\(i\\) of \\(X1\\) and level \\(j\\) of \\(X2\\). This model allows us to more formally state the hypotheses as First factor \\(X1\\) having say, levels \\(A\\) and \\(B\\). \\[ H_0: \\alpha_A = \\alpha_B = 0 \\] \\[ H_a: \\alpha_i \\neq 0 \\ \\text{for at least one}\\ i\\in\\{1=A,2=B\\} \\] Second factor \\(X2\\) with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\beta_C = \\beta_D = \\beta_E = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j\\in\\{1=C,2=D,3=E\\} \\] Does the effect of the first factor (\\(X1\\)) change for the different levels of the second factor (\\(X2\\))? In other words, is there an interaction between the two factors \\(X1\\) and \\(X2\\)? \\[ H_0: \\alpha\\beta_{ij} = 0 \\ \\text{for all } i,j \\] \\[ H_a: \\alpha\\beta_{ij} \\neq 0 \\ \\text{for at least one } i,j \\] R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ X1+X2+X1:X2, data=YourDataSet) Perform the ANOVA summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. Note that factors X3, X4, and so on could also be added + to the model if desired, but this would create a three-way, or four-way ANOVA model, and so on. X1:X2 denotes the interaction of the factors X1 and X2. It is not required, but is usually included. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. warp.aov <-  Saves the results of the ANOVA test as an object named ‘warp.aov’. aov( ‘aov()’ is a function in R used to perform the ANOVA. breaks \\(Y\\) is ‘breaks’, which is a numeric variable from the warpbreaks dataset.  ~  ‘~’ is the tilde symbol used to separate the left- and right-hand side in a model formula. wool  The first factor \\(X1\\) is ‘wool’, which is a qualitative variable in the warpbreaks dataset. In this case, wool is a factor with two levels. Use str(warpbreaks) to see this. + tension  The second factor \\(X2\\) is ‘tension’, which is another qualitative variable in the warpbreaks dataset. In this case, tension is a factor with three levels. Use str(warpbreaks) to see this. + wool:tension, The interaction of the two factors: wool and tension.  data = warpbreaks) ‘warpbreaks’ is a dataset in R. summary( ‘summary()’ shows the results of the ANOVA. warp.aov) ‘warp.aov’ is the name of the ANOVA.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## wool 1 451 450.7 3.765 0.058213 ## tension 2 2034 1017.1 8.498 0.000693 ## wool:tension 2 1003 501.4 4.189 0.021044 ## Residuals 48 5745 119.7 ## ## wool . ## tension *** ## wool:tension * ## Residuals ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow = c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. plot( ‘plot’ is a R function for the plotting of R objects. warp.aov, ‘warp.aov’ is the name of the ANOVA.  which = 1:2) Will show the Residuals vs Fitted and the Normal QQ-plot to check the ANOVA assumptions.  Click to View Output  Click to View Output. Explanation Hypotheses in Two-way ANOVA The hypotheses that can be tested in a two-way ANOVA that includes an interaction term are three-fold. Hypotheses about \\(\\alpha\\) where \\(\\alpha\\) has \\(m\\) levels. \\[ H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_m = 0 \\] \\[ H_a: \\alpha_i \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\} \\] Hypotheses about \\(\\beta\\) where \\(\\beta\\) has \\(q\\) levels. \\[ H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_q = 0 \\] \\[ H_a: \\beta_j \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,q\\} \\] Hypotheses about the interaction term \\(\\alpha\\beta\\). \\[ H_0: \\text{the effect of one factor is the same across all levels of the other factor} \\] \\[ H_a: \\text{the effect of one factor differs for at least one level of the other factor} \\] Expanding the ANOVA Model{#expanding} It turns out that more can be done with ANOVA than simply checking to see if the means of several groups differ. Reconsider the mathematical model of two-way ANOVA. \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] This model could be expanded to include any number of new terms in the model. The power of this approach is in the several questions (hypotheses) that can be posed to data simultaneously. What happens if your ANOVA fails both requirement tests? Example: BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") ## Rows: 204 Columns: 3 ## ── Column specification ────────────────────────── ## Delimiter: \",\" ## chr (2): Semester, Participation ## dbl (1): FinalGrade ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. BAanova <- aov(FinalGrade ~ Semester + Participation + Semester:Participation, data=BAFinalGrades) par(mfrow=c(1,2)) plot(BAanova, which=1:2, pch=16) The residual plot shows that the constant (equal) variance assumption raises some concern as the points within each group have varying spread. The spread of each group seems to gets closer and closer as we go from left to right as well as some straying point from the more clustered groups. Thus, showing the variance of the residuals are not constant and do not accurately show a relationship between each variable. For the Q-Q plot, it shows that the residuals of the data is not quite normally distributed. There are visible points at the lower and upper ends of the graph deviating away form the diagonal line, which represents a normal distribution. Thus, showing us that the residuals of the data are not normal distributed. Although both graphs indicate that the data fails to meet both requirements, this primarily means our findings may be less reliable due to the abnormalities in the data. As a result, we should interpret the results cautiously as we proceed with our Two-Way ANOVA test! Examples: warpbreaks (Two-way), CO2 (three-way)",
    "sentences": ["Graphics Y is a single quantitative variable of interest, like “heights” of BYU-Idaho students.", "X1 is a categorical (qualitative) variable like gender, with levels “boy” and “girl.” X2 is another categorical (qualitative) variable like “Classrank” with levels “Freshman”, “Sophomore”, and “Junior”.", "Of course, both X1 and X2 can have as many levels as you want to work with, but must have at least two levels each.", "Test(s) used for this category: Two-Way ANOVA (means) Used to answer THREE questions: the effects of factor 1 on the quantitative variable the effects of factor 2 on the quantitative variable the interaction between the two factors best paired with numerical summary looking at mean Analysis : Two-way ANOVA: Final Beginning Algebra Grades Question Examples: Based on your p-values and these graphs, which of the following is a correct conclusion to reach?", "xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1))) ANSWER : Among fourth graders, right handed boys have longer feet on average than left handed boys while the opposite is true for girls.", "Does the amount a student participates in the course affect the average final Beginning Algebra grades?", "Does the semester in which a student takes Beginning Algebra affect their average final grade?", "Does a student’s participation level vary depending on the semester?", "(Alternatively, does the semester influence a student’s participation?) BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") BAFinalGrades$Semester <- factor(BAFinalGrades$Semester, levels = c(\"Fall\",\"Winter\",\"Spring\")) ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=1)) + geom_point(color=\"firebrick1\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Spring Semester Seems to Have Higher Final Grades\", x=\"Semester\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades$Participation <- factor(BAFinalGrades$Participation, levels = c(\"Low\",\"Moderate\",\"High\")) ggplot(BAFinalGrades, aes(x=Participation, y=FinalGrade, group=1)) + geom_point(color=\"darkred\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Participation Seems to Lead to Higher Grades\", x=\"Participation\", y=\"Beginning Algebra Final Grades\") + theme_minimal() ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=Participation, color=Participation)) + geom_point() + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Beginning Algebra Seems Better during the Spring Semester\", x=\"BYU-Idaho Semesters\", y=\"Beginning Algebra Final Grades\") + theme_minimal()", "Tests Two-way ANOVA Overview A two-way ANOVA is only appropriate when all of the following are satisfied."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Quantitative Y | Multiple Categorical\r\nX"
  },
  {
    "id": 92,
    "title": "📍 Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single quantitative variable of interest, like “heights” of BYU-Idaho students. X1 is a categorical (qualitative) variable like gender, with levels “boy” and “girl.” X2 is another categorical (qualitative) variable like “Classrank” with levels “Freshman”, “Sophomore”, and “Junior”. Of course, both X1 and X2 can have as many levels as you want to work with, but must have at least two levels each. Test(s) used for this category: Two-Way ANOVA (means) Used to answer THREE questions: the effects of factor 1 on the quantitative variable the effects of factor 2 on the quantitative variable the interaction between the two factors best paired with numerical summary looking at mean Analysis : Two-way ANOVA: Final Beginning Algebra Grades Question Examples: Based on your p-values and these graphs, which of the following is a correct conclusion to reach? xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1))) ANSWER : Among fourth graders, right handed boys have longer feet on average than left handed boys while the opposite is true for girls. Does the amount a student participates in the course affect the average final Beginning Algebra grades? Does the semester in which a student takes Beginning Algebra affect their average final grade? Does a student’s participation level vary depending on the semester? (Alternatively, does the semester influence a student’s participation?) BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") BAFinalGrades$Semester <- factor(BAFinalGrades$Semester, levels = c(\"Fall\",\"Winter\",\"Spring\")) ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=1)) + geom_point(color=\"firebrick1\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Spring Semester Seems to Have Higher Final Grades\", x=\"Semester\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades$Participation <- factor(BAFinalGrades$Participation, levels = c(\"Low\",\"Moderate\",\"High\")) ggplot(BAFinalGrades, aes(x=Participation, y=FinalGrade, group=1)) + geom_point(color=\"darkred\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Participation Seems to Lead to Higher Grades\", x=\"Participation\", y=\"Beginning Algebra Final Grades\") + theme_minimal() ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=Participation, color=Participation)) + geom_point() + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Beginning Algebra Seems Better during the Spring Semester\", x=\"BYU-Idaho Semesters\", y=\"Beginning Algebra Final Grades\") + theme_minimal()",
    "sentences": ["Y is a single quantitative variable of interest, like “heights” of BYU-Idaho students.", "X1 is a categorical (qualitative) variable like gender, with levels “boy” and “girl.” X2 is another categorical (qualitative) variable like “Classrank” with levels “Freshman”, “Sophomore”, and “Junior”.", "Of course, both X1 and X2 can have as many levels as you want to work with, but must have at least two levels each.", "Test(s) used for this category: Two-Way ANOVA (means) Used to answer THREE questions: the effects of factor 1 on the quantitative variable the effects of factor 2 on the quantitative variable the interaction between the two factors best paired with numerical summary looking at mean Analysis : Two-way ANOVA: Final Beginning Algebra Grades Question Examples: Based on your p-values and these graphs, which of the following is a correct conclusion to reach?", "xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1))) ANSWER : Among fourth graders, right handed boys have longer feet on average than left handed boys while the opposite is true for girls.", "Does the amount a student participates in the course affect the average final Beginning Algebra grades?", "Does the semester in which a student takes Beginning Algebra affect their average final grade?", "Does a student’s participation level vary depending on the semester?", "(Alternatively, does the semester influence a student’s participation?) BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") BAFinalGrades$Semester <- factor(BAFinalGrades$Semester, levels = c(\"Fall\",\"Winter\",\"Spring\")) ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=1)) + geom_point(color=\"firebrick1\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Spring Semester Seems to Have Higher Final Grades\", x=\"Semester\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades$Participation <- factor(BAFinalGrades$Participation, levels = c(\"Low\",\"Moderate\",\"High\")) ggplot(BAFinalGrades, aes(x=Participation, y=FinalGrade, group=1)) + geom_point(color=\"darkred\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Participation Seems to Lead to Higher Grades\", x=\"Participation\", y=\"Beginning Algebra Final Grades\") + theme_minimal() ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=Participation, color=Participation)) + geom_point() + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Beginning Algebra Seems Better during the Spring Semester\", x=\"BYU-Idaho Semesters\", y=\"Beginning Algebra Final Grades\") + theme_minimal()"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 93,
    "title": "📍 Tests",
    "url": "index.html#tests",
    "content": "Tests Two-way ANOVA Overview A two-way ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses. Writing out the hypotheses can be very involved depending on whether you use the official “effects model” notation (very mathematically correct) or a simplified “means model” notation (which isn’t very mathematically correct, but gets the idea across in an acceptable way). Means Model Effects Model The first set of hypotheses are a “one-way” set of hypotheses for the first factor of the ANOVA. Factor: X1 with say, levels \\(A\\) and \\(B\\). \\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a “one-way” set of hypotheses, but for the second factor of the ANOVA. Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\mu_C = \\mu_D = \\mu_E = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one}\\ i\\in\\{1=C,2=D,3=E\\} \\] The third set of hypotheses are the most interesting hypotheses in a two-way ANOVA. These are called the interaction hypotheses. They test to see if the levels of one of the factors, say \\(X1\\), impact \\(Y\\) differently for the differing levels of the other factor, \\(X2\\). The hypotheses read formally as \\[ H_0: \\text{The effect of the first factor on Y} \\\\ \\text{is the same for all levels of the second factor.} \\] \\[ H_a: \\text{The effect of the first factor on Y is not the same} \\\\ \\text{for all levels of the second factor.} \\] A mathematically correct way to state the two-way ANOVA model is with the equation \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] In this model, \\(\\mu\\) is the grand mean (which is the average Y-value ignoring all information contained in the factors); \\(\\alpha_i\\) is the first factor \\(X1\\) with levels \\(A\\) and \\(B\\) (though there could be more levels in \\(X1\\) depending on your data); \\(\\beta_j\\) is the second factor with levels \\(C\\), \\(D\\), and \\(E\\) (though there could be fewer or more levels to this factor depending on your data); \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors which has \\(2\\times3=6\\) (may differ for your data) levels; and \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) is the normally distributed error term for each point \\(k\\) found within level \\(i\\) of \\(X1\\) and level \\(j\\) of \\(X2\\). This model allows us to more formally state the hypotheses as First factor \\(X1\\) having say, levels \\(A\\) and \\(B\\). \\[ H_0: \\alpha_A = \\alpha_B = 0 \\] \\[ H_a: \\alpha_i \\neq 0 \\ \\text{for at least one}\\ i\\in\\{1=A,2=B\\} \\] Second factor \\(X2\\) with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\beta_C = \\beta_D = \\beta_E = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j\\in\\{1=C,2=D,3=E\\} \\] Does the effect of the first factor (\\(X1\\)) change for the different levels of the second factor (\\(X2\\))? In other words, is there an interaction between the two factors \\(X1\\) and \\(X2\\)? \\[ H_0: \\alpha\\beta_{ij} = 0 \\ \\text{for all } i,j \\] \\[ H_a: \\alpha\\beta_{ij} \\neq 0 \\ \\text{for at least one } i,j \\] R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ X1+X2+X1:X2, data=YourDataSet) Perform the ANOVA summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. Note that factors X3, X4, and so on could also be added + to the model if desired, but this would create a three-way, or four-way ANOVA model, and so on. X1:X2 denotes the interaction of the factors X1 and X2. It is not required, but is usually included. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. warp.aov <-  Saves the results of the ANOVA test as an object named ‘warp.aov’. aov( ‘aov()’ is a function in R used to perform the ANOVA. breaks \\(Y\\) is ‘breaks’, which is a numeric variable from the warpbreaks dataset.  ~  ‘~’ is the tilde symbol used to separate the left- and right-hand side in a model formula. wool  The first factor \\(X1\\) is ‘wool’, which is a qualitative variable in the warpbreaks dataset. In this case, wool is a factor with two levels. Use str(warpbreaks) to see this. + tension  The second factor \\(X2\\) is ‘tension’, which is another qualitative variable in the warpbreaks dataset. In this case, tension is a factor with three levels. Use str(warpbreaks) to see this. + wool:tension, The interaction of the two factors: wool and tension.  data = warpbreaks) ‘warpbreaks’ is a dataset in R. summary( ‘summary()’ shows the results of the ANOVA. warp.aov) ‘warp.aov’ is the name of the ANOVA.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## wool 1 451 450.7 3.765 0.058213 ## tension 2 2034 1017.1 8.498 0.000693 ## wool:tension 2 1003 501.4 4.189 0.021044 ## Residuals 48 5745 119.7 ## ## wool . ## tension *** ## wool:tension * ## Residuals ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow = c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. plot( ‘plot’ is a R function for the plotting of R objects. warp.aov, ‘warp.aov’ is the name of the ANOVA.  which = 1:2) Will show the Residuals vs Fitted and the Normal QQ-plot to check the ANOVA assumptions.  Click to View Output  Click to View Output. Explanation Hypotheses in Two-way ANOVA The hypotheses that can be tested in a two-way ANOVA that includes an interaction term are three-fold. Hypotheses about \\(\\alpha\\) where \\(\\alpha\\) has \\(m\\) levels. \\[ H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_m = 0 \\] \\[ H_a: \\alpha_i \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\} \\] Hypotheses about \\(\\beta\\) where \\(\\beta\\) has \\(q\\) levels. \\[ H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_q = 0 \\] \\[ H_a: \\beta_j \\neq 0\\ \\text{for at least one}\\ i\\in\\{1,\\ldots,q\\} \\] Hypotheses about the interaction term \\(\\alpha\\beta\\). \\[ H_0: \\text{the effect of one factor is the same across all levels of the other factor} \\] \\[ H_a: \\text{the effect of one factor differs for at least one level of the other factor} \\] Expanding the ANOVA Model{#expanding} It turns out that more can be done with ANOVA than simply checking to see if the means of several groups differ. Reconsider the mathematical model of two-way ANOVA. \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] This model could be expanded to include any number of new terms in the model. The power of this approach is in the several questions (hypotheses) that can be posed to data simultaneously. What happens if your ANOVA fails both requirement tests? Example: BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") ## Rows: 204 Columns: 3 ## ── Column specification ────────────────────────── ## Delimiter: \",\" ## chr (2): Semester, Participation ## dbl (1): FinalGrade ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. BAanova <- aov(FinalGrade ~ Semester + Participation + Semester:Participation, data=BAFinalGrades) par(mfrow=c(1,2)) plot(BAanova, which=1:2, pch=16) The residual plot shows that the constant (equal) variance assumption raises some concern as the points within each group have varying spread. The spread of each group seems to gets closer and closer as we go from left to right as well as some straying point from the more clustered groups. Thus, showing the variance of the residuals are not constant and do not accurately show a relationship between each variable. For the Q-Q plot, it shows that the residuals of the data is not quite normally distributed. There are visible points at the lower and upper ends of the graph deviating away form the diagonal line, which represents a normal distribution. Thus, showing us that the residuals of the data are not normal distributed. Although both graphs indicate that the data fails to meet both requirements, this primarily means our findings may be less reliable due to the abnormalities in the data. As a result, we should interpret the results cautiously as we proceed with our Two-Way ANOVA test! Examples: warpbreaks (Two-way), CO2 (three-way)",
    "sentences": ["Two-way ANOVA Overview A two-way ANOVA is only appropriate when all of the following are satisfied.", "The sample(s) of data can be considered to be representative of their population(s).", "The data is normally distributed in each group.", "(This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same.", "(This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses.", "Writing out the hypotheses can be very involved depending on whether you use the official “effects model” notation (very mathematically correct) or a simplified “means model” notation (which isn’t very mathematically correct, but gets the idea across in an acceptable way).", "Means Model Effects Model The first set of hypotheses are a “one-way” set of hypotheses for the first factor of the ANOVA.", "Factor: X1 with say, levels \\(A\\) and \\(B\\).", "\\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a “one-way” set of hypotheses, but for the second factor of the ANOVA.", "Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\)."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 94,
    "title": "📍 Quantitative Y | Quantitative X",
    "url": "index.html#quantitativeyquantitativex",
    "content": "Quantitative Y | Quantitative X Graphics Y is a single quantitative variable of interest, like “height”. X is another single quantitative variable of interest, like “shoe-size”. This would imply we are using “shoe-size” (X) to explain “height” (Y). Test(s) used for this category: Simple Linear Regression Analysis : Simple Linear Regression: BYU-Idaho Math Tutoring Used to find the relationship between one independent variable (predictor) and one dependent variable(outcome): Is there a relationship between the length of children’s feet and the width of their foot? Example: Does the amount of tutoring a student receives correlate with their chapter exam scores? (Simple Linear Regression) ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams) Tests Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\). Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$\\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$$ \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is “equal to” the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to “God’s Law” or “Natural Law”. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are “random” and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)… \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced “y-hat”, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what follows… \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, “the change in the average y-value for a one unit change in the x-value.” It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, “the average y-value when x is zero.” It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful. R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command.  <-  This is the “left arrow” assignment operator that stores the results of your lm() code into mylm name. lm( lm(…) is an R function that stands for “Linear Model”. It performs a linear regression analysis for Y ~ X. Y  Y is your quantitative response variable. It is the name of one of the columns in your data set. ~  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X. X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value.  data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X. ) Closing parenthesis for the lm(…) function. summary(mylm) The summary command allows you to print the results of your linear regression that were previously saved in mylm name.  Click to Show Output  Click to View Output. Example output from a regression. Hover each piece to learn more. Call: lm(formula = dist ~ speed, data = cars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -29.069 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -9.525 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -2.272 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -2.272 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-2.272) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   9.215 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   43.201 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -17.5791 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   6.7584 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -2.601 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (-17.5791) by its standard error (6.7584). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of -17.5791 is -2.601 standard errors (6.7584) from zero, which is a fairly surprising distance as shown by the p-value. 0.0123 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). speed This is always the name of your X-variable in your lm(Y ~ X, …).   3.9324 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.4155 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 9.464 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” of the slope (3.9324) by its standard error (0.4155). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from zero, which is a really surprising distance as shown by the smallness of the p-value. 1.49e-12 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  15.38 For this particular regression, the estimate of \\(\\sigma\\) is 15.38. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 48 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 50 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 50-2 = 48. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.6511, In this particular regression, 65.11% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data.  0.6438 In this case, the value of 0.6438 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is found as the ratio of the MSR/MSE where MSR = SSR/(p-1) and MSE = SSE/(n-p) where n is the sample size and p is the number of parameters in the regression model.  89.57 This is the value of the F-statistic for the lm(dist ~ speed, data=cars) regression. Note that SSE = sum( cars.lm$res^2 ) = 11353.52 with n - p = 50 - 2 = 48 degrees of freedom for this data. Further, SSR = sum( (cars.lm$fit - mean(cars$dist))^2 ) = 21185.46 with p - 1 = 1 degree of freedom. So MSR = 21185.46 and MSE = 11353.52 / 48 = 236.5317. So MSR / MSE = 21185.46 / 236.5317 = 89.56711.  on 1 and 48 DF, The 1 degree of freedom is the SSR degrees of freedom (p-1). The 48 is the SSE degrees of freedom (n-p).  p-value: 1.49e-12 The p-value for an F-statistic is found by the code pf(89.56711, 1, 48, lower.tail=FALSE), which gives the probability of being more extreme than the observed F-statistic in an F distribution with 1 and 48 degrees of freedom. Check Assumptions 1, 2, 3, and 5 When making it in an r chunk make your height 3! { r, fig.height = 3 } par( The par(…) command stands for “Graphical PARameters”. It allows you to control various aspects of graphics in Base R. mfrow= This stands for “multiple frames filled by row”, which means, put lots of plots on the same row, starting with the plot on the left, then working towards the right as more plots are created. c( The combine function c(…) is used to specify how many rows and columns of graphics should be placed together. 1, This specifies that 1 row of graphics should be produced. 3 This states that 3 columns of graphics should be produced. ) Closing parenthesis for c(…) function. ) Closing parenthesis for par(…) function. plot( This version of plot(…) will actually create several regression diagnostic plots by default. mylm, This is the name of an lm object that you created previously. which= This allows you to select “which” regression diagnostic plots should be drawn. 1 Selecting 1, would give the residuals vs. fitted values plot only. : The colon allows you to select more than just one plot. 2 Selecting 2 also gives the Q-Q Plot of residuals. If you wanted to instead you could just use which=1 to get the residuals vs fitted values plot, then you could use qqPlot(mylm$residuals) to create a fancier Q-Q Plot of the residuals. ) Closing parenthesis for plot(…) function. plot( This version of plot(…) will be used to create a time-ordered plot of the residuals. The order of the residuals is the original order of the x-values in the original data set. If the original data set doesn’t have an order, then this plot is not interesting. mylm The lm object that you created previously. $ This allows you to access various elements from the regression that was performed. residuals This grabs the residuals for each observation in the regression. ) Closing parenthesis for plot(…) function.  Click to Show Output  Click to View Output. Which regression assumption(s) does the residuals versus fitted values plot diagnose? Linear Relation Constance Variance Which regression assumption(s) does the Q-Q Plot of the residuals diagnose? Normal Errors Which regression assumption(s) does the Residuals versus Order plot diagnose? (Remember, this plot can only be created when the data was collected in a specific order.) Independent Errors Plotting the Regression Line Base R ggplot2 To add the regression line to a scatterplot use the abline(...) command: plot( The plot(…) function is used to create a scatterplot with a y-axis (the vertical axis) and an x-axis (the horizontal axis). Y  This is the “response variable” of your regression. The thing you are interested in predicting. This is the name of a “numeric” column of data from the data set called YourDataSet. ~  The tilde “~” is used to relate Y to X and can be found on the top-left key of your keyboard. X,  This is the explanatory variable of your regression. It is the name of a “numeric” column of data from YourDataSet. data= The data= statement is used to specify the name of the data set where the columns of “X” and “Y” are located. YourDataSet This is the name of your data set, like KidsFeet or cars or airquality. ) Closing parenthesis for plot(…) function. abline( This stands for “a” (intercept) “b” (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm This is the name of an lm(…) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(…) function will locate these two values from within mylm and use them to add a line to your current plot(…). ) Closing parenthesis for abline(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm) You can customize the look of the regression line with abline( This stands for “a” (intercept) “b” (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm, This is the name of an lm(…) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(…) function will locate these two values from within mylm and use them to add a line to your current plot(…). lty= The lty= stands for “line type” and allows you to select between 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. 1, This creates a solid line. Remember, other options include: 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. lwd= The lwd= allows you to specify the width of the line. The default width is 1. Using lwd=2 would double the thickness, and so on. Any positive value is allowed. 1, Default line width. To make a thicker line, us 2 or 3… To make a thinner line, try 0.5, but 1 is already pretty thin. col= This allows you to specify the color of the line using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). “someColor” Type colors() in R for options. ) Closing parenthesis for abline(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm, lty=1, lwd=1, col=\"firebrick\") You can add points to the regression with… points( This is like plot(…) but adds points to the current plot(…) instead of creating a new plot. newY  newY should be a column of values from some data set. Or, use points(newX, newY) to add a single point to a graph. ~  This links Y to X in the plot. newX,  newX should be a column of values from some data set. It should be the same length as newY. If just a single value, use points(newX, newY) instead. data=YourDataSet,  If newY and newX come from a dataset, then use data= to tell the points(…) function what data set they come from. If newY and newX are just single values, then data= is not needed. col=“skyblue”, This allows you to specify the color of the points using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). pch=16 This allows you to specify the type of plotting symbol to be used for the points. Type ?pch and scroll half way down in the help file that appears to learn about other possible symbols. ) Closing parenthesis for points(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) points(7,40, pch=16, col=\"skyblue\", cex=2) text(7,40, \"New Dot\", pos=3, cex=0.5) points(dist ~ speed, data=filter(cars, mylm$res > 2), cex=.8, col=\"red\") abline(mylm, lty=1, lwd=1, col=\"firebrick\") To add the regression line to a scatterplot using the ggplot2 approach, first ensure: library(ggplot2) or library(tidyverse) is loaded. Then, use the geom_smooth(method = lm) command: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. YourDataSet,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. X, This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet.  y =  “y= ” declares which variable will become the y-axis of the graphic. Y This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function. + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. + Here the + is used to add yet another layer to ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. There are several other methods that could be used here.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(…) to place more complicated models on the plot.  se = FALSE se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. ) Closing parenthesis for the geom_smooth() function.  Click to Show Output  Click to View Output. ggplot(cars, aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = \"lm\", formula=y~x, se=FALSE) There are a number of ways to customize the appearance of the regression line: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. speed,  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet. y =  “y= ” declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic.  + Here the + is used to add yet another layer to ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(…) to place more complicated models on the plot.  se = FALSE, se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.  size = 2, Use size = 2 to adjust the thickness of the line to size 2.  color = “orange”, Use color = “orange” to change the color of the line to orange.   linetype = “dashed” Use linetype = “dashed” to change the solid line to a dashed line. Some linetype options include “dashed”, “dotted”, “longdash”, “dotdash”, etc. ) Closing parenthesis for the geom_smooth() function.  Click to Show Output  Click to View Output. In addition to customizing the regression line, you can customize the points, add points, add lines, and much more. ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. speed,  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet. y =  “y= ” declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point( geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. size = 1.5, Use size = 1.5 to change the size of the points.  color = “skyblue” Use color = “skyblue” to change the color of the points to Brother Saunders’ favorite color.  alpha = 0.5 Use alpha = 0.5 to change the transparency of the points to 0.5. ) Closing parenthesis of geom_point() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in ways similar to lm(…) to place more complicated models on the plot.  se = FALSE, se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.  color = “navy”, Use color = “navy” to change the color of the line to navy blue.  size = 1.5 Use size = 1.5 to adjust the thickness of the line to 1.5. ) Closing parenthesis of geom_smooth() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_hline( Use geom_hline() to add a horizontal line at a specified y-intercept. You can also use geom_vline(xintercept = some_number) to add a vertical line to the graph. yintercept = Use “yintercept =” to tell geom_hline() that you are going to declare a y intercept for the horizontal line.  75 75 is the value of the y-intercept. , color = “firebrick” Use color = “firebrick” to change the color of the horizontal line to firebrick red. , size = 1, Use size = 1 to adjust the thickness of the horizontal line to size 1.              linetype = “longdash” Use linetype = “longdash” to change the solid line to a dashed line with longer dashes. Some linetype options include “dashed”, “dotted”, “longdash”, “dotdash”, etc. , alpha = 0.5 Use alpha = 0.5 to change the transparency of the horizontal line to 0.5. ) Closing parenthesis of geom_hline function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_segment( geom_segment() allows you to add a line segment to ggplot() by using specified start and end points. x = “x =” tells geom_segment() that you are going to declare the x-coordinate for the starting point of the line segment.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the starting point of the line segment.  y =“y =” tells geom_segment() that you are going to declare the y-coordinate for the starting point of the line segment.  75, 75 is a number on the y-axis of your graph. It is the y-coordinate of the starting point of the line segment.  xend = “xend =” tells geom_segment() that you are going to declare the x-coordinate for the end point of the line segment.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the end point of the line segment.  yend = “yend =” tells geom_segment() that you are going to declare the y-coordinate for the end point of the line segment.  38, 38 is a number on the y-axis of your graph. It is the y-coordinate of the end point of the line segment.                size = 1 Use size = 1 to adjust the thickness of the line segment. , color = “lightgray” Use color = “lightgray” to change the color of the line segment to light gray. , linetype = “longdash” Use *linetype = “longdash* to change the solid line segment to a dashed one. Some linetype options include”dashed”, “dotted”, “longdash”, “dotdash”, etc. ) Closing parenthesis for geom_segment() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_point( geom_point() can also be used to add individual points to the graph. Simply declare the x and y coordinates of the point you want to plot. x = “x =” tells geom_point() that you are going to declare the x-coordinate for the point.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the point.  y = “y =” tells geom_point() that you are going to declare the y-coordinate for the point.  75 75 is a number on the y-axis of your graph. It is the y-coordinate of the point. , size = 3 Use size = 3 to make the point stand out more. , color = “firebrick” Use color = “firebrick” to change the color of the point to firebrick red. ) Closing parenthesis of the geom_point() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_text( geom_text() allows you to add customized text anywhere on the graph. It is very similar to the base R equivalent, text(…). x = “x =” tells geom_text() that you are going to declare the x-coordinate for the text.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the text.  y = “y =” tells geom_text() that you are going to declare the y-coordinate for the text.  84, 84 is a number on the y-axis of your graph. It is the y-coordinate of the text.  label = “label =” tells geom_text() that you are going to give it the label.  “My Point (14, 75)”, “My Point (14, 75)” is the text that will appear on the graph.             color = “navy” Use color = “navy” to change the color of the text to navy blue. , size = 3 Use size = 3 to change the size of the text. ) Closing parenthesis of the geom_text() function.  + The + allows you to add more layers to the framework provided by ggplot().   theme_minimal() Add a minimalistic theme to the graph. There are many other themes that you can try out.  Click to Show Output  Click to View Output. ## `geom_smooth()` using formula = 'y ~ x' Accessing Parts of the Regression Finally, note that the mylm object contains the names(mylm) of mylm$coefficients Contains two values. The first is the estimated \\(y\\)-intercept. The second is the estimated slope. ## (Intercept) speed ## -17.579095 3.932409 mylm$residuals Contains the residuals from the regression in the same order as the actual dataset. ## 1 2 3 4 ## 3.849460 11.849460 -5.947766 12.052234 ## 5 6 7 8 ## 2.119825 -7.812584 -3.744993 4.255007 ## 9 10 11 12 ## 12.255007 -8.677401 2.322599 -15.609810 ## 13 14 15 16 ## -9.609810 -5.609810 -1.609810 -7.542219 ## 17 18 19 20 ## 0.457781 0.457781 12.457781 -11.474628 ## 21 22 23 24 ## -1.474628 22.525372 42.525372 -21.407036 ## 25 26 27 28 ## -15.407036 12.592964 -13.339445 -5.339445 ## 29 30 31 32 ## -17.271854 -9.271854 0.728146 -11.204263 ## 33 34 35 36 ## 2.795737 22.795737 30.795737 -21.136672 ## 37 38 39 40 ## -11.136672 10.863328 -29.069080 -13.069080 ## 41 42 43 44 ## -9.069080 -5.069080 2.930920 -2.933898 ## 45 46 47 48 ## -18.866307 -6.798715 15.201285 16.201285 ## 49 50 ## 43.201285 4.268876 mylm$fitted.values The values of \\(\\hat{Y}\\) in the same order as the original dataset. ## 1 2 3 4 5 ## -1.849460 -1.849460 9.947766 9.947766 13.880175 ## 6 7 8 9 10 ## 17.812584 21.744993 21.744993 21.744993 25.677401 ## 11 12 13 14 15 ## 25.677401 29.609810 29.609810 29.609810 29.609810 ## 16 17 18 19 20 ## 33.542219 33.542219 33.542219 33.542219 37.474628 ## 21 22 23 24 25 ## 37.474628 37.474628 37.474628 41.407036 41.407036 ## 26 27 28 29 30 ## 41.407036 45.339445 45.339445 49.271854 49.271854 ## 31 32 33 34 35 ## 49.271854 53.204263 53.204263 53.204263 53.204263 ## 36 37 38 39 40 ## 57.136672 57.136672 57.136672 61.069080 61.069080 ## 41 42 43 44 45 ## 61.069080 61.069080 61.069080 68.933898 72.866307 ## 46 47 48 49 50 ## 76.798715 76.798715 76.798715 76.798715 80.731124 mylm$… several other things that will not be explained here. Making Predictions predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ) Closing parenthesis for the data.frame(…) function. ) Closing parenthesis for the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12)) 1   29.60981 The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(…) function.  interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. “prediction” This specifies that a prediction interval will be included with the predicted value. A prediction interval gives you a 95% confidence interval that captures 95% of the data, or \\(Y_i\\) values for the specific \\(X\\)-value specified in the prediction. ) Closing parenthesis of the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"prediction\")   fit The “fit” is the predicted value.   lwr The “lwr” is the lower bound.   upr The “upr” is the upper bound. 1 29.60981 In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet. -1.749529 This is the lower bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us the stopping distance could be as quick as -1.749529 feet (or 0 because distance can’t go negative). 60.96915 This is the upper bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us that the actual stopping distance could be as high as 60.96915 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(…) function.  interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. “confidence” This specifies that a confidence interval for the prediction should be provided. This is of use whenever your interest is in just estimating the average y-value, not the actual y-values. ) Closing parenthesis of the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"confidence\")   fit The “fit” is the predicted value.   lwr The “lwr” is the lower bound.   upr The “upr” is the upper bound. 1 29.60981 In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. 24.39514 This is the lower bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is greater than this value. 34.82448 This is the upper bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is less than this value. Finding Confidence Intervals for Model Parameters confint( The R function confint(…) allows you to use an lm(…) object to compute confidence intervals for one or more parameters (like \\(\\beta_0\\) or \\(\\beta_1\\)) in your model. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  level = “level =” tells the confint(…) function that you are going to declare at what level of confidence you want the interval. The default is “level = 0.95.” If you want to find 95% confidence intervals for your parameters, then just run confint(mylm).  someConfidenceLevel someConfidenceLevel is simply a confidence level you choose when you want something other than a 95% confidence interval. Some examples of appropriate levels include 0.90 and 0.99. ) Closing parenthesis for confint(..) function. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.90)     5 % The lower bound of a 90% confidence interval occurs at the 5th percentile. This is because at 90% confidence, 10% is left in the tails, with 5% on each end. The upper bound will thus end at the 95th percentile, hence the 5% and 95% as the column names.   95 % The upper bound of a 90% confidence interval ends at the 95th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -28.914514 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -6.243676 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.235501 and 4.629317. 3.235501 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. 4.629317 This is the upper bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.95)     2.5 % The lower bound of a 95% confidence interval occurs at the 2.5th percentile. This is because at 95% confidence, 5% is left in the tails, with 2.5% on each end. The upper bound will thus end at the 97.5th percentile, hence the 2.5% and 97.5% as the column names for the lower and upper bounds, respectively.   97.5 % The upper bound of a 95% confidence interval ends at the 97.5th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -31.167850 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -3.990340 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.096964 and 4.767853. 3.096964 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.096964 and 4.767853 4.767853 This is the upper bound of the confidence interval. In this case, you can be 95% confident that the slope lies between 3.096964 and 4.767853 Explanation Linear regression has a rich mathematical theory behind it. This is because it uses a mathematical function and a random error term to describe the regression relation between a response variable \\(Y\\) and an explanatory variable called \\(X\\). Expand each element below to learn more. Regression Cheat Sheet (Expand) Term Pronunciation Meaning Math R Code \\(Y_i\\) $Y_i$ “why-eye” The data \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)\\) $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$ YourDataSet$YourYvariable \\(\\hat{Y}_i\\) $\\hat{Y}_i$ “why-hat-eye” The fitted line \\(\\hat{Y}_i = b_0 + b_1 X_i\\) $\\hat{Y}_i = b_0 + b_1 X_i$ lmObject$fitted.values \\(E\\{Y_i\\}\\) $E\\{Y_i\\}$ “expected value of why-eye” True mean y-value \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) $E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i$ <none> \\(\\beta_0\\) $\\beta_0$ “beta-zero” True y-intercept <none> <none> \\(\\beta_1\\) $\\beta_1$ “beta-one” True slope <none> <none> \\(b_0\\) $b_0$ “b-zero” Estimated y-intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) $b_0 = \\bar{Y} - b_1\\bar{X} b_0 <- mean(Y) - b_1*mean(X)$ \\(b_1\\) $b_1$ “b-one” Estimated slope \\(b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\) $b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})} {\\sum(X_i - \\bar{X})^2}$ b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) \\(\\epsilon_i\\) $\\epsilon_i$ “epsilon-eye” Distance of dot to true line \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) $\\epsilon_i = Y_i - E\\{Y_i\\}$ <none> \\(r_i\\) $r_i$ “r-eye” or “residual-eye” Distance of dot to estimated line \\(r_i = Y_i - \\hat{Y}_i\\) $r_i = Y_i - \\hat{Y}_i$ lmObject$residuals \\(\\sigma^2\\) $\\sigma^2$ “sigma-squared” Variance of the \\(\\epsilon_i\\) \\(Var\\{\\epsilon_i\\} = \\sigma^2\\)$Var\\{\\epsilon_i\\} = \\sigma^2$ <none> \\(MSE\\) $MSE$ “mean squared error” Estimate of \\(\\sigma^2\\) \\(MSE = \\frac{SSE}{n-p}\\)$MSE = \\frac{SSE}{n-p}$ sum( lmObject$res^2 ) / (n - p) \\(SSE\\) $SSE$ “sum of squared error” (residuals) Measure of dot’s total deviation from the line \\(SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\)$SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$ sum( lmObject$res^2 ) \\(SSR\\) $SSR$ “sum of squared regression error” Measure of line’s deviation from y-bar \\(SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\)$SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2$ sum( (lmObject$fit - mean(YourData$Y))^2 ) \\(SSTO\\) $SSTO$ “total sum of squares” Measure of total variation in Y \\(SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\)$SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2$ sum( (YourData$Y - mean(YourData$Y))^2 ) \\(R^2\\) $R^2$ “R-squared” Proportion of variation in Y explained by the regression \\(R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\)$R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}$ SSR/SSTO \\(r\\) $r$ “r” Correlation between X and Y. \\(r = \\sqrt{R^2}\\)$r = \\sqrt{R^2}$ sqrt(R^2) \\(\\hat{Y}_h\\) $\\hat{Y}_h$ “why-hat-aitch” Estimated mean y-value for some x-value called \\(X_h\\) \\(\\hat{Y}_h = b_0 + b_1 X_h\\)$\\hat{Y}_h = b_0 + b_1 X_h$ predict(lmObject, data.frame(XvarName=#)) \\(X_h\\) $X_h$ “ex-aitch” Some x-value, not necessarily one of the \\(X_i\\) values used in the regression \\(X_h =\\) some number$X_h = $ Xh = # Confidence Interval “confidence interval” Estimated bounds at a certain level of confidence for a parameter \\(b_0 \\pm t^* \\cdot s_{b_0}\\)b_0 \\pm t^* \\cdot s_{b_0} or \\(b_1 \\pm t^* \\cdot s_{b_1}\\)b_1 \\pm t^* \\cdot s_{b_1} confint(mylm, level = someConfidenceLevel) Parameter Estimate \\(\\beta_0\\) \\(b_0\\) \\(\\beta_1\\) \\(b_1\\) \\(\\epsilon_i\\) \\(r_i\\) \\(\\sigma^2\\) \\(MSE\\) \\(\\sigma\\) \\(\\sqrt{MSE}\\), the Residual standard error The Mathematical Model (Expand) \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)… There are three main elements to the mathematical model of regression. Each of these three elements is pictured below in the “Regression Relation Diagram.” Study both the three bullet points and their visual representations in the plot below for a clearer understanding. The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read more…) The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as “natural law” or “God’s law”. It is some true line that is unknown to us. The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line. The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\). Note: \\(E\\{Y\\}\\) is pronounced “the expected value of y” because, well… the mean is the typical, average, or “expected” value. The dots, i.e., the regression relation plus an error term: \\(Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}} + \\underbrace{\\epsilon_i}_\\text{error term} \\quad \\text{where} \\ \\epsilon_i\\sim N(0,\\sigma^2)\\) (Read more…) This is shown by the dots in the graph below. This is the data. In regression, the assumption is that the y-value for individual \\(i\\), denoted by \\(Y_i\\), was “created” by adding an error term \\(\\epsilon_i\\) to each individual’s “expected” value \\(\\beta_0 + \\beta_1 X_i\\). Note the “order of creation” would require first knowing an indivual’s x-value, \\(X_i\\), then their expected value from the regression relation \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) and then adding their \\(\\epsilon_i\\) value to the result. The \\(\\epsilon_i\\) allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance \\(\\epsilon_i\\) from the line. Note: \\(Y_i\\) is pronounced “why-eye” because it is the y-value for individual \\(i\\). Sometimes also called “why-sub-eye” because \\(i\\) is in the subscript of \\(Y\\). The estimated line, i.e., the line we get from a sample of data. \\(\\underbrace{\\hat{Y}_i}_{\\substack{\\text{estimated mean} \\\\ \\text{y-value}}} = \\underbrace{b_0 + b_1 X_i}_\\text{estimated regression equation}\\) (Read more…) The estimated line is shown by the solid line in the graph below. \\(\\hat{Y}\\) is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation \\(E\\{Y\\}\\). So \\(\\hat{Y}\\) is interpreted as the estimated average (or mean) \\(Y\\)-value for any given \\(X\\)-value. Thus, \\(b_0\\) is the estimated y-intercept and \\(b_1\\) is the estimated slope. The b’s are sample statistics, like \\(\\bar{x}\\) and the \\(\\beta\\)’s are population parameters like \\(\\mu\\). The \\(b\\)’s estimate the \\(\\beta\\)’s. Note: \\(\\hat{Y}_i\\) is pronounced “why-hat-eye” and is known as the “estimated y-value” or “fitted y-value” because it is the y-value you get from \\(b_0 + b_1 X_i\\). It is always different from \\(Y_i\\) because dots are rarely if ever exactly on the estimated regression line. This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line). Something to ponder: The true line, when coupled with the error terms, “creates” the data. The estimated (or fitted) line uses the sampled data to try to “re-create” the true line. We could loosely call this the “order of creation” as shown by the following diagram. par(mfrow=c(1,3), mai=c(.2,.2,.4,.1)) plot(y ~ x, col=\"white\", main=\"A Law is Given\", yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, main=\"Data is Created\", xaxt='n', yaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, xaxt='n', yaxt='n', main=\"The Law is Estimated\") curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) A Law is Given Data is Created The Law is Estimated \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) \\(Y_i = E\\{Y_i\\} + \\epsilon_i\\) \\(\\hat{Y}_i = b_0 + b_1 X_i\\) The true line is the “law”. The \\(Y_i\\) are created by adding \\(\\epsilon_i\\) to \\(E\\{Y_i\\}\\) where \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\). The law is estimated with \\(\\hat{Y}_i\\) which is given with lm(...). Click open the “Code” buttom below to the right to find code that runs a simulation demonstrating this “order of creation”. ## Simulating Data from a Regression Model ## This R-chunk is meant to be played in your R Console. ## It allows you to explore how the various elements ## of the regression model combine together to \"create\" ## data and then use the data to \"re-create\" the line. set.seed(101) #Allows us to always get the same \"random\" sample #Change to a new number to get a new sample n <- 3 #set the sample size X_i <- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45. beta0 <- 3 #Our choice for the y-intercept. beta1 <- .1 #Our choice for the slope. sigma <- 12.5 #Our choice for the std. deviation of the error terms. epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data View(fabData) #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it. fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData. summary(fab.lm) #Summarize your model. plot(y ~ x, data=fabData) #Plot the data. abline(fab.lm) #Add the estimated regression line to your plot. # Now for something you can't do in real life... but since we created the data... abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). legend(\"topleft\", legend=c(\"True Line\", \"Estimated Line\"), lty=c(2,1), bty=\"n\") #Add a legend to your plot specifying which line is which. Interpreting the Model Parameters (Expand) \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted as… The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model. If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\). The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\). It is often misunderstood to be the “average change in y” or just “the change in y” but it is more correctly referred to as the “change in the average y”. To better see this, consider the three graphics shown below. par(mfrow=c(1,3)) hist(mtcars$mpg, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Number of Vehicles\", xlab=\"Gas Mileage (mpg)\", col=\"skyblue\") boxplot(mpg ~ cyl, data=mtcars, border=\"skyblue\", boxwex=0.5, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Number of Cylinders of Engine (cyl)\") plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=\"skyblue\", main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Quarter Mile Time (qsec)\") abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=\"darkgray\") mtext(side=3, text=\"Automatic Transmissions Only (am==0)\", cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=\"gray\") The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09. The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg. The scatterplot on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value. Residuals and Errors (Expand) \\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true error… Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\). The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\). We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summary… Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\). \\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms. Keep in mind the idea that the errors \\(\\epsilon_i\\) “created” the data and that the residuals \\(r_i\\) are computed after using the data to “re-create” the line. Residuals have many uses in regression analysis. They allow us to diagnose the regression assumptions, See the “Assumptions” section below for more details. estimate the regression relation, See the “Estimating the Model Parameters” section below for more details. estimate the variance of the error terms, See the “Estimating the Model Variance” section below for more details. and assess the fit of the regression relation. See the “Assessing the Fit of a Regression” section below for more details. Assessing the Fit of a Regression (Expand) \\(R^2\\), SSTO, SSR, and SSE… Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line. par(mfrow=c(1,3), mai=c(.1,.1,.5,.1)) set.seed(2) x <- runif(30,0,20) y1 <- 2 + 3.5*x + rnorm(30,0,2) y2 <- 2 + 3.5*x + rnorm(30,0,8) y3 <- 2 + 3.5*x + rnorm(30,0,27) plot(y1 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Excellent Fit\") abline(lm(y1 ~ x), col=\"gray\") plot(y2 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Good Fit\") abline(lm(y2 ~ x), col=\"gray\") plot(y3 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Poor Fit\") abline(lm(y3 ~ x), col=\"gray\") A common way to measure the fit of a regression is with correlation. While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\). (If you are a Math 325 student, just stick with correlation for now and skip on to the next section of this Explanation tab. If you are a Math 425 student, it is critical that you come to understand \\(R^2\\) deeply, so read on.) Before you can understand \\(R^2\\), you must understand three important “sums of squares”. (Read more about sums…) A sum is just a fancy word for adding things together. \\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand. So we use the symbol \\(\\Sigma\\) to denote the word “sum”. Further, we use a subscript \\(\\Sigma_{i=1}\\) to state what value the sum is beginning with, and a superscript \\(\\Sigma_{i=1}^6\\) to state the value we are ending at. This gives \\[ \\sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Test your knowledge, do you see why the answer is 6 to the sum below? \\[ \\sum_{i=1}^3 i = 6 \\] Computing sums in R is fairly easy. Type the following codes in your R Console. sum(1:6) #gives the answer of 21 sum(1:3) #gives the answer of 6 However, sums really become useful when used with a data set. Each row of a data set represents an “individual’s” data. We can reference each individual with a row number. In the data below, individual 3, denoted by \\(i=3\\), has a speed of 7 and a dist of 4. pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3) Individual speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 To compute the sum of the speed column, use sum(speed). If we divided this sum by 6, we would get the mean of speed mean(speed). In fact, the two most used statistics mean(...) and sd(...) both use sums. Take a moment to review the formulas for mean and standard deviation. It is strongly recommended that you study the Explanation tab for both as well. We’ll wait. See you back here shortly. … Welcome back. Suppose we let X = speed and Y = dist. Then \\(X_3 = 7\\) and \\(Y_3 = 4\\) because we are accessing row 3 of both the \\(X\\) (or speed) column and \\(Y\\) (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, sum(speed) would be written mathematically as \\(\\sum_{i=1}^6 X_i\\) and sum(dist) would be written as \\(\\sum_{i=1}^6 Y_i\\). Sum of Squared Errors Sum of Squares Regression Total Sum of Squares \\(\\text{SSE} = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\) \\(\\text{SSR} = \\sum_{i=1}^n \\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\) \\(\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\\) Measures how much the residuals deviate from the line. Measures how much the regression line deviates from the average y-value. Measures how much the y-values deviate from the average y-value. Equals SSTO - SSR Equals SSTO - SSE Equals SSE + SSR sum( (Y - mylm$fit)^2 ) sum( (mylm$fit - mean(Y))^2 ) sum( (Y - mean(Y))^2 ) It is important to remember that SSE and SSR split up SSTO, so that \\[ \\text{SSTO} = \\text{SSE} + \\text{SSR} \\] This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works. The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called \\(R^2\\) (“r-squared”). R-Squared (\\(R^2\\)) \\[ \\underbrace{R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}}_\\text{Interpretation: Proportion of variation in Y explained by the regression.} \\] The smallest \\(R^2\\) can be is zero, and the largest it can be is 1. This is because \\(SSR\\) must be between 0 and SSTO, inclusive. Residual Plots & Regression Assumptions (Expand) Residuals vs. fitted-values, Q-Q Plot of the residuals, and residuals vs. order plots… There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate. Each assumption is labeled in the regression equation below. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions. (Read more…) As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not. Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3 The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(\\hat{Y}_i\\). The residuals are the \\(r_i\\). This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable. | Show Examples | (Read more…) The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption. The linear relation is assumed to be satisfied if there are no apparent trends in the plot. The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values. The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance. set.seed(2) X <- rnorm(30,15,3) notLin <- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8)) notLin.lm <- lm(Y~X, data=notLin) set.seed(15) Lin <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) Lin.lm <- lm(Y~X, data=Lin) par(mfrow=c(3,3), mai=c(.25,.25,.25,.25), mgp=c(1,.75,0)) plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Not Linear\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(notLin.lm$fitted.values,notLin.lm$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Good: Linear, Constant Variance\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) set.seed(6) notCon <- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5)) notCon.lm <- lm(Y~X, data=notCon) LinO <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) LinO[1] <- LinO[1]^2 LinO.lm <- lm(Y~X, data=LinO) plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Unconstant Variance\", cex.main=0.95, yaxt='n', xaxt='n', col=\"firebrick\") polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) # plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Outliers\", cex.main=0.95) # abline(h=0) tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(Girth ~ Volume, data=trees[-31,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(Height ~ Volume, data=trees) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) abline(h=0) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals, f=.4) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(weight ~ repwt, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(weight ~ repht, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) abline(h=0) Q-Q Plot of the Residuals: Checks Assumption #2 The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption. | Show Examples | (Read more…) There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot. Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) set.seed(123) tmp <- rnorm(100) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"skyblue\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Normal\", col=\"skyblue\") tmp <- Davis$weight qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Right-skewed\", breaks=15, col=\"firebrick\") par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- rbeta(100, 5,1) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Left-skewed\", breaks=seq(min(tmp),max(tmp), length.out=13), col=\"firebrick\") tmp <- rbeta(100,2,2) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Heavy-tailed\", col=\"firebrick\") Residuals versus Order Plot: Checks Assumption #5 When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated. | Show Examples | (Read more…) Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Good: No Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: General Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ qsec, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: Interesting Patterns\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),]) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Bad: Obvious Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") Problems from Failed Assumptions There are various problems that can arise when certain of the regression assupmtions are not satisfied. Lack of Linearity When the linearity assumption is violated, pretty much everything we obtain from the regression summary is no longer meaningful. The y-intercept estimate can be drastically off from its actual true value. Important model information is lost by trying to use a simple slope term \\(\\beta_1\\) to describe the model with respect to \\(X\\). The residual standard error will be much higher than it otherwise would be because of curvature patterns in the data that the line cannot capture. Thus, R-squared will be lower than it otherwise should be. P-values can become non-significant, when in fact there is a strong pattern in the data, but that pattern just cannot be captured by a simple line. *Normality of the errors is often put into question as well when a simplified line is used to try to capture a more complicated curved model. The plot below demonstrate these difficulties. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 7.5 #True slope beta_2 <- -0.25 #True bend X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors Y_i <- beta_0 + beta_1*X_i + beta_2*X_i^2 + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Non-Linear Relation\") abline(mylm, col=\"gray\") #Add fitted line to plot curve(beta_0 + beta_1*x + beta_2*x^2, col=\"gray\", lty=2, add=TRUE) #Add True line to plot #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (True value:\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (True value:\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (True value:\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Non-normal Error Terms When the normality assumption for the error terms is violated, not all is lost. In fact, the estimate of the slope and intercept are still often fairly meaningful. However, it is unwise to put too much trust in the residual standard error as an estimate of the standard deviation \\(\\sigma\\). This is because the standard deviation in skewed distributions does not carry the same meaning it has in normal distributions. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rchisq(n, 1)*3 - 1 #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Unconstant Variance When variance of the error term changes across the regression, the regression approximates the “average variance” of the errors because the regression is still assuming the variance is constant across the regression. The estimates of the slope and intercept are still typically quite good, and can be used for interpretation. The residual standard error however should not be considered to be meaningful as it will be too large on one end of the regression and too small on the other end. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma + X_i) #normally distributed errors #with increasing variance Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Variance Varies (Non-Constant)\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 13.82 Slope 3.5 3.768 Sigma 2.5 13.02 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", round(mean(sigma + X_i), 2), \", mean)\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Normality Violated As silly as it sounds, if the only problem with the regression is the lack of normality of the error terms, it isn’t all that big of a problem. Depending on how non-normal the residuals appear, there could be some skewing to the residual standard error, but otherwise, the slope and intercept are still interpretable and meaningful. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- runif(n, -sqrt(12*sigma^2)/2, sqrt(12*sigma^2)/2) #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 14.7 Slope 3.5 3.449 Sigma 2.5 2.317 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Independence Assumption Violated While the slope and intercept are often still meaningful when the independence assumption is violated, the residual standard error is unnecessarily large in this case. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, 2.5) + (1:n -n/2)*.5 #normal, but correlated errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Independence Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 16.47 Slope 3.5 3.296 Sigma 2.5 4.819 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Outliers Present While outliers do not violate any of the regression assumptions, they do pose substantial difficulties for the least squares regression estimates of the slope and intercept. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors epsilon_i[3] <- ifelse(X_i[3] < 10, runif(1,25,35), -runif(1,25,35)) #create outlier Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"An Outlier Present\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 13.18 Slope 3.5 3.447 Sigma 2.5 6.389 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) The material below this section is meant for Math 425 students only. Estimating the Model Parameters (Expand) How to get \\(b_0\\) and \\(b_1\\): least squares & maximum likelihood… There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical. The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas. Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 <- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\). When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\). Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i. \\label{exp} \\end{equation}\\] Least Squares{#leastSquares} To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\). \\[ Q = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2 \\] Then we use the function Q as if it were a function of \\(\\beta_0\\) and \\(\\beta_1\\). Ironically, the values of \\(Y\\) and \\(X\\) are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we don’t know are the values of \\(\\beta_0\\) and \\(\\beta_1\\). This least squares applet is a good way to explore how various choices of the slope and intercept yield different values of the “sum of squared residuals”. But it turns out that there is one “best” choice of the slope and intercept that yields a “smallest” value of the “sum of squared residuals.” This best choice can actually be found using calculus by taking the partial derivatives of \\(Q\\) with respect to both \\(\\beta_0\\) and \\(\\beta_1\\). \\[ \\frac{\\partial Q}{\\partial \\beta_0} = -2\\sum (Y_i - \\beta_0 - \\beta_1X_i) \\] \\[ \\frac{\\partial Q}{\\partial \\beta_1} = -2\\sum X_i(Y_i-\\beta_0-\\beta_1X_i) \\] Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize \\(Q\\) for a given set of data. After all the calculations are completed we find the values of the parameter estimators \\(b_0\\) and \\(b_1\\) (of \\(\\beta_0\\) and \\(\\beta_1\\), respectively) are as stated previously. Maximum Likelihood{#mle} The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of \\(\\beta_0\\) and \\(\\beta_1\\) which minime the least squares \\(Q\\) function, we choose the values of \\(\\beta_0\\) and \\(\\beta_1\\) which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the \\(Y_i\\) for all observations \\(i=1,\\ldots,n\\). We can do this rather simply by using the assumption that the errors, \\(\\epsilon_i\\) are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if \\(f(Y_i)\\) denotes the probability density function for \\(Y_i\\), then the joint probability density for all \\(Y_i\\), \\(f(Y_1,\\ldots,Y_n)\\) is given by \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) \\] Since each \\(Y_i\\) is assumed to be normally distributed with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\) (see model (\\(\\ref{model}\\))) we have that \\[ f(Y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{Y_i-\\beta_0-\\beta_1X_i}{\\sigma}\\right)^2\\right]} \\] which provides the joint probability as \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] The likelihood function \\(L\\) is then given by consider the \\(Y_i\\) and \\(X_i\\) fixed and the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) as the variables in the function. \\[ L(\\beta_0,\\beta_1,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] Instead of taking partial derivatives of \\(L\\) directly (with respect to all parameters) we take the partial derivatives of the \\(\\log\\) of \\(L\\), which is easier to work with. In a similar, but more difficult calculation, to that of minimizing \\(Q\\), we obtain the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) which maximize the log of \\(L\\), and which therefore maximize \\(L\\). (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate \\(\\hat{\\sigma}^2\\) of \\(\\sigma^2\\). \\[ \\hat{\\sigma}^2 = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n} \\] Estimating the Model Variance (Expand) Estimating \\(\\sigma^2\\) with MSE… As shown previously in the “Estimating Model Parameters” section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation. Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\). This estimate turns out to be a biased estimator. This means that it is consistently wrong in its estimates of \\(\\sigma^2\\). If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong. This is bad. Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\). Without going into all the details, to “fix” the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently. To find the correct degrees of freedom, we have to notice that the \\(\\hat{Y}_i\\) in the numerator of \\(\\widehat{\\sigma}^2\\) is defined by \\[\\begin{equation} \\widehat{Y}_i = b_0 + b_1X_i \\label{hatY} \\end{equation}\\] From this equation, we notice that two means, \\(\\bar{X}\\) and \\(\\bar{Y}\\), were estimated from the data in order to obtain \\(\\hat{Y}_i\\). (See the formulas for \\(b_0\\) and \\(b_1\\) above, and note how they use both \\(\\bar{X}\\) and \\(\\bar{Y}\\) in their calculation.) Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for \\(\\hat{\\sigma}^2\\) should be \\(n-2\\) instead of \\(n\\). Some incredibly long calculations will show that the “fixed” estimator \\[\\begin{equation} s^2 = MSE = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n-2} \\quad \\text{(Unbiased Estimator of $\\sigma^2$)} \\end{equation}\\] is an unbiased estimator of \\(\\sigma^2\\). Here \\(MSE\\) stands for mean squared error, which is the most obvious name for a formula that squares the errors \\(Y_i-\\hat{Y}_i\\) then adds them up and divides by their degrees of freedom. Similarly, we call the numerator \\(\\sum(Y_i-\\hat{Y}_i)^2\\) the sum of the squared errors, denoted by \\(SSE\\). It is also important to note that the errors are often denoted by \\(r_i = Y_i-\\hat{Y}_i\\), the residuals. Putting this all together we get the following equivalent statements for \\(MSE\\). \\[\\begin{equation} s^2 = MSE = \\frac{SSE}{n-2} = \\frac{\\sum(Y_i-\\widehat{Y}_i)^2}{n-2} = \\frac{\\sum r_i^2}{n-2} \\end{equation}\\] As a final note, even though the expected value \\(E\\{MSE\\} = \\sigma^2\\), which shows \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\), it unfortunately isn’t true that \\(\\sqrt{MSE}\\) is an unbiased estimator of \\(\\sigma\\). This presents a few problems later on, but these are minimal enough that we can overlook the issue and move forward. Transformations (Expand) \\(Y'\\), \\(X'\\), and returning to the original space… Y transformations are denoted by y-prime, written \\(Y'\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\). \\[ Y' = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y' = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y' = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y' = \\log(Y)\\) lm(log(Y) ~ X) 0.25 \\(Y' = \\sqrt(\\sqrt(Y))\\) lm(sqrt(sqrt(Y)) ~ X) 0.5 \\(Y' = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y' = Y\\) lm(Y ~ X) 2 \\(Y' = Y^2\\) lm(Y^2 ~ X) Using “maximum-likelihood” estimation, the Box-Cox procedure can actually automatically detect the “optimal” value of \\(\\lambda\\) to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise. Scatterplot Recognition Box-Cox Suggestion An Example Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\). set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Ya <- 1/sqrt(Y) #1/Y^2 Lam = -2 Yb <- 1/Y #1/Y Lam = -1 Yc <- exp(.02*Y) #log(Y) Lam = 0 Yd <- Y^2 #sqrt(Y) Lam = 0.5 Ye <- Y #Y Lam = 1 Yf <- sqrt(Y) #Y^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Ya ~ X, main=expression(paste(\"Use \", lambda == -2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ya^-2 ~ X)) curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yb ~ X, main=expression(paste(\"Use \", lambda == -1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yb^-1 ~ X)) curve(1/(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yc ~ X, main=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(log(Yc) ~ X)) curve(exp(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yd ~ X, main=expression(paste(\"Use \", lambda == 0.5)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(sqrt(Yd) ~ X)) curve((b[1] + b[2]*x)^2, add=TRUE, col=\"green\", lwd=2) plot(Ye ~ X, main=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ye ~ X)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yf ~ X, main=expression(paste(\"Use \", lambda == 2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yf^2 ~ X)) curve(sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try. par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0)) boxCox(lm(Ya ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -2)), line=.5) boxCox(lm(Yb ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -1)), line=.5) boxCox(lm(Yc ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), line=.5) boxCox(lm(Yd ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0.5)), line=.5) boxCox(lm(Ye ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), line=.5) boxCox(lm(Yf ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 2)), line=.5) An Example Suppose we were running a simple linear regression on the cars dataset. This would be done with the code cars.lm <- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesn’t quite fit the data as well as we would hope. Instead, the data looks a little curved. cars.lm <-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") abline(cars.lm, col=\"gray\") Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful. library(car) boxCox(cars.lm) The output from the boxCox(...) function looks as follows. This plot tells use to use the \\(\\lambda = 0.5\\) transformation, so that \\(Y' = Y^0.5 = \\sqrt{Y}\\). (To see this yourself, click on the “Box-Cox Suggestion” tab above, as well as on the “Scatterplot Recognition” tab.) Now, a transformation regression is performed using sqrt(Y) in place of Y as follows: cars.lm.t <- lm(sqrt(dist) ~ speed, data=cars) summary(cars.lm.t)   Estimate Std. Error t value Pr(> (Intercept) 1.277 0.4844 2.636 0.01126 speed 0.3224 0.02978 10.83 1.773e-14 Then, \\[ \\widehat{Y}_i' = 1.277 + 0.3224 X_i \\] And replacing \\(\\hat{Y}_i' = \\sqrt{\\hat{Y}_i}\\) we have \\[ \\sqrt{\\widehat{Y}_i} = 1.277 + 0.3224 X_i \\] Solving for \\(\\hat{Y}_i\\) gives \\[ \\widehat{Y}_i = (1.277 + 0.3224 X_i)^2 \\] Which, using curve((1.277 + 0.3224*x)^2, add=TRUE) (see code for details) looks like this: plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") curve( (1.277 + 0.3224*x)^2, add=TRUE, col=\"firebrick\") X-Transformations X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them. The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated. The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation. set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Xa <- 1/sqrt(X) #1/X^2 Lam = -2 Xb <- 1/X #1/X Lam = -1 Xc <- exp(.02*X) #log(X) Lam = 0 Xd <- X^2 #sqrt(X) Lam = 0.5 Xe <- X #X Lam = 1 Xf <- sqrt(X) #X^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Y ~ Xa, main=expression(paste(\"Use \", X*minute == X^-2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xa^-2))) curve(b[1] + b[2]*x^-2, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xb, main=expression(paste(\"Use \", X*minute == X^-1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xb^-1))) curve(b[1] + b[2]*x^-1, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xc, main=expression(paste(\"Use \", X*minute == log(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ log(Xc))) curve(b[1] + b[2]*log(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xd, main=expression(paste(\"Use \", X*minute == sqrt(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ sqrt(Xd))) curve(b[1] + b[2]*sqrt(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xe, main=expression(paste(\"Use \", X*minute == X, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ Xe)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xf, main=expression(paste(\"Use \", X*minute == X^2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xf^2))) curve(b[1] + b[2]*x^2, add=TRUE, col=\"green\", lwd=2) Inference for the Model Parameters (Expand) t test formulas, sampling distributions, confidence intervals, and F tests… When fitting the regression model given by the equation \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] to a sample of data, we typically test hypotheses about the parameters \\(\\beta_0\\), \\(\\beta_1\\), or both. Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_0 - \\overbrace{0}^\\text{a number}}{s_{b_0}}\\] This is the formula for the test statistic. It measures how far the estimated y-intercept \\(b_0\\) is from the null hypothesis for \\(\\beta_0\\) in units of “standard errors of \\(b_0\\)”. Thus the division by \\(s_{b_0}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0, it could be any number. \\(H_0: \\beta_1 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_1\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_1 - \\overbrace{0}^\\text{a number}}{s_{b_1}}\\] This is the formula for the test statistic. It measures how far the estimated slope \\(b_1\\) is from the null hypothesis for \\(\\beta_1\\) in units of “standard errors of \\(b_1\\)”. Thus the division by \\(s_{b_1}\\). Though the hypothesized value of \\(\\beta_1\\) is typically 0, it could be any number. Left-tailed p-value = pt(-abs(tvalue), degrees of freedom). Double it to get the two-sided p-value. In R, these values correspond to the output summary of an lm as follows. (Show Example) Consider the cars data in R. Suppose we used the regression model given by \\[ \\underbrace{Y_i}_\\text{Feet to Stop} = \\beta_0 + \\beta_1 \\underbrace{X_{i}}_\\text{mph} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\quad \\sim N(0,\\sigma^2) \\] to model the feet a vehicle (from the 1920’s) takes to stop when traveling at a certain speed (in miles per hour, mph) prior to stopping. When the regression is performed and summarized in R, it is always testing the following two hypotheses: \\[ H_0: \\beta_0 = 0 \\quad\\quad H_0: \\beta_1 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\quad\\quad H_a: \\beta_1 \\neq 0 \\] To perform the test of these hypotheses for the regression stated above, we would run the following codes in R. cars.lm <- lm(dist ~ speed, data=cars) pander(summary(cars.lm)$coefficients) These would produce summary output like the following, but the following output has been labeled with the math notation corresponding to each value. Let’s emphasize what is happening in this summary output table. First, here is how the “t value” is calculated for the “(Intercept)” in the summary table above. \\[ t = \\frac{b_0-0}{s_{b_0}} = \\frac{-17.58 - 0}{6.758} = -2.601 \\] Second, here is a visual representation of how the P-value, the “Pr(>|t|)” as it is called in the summary table above, is calculated for this test statistic. (Click the graph to view an interactive applet showing this calculation.) Notice both ends of the t-distribution are being shaded to compute the P-value because the alternative hypothesis was \\(H_a: \\beta_0 \\neq 0\\). To compute the P-value in R, we use the “percentile function for the t-distribution” called pt( ). This function requires two things, the t-value and the degrees of freedom, in our case pt(-2.601, 48). Note the degrees of freedom (df) are 48 because the sample size is \\(n=50\\) and there are two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) in our regression model. Running this code in R gives: pt(-2.601, 48) = 0.00616 However, note that this value is only half of the actual P-value of 0.0123. To get the “two-sided” P-value (note that our alternative hypothesis used a \\(\\neq\\) symbol) we need to double this left-tailed P-value. 2*pt(-2.601, 48)) = 0.0123 Finally, note that the same procedure can be used to test hypotheses that use a value other than 0 in the null and alternative. For example, to test the hypotheses: \\[ H_0: \\beta_1 = 3 \\\\ H_a: \\beta_1 \\neq 3 \\] Use the t-formula \\[ t = \\frac{b_1 - 3}{s_{b_1}} = \\frac{\\overbrace{3.932}^{b_1} - \\overbrace{3}^{H_0}}{\\underbrace{0.4155}_{s_{b_1}}} = 2.243 \\] then the P-value is calculated in R by 2*pt(-abs(2.243), 48) = 0.0295495 To obtain confidence intervals in R use confint(mylm). Confidence Interval Formula Standard Error \\(\\beta_0\\) \\(b_0 \\pm\\) \\(t^*\\) This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\). Use qt(0.975, df) to get \\(t*\\) in R. \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_0\\). \\(s_{b_0}\\) The standard error of \\(b_0\\), denoted by \\(s_{b_0}\\) is provided in the regression summary output under the column header called “Std. Error” for the “(Intercept)” row of the output. It is calculated using the formula shown below. \\[s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\] This is called the “estimated variance of \\(b_0\\)”. Taking the square root of this number gives the “standard error of \\(b_0\\)”. \\(\\beta_1\\) \\(b_1 \\pm\\) \\(t^*\\) This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\) (sample size - number of parameters in the regression model). Use qt(0.975, df) to get \\(t*\\) in R \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_1\\). \\(s_{b_1}\\) The standard error of \\(b_1\\), denoted by \\(s_{b_1}\\) is provided in the regression summary output under the column header called “Std. It is calculated using the formula shown below. \\[s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\] This is called the “estimated variance of \\(b_1\\)”. Taking the square root of this number gives the “standard error of \\(b_1\\)”. To be more exact, the types of inference we are interested in are the following. Determine if there is evidence of a meaningful linear relationship in the data. If \\(\\beta_1 = 0\\), then there is no relation between \\(X\\) and \\(E\\{Y\\}\\). Hence we might be interested in testing the hypotheses \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) is some hypothesized number. To provide a confidence interval for the true value of \\(\\beta_1\\). Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the sampling distribution of the estimate \\(b_1\\) of the parameter \\(\\beta_1\\). And, while we are at it, we may as well come to understand the sampling distribution of the estimate \\(b_0\\) of the parameter \\(\\beta_0\\). Review sampling distributions from Math 221. Since \\(b_1\\) is an estimate, it will vary from sample to sample, even though the truth, \\(\\beta_1\\), remains fixed. (The same holds for \\(b_0\\) and \\(\\beta_0\\).) It turns out that the sampling distribution of \\(b_1\\) (where the \\(X\\) values remain fixed from study to study) is normal with mean and variance: \\[ \\mu_{b_1} = \\beta_1 \\] \\[ \\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} \\] ## Simulation to Show relationship between Standard Errors ##----------------------------------------------- ## Edit anything in this area... n <- 100 #sample size Xstart <- 30 #lower-bound for x-axis Xstop <- 100 #upper-bound for x-axis beta_0 <- 2 #choice of true y-intercept beta_1 <- 3.5 #choice of true slope sigma <- 13.8 #choice of st. deviation of error terms ## End of Editable area. ##----------------------------------------------- # Create X, which will be used in the next R-chunk. X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) ## After playing this chunk, play the next chunk as well. To see that this is true, consider the regression model with values specified for each parameter as follows. \\[ Y_i = \\overbrace{\\beta_0}^{2} + \\overbrace{\\beta_1}^{3.5} X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\overbrace{\\sigma^2}^{\\sigma=13.8}) \\] Using the equations above for \\(\\mu_{b_1}\\) and \\(\\sigma^2_{b_1}\\) we obtain that the mean of the sampling distribution of \\(b_1\\) will be \\(\\mu_{b_1} = \\beta_1 = 3.5\\) Further, we see that the variance of the sampling distribution of \\(b_1\\) will be \\(\\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} = \\frac{13.8^2}{4.25\\times 10^{4}}\\) Taking the square root of the variance, the standard deviation of the sampling distribution of \\(b_1\\) will be \\(\\sigma_{b_1} = 0.067\\). That’s very nice. But to really believe it, let’s run a simulation ourselves. The “Code” below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of \\(b_1\\) and \\(b_0\\) for that regression. After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of \\(b_0\\) is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of \\(b_1\\) is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of \\(\\beta_0\\) or \\(\\beta_1\\), respectively. Also, the “Std. Error” of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of \\(\\sigma_{b_0}\\) and \\(\\sigma_{b_1}\\), respectively. N <- 5000 #number of times to pull a random sample storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N) for (i in 1:N){ Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model mylm <- lm(Y ~ X) storage_b0[i] <- coef(mylm)[1] storage_b1[i] <- coef(mylm)[2] storage_rmse[i] <- summary(mylm)$sigma } layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3)) Ystart <- 0 #min(0,min(Y)) Ystop <- 500 #max(max(Y), 0) Yrange <- Ystop - Ystart plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), ylim=c(Ystart, Ystop), pch=16, col=\"gray\", main=\"Regression Lines from many Samples (gray lines) \\n Plus Residual Standard Deviation Lines (green lines)\") text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1) text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1) text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1) for (i in 1:N){ abline(storage_b0[i], storage_b1[i], col=\"darkgray\") } abline(beta_0, beta_1, col=\"green\", lwd=3) abline(beta_0+sigma, beta_1, col=\"green\", lwd=2) abline(beta_0-sigma, beta_1, col=\"green\", lwd=2) abline(beta_0+2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0-2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0+3*sigma, beta_1, col=\"green\", lwd=.5) abline(beta_0-3*sigma, beta_1, col=\"green\", lwd=.5) par(mai=c(1,.6,.5,.01)) addnorm <- function(m,s, col=\"firebrick\"){ curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2) lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col) lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col) lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col) lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col) lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col) lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col) lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col) legend(\"topleft\", legend=paste(\"Std. Error = \", round(s,3)), cex=0.7, bty=\"n\") } h0 <- hist(storage_b0, col=\"skyblue3\", main=\"Sampling Distribution\\n Y-intercept\", xlab=expression(paste(\"Estimates of \", beta[0], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m0 <- mean(storage_b0) s0 <- sd(storage_b0) addnorm(m0,s0, col=\"green\") h1 <- hist(storage_b1, col=\"skyblue3\", main=\"Sampling Distribution\\n Slope\", xlab=expression(paste(\"Estimates of \", beta[1], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m1 <- mean(storage_b1) s1 <- sd(storage_b1) addnorm(m1,s1, col=\"green\") t Tests{#tTests} Using the information above about the sampling distributions of \\(b_1\\) and \\(b_0\\), an immediate choice of statistical test to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) can be zero, or any other value, is a t test given by \\[ t = \\frac{b_1 - \\beta_{10}}{s_{b_1}} \\] where \\(s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\). (You may want to review the section “Estimating the Model Variance” of this file to know where MSE came from.) With quite a bit of work it has been shown that \\(t\\) is distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. The nearly identical test statistic for testing \\[ H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_0 \\neq \\beta_{00} \\] is given by \\[ t = \\frac{b_0 - \\beta_{00}}{s_{b_0}} \\] where \\(s^2_{b_0} = MSE\\left[\\frac{1}{n}+\\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\). This version of \\(t\\) has also been shown to be distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. Confidence Intervals Creating a confidence interval for either \\(\\beta_1\\) or \\(\\beta_0\\) follows immediately from these results using the formulas \\[ b_1 \\pm t^*_{n-2}\\cdot s_{b_1} \\] \\[ b_0 \\pm t^*_{n-2}\\cdot s_{b_0} \\] where \\(t^*_{n-2}\\) is the critical value from a t distribution with \\(n-2\\) degrees of freedom corresponding to the chosen confidence level. F tests{#Ftests} Another way to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\quad\\quad \\text{or} \\quad\\quad H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\quad\\quad \\ \\ \\quad \\quad H_a: \\beta_0 \\neq \\beta_{00} \\] is with an \\(F\\) Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an \\(F\\) test is very general and can be used in many places that a t test cannot. In its most general form, the \\(F\\) test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that \\(H_0:\\beta_1=0\\) against the alternative that \\(H_a: \\beta_1\\neq 0\\), we are essentially comparing two models against each other. If \\(\\beta_1=0\\), then the corresponding model would be \\(E\\{Y_i\\} = \\beta_0\\). If \\(\\beta_1\\neq0\\), then the model remains \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the \\(F\\) Test, that the null model (reduced model) have fewer “free” parameters than the alternative model (full model). To demonstrate what we mean by “free” parameters, consider the following example. Say we wanted to test the hypothesis that \\(H_0:\\beta_1 = 2.5\\) against the alternative that \\(\\beta_1\\neq2.5\\). Then the null, or reduced model, would be \\(E\\{Y_i\\}=\\beta_0+2.5X_i\\). The alternative, or full model, would be \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). Thus, the null (reduced) model contains only one “free” parameter because \\(\\beta_1\\) has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two “free” parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model. Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing \\(H_0: \\beta_1=0\\) against \\(H_a:\\beta_1\\neq0\\) we have the partition \\[ \\underbrace{Y_i-\\bar{Y}}_{Total} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{Regression} + \\underbrace{Y_i-\\hat{Y}_i}_{Error} \\] The reason we use \\(\\bar{Y}\\) for the null model is that \\(\\bar{Y}\\) is the unbiased estimator of \\(\\beta_0\\) for the null model, \\(E\\{Y_i\\} = \\beta_0\\). Thus we would compute the following sums of squares: \\[ SSTO = \\sum(Y_i-\\bar{Y})^2 \\] \\[ SSR = \\sum(\\hat{Y}_i-\\bar{Y})^2 \\] \\[ SSE = \\sum(Y_i-\\hat{Y}_i)^2 \\] and note that \\(SSTO = SSR + SSE\\). Important to note is that \\(SSTO\\) uses the difference between the observations \\(Y_i\\) and the null (reduced) model. The \\(SSR\\) uses the diffences between the alternative (full) and null (reduced) model. The \\(SSE\\) uses the differences between the observations \\(Y_i\\) and the alternative (full) model. From these we could set up a General \\(F\\) table of the form   Sum Sq Df Mean Sq F Value Model Error \\(SSR\\) \\(df_R-df_F\\) \\(\\frac{SSR}{df_R-df_F}\\) \\(\\frac{SSR}{df_R-df_F}\\cdot\\frac{df_F}{SSE}\\) Residual Error \\(SSE\\) \\(df_F\\) \\(\\frac{SSE}{df_F}\\) Total Error \\(SSTO\\) \\(df_R\\) Prediction and Confidence Intervals for \\(\\hat{Y}_h\\) (Expand) predict(…, interval=“prediction”)… It is a common mistake to assume that averages (means) describe individuals. They do not. So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line. Interval R Code Math Equation When to Use Prediction predict(..., interval=\"prediction\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\text{Pred}\\ Y}\\) Predict an individual’s value. Confidence predict(..., interval=\"confidence\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\hat{Y}}\\) Estimate location of the mean y-value. predict(mylm, data.frame(XvarName = number), interval=...) For example, consider this graph. Then click here to read about the graph. Notice the three dots above 15 mph in the graph. Each of these dots show a car that was going 15 mph when it applied the brakes. However, stopping distances of the three individual cars differ with one at 20 feet, one at 26 feet and one at 54 feet. The regression line represents the average stopping distance of cars. In this case, cars going 15 mph are estimated to have an average stopping distance of about 40 feet, as shown by the line. But individual vehicles, all going the same speed of 15 mph, varied from stopping distances of 20 feet up to 54 feet! So, to predict that a car going 15 mph will take 41.4 feet to stop, doesn’t tell the whole story. Far more revealing is the complete statement, “Cars going 15 mph are predicted to take anywhere from 10.2 to 72.6 feet to stop, with an average stopping distance of 41.4 feet.” This is called the “prediction interval” and is shown in the graph in blue. It is obtained in R with the codes: cars.lm <- lm(dist ~ speed, data=cars) predict(cars.lm, data.frame(speed=15), interval=\"prediction\") cars.lm <- lm(dist ~ speed, data=cars) pander(predict(cars.lm, data.frame(speed=15), interval=\"prediction\")) fit lwr upr 41.41 10.17 72.64 plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=\"firebrick2\", cex=1.5) cars.lm <- lm(dist ~ speed, data=cars) abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3)) abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2)) abline(v=15, lty=2, col=\"firebrick\") preds <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12) lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8)) lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8)) Now, for the details behind prediction intervals and confidence intervals. Let’s begin by recalling some details (from the section “Inference for the Model Parameters”) about the standard error of the y-intercept, \\(b_0\\). Recall that the y-intercept is the average y-value for the given x-value of \\(x=0\\). Recall further that the formula for the standard error of \\(b_0\\) is given by the formula \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] If we wanted to be more exact with this formula, we would write it as \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{(0-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice the addition of \\((0 - \\bar{X})^2\\) instead of just \\(\\bar{X}^2\\) in the numerator of the right-most part of the equation? This more complete statement obviously would reduce to just \\(\\bar{X}^2\\), but that is only because \\(X=0\\) when we are working with the y-intercept, \\(b_0\\). We could be working with other values of \\(X\\) than just zero. Let’s take a quick detour and talk notation for a second. Typically, \\(X_i\\) and \\(Y_i\\) are used to denote the x-value and y-value of points that are contained in our data set. When we want to reference a point that wasn’t within our original data set, we use the notation \\(X_h\\) and \\(Y_h\\). (The letter h is close to i, but different from i, so why not. There is really no other reason to use h.) Thus, \\(Y_h\\) is the y-value for the \\(X_h\\) x-value, neither of which were included in our original regression of \\(X_i\\)’s and \\(Y_i\\)’s. Now, back to the previous discussion. If \\(X_h = 0\\), then \\(\\hat{Y}_h\\) is the y-intercept, so \\(\\hat{Y}_h = b_0\\) when \\(X_h=0\\). So, we could write, \\[ s^2_{\\hat{Y}_h} = MSE\\left[\\frac{1}{n} + \\frac{(X_h-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice how the \\(b_0\\) in \\(s_{b_0}\\) was replaced with \\(\\hat{Y}_h\\) to get \\(s_{\\hat{Y}_h}\\) and the 0 in \\((0 - \\bar{X})^2\\) was replaced with \\(X_h\\) to get \\((X_h - \\bar{X})^2\\)? Interesting. We now have a formula that would give us the standard error of \\(\\hat{Y}_h\\) for any \\(X_h\\) value, not just \\(X_h = 0\\), or the y-intercept, \\(b_0\\). That is fantastic. It would look like this if plotted. Notice how the gray region is showing the standard error for each \\(\\hat{Y}_h\\) value? (It is technically showing the confidence interval for \\(E\\{Y_h\\}\\) at every possible \\(X_h\\) value, but that is just \\(\\hat{Y}_h \\pm t^* \\cdot s_{\\hat{Y}_h}\\).) ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + theme_bw() ## `geom_smooth()` using formula = 'y ~ x' Confidence Interval for \\(\\hat{Y}_h\\) \\[ \\hat{Y}_h \\pm t^* s_{\\hat{Y}_h} \\quad \\text{where} \\ s_{\\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] The confidence interval is a wonderful tool for estimating \\(E\\{Y_h\\}\\), the “true” average y-value for a given x-value of \\(X_h\\). However, it is not valuable for predicting an individual dot, or \\(Y_h\\) value. Notice how few of the dots of the regression are actually contained within the confidence interval band in the plot? The confidence interval does not really predict where the dots will land, just where the average y-value is located for each x-value. Remember the 68-95-99.7 Rule of the normal distribution? If not, here is a link back to that concept in the Math 221 textbook. This rule states that roughly 95% of data, when normally distributed, will be between \\(z=-2\\) and \\(z=2\\) standard deviations from the mean. So, is going two “residual standard errors” to both sides of the regression line enough to capture 95% of the data? The answer is, not quite. The reason for this is because our knowledge of where the true mean lies is uncertain. (Notice the confidence interval band shown in the plot.) However, adding two standard errors to the edges of the confidence band would get us in the right place. In other words, there are two sources of variability at play here, (1) our uncertaintity in where the regression line is sitting, and (2) the natural variability of the data points around the line. Thus, the “prediction interval” requires accounting for both of these sources of variability to produce the following equation. Prediction Interval for \\(Y_h\\) \\[ \\hat{Y}_h \\pm t^* s_{Pred \\hat{Y}_h} \\quad \\text{where} \\ s_{Pred \\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + 1 + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] This formula provides a useful band for identifying a region where we are 95% confident that a new observation for \\(Y_h\\) will land, given the value of \\(X_h\\). It looks as follows. Notice the prediction interval is much wider than the confidence interval. This is because data varies far more than do means. Prediction is for where the individual data points will land, confidence is for where the mean will land. cars.lm <- lm(dist ~ speed, data=cars) predy <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + geom_segment(aes(x=15, xend=15, y=predy[2], yend=predy[3]), lwd=4, color=rgb(.5,.7,.5,.01)) + geom_point(aes(x=15, y=predy[1]), cex=2, color=\"skyblue\", pch=15) + theme_bw() Lowess (and Loess) Curves (Expand) A non-parametric approach to estimating \\(E\\{Y_i\\}\\)… Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value. Using Base R # Select the relevant columns and remove NA values air2 <- airquality %>% dplyr::select(Temp, Ozone) %>% # Use dplyr's select function explicitly drop_na() # Drop rows with NA values in Temp or Ozone plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") lines(lowess(air2$Ozone, air2$Temp), col=\"firebrick\") Using ggplot2 # Your dataset (air2) should have been created earlier, ensuring no NAs air2 <- na.omit(airquality[c(\"Temp\", \"Ozone\")]) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=\"darkgray\") + # Add points with dark gray color geom_smooth(se=FALSE, method=\"loess\", method.args = list(degree=1)) + # Lowess curve, no standard error shading theme_bw() # Black and white theme ## `geom_smooth()` using formula = 'y ~ x' # Optionally, allow for predictions as well as the graph: # Uncomment the following lines if you want predictions as well: # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fitted, x=Ozone)) # Use fitted values from loess Advantages Disadvantages Quick. Good at ignoring outliers. Good at capturing the general pattern in the data. Good for making predictions within the scope of the data. No mathematical model. Not interpretable. No p-values. No adjusted R-squared. How it Works The Lowess curve localizes the regression model to a “neighborhood” of points, and then joins these localized regressions together into a smooth line. It minimizes the effect of outliers, and let’s the data “speak for itself”. As a downside, it is not interpretable, and has no final way to write the model mathematically. All the same, it is a very powerful tool for identifying an appropriate model, or verifying the fit of a model, or making predictions when no reasonable model does an adequate job. Study this graphic and the explanations below to learn how it works. Recommendation: run the code in this “Code” chunk to the right in your Console, and flip through the resulting graphics. X <- cars$speed Y <- cars$dist X <- X[!is.na(X) & !is.na(Y)] Y <- Y[!is.na(X) & !is.na(Y)] f <- 1/2 n <- length(X) lfit <- rep(NA,n) for (xh in 1:n){ xdists <- X - X[xh] nn <- floor(n*f) r <- sort(abs(xdists))[nn] xdists.nbrhd <- which(abs(xdists) < r) w <- rep(0, length(xdists)) w[xdists.nbrhd] <- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w), col=rgb(.2,.2,.2,.3), cex=1.5, yaxt='n', xaxt='n', xlab=\"\", ylab=\"\") points(Y[xh] ~ X[xh], pch=16, col=\"orange\") lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2)) cat(\"\\n\\n\") readline(prompt=paste0(\"Center point is point #\", xh, \"... Press [enter] to continue...\")) MADnotThereYet <- TRUE count <- 0 while(MADnotThereYet){ readline(prompt=paste0(\"\\n Adjusting line to account for outliers in the y-direction... Press [enter] to continue...\")) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"wheat\", add=TRUE) MAD <- median(abs(lmc$res)) resm <- lmc$res/(6*MAD) resm[resm>1] <- 1 bisq <- (1-resm^2)^2 w <- w*bisq obs <- coef(lmc) lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) count <- count + 1 if ( (sum(abs(obs-lmc$coef))<.1) | (count > 3)) MADnotThereYet <- FALSE } curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"green\", add=TRUE) points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=\"green\") readline(prompt=paste0(\"\\n Use final line to get fitted value for this point... Press [enter] to continue to next point...\")) lfit[xh] <- predict(lmc, data.frame(X=X[xh])) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") if (xh == n){ readline(prompt=paste0(\"\\n Press [enter] to see actual Lowess curve...\")) lines(lowess(X,Y, f=f), col=\"firebrick\") legend(\"topleft\", bty=\"n\", legend=\"Actual lowess Curve using lowess(...)\", col=\"firebrick\", lty=1) } } Select a fraction of the data to use for the “neighborhood” of points (shown in blue in the graph above). The lowess function in R uses “f=2/3” and the loess function uses “span=0.75” for this value, which selects the nearest two-thirds or 75% of the data, respectively, depending on which function you use. For this example, we set the fraction of points at 50%. Both functions can be set to whatever you want. Pick any point in the regression, eventually selecting all points one at a time. The selected point becomes the “center” of a “neighborhood” of points surrounding it. In this example, the center point is in orange, and the neighboring points are in blue. Use the points within the neighborhood to fit a regression line. However, make the regression depend most on points closest to “center” and least on points furthest from “center.” This is called a weighted regression. Weights are decided according to what is called the tricubic weight function, so that the weight \\(w\\) given to point \\(j\\) of the neighborhood of points is defined by \\[ w_j = \\left(1- \\left( \\frac{|X_c - X_j|}{\\max_k |X_c - X_k|}\\right)^3\\right)^3 \\] where \\(X_c\\) is the x-value of the “center” dot and \\(X_j\\) is the x-value of any other dot in the neighborhood. The fitted-value of \\(\\hat{Y}_c\\) is obtained for the center point \\(X_c\\) of the current regression. This point is used as the Lowess (or Loess) curve’s value at that particular x-value. Well, almost. It’s a first guess at where this value will end up, but there’s a little more to the algorithm before we are done. Initial guesses for each of these fitted values are obtained for each point in the regression. Now each local regression for each neighborhood is re-run a few times in such a way the the effect of outliers is minimized. The final line for each neighborhood is obtained by the following steps. Compute all residuals for points in the neighborhood of the current regression, denoted by \\(r_i\\). Then compute the MAD, median absolute deviation, of the residuals \\(MAD = \\text{median} (|r_1|, |r_2|, \\ldots)\\). Divide all residuals by 6 times the MAD: \\(u_i = r_i/(6\\cdot MAD)\\) (If \\(r_i > 6\\cdot MAD\\) then set \\(u_i = 0\\).) Compute what are called bisquare weights using the formula: \\(b_i = (1 - u_i^2)^2\\) Perform a regression using the weights \\(w_i = w_i b_i\\) Repeat the above process with the new weights \\(w_i\\) until the weights stop changing very much. The final fitted values for each \\(X\\)-value in the regression are obtained from the final regression line for each neighborhood. These fitted values make up the Lowess (or loess) curve. Note that the default of the loess function in R is to use quadratic regressions in each neighborhood instead of linear regressions. This can be controlled with the loess option of “degree=2” (quadratic fits) or “degree = 1”. In the lowess function only a linear regression in each neighborhood is allowed. Examples: bodyweight, cars Quantitative Y | Multiple X Graphics Y is a single quantitative variable of interest, like height. While we could use an X1 of “shoe-size” to explain height, we might also want to use a second x-variable, X2, like “gender” to help explain height. Further x-variables could also be used. Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a student’s final exam grade? How do factors like age, weight, exercise level, and diet impact a person’s blood pressure? Examples: Is one Navy base more prone to heatwaves than the other? The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH) Tests Multiple Linear Regression Multiple regression is a way to understand how different factors affect something we’re interested in. It’s like having more than one clue to solve a mystery. Here’s what you need to know: We can use several different pieces of information (called variables) to predict an outcome. There are many ways to combine these pieces of information. We start with simple combinations and build up to more complex ones, kind of like building with Lego blocks. This helps us make better predictions and understand complex situations more clearly. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model is a bit like using the same ingredient twice in a recipe, but in different ways. It uses the same factor (\\(X_i^2\\)) in two different forms: Once in its regular form (\\(\\beta_1 X_i\\)) Once squared (\\(\\beta_2 X_i^2\\)) This squared part is what we call the “quadratic” term. It helps us describe relationships that aren’t just straight lines, but have curves to them. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and “steepness” of the Model: negative values face down, positive values face up; large values imply “steeper” parabolas and low values imply “flatter” parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)’s explanation. An Example Using the airquality data set, we run the following “quadratic” regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our “quadratic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. Temp Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These “Estimates” can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(Month^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of Month from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, )   Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will “open down” (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be “month zero.” Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the “cubic” term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the “base slope coefficient” and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following “cubic” regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our “cubic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. uptake Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^3) \\(X_{i}^3\\), where the function I(…) protects the cubing of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will “open up”. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The “two-lines” model is a way to compare two different groups in statistics using numbers. Here’s how it works: It uses two types of numbers: A regular number that can be any value (\\(X_{1i}\\)) A special number that’s either 0 or 1 (called an “indicator variable”, \\(X_{2i}\\)) The special number (\\(X_{2i}\\)) helps us turn group information (like “Group A” or “Group B”) into numbers we can use in calculations. This model creates two separate lines on a graph: One line for Group A One line for Group B By using this method, we can easily see and compare how the two groups are different from each other. Think of it like having two recipes for the same dish, but with slightly different ingredients. This model helps us see how those small changes affect the final result. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the “base-line” of the model, the “Group 0” line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the “base-line” line. \\(\\beta_3\\) Called the “interaction” term. Controls the change in the slope for the second line in the model as compared to the slope of the “base-line” line. An Example Using the mtcars data set, we run the following “two-lines” regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our “two-lines” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. mpg Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) (beta zero): This is like the starting point. It’s estimated by something called the “Intercept”. \\(\\beta_1\\) (beta one): This shows how one thing changes when another thing (called “qsec”) changes. It’s estimated to be 1.439. \\(\\beta_2\\) (beta two): This shows how something else (called “am”) affects the overall result. It’s estimated to be -14.51. \\(\\beta_3\\) (beta three): This shows how “qsec” and “am” work together to affect the result. It’s estimated to be 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called “3D” regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to “bend”. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the “Temp” direction is estimated to be 2.659 and the slope in the “Month” direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction term’s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the “slopes” in each direction to change, creating a “curved” surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called “HD”, or “High Dimensional”, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isn’t really even possible to “mentally connect” with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)’s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the “Wind” direction is estimated to be -3.334. The slope in the “Temp” direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the “Solar.R” direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here. R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the “Overview” section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the “column (c) bind” function and it joins together things as columns. Res =  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals.  panel=panel.smooth,  This places a lowess smoothing line on each scatterplot. col =  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(“color1”,“color2”, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for “linear model.” Y Y must be a “numeric” vector of the quantitative response variable.  ~  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats “numeric” variables as quantitative and “character” or “factor” variables as qualitative. R will automatcially recode qualitative variables to become “numeric” variables using a 0,1 encoding. See the Explanation tab for details.  + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details.  + …, * ... emphasizes that as many explanatory variables as are desired can be included in the model.  data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(…) function displays the results of an lm(…) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(…) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -4.3818 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -2.2696 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.1344 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   1.7058 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   5.8752 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero.   2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (26.6248479) by its standard error (2.1829432). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a “star”. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + …).   -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model.   0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” by its standard error. It gives the “number of standard errors” away from zero that the “estimate” has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + …).   5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\).   2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot “.” implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like.   0.0004029 This is the estimate of the coefficient of the interaction term.   0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data.  0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that “at least one is not.”  34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom.  on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero.  p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that “at least one” of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the “Overview” sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =… Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “prediction”) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “confidence”) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BIC… There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (“R-squared”). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The “simplest” but “best” model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it “an information criterion (AIC)” when he published the method and other people later began calling it the “Akaike Information Criterion.”) Model Selection (Expand) pairs plots, added variable plots, and pattern recognition… Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots {.tabset .tabset-pills} A useful visualization tool for model selection is the “pairs plot.” This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice that… the y-axis of each plot is found by locating the variable name (like “mpg”) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like “disp”) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model {.tabset .tabset-pills} Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldn’t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Let’s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a model’s ability to generalize to new data… The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesn’t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, let’s remind ourselves why we use regression models in the first place. The main goal is to capture the “essence” of the data. In other words, the general pattern is what we are after. We want a model that tells us how “all such” data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the “complicated model” is overfitting the original data. It does not capture the “essence” of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-value… The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the model… The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions{#check} The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read more…) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) Outlier Analysis (Expand) Cook’s Distances and Leverage Values… The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs. fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cook’s Distances The idea behind Cook’s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article “Detection of Influential Observation in Linear Regression” (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, let’s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2))   1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the “differences” in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cook’s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cook’s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cook’s Distances using the code cooks.distance(lmObject). Also, a graph of Cook’s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of “leverage” and is “pulling” the regression toward itself. A value near 0 implies the point is just “one of many” and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that “minimize” the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)’s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the “hat matrix” \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the “leverage values” and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a “lot of pull,” and values close to 0 represent “little pull.” In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with “lots of leverage” and a large “Cook’s Distance” are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regression… Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class. Examples: Civic Vs Corolla cadillacs Binomial Y | Quantitative X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t. We might use an explanatory variable X of “height” to see if taller students are more likely to get an A in Math 325 than shorter students. (They aren’t, if you were wondering.) Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1’s and 0’s (this does happen or this doesn’t happen): Does the width of a child’s foot predict whether or not they are right-handed? What is the probability the cat adopted is a male based on how many days it took them to get adopted? Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted? (Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\") Tests Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The “P” stands for “Probability that…” \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1… This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)… in other words, the “|” says “given” and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the “natural constant” number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14…) Part What it means \\(P(Y_i = 1| x_i)\\) - The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s) - The symbol used to represent the notation in logisitc regression is \\(\\pi_i\\) \\(Y_i\\) (Responsive Variable) Describes the probability of $Y_i = $ 1 or 0 for a given scenario. \\(x_i\\) (Explanatory variable) Describes the observed value and how it effects the \\(Y_i\\) \\(\\frac{e^{\\text{stuff}}}{1+e^{\\text{stuff}}}\\) The content for “stuff” being simple linear regression of the form \\(\\beta_0 + \\beta_{1x_i}\\) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -1.5651 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.6648 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -0.2460 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   0.7276 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   2.2691 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The test statistic is a regular old z-score. It is most reliable when the sample size is “large.” It is a measurement of the number of standard errors the estimate is from 0.   Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds.   1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, …).   -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds.   0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a “star”. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about.   Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\).  43.230  on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates.  29.732 This can be calculated by sum(log(myglm$res^2)).  on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, “A version of Akaike’s An Information Criterion…” The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model.  33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 ## disp -0.014604 0.005168 -2.826 0.00471 ## ## (Intercept) * ## disp ** ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0’s or 1’s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == “B” or height < 68. In other words, Y needs to be a collection of 0’s and 1’s.  ~  The tilde is read “Y on X.” X, X is some quantitative variable.  data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(…) function. curve( A function in R that draws a “curve” on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the “Intercept” estimate from your logistic regression summary output.  \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case “x” in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula.  exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator.  add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(…) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set.  aes( The aes(…) function stands for “aesthetics” and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The plus sign adds a new layer to the ggplot.   geom_point( Tells the plot to add the physical geometry of “points” to the plot. ) Closing parenthesis for the geom_point() function.  + Add another layer to the plot.   geom_smooth( Add a smoothing line to the graph. method=“glm”, This adds a general linear model to the graph. method.args = list(family=“binomial”), This tells the method=“glm” to choose specifically the “binomial” model, otherwise known as the “logistic regression” model.  se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function.  + Add another layer to the ggplot.   theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),  The xVariableName is the same one you used in your glm(y ~ x, …) statement for “x”. Input any desired value for the “someNumericValue” spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016 Explanation Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal). The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\). Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page. Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful. Checking Model Assumptions {#diagnostics} The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit. Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease. Binomial Y | Multiple X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t. We might use an explanatory variable X1 of “height” and a second explanatory variable X2 of “gender” to try to predict whether or not a student will get an A in Math 325. Test(s) used for this category: Multiple Logistic Regression Model Tests Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two. Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. …, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -2.9446 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.2364 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.0000 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q   0.2776 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   1.9968 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The Z-score testing the hypothesis that \\(\\beta_j=0\\)   Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, …).   0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\).   1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\).   4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\).   3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\).   0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\).   0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a “star”. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\).   471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option.   Null Deviance: The deviance of the null model.  800.44 In this case, the null deviance is 800.44  on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression.  256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit.  on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model.  272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities ## numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) -4.97603 0.65316 -7.618 ## Time 0.39111 0.04979 7.854 ## Diet2 0.58283 1.04061 0.560 ## Diet3 -8.44476 4.32324 -1.953 ## Diet4 -67.41995 3768.10023 -0.018 ## Time:Diet2 0.02391 0.08679 0.275 ## Time:Diet3 1.20955 0.51249 2.360 ## Time:Diet4 8.83168 471.01259 0.019 ## Pr(>|z|) ## (Intercept) 2.57e-14 *** ## Time 4.02e-15 *** ## Diet2 0.5754 ## Diet3 0.0508 . ## Diet4 0.9857 ## Time:Diet2 0.7830 ## Time:Diet3 0.0183 * ## Time:Diet4 0.9850 ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, …), X2 = c(Value 1, Value 2, …), …) command. You should see the GSS example file for an example of how to use this function. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the “b” vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(“SomeColor”,“DifferentColor”,…) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(…) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==“B”.  ~  The formula operator in R. X1, The first x-variable in your glm code.  data = YourDataSet, Specify the name of your data set.  pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(…) function computes e^(stuff) in R and stands for the “exponential function.” (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(…) function requires that you call the x-variable “x” although you can change this behavior using xname=“SomeOtherName” if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)).  col = palette()[1], Pull the first color from the color palette for the first curve.  add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters.  col = palette()[2], Set the color of this second curve to the second color in the palette.  add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. “topright”, Typically the legend is placed in the top-right corner of the graph. But it could be placed “top”, “topleft”, “left”, “bottomleft”, “bottom”, “center”, “right”, or “bottomright”.  legend = Why you have to write legend again, no one knows, but you do. c(“Lable 1”, “Label 2”), These are the values that will appear in the legend, one entry on each line of the legend.  col = palette(), This specifies the colors of the symbols in the legend. First color goes with “Label 1”, and second goes with “Label 2” and so on.  lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist.  bty = Specifies the “box type” (bty) that should be drawn around the legend. ‘n’) By placing a “no” option (‘n’) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot.  aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + Add a layer to the ggplot.   geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function.  + Add a layer to the ggplot.   geom_smooth(method=“glm”, method.args=list(family=“binomial”), se=FALSE, Add the logistic regression curve to the plot.   aes(color=“X2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function.  + Add a layer to the plot.   theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x' Explanation Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative. The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations. Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant. Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression. Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\). Categorical Y | Categorical X Graphics Y is a single categorical variable of interest, like gender. X is another categorical variable of interest, like “hair color”. This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender. Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year? Whats the probability that you have brown hair and blue eyes? Tests Add your own notes about appropriate inferential procedures for this type of data here.",
    "sentences": ["Graphics Y is a single quantitative variable of interest, like “height”.", "X is another single quantitative variable of interest, like “shoe-size”.", "This would imply we are using “shoe-size” (X) to explain “height” (Y).", "Test(s) used for this category: Simple Linear Regression Analysis : Simple Linear Regression: BYU-Idaho Math Tutoring Used to find the relationship between one independent variable (predictor) and one dependent variable(outcome): Is there a relationship between the length of children’s feet and the width of their foot?", "Example: Does the amount of tutoring a student receives correlate with their chapter exam scores?", "(Simple Linear Regression) ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams)", "Tests Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\).", "Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$\\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$$ \\(Y_i\\) The response variable.", "The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size.", "\\(=\\) This states that we are assuming \\(Y_i\\) was created, or is “equal to” the formula that will follow on the right-hand-side of the equation."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Quantitative Y | Quantitative X"
  },
  {
    "id": 95,
    "title": "📍 Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single quantitative variable of interest, like “height”. X is another single quantitative variable of interest, like “shoe-size”. This would imply we are using “shoe-size” (X) to explain “height” (Y). Test(s) used for this category: Simple Linear Regression Analysis : Simple Linear Regression: BYU-Idaho Math Tutoring Used to find the relationship between one independent variable (predictor) and one dependent variable(outcome): Is there a relationship between the length of children’s feet and the width of their foot? Example: Does the amount of tutoring a student receives correlate with their chapter exam scores? (Simple Linear Regression) ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams)",
    "sentences": ["Y is a single quantitative variable of interest, like “height”.", "X is another single quantitative variable of interest, like “shoe-size”.", "This would imply we are using “shoe-size” (X) to explain “height” (Y).", "Test(s) used for this category: Simple Linear Regression Analysis : Simple Linear Regression: BYU-Idaho Math Tutoring Used to find the relationship between one independent variable (predictor) and one dependent variable(outcome): Is there a relationship between the length of children’s feet and the width of their foot?", "Example: Does the amount of tutoring a student receives correlate with their chapter exam scores?", "(Simple Linear Regression) ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams)"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 96,
    "title": "📍 Tests",
    "url": "index.html#tests",
    "content": "Tests Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\). Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$\\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$$ \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is “equal to” the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to “God’s Law” or “Natural Law”. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are “random” and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)… \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced “y-hat”, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what follows… \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, “the change in the average y-value for a one unit change in the x-value.” It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, “the average y-value when x is zero.” It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful. R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command.  <-  This is the “left arrow” assignment operator that stores the results of your lm() code into mylm name. lm( lm(…) is an R function that stands for “Linear Model”. It performs a linear regression analysis for Y ~ X. Y  Y is your quantitative response variable. It is the name of one of the columns in your data set. ~  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X. X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value.  data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X. ) Closing parenthesis for the lm(…) function. summary(mylm) The summary command allows you to print the results of your linear regression that were previously saved in mylm name.  Click to Show Output  Click to View Output. Example output from a regression. Hover each piece to learn more. Call: lm(formula = dist ~ speed, data = cars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -29.069 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -9.525 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -2.272 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -2.272 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-2.272) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   9.215 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   43.201 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -17.5791 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   6.7584 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -2.601 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (-17.5791) by its standard error (6.7584). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of -17.5791 is -2.601 standard errors (6.7584) from zero, which is a fairly surprising distance as shown by the p-value. 0.0123 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). speed This is always the name of your X-variable in your lm(Y ~ X, …).   3.9324 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.4155 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 9.464 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” of the slope (3.9324) by its standard error (0.4155). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from zero, which is a really surprising distance as shown by the smallness of the p-value. 1.49e-12 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  15.38 For this particular regression, the estimate of \\(\\sigma\\) is 15.38. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 48 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 50 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 50-2 = 48. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.6511, In this particular regression, 65.11% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data.  0.6438 In this case, the value of 0.6438 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is found as the ratio of the MSR/MSE where MSR = SSR/(p-1) and MSE = SSE/(n-p) where n is the sample size and p is the number of parameters in the regression model.  89.57 This is the value of the F-statistic for the lm(dist ~ speed, data=cars) regression. Note that SSE = sum( cars.lm$res^2 ) = 11353.52 with n - p = 50 - 2 = 48 degrees of freedom for this data. Further, SSR = sum( (cars.lm$fit - mean(cars$dist))^2 ) = 21185.46 with p - 1 = 1 degree of freedom. So MSR = 21185.46 and MSE = 11353.52 / 48 = 236.5317. So MSR / MSE = 21185.46 / 236.5317 = 89.56711.  on 1 and 48 DF, The 1 degree of freedom is the SSR degrees of freedom (p-1). The 48 is the SSE degrees of freedom (n-p).  p-value: 1.49e-12 The p-value for an F-statistic is found by the code pf(89.56711, 1, 48, lower.tail=FALSE), which gives the probability of being more extreme than the observed F-statistic in an F distribution with 1 and 48 degrees of freedom. Check Assumptions 1, 2, 3, and 5 When making it in an r chunk make your height 3! { r, fig.height = 3 } par( The par(…) command stands for “Graphical PARameters”. It allows you to control various aspects of graphics in Base R. mfrow= This stands for “multiple frames filled by row”, which means, put lots of plots on the same row, starting with the plot on the left, then working towards the right as more plots are created. c( The combine function c(…) is used to specify how many rows and columns of graphics should be placed together. 1, This specifies that 1 row of graphics should be produced. 3 This states that 3 columns of graphics should be produced. ) Closing parenthesis for c(…) function. ) Closing parenthesis for par(…) function. plot( This version of plot(…) will actually create several regression diagnostic plots by default. mylm, This is the name of an lm object that you created previously. which= This allows you to select “which” regression diagnostic plots should be drawn. 1 Selecting 1, would give the residuals vs. fitted values plot only. : The colon allows you to select more than just one plot. 2 Selecting 2 also gives the Q-Q Plot of residuals. If you wanted to instead you could just use which=1 to get the residuals vs fitted values plot, then you could use qqPlot(mylm$residuals) to create a fancier Q-Q Plot of the residuals. ) Closing parenthesis for plot(…) function. plot( This version of plot(…) will be used to create a time-ordered plot of the residuals. The order of the residuals is the original order of the x-values in the original data set. If the original data set doesn’t have an order, then this plot is not interesting. mylm The lm object that you created previously. $ This allows you to access various elements from the regression that was performed. residuals This grabs the residuals for each observation in the regression. ) Closing parenthesis for plot(…) function.  Click to Show Output  Click to View Output. Which regression assumption(s) does the residuals versus fitted values plot diagnose? Linear Relation Constance Variance Which regression assumption(s) does the Q-Q Plot of the residuals diagnose? Normal Errors Which regression assumption(s) does the Residuals versus Order plot diagnose? (Remember, this plot can only be created when the data was collected in a specific order.) Independent Errors Plotting the Regression Line Base R ggplot2 To add the regression line to a scatterplot use the abline(...) command: plot( The plot(…) function is used to create a scatterplot with a y-axis (the vertical axis) and an x-axis (the horizontal axis). Y  This is the “response variable” of your regression. The thing you are interested in predicting. This is the name of a “numeric” column of data from the data set called YourDataSet. ~  The tilde “~” is used to relate Y to X and can be found on the top-left key of your keyboard. X,  This is the explanatory variable of your regression. It is the name of a “numeric” column of data from YourDataSet. data= The data= statement is used to specify the name of the data set where the columns of “X” and “Y” are located. YourDataSet This is the name of your data set, like KidsFeet or cars or airquality. ) Closing parenthesis for plot(…) function. abline( This stands for “a” (intercept) “b” (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm This is the name of an lm(…) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(…) function will locate these two values from within mylm and use them to add a line to your current plot(…). ) Closing parenthesis for abline(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm) You can customize the look of the regression line with abline( This stands for “a” (intercept) “b” (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm, This is the name of an lm(…) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(…) function will locate these two values from within mylm and use them to add a line to your current plot(…). lty= The lty= stands for “line type” and allows you to select between 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. 1, This creates a solid line. Remember, other options include: 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. lwd= The lwd= allows you to specify the width of the line. The default width is 1. Using lwd=2 would double the thickness, and so on. Any positive value is allowed. 1, Default line width. To make a thicker line, us 2 or 3… To make a thinner line, try 0.5, but 1 is already pretty thin. col= This allows you to specify the color of the line using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). “someColor” Type colors() in R for options. ) Closing parenthesis for abline(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm, lty=1, lwd=1, col=\"firebrick\") You can add points to the regression with… points( This is like plot(…) but adds points to the current plot(…) instead of creating a new plot. newY  newY should be a column of values from some data set. Or, use points(newX, newY) to add a single point to a graph. ~  This links Y to X in the plot. newX,  newX should be a column of values from some data set. It should be the same length as newY. If just a single value, use points(newX, newY) instead. data=YourDataSet,  If newY and newX come from a dataset, then use data= to tell the points(…) function what data set they come from. If newY and newX are just single values, then data= is not needed. col=“skyblue”, This allows you to specify the color of the points using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). pch=16 This allows you to specify the type of plotting symbol to be used for the points. Type ?pch and scroll half way down in the help file that appears to learn about other possible symbols. ) Closing parenthesis for points(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) points(7,40, pch=16, col=\"skyblue\", cex=2) text(7,40, \"New Dot\", pos=3, cex=0.5) points(dist ~ speed, data=filter(cars, mylm$res > 2), cex=.8, col=\"red\") abline(mylm, lty=1, lwd=1, col=\"firebrick\") To add the regression line to a scatterplot using the ggplot2 approach, first ensure: library(ggplot2) or library(tidyverse) is loaded. Then, use the geom_smooth(method = lm) command: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. YourDataSet,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. X, This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet.  y =  “y= ” declares which variable will become the y-axis of the graphic. Y This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function. + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. + Here the + is used to add yet another layer to ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. There are several other methods that could be used here.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(…) to place more complicated models on the plot.  se = FALSE se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. ) Closing parenthesis for the geom_smooth() function.  Click to Show Output  Click to View Output. ggplot(cars, aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = \"lm\", formula=y~x, se=FALSE) There are a number of ways to customize the appearance of the regression line: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. speed,  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet. y =  “y= ” declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic.  + Here the + is used to add yet another layer to ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(…) to place more complicated models on the plot.  se = FALSE, se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.  size = 2, Use size = 2 to adjust the thickness of the line to size 2.  color = “orange”, Use color = “orange” to change the color of the line to orange.   linetype = “dashed” Use linetype = “dashed” to change the solid line to a dashed line. Some linetype options include “dashed”, “dotted”, “longdash”, “dotdash”, etc. ) Closing parenthesis for the geom_smooth() function.  Click to Show Output  Click to View Output. In addition to customizing the regression line, you can customize the points, add points, add lines, and much more. ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. speed,  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet. y =  “y= ” declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point( geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. size = 1.5, Use size = 1.5 to change the size of the points.  color = “skyblue” Use color = “skyblue” to change the color of the points to Brother Saunders’ favorite color.  alpha = 0.5 Use alpha = 0.5 to change the transparency of the points to 0.5. ) Closing parenthesis of geom_point() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in ways similar to lm(…) to place more complicated models on the plot.  se = FALSE, se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.  color = “navy”, Use color = “navy” to change the color of the line to navy blue.  size = 1.5 Use size = 1.5 to adjust the thickness of the line to 1.5. ) Closing parenthesis of geom_smooth() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_hline( Use geom_hline() to add a horizontal line at a specified y-intercept. You can also use geom_vline(xintercept = some_number) to add a vertical line to the graph. yintercept = Use “yintercept =” to tell geom_hline() that you are going to declare a y intercept for the horizontal line.  75 75 is the value of the y-intercept. , color = “firebrick” Use color = “firebrick” to change the color of the horizontal line to firebrick red. , size = 1, Use size = 1 to adjust the thickness of the horizontal line to size 1.              linetype = “longdash” Use linetype = “longdash” to change the solid line to a dashed line with longer dashes. Some linetype options include “dashed”, “dotted”, “longdash”, “dotdash”, etc. , alpha = 0.5 Use alpha = 0.5 to change the transparency of the horizontal line to 0.5. ) Closing parenthesis of geom_hline function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_segment( geom_segment() allows you to add a line segment to ggplot() by using specified start and end points. x = “x =” tells geom_segment() that you are going to declare the x-coordinate for the starting point of the line segment.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the starting point of the line segment.  y =“y =” tells geom_segment() that you are going to declare the y-coordinate for the starting point of the line segment.  75, 75 is a number on the y-axis of your graph. It is the y-coordinate of the starting point of the line segment.  xend = “xend =” tells geom_segment() that you are going to declare the x-coordinate for the end point of the line segment.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the end point of the line segment.  yend = “yend =” tells geom_segment() that you are going to declare the y-coordinate for the end point of the line segment.  38, 38 is a number on the y-axis of your graph. It is the y-coordinate of the end point of the line segment.                size = 1 Use size = 1 to adjust the thickness of the line segment. , color = “lightgray” Use color = “lightgray” to change the color of the line segment to light gray. , linetype = “longdash” Use *linetype = “longdash* to change the solid line segment to a dashed one. Some linetype options include”dashed”, “dotted”, “longdash”, “dotdash”, etc. ) Closing parenthesis for geom_segment() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_point( geom_point() can also be used to add individual points to the graph. Simply declare the x and y coordinates of the point you want to plot. x = “x =” tells geom_point() that you are going to declare the x-coordinate for the point.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the point.  y = “y =” tells geom_point() that you are going to declare the y-coordinate for the point.  75 75 is a number on the y-axis of your graph. It is the y-coordinate of the point. , size = 3 Use size = 3 to make the point stand out more. , color = “firebrick” Use color = “firebrick” to change the color of the point to firebrick red. ) Closing parenthesis of the geom_point() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_text( geom_text() allows you to add customized text anywhere on the graph. It is very similar to the base R equivalent, text(…). x = “x =” tells geom_text() that you are going to declare the x-coordinate for the text.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the text.  y = “y =” tells geom_text() that you are going to declare the y-coordinate for the text.  84, 84 is a number on the y-axis of your graph. It is the y-coordinate of the text.  label = “label =” tells geom_text() that you are going to give it the label.  “My Point (14, 75)”, “My Point (14, 75)” is the text that will appear on the graph.             color = “navy” Use color = “navy” to change the color of the text to navy blue. , size = 3 Use size = 3 to change the size of the text. ) Closing parenthesis of the geom_text() function.  + The + allows you to add more layers to the framework provided by ggplot().   theme_minimal() Add a minimalistic theme to the graph. There are many other themes that you can try out.  Click to Show Output  Click to View Output. ## `geom_smooth()` using formula = 'y ~ x' Accessing Parts of the Regression Finally, note that the mylm object contains the names(mylm) of mylm$coefficients Contains two values. The first is the estimated \\(y\\)-intercept. The second is the estimated slope. ## (Intercept) speed ## -17.579095 3.932409 mylm$residuals Contains the residuals from the regression in the same order as the actual dataset. ## 1 2 3 4 ## 3.849460 11.849460 -5.947766 12.052234 ## 5 6 7 8 ## 2.119825 -7.812584 -3.744993 4.255007 ## 9 10 11 12 ## 12.255007 -8.677401 2.322599 -15.609810 ## 13 14 15 16 ## -9.609810 -5.609810 -1.609810 -7.542219 ## 17 18 19 20 ## 0.457781 0.457781 12.457781 -11.474628 ## 21 22 23 24 ## -1.474628 22.525372 42.525372 -21.407036 ## 25 26 27 28 ## -15.407036 12.592964 -13.339445 -5.339445 ## 29 30 31 32 ## -17.271854 -9.271854 0.728146 -11.204263 ## 33 34 35 36 ## 2.795737 22.795737 30.795737 -21.136672 ## 37 38 39 40 ## -11.136672 10.863328 -29.069080 -13.069080 ## 41 42 43 44 ## -9.069080 -5.069080 2.930920 -2.933898 ## 45 46 47 48 ## -18.866307 -6.798715 15.201285 16.201285 ## 49 50 ## 43.201285 4.268876 mylm$fitted.values The values of \\(\\hat{Y}\\) in the same order as the original dataset. ## 1 2 3 4 5 ## -1.849460 -1.849460 9.947766 9.947766 13.880175 ## 6 7 8 9 10 ## 17.812584 21.744993 21.744993 21.744993 25.677401 ## 11 12 13 14 15 ## 25.677401 29.609810 29.609810 29.609810 29.609810 ## 16 17 18 19 20 ## 33.542219 33.542219 33.542219 33.542219 37.474628 ## 21 22 23 24 25 ## 37.474628 37.474628 37.474628 41.407036 41.407036 ## 26 27 28 29 30 ## 41.407036 45.339445 45.339445 49.271854 49.271854 ## 31 32 33 34 35 ## 49.271854 53.204263 53.204263 53.204263 53.204263 ## 36 37 38 39 40 ## 57.136672 57.136672 57.136672 61.069080 61.069080 ## 41 42 43 44 45 ## 61.069080 61.069080 61.069080 68.933898 72.866307 ## 46 47 48 49 50 ## 76.798715 76.798715 76.798715 76.798715 80.731124 mylm$… several other things that will not be explained here. Making Predictions predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ) Closing parenthesis for the data.frame(…) function. ) Closing parenthesis for the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12)) 1   29.60981 The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(…) function.  interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. “prediction” This specifies that a prediction interval will be included with the predicted value. A prediction interval gives you a 95% confidence interval that captures 95% of the data, or \\(Y_i\\) values for the specific \\(X\\)-value specified in the prediction. ) Closing parenthesis of the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"prediction\")   fit The “fit” is the predicted value.   lwr The “lwr” is the lower bound.   upr The “upr” is the upper bound. 1 29.60981 In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet. -1.749529 This is the lower bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us the stopping distance could be as quick as -1.749529 feet (or 0 because distance can’t go negative). 60.96915 This is the upper bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us that the actual stopping distance could be as high as 60.96915 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(…) function.  interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. “confidence” This specifies that a confidence interval for the prediction should be provided. This is of use whenever your interest is in just estimating the average y-value, not the actual y-values. ) Closing parenthesis of the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"confidence\")   fit The “fit” is the predicted value.   lwr The “lwr” is the lower bound.   upr The “upr” is the upper bound. 1 29.60981 In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. 24.39514 This is the lower bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is greater than this value. 34.82448 This is the upper bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is less than this value. Finding Confidence Intervals for Model Parameters confint( The R function confint(…) allows you to use an lm(…) object to compute confidence intervals for one or more parameters (like \\(\\beta_0\\) or \\(\\beta_1\\)) in your model. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  level = “level =” tells the confint(…) function that you are going to declare at what level of confidence you want the interval. The default is “level = 0.95.” If you want to find 95% confidence intervals for your parameters, then just run confint(mylm).  someConfidenceLevel someConfidenceLevel is simply a confidence level you choose when you want something other than a 95% confidence interval. Some examples of appropriate levels include 0.90 and 0.99. ) Closing parenthesis for confint(..) function. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.90)     5 % The lower bound of a 90% confidence interval occurs at the 5th percentile. This is because at 90% confidence, 10% is left in the tails, with 5% on each end. The upper bound will thus end at the 95th percentile, hence the 5% and 95% as the column names.   95 % The upper bound of a 90% confidence interval ends at the 95th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -28.914514 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -6.243676 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.235501 and 4.629317. 3.235501 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. 4.629317 This is the upper bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.95)     2.5 % The lower bound of a 95% confidence interval occurs at the 2.5th percentile. This is because at 95% confidence, 5% is left in the tails, with 2.5% on each end. The upper bound will thus end at the 97.5th percentile, hence the 2.5% and 97.5% as the column names for the lower and upper bounds, respectively.   97.5 % The upper bound of a 95% confidence interval ends at the 97.5th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -31.167850 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -3.990340 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.096964 and 4.767853. 3.096964 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.096964 and 4.767853 4.767853 This is the upper bound of the confidence interval. In this case, you can be 95% confident that the slope lies between 3.096964 and 4.767853 Explanation Linear regression has a rich mathematical theory behind it. This is because it uses a mathematical function and a random error term to describe the regression relation between a response variable \\(Y\\) and an explanatory variable called \\(X\\). Expand each element below to learn more. Regression Cheat Sheet (Expand) Term Pronunciation Meaning Math R Code \\(Y_i\\) $Y_i$ “why-eye” The data \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)\\) $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$ YourDataSet$YourYvariable \\(\\hat{Y}_i\\) $\\hat{Y}_i$ “why-hat-eye” The fitted line \\(\\hat{Y}_i = b_0 + b_1 X_i\\) $\\hat{Y}_i = b_0 + b_1 X_i$ lmObject$fitted.values \\(E\\{Y_i\\}\\) $E\\{Y_i\\}$ “expected value of why-eye” True mean y-value \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) $E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i$ <none> \\(\\beta_0\\) $\\beta_0$ “beta-zero” True y-intercept <none> <none> \\(\\beta_1\\) $\\beta_1$ “beta-one” True slope <none> <none> \\(b_0\\) $b_0$ “b-zero” Estimated y-intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) $b_0 = \\bar{Y} - b_1\\bar{X} b_0 <- mean(Y) - b_1*mean(X)$ \\(b_1\\) $b_1$ “b-one” Estimated slope \\(b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\) $b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})} {\\sum(X_i - \\bar{X})^2}$ b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) \\(\\epsilon_i\\) $\\epsilon_i$ “epsilon-eye” Distance of dot to true line \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) $\\epsilon_i = Y_i - E\\{Y_i\\}$ <none> \\(r_i\\) $r_i$ “r-eye” or “residual-eye” Distance of dot to estimated line \\(r_i = Y_i - \\hat{Y}_i\\) $r_i = Y_i - \\hat{Y}_i$ lmObject$residuals \\(\\sigma^2\\) $\\sigma^2$ “sigma-squared” Variance of the \\(\\epsilon_i\\) \\(Var\\{\\epsilon_i\\} = \\sigma^2\\)$Var\\{\\epsilon_i\\} = \\sigma^2$ <none> \\(MSE\\) $MSE$ “mean squared error” Estimate of \\(\\sigma^2\\) \\(MSE = \\frac{SSE}{n-p}\\)$MSE = \\frac{SSE}{n-p}$ sum( lmObject$res^2 ) / (n - p) \\(SSE\\) $SSE$ “sum of squared error” (residuals) Measure of dot’s total deviation from the line \\(SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\)$SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$ sum( lmObject$res^2 ) \\(SSR\\) $SSR$ “sum of squared regression error” Measure of line’s deviation from y-bar \\(SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\)$SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2$ sum( (lmObject$fit - mean(YourData$Y))^2 ) \\(SSTO\\) $SSTO$ “total sum of squares” Measure of total variation in Y \\(SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\)$SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2$ sum( (YourData$Y - mean(YourData$Y))^2 ) \\(R^2\\) $R^2$ “R-squared” Proportion of variation in Y explained by the regression \\(R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\)$R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}$ SSR/SSTO \\(r\\) $r$ “r” Correlation between X and Y. \\(r = \\sqrt{R^2}\\)$r = \\sqrt{R^2}$ sqrt(R^2) \\(\\hat{Y}_h\\) $\\hat{Y}_h$ “why-hat-aitch” Estimated mean y-value for some x-value called \\(X_h\\) \\(\\hat{Y}_h = b_0 + b_1 X_h\\)$\\hat{Y}_h = b_0 + b_1 X_h$ predict(lmObject, data.frame(XvarName=#)) \\(X_h\\) $X_h$ “ex-aitch” Some x-value, not necessarily one of the \\(X_i\\) values used in the regression \\(X_h =\\) some number$X_h = $ Xh = # Confidence Interval “confidence interval” Estimated bounds at a certain level of confidence for a parameter \\(b_0 \\pm t^* \\cdot s_{b_0}\\)b_0 \\pm t^* \\cdot s_{b_0} or \\(b_1 \\pm t^* \\cdot s_{b_1}\\)b_1 \\pm t^* \\cdot s_{b_1} confint(mylm, level = someConfidenceLevel) Parameter Estimate \\(\\beta_0\\) \\(b_0\\) \\(\\beta_1\\) \\(b_1\\) \\(\\epsilon_i\\) \\(r_i\\) \\(\\sigma^2\\) \\(MSE\\) \\(\\sigma\\) \\(\\sqrt{MSE}\\), the Residual standard error The Mathematical Model (Expand) \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)… There are three main elements to the mathematical model of regression. Each of these three elements is pictured below in the “Regression Relation Diagram.” Study both the three bullet points and their visual representations in the plot below for a clearer understanding. The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read more…) The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as “natural law” or “God’s law”. It is some true line that is unknown to us. The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line. The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\). Note: \\(E\\{Y\\}\\) is pronounced “the expected value of y” because, well… the mean is the typical, average, or “expected” value. The dots, i.e., the regression relation plus an error term: \\(Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}} + \\underbrace{\\epsilon_i}_\\text{error term} \\quad \\text{where} \\ \\epsilon_i\\sim N(0,\\sigma^2)\\) (Read more…) This is shown by the dots in the graph below. This is the data. In regression, the assumption is that the y-value for individual \\(i\\), denoted by \\(Y_i\\), was “created” by adding an error term \\(\\epsilon_i\\) to each individual’s “expected” value \\(\\beta_0 + \\beta_1 X_i\\). Note the “order of creation” would require first knowing an indivual’s x-value, \\(X_i\\), then their expected value from the regression relation \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) and then adding their \\(\\epsilon_i\\) value to the result. The \\(\\epsilon_i\\) allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance \\(\\epsilon_i\\) from the line. Note: \\(Y_i\\) is pronounced “why-eye” because it is the y-value for individual \\(i\\). Sometimes also called “why-sub-eye” because \\(i\\) is in the subscript of \\(Y\\). The estimated line, i.e., the line we get from a sample of data. \\(\\underbrace{\\hat{Y}_i}_{\\substack{\\text{estimated mean} \\\\ \\text{y-value}}} = \\underbrace{b_0 + b_1 X_i}_\\text{estimated regression equation}\\) (Read more…) The estimated line is shown by the solid line in the graph below. \\(\\hat{Y}\\) is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation \\(E\\{Y\\}\\). So \\(\\hat{Y}\\) is interpreted as the estimated average (or mean) \\(Y\\)-value for any given \\(X\\)-value. Thus, \\(b_0\\) is the estimated y-intercept and \\(b_1\\) is the estimated slope. The b’s are sample statistics, like \\(\\bar{x}\\) and the \\(\\beta\\)’s are population parameters like \\(\\mu\\). The \\(b\\)’s estimate the \\(\\beta\\)’s. Note: \\(\\hat{Y}_i\\) is pronounced “why-hat-eye” and is known as the “estimated y-value” or “fitted y-value” because it is the y-value you get from \\(b_0 + b_1 X_i\\). It is always different from \\(Y_i\\) because dots are rarely if ever exactly on the estimated regression line. This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line). Something to ponder: The true line, when coupled with the error terms, “creates” the data. The estimated (or fitted) line uses the sampled data to try to “re-create” the true line. We could loosely call this the “order of creation” as shown by the following diagram. par(mfrow=c(1,3), mai=c(.2,.2,.4,.1)) plot(y ~ x, col=\"white\", main=\"A Law is Given\", yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, main=\"Data is Created\", xaxt='n', yaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, xaxt='n', yaxt='n', main=\"The Law is Estimated\") curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) A Law is Given Data is Created The Law is Estimated \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) \\(Y_i = E\\{Y_i\\} + \\epsilon_i\\) \\(\\hat{Y}_i = b_0 + b_1 X_i\\) The true line is the “law”. The \\(Y_i\\) are created by adding \\(\\epsilon_i\\) to \\(E\\{Y_i\\}\\) where \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\). The law is estimated with \\(\\hat{Y}_i\\) which is given with lm(...). Click open the “Code” buttom below to the right to find code that runs a simulation demonstrating this “order of creation”. ## Simulating Data from a Regression Model ## This R-chunk is meant to be played in your R Console. ## It allows you to explore how the various elements ## of the regression model combine together to \"create\" ## data and then use the data to \"re-create\" the line. set.seed(101) #Allows us to always get the same \"random\" sample #Change to a new number to get a new sample n <- 3 #set the sample size X_i <- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45. beta0 <- 3 #Our choice for the y-intercept. beta1 <- .1 #Our choice for the slope. sigma <- 12.5 #Our choice for the std. deviation of the error terms. epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data View(fabData) #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it. fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData. summary(fab.lm) #Summarize your model. plot(y ~ x, data=fabData) #Plot the data. abline(fab.lm) #Add the estimated regression line to your plot. # Now for something you can't do in real life... but since we created the data... abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). legend(\"topleft\", legend=c(\"True Line\", \"Estimated Line\"), lty=c(2,1), bty=\"n\") #Add a legend to your plot specifying which line is which. Interpreting the Model Parameters (Expand) \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted as… The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model. If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\). The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\). It is often misunderstood to be the “average change in y” or just “the change in y” but it is more correctly referred to as the “change in the average y”. To better see this, consider the three graphics shown below. par(mfrow=c(1,3)) hist(mtcars$mpg, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Number of Vehicles\", xlab=\"Gas Mileage (mpg)\", col=\"skyblue\") boxplot(mpg ~ cyl, data=mtcars, border=\"skyblue\", boxwex=0.5, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Number of Cylinders of Engine (cyl)\") plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=\"skyblue\", main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Quarter Mile Time (qsec)\") abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=\"darkgray\") mtext(side=3, text=\"Automatic Transmissions Only (am==0)\", cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=\"gray\") The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09. The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg. The scatterplot on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value. Residuals and Errors (Expand) \\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true error… Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\). The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\). We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summary… Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\). \\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms. Keep in mind the idea that the errors \\(\\epsilon_i\\) “created” the data and that the residuals \\(r_i\\) are computed after using the data to “re-create” the line. Residuals have many uses in regression analysis. They allow us to diagnose the regression assumptions, See the “Assumptions” section below for more details. estimate the regression relation, See the “Estimating the Model Parameters” section below for more details. estimate the variance of the error terms, See the “Estimating the Model Variance” section below for more details. and assess the fit of the regression relation. See the “Assessing the Fit of a Regression” section below for more details. Assessing the Fit of a Regression (Expand) \\(R^2\\), SSTO, SSR, and SSE… Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line. par(mfrow=c(1,3), mai=c(.1,.1,.5,.1)) set.seed(2) x <- runif(30,0,20) y1 <- 2 + 3.5*x + rnorm(30,0,2) y2 <- 2 + 3.5*x + rnorm(30,0,8) y3 <- 2 + 3.5*x + rnorm(30,0,27) plot(y1 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Excellent Fit\") abline(lm(y1 ~ x), col=\"gray\") plot(y2 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Good Fit\") abline(lm(y2 ~ x), col=\"gray\") plot(y3 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Poor Fit\") abline(lm(y3 ~ x), col=\"gray\") A common way to measure the fit of a regression is with correlation. While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\). (If you are a Math 325 student, just stick with correlation for now and skip on to the next section of this Explanation tab. If you are a Math 425 student, it is critical that you come to understand \\(R^2\\) deeply, so read on.) Before you can understand \\(R^2\\), you must understand three important “sums of squares”. (Read more about sums…) A sum is just a fancy word for adding things together. \\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand. So we use the symbol \\(\\Sigma\\) to denote the word “sum”. Further, we use a subscript \\(\\Sigma_{i=1}\\) to state what value the sum is beginning with, and a superscript \\(\\Sigma_{i=1}^6\\) to state the value we are ending at. This gives \\[ \\sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Test your knowledge, do you see why the answer is 6 to the sum below? \\[ \\sum_{i=1}^3 i = 6 \\] Computing sums in R is fairly easy. Type the following codes in your R Console. sum(1:6) #gives the answer of 21 sum(1:3) #gives the answer of 6 However, sums really become useful when used with a data set. Each row of a data set represents an “individual’s” data. We can reference each individual with a row number. In the data below, individual 3, denoted by \\(i=3\\), has a speed of 7 and a dist of 4. pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3) Individual speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 To compute the sum of the speed column, use sum(speed). If we divided this sum by 6, we would get the mean of speed mean(speed). In fact, the two most used statistics mean(...) and sd(...) both use sums. Take a moment to review the formulas for mean and standard deviation. It is strongly recommended that you study the Explanation tab for both as well. We’ll wait. See you back here shortly. … Welcome back. Suppose we let X = speed and Y = dist. Then \\(X_3 = 7\\) and \\(Y_3 = 4\\) because we are accessing row 3 of both the \\(X\\) (or speed) column and \\(Y\\) (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, sum(speed) would be written mathematically as \\(\\sum_{i=1}^6 X_i\\) and sum(dist) would be written as \\(\\sum_{i=1}^6 Y_i\\). Sum of Squared Errors Sum of Squares Regression Total Sum of Squares \\(\\text{SSE} = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\) \\(\\text{SSR} = \\sum_{i=1}^n \\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\) \\(\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\\) Measures how much the residuals deviate from the line. Measures how much the regression line deviates from the average y-value. Measures how much the y-values deviate from the average y-value. Equals SSTO - SSR Equals SSTO - SSE Equals SSE + SSR sum( (Y - mylm$fit)^2 ) sum( (mylm$fit - mean(Y))^2 ) sum( (Y - mean(Y))^2 ) It is important to remember that SSE and SSR split up SSTO, so that \\[ \\text{SSTO} = \\text{SSE} + \\text{SSR} \\] This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works. The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called \\(R^2\\) (“r-squared”). R-Squared (\\(R^2\\)) \\[ \\underbrace{R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}}_\\text{Interpretation: Proportion of variation in Y explained by the regression.} \\] The smallest \\(R^2\\) can be is zero, and the largest it can be is 1. This is because \\(SSR\\) must be between 0 and SSTO, inclusive. Residual Plots & Regression Assumptions (Expand) Residuals vs. fitted-values, Q-Q Plot of the residuals, and residuals vs. order plots… There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate. Each assumption is labeled in the regression equation below. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions. (Read more…) As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not. Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3 The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(\\hat{Y}_i\\). The residuals are the \\(r_i\\). This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable. | Show Examples | (Read more…) The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption. The linear relation is assumed to be satisfied if there are no apparent trends in the plot. The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values. The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance. set.seed(2) X <- rnorm(30,15,3) notLin <- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8)) notLin.lm <- lm(Y~X, data=notLin) set.seed(15) Lin <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) Lin.lm <- lm(Y~X, data=Lin) par(mfrow=c(3,3), mai=c(.25,.25,.25,.25), mgp=c(1,.75,0)) plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Not Linear\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(notLin.lm$fitted.values,notLin.lm$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Good: Linear, Constant Variance\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) set.seed(6) notCon <- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5)) notCon.lm <- lm(Y~X, data=notCon) LinO <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) LinO[1] <- LinO[1]^2 LinO.lm <- lm(Y~X, data=LinO) plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Unconstant Variance\", cex.main=0.95, yaxt='n', xaxt='n', col=\"firebrick\") polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) # plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Outliers\", cex.main=0.95) # abline(h=0) tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(Girth ~ Volume, data=trees[-31,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(Height ~ Volume, data=trees) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) abline(h=0) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals, f=.4) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(weight ~ repwt, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(weight ~ repht, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) abline(h=0) Q-Q Plot of the Residuals: Checks Assumption #2 The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption. | Show Examples | (Read more…) There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot. Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) set.seed(123) tmp <- rnorm(100) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"skyblue\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Normal\", col=\"skyblue\") tmp <- Davis$weight qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Right-skewed\", breaks=15, col=\"firebrick\") par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- rbeta(100, 5,1) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Left-skewed\", breaks=seq(min(tmp),max(tmp), length.out=13), col=\"firebrick\") tmp <- rbeta(100,2,2) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Heavy-tailed\", col=\"firebrick\") Residuals versus Order Plot: Checks Assumption #5 When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated. | Show Examples | (Read more…) Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Good: No Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: General Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ qsec, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: Interesting Patterns\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),]) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Bad: Obvious Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") Problems from Failed Assumptions There are various problems that can arise when certain of the regression assupmtions are not satisfied. Lack of Linearity When the linearity assumption is violated, pretty much everything we obtain from the regression summary is no longer meaningful. The y-intercept estimate can be drastically off from its actual true value. Important model information is lost by trying to use a simple slope term \\(\\beta_1\\) to describe the model with respect to \\(X\\). The residual standard error will be much higher than it otherwise would be because of curvature patterns in the data that the line cannot capture. Thus, R-squared will be lower than it otherwise should be. P-values can become non-significant, when in fact there is a strong pattern in the data, but that pattern just cannot be captured by a simple line. *Normality of the errors is often put into question as well when a simplified line is used to try to capture a more complicated curved model. The plot below demonstrate these difficulties. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 7.5 #True slope beta_2 <- -0.25 #True bend X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors Y_i <- beta_0 + beta_1*X_i + beta_2*X_i^2 + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Non-Linear Relation\") abline(mylm, col=\"gray\") #Add fitted line to plot curve(beta_0 + beta_1*x + beta_2*x^2, col=\"gray\", lty=2, add=TRUE) #Add True line to plot #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (True value:\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (True value:\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (True value:\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Non-normal Error Terms When the normality assumption for the error terms is violated, not all is lost. In fact, the estimate of the slope and intercept are still often fairly meaningful. However, it is unwise to put too much trust in the residual standard error as an estimate of the standard deviation \\(\\sigma\\). This is because the standard deviation in skewed distributions does not carry the same meaning it has in normal distributions. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rchisq(n, 1)*3 - 1 #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Unconstant Variance When variance of the error term changes across the regression, the regression approximates the “average variance” of the errors because the regression is still assuming the variance is constant across the regression. The estimates of the slope and intercept are still typically quite good, and can be used for interpretation. The residual standard error however should not be considered to be meaningful as it will be too large on one end of the regression and too small on the other end. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma + X_i) #normally distributed errors #with increasing variance Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Variance Varies (Non-Constant)\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 13.82 Slope 3.5 3.768 Sigma 2.5 13.02 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", round(mean(sigma + X_i), 2), \", mean)\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Normality Violated As silly as it sounds, if the only problem with the regression is the lack of normality of the error terms, it isn’t all that big of a problem. Depending on how non-normal the residuals appear, there could be some skewing to the residual standard error, but otherwise, the slope and intercept are still interpretable and meaningful. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- runif(n, -sqrt(12*sigma^2)/2, sqrt(12*sigma^2)/2) #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 14.7 Slope 3.5 3.449 Sigma 2.5 2.317 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Independence Assumption Violated While the slope and intercept are often still meaningful when the independence assumption is violated, the residual standard error is unnecessarily large in this case. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, 2.5) + (1:n -n/2)*.5 #normal, but correlated errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Independence Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 16.47 Slope 3.5 3.296 Sigma 2.5 4.819 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Outliers Present While outliers do not violate any of the regression assumptions, they do pose substantial difficulties for the least squares regression estimates of the slope and intercept. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors epsilon_i[3] <- ifelse(X_i[3] < 10, runif(1,25,35), -runif(1,25,35)) #create outlier Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"An Outlier Present\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 13.18 Slope 3.5 3.447 Sigma 2.5 6.389 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) The material below this section is meant for Math 425 students only. Estimating the Model Parameters (Expand) How to get \\(b_0\\) and \\(b_1\\): least squares & maximum likelihood… There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical. The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas. Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 <- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\). When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\). Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i. \\label{exp} \\end{equation}\\] Least Squares{#leastSquares} To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\). \\[ Q = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2 \\] Then we use the function Q as if it were a function of \\(\\beta_0\\) and \\(\\beta_1\\). Ironically, the values of \\(Y\\) and \\(X\\) are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we don’t know are the values of \\(\\beta_0\\) and \\(\\beta_1\\). This least squares applet is a good way to explore how various choices of the slope and intercept yield different values of the “sum of squared residuals”. But it turns out that there is one “best” choice of the slope and intercept that yields a “smallest” value of the “sum of squared residuals.” This best choice can actually be found using calculus by taking the partial derivatives of \\(Q\\) with respect to both \\(\\beta_0\\) and \\(\\beta_1\\). \\[ \\frac{\\partial Q}{\\partial \\beta_0} = -2\\sum (Y_i - \\beta_0 - \\beta_1X_i) \\] \\[ \\frac{\\partial Q}{\\partial \\beta_1} = -2\\sum X_i(Y_i-\\beta_0-\\beta_1X_i) \\] Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize \\(Q\\) for a given set of data. After all the calculations are completed we find the values of the parameter estimators \\(b_0\\) and \\(b_1\\) (of \\(\\beta_0\\) and \\(\\beta_1\\), respectively) are as stated previously. Maximum Likelihood{#mle} The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of \\(\\beta_0\\) and \\(\\beta_1\\) which minime the least squares \\(Q\\) function, we choose the values of \\(\\beta_0\\) and \\(\\beta_1\\) which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the \\(Y_i\\) for all observations \\(i=1,\\ldots,n\\). We can do this rather simply by using the assumption that the errors, \\(\\epsilon_i\\) are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if \\(f(Y_i)\\) denotes the probability density function for \\(Y_i\\), then the joint probability density for all \\(Y_i\\), \\(f(Y_1,\\ldots,Y_n)\\) is given by \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) \\] Since each \\(Y_i\\) is assumed to be normally distributed with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\) (see model (\\(\\ref{model}\\))) we have that \\[ f(Y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{Y_i-\\beta_0-\\beta_1X_i}{\\sigma}\\right)^2\\right]} \\] which provides the joint probability as \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] The likelihood function \\(L\\) is then given by consider the \\(Y_i\\) and \\(X_i\\) fixed and the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) as the variables in the function. \\[ L(\\beta_0,\\beta_1,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] Instead of taking partial derivatives of \\(L\\) directly (with respect to all parameters) we take the partial derivatives of the \\(\\log\\) of \\(L\\), which is easier to work with. In a similar, but more difficult calculation, to that of minimizing \\(Q\\), we obtain the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) which maximize the log of \\(L\\), and which therefore maximize \\(L\\). (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate \\(\\hat{\\sigma}^2\\) of \\(\\sigma^2\\). \\[ \\hat{\\sigma}^2 = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n} \\] Estimating the Model Variance (Expand) Estimating \\(\\sigma^2\\) with MSE… As shown previously in the “Estimating Model Parameters” section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation. Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\). This estimate turns out to be a biased estimator. This means that it is consistently wrong in its estimates of \\(\\sigma^2\\). If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong. This is bad. Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\). Without going into all the details, to “fix” the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently. To find the correct degrees of freedom, we have to notice that the \\(\\hat{Y}_i\\) in the numerator of \\(\\widehat{\\sigma}^2\\) is defined by \\[\\begin{equation} \\widehat{Y}_i = b_0 + b_1X_i \\label{hatY} \\end{equation}\\] From this equation, we notice that two means, \\(\\bar{X}\\) and \\(\\bar{Y}\\), were estimated from the data in order to obtain \\(\\hat{Y}_i\\). (See the formulas for \\(b_0\\) and \\(b_1\\) above, and note how they use both \\(\\bar{X}\\) and \\(\\bar{Y}\\) in their calculation.) Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for \\(\\hat{\\sigma}^2\\) should be \\(n-2\\) instead of \\(n\\). Some incredibly long calculations will show that the “fixed” estimator \\[\\begin{equation} s^2 = MSE = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n-2} \\quad \\text{(Unbiased Estimator of $\\sigma^2$)} \\end{equation}\\] is an unbiased estimator of \\(\\sigma^2\\). Here \\(MSE\\) stands for mean squared error, which is the most obvious name for a formula that squares the errors \\(Y_i-\\hat{Y}_i\\) then adds them up and divides by their degrees of freedom. Similarly, we call the numerator \\(\\sum(Y_i-\\hat{Y}_i)^2\\) the sum of the squared errors, denoted by \\(SSE\\). It is also important to note that the errors are often denoted by \\(r_i = Y_i-\\hat{Y}_i\\), the residuals. Putting this all together we get the following equivalent statements for \\(MSE\\). \\[\\begin{equation} s^2 = MSE = \\frac{SSE}{n-2} = \\frac{\\sum(Y_i-\\widehat{Y}_i)^2}{n-2} = \\frac{\\sum r_i^2}{n-2} \\end{equation}\\] As a final note, even though the expected value \\(E\\{MSE\\} = \\sigma^2\\), which shows \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\), it unfortunately isn’t true that \\(\\sqrt{MSE}\\) is an unbiased estimator of \\(\\sigma\\). This presents a few problems later on, but these are minimal enough that we can overlook the issue and move forward. Transformations (Expand) \\(Y'\\), \\(X'\\), and returning to the original space… Y transformations are denoted by y-prime, written \\(Y'\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\). \\[ Y' = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y' = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y' = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y' = \\log(Y)\\) lm(log(Y) ~ X) 0.25 \\(Y' = \\sqrt(\\sqrt(Y))\\) lm(sqrt(sqrt(Y)) ~ X) 0.5 \\(Y' = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y' = Y\\) lm(Y ~ X) 2 \\(Y' = Y^2\\) lm(Y^2 ~ X) Using “maximum-likelihood” estimation, the Box-Cox procedure can actually automatically detect the “optimal” value of \\(\\lambda\\) to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise. Scatterplot Recognition Box-Cox Suggestion An Example Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\). set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Ya <- 1/sqrt(Y) #1/Y^2 Lam = -2 Yb <- 1/Y #1/Y Lam = -1 Yc <- exp(.02*Y) #log(Y) Lam = 0 Yd <- Y^2 #sqrt(Y) Lam = 0.5 Ye <- Y #Y Lam = 1 Yf <- sqrt(Y) #Y^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Ya ~ X, main=expression(paste(\"Use \", lambda == -2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ya^-2 ~ X)) curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yb ~ X, main=expression(paste(\"Use \", lambda == -1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yb^-1 ~ X)) curve(1/(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yc ~ X, main=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(log(Yc) ~ X)) curve(exp(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yd ~ X, main=expression(paste(\"Use \", lambda == 0.5)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(sqrt(Yd) ~ X)) curve((b[1] + b[2]*x)^2, add=TRUE, col=\"green\", lwd=2) plot(Ye ~ X, main=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ye ~ X)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yf ~ X, main=expression(paste(\"Use \", lambda == 2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yf^2 ~ X)) curve(sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try. par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0)) boxCox(lm(Ya ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -2)), line=.5) boxCox(lm(Yb ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -1)), line=.5) boxCox(lm(Yc ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), line=.5) boxCox(lm(Yd ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0.5)), line=.5) boxCox(lm(Ye ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), line=.5) boxCox(lm(Yf ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 2)), line=.5) An Example Suppose we were running a simple linear regression on the cars dataset. This would be done with the code cars.lm <- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesn’t quite fit the data as well as we would hope. Instead, the data looks a little curved. cars.lm <-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") abline(cars.lm, col=\"gray\") Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful. library(car) boxCox(cars.lm) The output from the boxCox(...) function looks as follows. This plot tells use to use the \\(\\lambda = 0.5\\) transformation, so that \\(Y' = Y^0.5 = \\sqrt{Y}\\). (To see this yourself, click on the “Box-Cox Suggestion” tab above, as well as on the “Scatterplot Recognition” tab.) Now, a transformation regression is performed using sqrt(Y) in place of Y as follows: cars.lm.t <- lm(sqrt(dist) ~ speed, data=cars) summary(cars.lm.t)   Estimate Std. Error t value Pr(> (Intercept) 1.277 0.4844 2.636 0.01126 speed 0.3224 0.02978 10.83 1.773e-14 Then, \\[ \\widehat{Y}_i' = 1.277 + 0.3224 X_i \\] And replacing \\(\\hat{Y}_i' = \\sqrt{\\hat{Y}_i}\\) we have \\[ \\sqrt{\\widehat{Y}_i} = 1.277 + 0.3224 X_i \\] Solving for \\(\\hat{Y}_i\\) gives \\[ \\widehat{Y}_i = (1.277 + 0.3224 X_i)^2 \\] Which, using curve((1.277 + 0.3224*x)^2, add=TRUE) (see code for details) looks like this: plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") curve( (1.277 + 0.3224*x)^2, add=TRUE, col=\"firebrick\") X-Transformations X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them. The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated. The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation. set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Xa <- 1/sqrt(X) #1/X^2 Lam = -2 Xb <- 1/X #1/X Lam = -1 Xc <- exp(.02*X) #log(X) Lam = 0 Xd <- X^2 #sqrt(X) Lam = 0.5 Xe <- X #X Lam = 1 Xf <- sqrt(X) #X^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Y ~ Xa, main=expression(paste(\"Use \", X*minute == X^-2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xa^-2))) curve(b[1] + b[2]*x^-2, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xb, main=expression(paste(\"Use \", X*minute == X^-1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xb^-1))) curve(b[1] + b[2]*x^-1, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xc, main=expression(paste(\"Use \", X*minute == log(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ log(Xc))) curve(b[1] + b[2]*log(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xd, main=expression(paste(\"Use \", X*minute == sqrt(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ sqrt(Xd))) curve(b[1] + b[2]*sqrt(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xe, main=expression(paste(\"Use \", X*minute == X, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ Xe)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xf, main=expression(paste(\"Use \", X*minute == X^2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xf^2))) curve(b[1] + b[2]*x^2, add=TRUE, col=\"green\", lwd=2) Inference for the Model Parameters (Expand) t test formulas, sampling distributions, confidence intervals, and F tests… When fitting the regression model given by the equation \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] to a sample of data, we typically test hypotheses about the parameters \\(\\beta_0\\), \\(\\beta_1\\), or both. Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_0 - \\overbrace{0}^\\text{a number}}{s_{b_0}}\\] This is the formula for the test statistic. It measures how far the estimated y-intercept \\(b_0\\) is from the null hypothesis for \\(\\beta_0\\) in units of “standard errors of \\(b_0\\)”. Thus the division by \\(s_{b_0}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0, it could be any number. \\(H_0: \\beta_1 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_1\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_1 - \\overbrace{0}^\\text{a number}}{s_{b_1}}\\] This is the formula for the test statistic. It measures how far the estimated slope \\(b_1\\) is from the null hypothesis for \\(\\beta_1\\) in units of “standard errors of \\(b_1\\)”. Thus the division by \\(s_{b_1}\\). Though the hypothesized value of \\(\\beta_1\\) is typically 0, it could be any number. Left-tailed p-value = pt(-abs(tvalue), degrees of freedom). Double it to get the two-sided p-value. In R, these values correspond to the output summary of an lm as follows. (Show Example) Consider the cars data in R. Suppose we used the regression model given by \\[ \\underbrace{Y_i}_\\text{Feet to Stop} = \\beta_0 + \\beta_1 \\underbrace{X_{i}}_\\text{mph} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\quad \\sim N(0,\\sigma^2) \\] to model the feet a vehicle (from the 1920’s) takes to stop when traveling at a certain speed (in miles per hour, mph) prior to stopping. When the regression is performed and summarized in R, it is always testing the following two hypotheses: \\[ H_0: \\beta_0 = 0 \\quad\\quad H_0: \\beta_1 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\quad\\quad H_a: \\beta_1 \\neq 0 \\] To perform the test of these hypotheses for the regression stated above, we would run the following codes in R. cars.lm <- lm(dist ~ speed, data=cars) pander(summary(cars.lm)$coefficients) These would produce summary output like the following, but the following output has been labeled with the math notation corresponding to each value. Let’s emphasize what is happening in this summary output table. First, here is how the “t value” is calculated for the “(Intercept)” in the summary table above. \\[ t = \\frac{b_0-0}{s_{b_0}} = \\frac{-17.58 - 0}{6.758} = -2.601 \\] Second, here is a visual representation of how the P-value, the “Pr(>|t|)” as it is called in the summary table above, is calculated for this test statistic. (Click the graph to view an interactive applet showing this calculation.) Notice both ends of the t-distribution are being shaded to compute the P-value because the alternative hypothesis was \\(H_a: \\beta_0 \\neq 0\\). To compute the P-value in R, we use the “percentile function for the t-distribution” called pt( ). This function requires two things, the t-value and the degrees of freedom, in our case pt(-2.601, 48). Note the degrees of freedom (df) are 48 because the sample size is \\(n=50\\) and there are two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) in our regression model. Running this code in R gives: pt(-2.601, 48) = 0.00616 However, note that this value is only half of the actual P-value of 0.0123. To get the “two-sided” P-value (note that our alternative hypothesis used a \\(\\neq\\) symbol) we need to double this left-tailed P-value. 2*pt(-2.601, 48)) = 0.0123 Finally, note that the same procedure can be used to test hypotheses that use a value other than 0 in the null and alternative. For example, to test the hypotheses: \\[ H_0: \\beta_1 = 3 \\\\ H_a: \\beta_1 \\neq 3 \\] Use the t-formula \\[ t = \\frac{b_1 - 3}{s_{b_1}} = \\frac{\\overbrace{3.932}^{b_1} - \\overbrace{3}^{H_0}}{\\underbrace{0.4155}_{s_{b_1}}} = 2.243 \\] then the P-value is calculated in R by 2*pt(-abs(2.243), 48) = 0.0295495 To obtain confidence intervals in R use confint(mylm). Confidence Interval Formula Standard Error \\(\\beta_0\\) \\(b_0 \\pm\\) \\(t^*\\) This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\). Use qt(0.975, df) to get \\(t*\\) in R. \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_0\\). \\(s_{b_0}\\) The standard error of \\(b_0\\), denoted by \\(s_{b_0}\\) is provided in the regression summary output under the column header called “Std. Error” for the “(Intercept)” row of the output. It is calculated using the formula shown below. \\[s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\] This is called the “estimated variance of \\(b_0\\)”. Taking the square root of this number gives the “standard error of \\(b_0\\)”. \\(\\beta_1\\) \\(b_1 \\pm\\) \\(t^*\\) This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\) (sample size - number of parameters in the regression model). Use qt(0.975, df) to get \\(t*\\) in R \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_1\\). \\(s_{b_1}\\) The standard error of \\(b_1\\), denoted by \\(s_{b_1}\\) is provided in the regression summary output under the column header called “Std. It is calculated using the formula shown below. \\[s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\] This is called the “estimated variance of \\(b_1\\)”. Taking the square root of this number gives the “standard error of \\(b_1\\)”. To be more exact, the types of inference we are interested in are the following. Determine if there is evidence of a meaningful linear relationship in the data. If \\(\\beta_1 = 0\\), then there is no relation between \\(X\\) and \\(E\\{Y\\}\\). Hence we might be interested in testing the hypotheses \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) is some hypothesized number. To provide a confidence interval for the true value of \\(\\beta_1\\). Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the sampling distribution of the estimate \\(b_1\\) of the parameter \\(\\beta_1\\). And, while we are at it, we may as well come to understand the sampling distribution of the estimate \\(b_0\\) of the parameter \\(\\beta_0\\). Review sampling distributions from Math 221. Since \\(b_1\\) is an estimate, it will vary from sample to sample, even though the truth, \\(\\beta_1\\), remains fixed. (The same holds for \\(b_0\\) and \\(\\beta_0\\).) It turns out that the sampling distribution of \\(b_1\\) (where the \\(X\\) values remain fixed from study to study) is normal with mean and variance: \\[ \\mu_{b_1} = \\beta_1 \\] \\[ \\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} \\] ## Simulation to Show relationship between Standard Errors ##----------------------------------------------- ## Edit anything in this area... n <- 100 #sample size Xstart <- 30 #lower-bound for x-axis Xstop <- 100 #upper-bound for x-axis beta_0 <- 2 #choice of true y-intercept beta_1 <- 3.5 #choice of true slope sigma <- 13.8 #choice of st. deviation of error terms ## End of Editable area. ##----------------------------------------------- # Create X, which will be used in the next R-chunk. X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) ## After playing this chunk, play the next chunk as well. To see that this is true, consider the regression model with values specified for each parameter as follows. \\[ Y_i = \\overbrace{\\beta_0}^{2} + \\overbrace{\\beta_1}^{3.5} X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\overbrace{\\sigma^2}^{\\sigma=13.8}) \\] Using the equations above for \\(\\mu_{b_1}\\) and \\(\\sigma^2_{b_1}\\) we obtain that the mean of the sampling distribution of \\(b_1\\) will be \\(\\mu_{b_1} = \\beta_1 = 3.5\\) Further, we see that the variance of the sampling distribution of \\(b_1\\) will be \\(\\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} = \\frac{13.8^2}{4.25\\times 10^{4}}\\) Taking the square root of the variance, the standard deviation of the sampling distribution of \\(b_1\\) will be \\(\\sigma_{b_1} = 0.067\\). That’s very nice. But to really believe it, let’s run a simulation ourselves. The “Code” below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of \\(b_1\\) and \\(b_0\\) for that regression. After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of \\(b_0\\) is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of \\(b_1\\) is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of \\(\\beta_0\\) or \\(\\beta_1\\), respectively. Also, the “Std. Error” of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of \\(\\sigma_{b_0}\\) and \\(\\sigma_{b_1}\\), respectively. N <- 5000 #number of times to pull a random sample storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N) for (i in 1:N){ Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model mylm <- lm(Y ~ X) storage_b0[i] <- coef(mylm)[1] storage_b1[i] <- coef(mylm)[2] storage_rmse[i] <- summary(mylm)$sigma } layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3)) Ystart <- 0 #min(0,min(Y)) Ystop <- 500 #max(max(Y), 0) Yrange <- Ystop - Ystart plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), ylim=c(Ystart, Ystop), pch=16, col=\"gray\", main=\"Regression Lines from many Samples (gray lines) \\n Plus Residual Standard Deviation Lines (green lines)\") text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1) text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1) text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1) for (i in 1:N){ abline(storage_b0[i], storage_b1[i], col=\"darkgray\") } abline(beta_0, beta_1, col=\"green\", lwd=3) abline(beta_0+sigma, beta_1, col=\"green\", lwd=2) abline(beta_0-sigma, beta_1, col=\"green\", lwd=2) abline(beta_0+2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0-2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0+3*sigma, beta_1, col=\"green\", lwd=.5) abline(beta_0-3*sigma, beta_1, col=\"green\", lwd=.5) par(mai=c(1,.6,.5,.01)) addnorm <- function(m,s, col=\"firebrick\"){ curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2) lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col) lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col) lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col) lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col) lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col) lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col) lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col) legend(\"topleft\", legend=paste(\"Std. Error = \", round(s,3)), cex=0.7, bty=\"n\") } h0 <- hist(storage_b0, col=\"skyblue3\", main=\"Sampling Distribution\\n Y-intercept\", xlab=expression(paste(\"Estimates of \", beta[0], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m0 <- mean(storage_b0) s0 <- sd(storage_b0) addnorm(m0,s0, col=\"green\") h1 <- hist(storage_b1, col=\"skyblue3\", main=\"Sampling Distribution\\n Slope\", xlab=expression(paste(\"Estimates of \", beta[1], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m1 <- mean(storage_b1) s1 <- sd(storage_b1) addnorm(m1,s1, col=\"green\") t Tests{#tTests} Using the information above about the sampling distributions of \\(b_1\\) and \\(b_0\\), an immediate choice of statistical test to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) can be zero, or any other value, is a t test given by \\[ t = \\frac{b_1 - \\beta_{10}}{s_{b_1}} \\] where \\(s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\). (You may want to review the section “Estimating the Model Variance” of this file to know where MSE came from.) With quite a bit of work it has been shown that \\(t\\) is distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. The nearly identical test statistic for testing \\[ H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_0 \\neq \\beta_{00} \\] is given by \\[ t = \\frac{b_0 - \\beta_{00}}{s_{b_0}} \\] where \\(s^2_{b_0} = MSE\\left[\\frac{1}{n}+\\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\). This version of \\(t\\) has also been shown to be distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. Confidence Intervals Creating a confidence interval for either \\(\\beta_1\\) or \\(\\beta_0\\) follows immediately from these results using the formulas \\[ b_1 \\pm t^*_{n-2}\\cdot s_{b_1} \\] \\[ b_0 \\pm t^*_{n-2}\\cdot s_{b_0} \\] where \\(t^*_{n-2}\\) is the critical value from a t distribution with \\(n-2\\) degrees of freedom corresponding to the chosen confidence level. F tests{#Ftests} Another way to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\quad\\quad \\text{or} \\quad\\quad H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\quad\\quad \\ \\ \\quad \\quad H_a: \\beta_0 \\neq \\beta_{00} \\] is with an \\(F\\) Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an \\(F\\) test is very general and can be used in many places that a t test cannot. In its most general form, the \\(F\\) test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that \\(H_0:\\beta_1=0\\) against the alternative that \\(H_a: \\beta_1\\neq 0\\), we are essentially comparing two models against each other. If \\(\\beta_1=0\\), then the corresponding model would be \\(E\\{Y_i\\} = \\beta_0\\). If \\(\\beta_1\\neq0\\), then the model remains \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the \\(F\\) Test, that the null model (reduced model) have fewer “free” parameters than the alternative model (full model). To demonstrate what we mean by “free” parameters, consider the following example. Say we wanted to test the hypothesis that \\(H_0:\\beta_1 = 2.5\\) against the alternative that \\(\\beta_1\\neq2.5\\). Then the null, or reduced model, would be \\(E\\{Y_i\\}=\\beta_0+2.5X_i\\). The alternative, or full model, would be \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). Thus, the null (reduced) model contains only one “free” parameter because \\(\\beta_1\\) has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two “free” parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model. Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing \\(H_0: \\beta_1=0\\) against \\(H_a:\\beta_1\\neq0\\) we have the partition \\[ \\underbrace{Y_i-\\bar{Y}}_{Total} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{Regression} + \\underbrace{Y_i-\\hat{Y}_i}_{Error} \\] The reason we use \\(\\bar{Y}\\) for the null model is that \\(\\bar{Y}\\) is the unbiased estimator of \\(\\beta_0\\) for the null model, \\(E\\{Y_i\\} = \\beta_0\\). Thus we would compute the following sums of squares: \\[ SSTO = \\sum(Y_i-\\bar{Y})^2 \\] \\[ SSR = \\sum(\\hat{Y}_i-\\bar{Y})^2 \\] \\[ SSE = \\sum(Y_i-\\hat{Y}_i)^2 \\] and note that \\(SSTO = SSR + SSE\\). Important to note is that \\(SSTO\\) uses the difference between the observations \\(Y_i\\) and the null (reduced) model. The \\(SSR\\) uses the diffences between the alternative (full) and null (reduced) model. The \\(SSE\\) uses the differences between the observations \\(Y_i\\) and the alternative (full) model. From these we could set up a General \\(F\\) table of the form   Sum Sq Df Mean Sq F Value Model Error \\(SSR\\) \\(df_R-df_F\\) \\(\\frac{SSR}{df_R-df_F}\\) \\(\\frac{SSR}{df_R-df_F}\\cdot\\frac{df_F}{SSE}\\) Residual Error \\(SSE\\) \\(df_F\\) \\(\\frac{SSE}{df_F}\\) Total Error \\(SSTO\\) \\(df_R\\) Prediction and Confidence Intervals for \\(\\hat{Y}_h\\) (Expand) predict(…, interval=“prediction”)… It is a common mistake to assume that averages (means) describe individuals. They do not. So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line. Interval R Code Math Equation When to Use Prediction predict(..., interval=\"prediction\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\text{Pred}\\ Y}\\) Predict an individual’s value. Confidence predict(..., interval=\"confidence\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\hat{Y}}\\) Estimate location of the mean y-value. predict(mylm, data.frame(XvarName = number), interval=...) For example, consider this graph. Then click here to read about the graph. Notice the three dots above 15 mph in the graph. Each of these dots show a car that was going 15 mph when it applied the brakes. However, stopping distances of the three individual cars differ with one at 20 feet, one at 26 feet and one at 54 feet. The regression line represents the average stopping distance of cars. In this case, cars going 15 mph are estimated to have an average stopping distance of about 40 feet, as shown by the line. But individual vehicles, all going the same speed of 15 mph, varied from stopping distances of 20 feet up to 54 feet! So, to predict that a car going 15 mph will take 41.4 feet to stop, doesn’t tell the whole story. Far more revealing is the complete statement, “Cars going 15 mph are predicted to take anywhere from 10.2 to 72.6 feet to stop, with an average stopping distance of 41.4 feet.” This is called the “prediction interval” and is shown in the graph in blue. It is obtained in R with the codes: cars.lm <- lm(dist ~ speed, data=cars) predict(cars.lm, data.frame(speed=15), interval=\"prediction\") cars.lm <- lm(dist ~ speed, data=cars) pander(predict(cars.lm, data.frame(speed=15), interval=\"prediction\")) fit lwr upr 41.41 10.17 72.64 plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=\"firebrick2\", cex=1.5) cars.lm <- lm(dist ~ speed, data=cars) abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3)) abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2)) abline(v=15, lty=2, col=\"firebrick\") preds <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12) lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8)) lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8)) Now, for the details behind prediction intervals and confidence intervals. Let’s begin by recalling some details (from the section “Inference for the Model Parameters”) about the standard error of the y-intercept, \\(b_0\\). Recall that the y-intercept is the average y-value for the given x-value of \\(x=0\\). Recall further that the formula for the standard error of \\(b_0\\) is given by the formula \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] If we wanted to be more exact with this formula, we would write it as \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{(0-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice the addition of \\((0 - \\bar{X})^2\\) instead of just \\(\\bar{X}^2\\) in the numerator of the right-most part of the equation? This more complete statement obviously would reduce to just \\(\\bar{X}^2\\), but that is only because \\(X=0\\) when we are working with the y-intercept, \\(b_0\\). We could be working with other values of \\(X\\) than just zero. Let’s take a quick detour and talk notation for a second. Typically, \\(X_i\\) and \\(Y_i\\) are used to denote the x-value and y-value of points that are contained in our data set. When we want to reference a point that wasn’t within our original data set, we use the notation \\(X_h\\) and \\(Y_h\\). (The letter h is close to i, but different from i, so why not. There is really no other reason to use h.) Thus, \\(Y_h\\) is the y-value for the \\(X_h\\) x-value, neither of which were included in our original regression of \\(X_i\\)’s and \\(Y_i\\)’s. Now, back to the previous discussion. If \\(X_h = 0\\), then \\(\\hat{Y}_h\\) is the y-intercept, so \\(\\hat{Y}_h = b_0\\) when \\(X_h=0\\). So, we could write, \\[ s^2_{\\hat{Y}_h} = MSE\\left[\\frac{1}{n} + \\frac{(X_h-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice how the \\(b_0\\) in \\(s_{b_0}\\) was replaced with \\(\\hat{Y}_h\\) to get \\(s_{\\hat{Y}_h}\\) and the 0 in \\((0 - \\bar{X})^2\\) was replaced with \\(X_h\\) to get \\((X_h - \\bar{X})^2\\)? Interesting. We now have a formula that would give us the standard error of \\(\\hat{Y}_h\\) for any \\(X_h\\) value, not just \\(X_h = 0\\), or the y-intercept, \\(b_0\\). That is fantastic. It would look like this if plotted. Notice how the gray region is showing the standard error for each \\(\\hat{Y}_h\\) value? (It is technically showing the confidence interval for \\(E\\{Y_h\\}\\) at every possible \\(X_h\\) value, but that is just \\(\\hat{Y}_h \\pm t^* \\cdot s_{\\hat{Y}_h}\\).) ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + theme_bw() ## `geom_smooth()` using formula = 'y ~ x' Confidence Interval for \\(\\hat{Y}_h\\) \\[ \\hat{Y}_h \\pm t^* s_{\\hat{Y}_h} \\quad \\text{where} \\ s_{\\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] The confidence interval is a wonderful tool for estimating \\(E\\{Y_h\\}\\), the “true” average y-value for a given x-value of \\(X_h\\). However, it is not valuable for predicting an individual dot, or \\(Y_h\\) value. Notice how few of the dots of the regression are actually contained within the confidence interval band in the plot? The confidence interval does not really predict where the dots will land, just where the average y-value is located for each x-value. Remember the 68-95-99.7 Rule of the normal distribution? If not, here is a link back to that concept in the Math 221 textbook. This rule states that roughly 95% of data, when normally distributed, will be between \\(z=-2\\) and \\(z=2\\) standard deviations from the mean. So, is going two “residual standard errors” to both sides of the regression line enough to capture 95% of the data? The answer is, not quite. The reason for this is because our knowledge of where the true mean lies is uncertain. (Notice the confidence interval band shown in the plot.) However, adding two standard errors to the edges of the confidence band would get us in the right place. In other words, there are two sources of variability at play here, (1) our uncertaintity in where the regression line is sitting, and (2) the natural variability of the data points around the line. Thus, the “prediction interval” requires accounting for both of these sources of variability to produce the following equation. Prediction Interval for \\(Y_h\\) \\[ \\hat{Y}_h \\pm t^* s_{Pred \\hat{Y}_h} \\quad \\text{where} \\ s_{Pred \\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + 1 + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] This formula provides a useful band for identifying a region where we are 95% confident that a new observation for \\(Y_h\\) will land, given the value of \\(X_h\\). It looks as follows. Notice the prediction interval is much wider than the confidence interval. This is because data varies far more than do means. Prediction is for where the individual data points will land, confidence is for where the mean will land. cars.lm <- lm(dist ~ speed, data=cars) predy <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + geom_segment(aes(x=15, xend=15, y=predy[2], yend=predy[3]), lwd=4, color=rgb(.5,.7,.5,.01)) + geom_point(aes(x=15, y=predy[1]), cex=2, color=\"skyblue\", pch=15) + theme_bw() Lowess (and Loess) Curves (Expand) A non-parametric approach to estimating \\(E\\{Y_i\\}\\)… Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value. Using Base R # Select the relevant columns and remove NA values air2 <- airquality %>% dplyr::select(Temp, Ozone) %>% # Use dplyr's select function explicitly drop_na() # Drop rows with NA values in Temp or Ozone plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") lines(lowess(air2$Ozone, air2$Temp), col=\"firebrick\") Using ggplot2 # Your dataset (air2) should have been created earlier, ensuring no NAs air2 <- na.omit(airquality[c(\"Temp\", \"Ozone\")]) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=\"darkgray\") + # Add points with dark gray color geom_smooth(se=FALSE, method=\"loess\", method.args = list(degree=1)) + # Lowess curve, no standard error shading theme_bw() # Black and white theme ## `geom_smooth()` using formula = 'y ~ x' # Optionally, allow for predictions as well as the graph: # Uncomment the following lines if you want predictions as well: # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fitted, x=Ozone)) # Use fitted values from loess Advantages Disadvantages Quick. Good at ignoring outliers. Good at capturing the general pattern in the data. Good for making predictions within the scope of the data. No mathematical model. Not interpretable. No p-values. No adjusted R-squared. How it Works The Lowess curve localizes the regression model to a “neighborhood” of points, and then joins these localized regressions together into a smooth line. It minimizes the effect of outliers, and let’s the data “speak for itself”. As a downside, it is not interpretable, and has no final way to write the model mathematically. All the same, it is a very powerful tool for identifying an appropriate model, or verifying the fit of a model, or making predictions when no reasonable model does an adequate job. Study this graphic and the explanations below to learn how it works. Recommendation: run the code in this “Code” chunk to the right in your Console, and flip through the resulting graphics. X <- cars$speed Y <- cars$dist X <- X[!is.na(X) & !is.na(Y)] Y <- Y[!is.na(X) & !is.na(Y)] f <- 1/2 n <- length(X) lfit <- rep(NA,n) for (xh in 1:n){ xdists <- X - X[xh] nn <- floor(n*f) r <- sort(abs(xdists))[nn] xdists.nbrhd <- which(abs(xdists) < r) w <- rep(0, length(xdists)) w[xdists.nbrhd] <- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w), col=rgb(.2,.2,.2,.3), cex=1.5, yaxt='n', xaxt='n', xlab=\"\", ylab=\"\") points(Y[xh] ~ X[xh], pch=16, col=\"orange\") lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2)) cat(\"\\n\\n\") readline(prompt=paste0(\"Center point is point #\", xh, \"... Press [enter] to continue...\")) MADnotThereYet <- TRUE count <- 0 while(MADnotThereYet){ readline(prompt=paste0(\"\\n Adjusting line to account for outliers in the y-direction... Press [enter] to continue...\")) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"wheat\", add=TRUE) MAD <- median(abs(lmc$res)) resm <- lmc$res/(6*MAD) resm[resm>1] <- 1 bisq <- (1-resm^2)^2 w <- w*bisq obs <- coef(lmc) lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) count <- count + 1 if ( (sum(abs(obs-lmc$coef))<.1) | (count > 3)) MADnotThereYet <- FALSE } curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"green\", add=TRUE) points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=\"green\") readline(prompt=paste0(\"\\n Use final line to get fitted value for this point... Press [enter] to continue to next point...\")) lfit[xh] <- predict(lmc, data.frame(X=X[xh])) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") if (xh == n){ readline(prompt=paste0(\"\\n Press [enter] to see actual Lowess curve...\")) lines(lowess(X,Y, f=f), col=\"firebrick\") legend(\"topleft\", bty=\"n\", legend=\"Actual lowess Curve using lowess(...)\", col=\"firebrick\", lty=1) } } Select a fraction of the data to use for the “neighborhood” of points (shown in blue in the graph above). The lowess function in R uses “f=2/3” and the loess function uses “span=0.75” for this value, which selects the nearest two-thirds or 75% of the data, respectively, depending on which function you use. For this example, we set the fraction of points at 50%. Both functions can be set to whatever you want. Pick any point in the regression, eventually selecting all points one at a time. The selected point becomes the “center” of a “neighborhood” of points surrounding it. In this example, the center point is in orange, and the neighboring points are in blue. Use the points within the neighborhood to fit a regression line. However, make the regression depend most on points closest to “center” and least on points furthest from “center.” This is called a weighted regression. Weights are decided according to what is called the tricubic weight function, so that the weight \\(w\\) given to point \\(j\\) of the neighborhood of points is defined by \\[ w_j = \\left(1- \\left( \\frac{|X_c - X_j|}{\\max_k |X_c - X_k|}\\right)^3\\right)^3 \\] where \\(X_c\\) is the x-value of the “center” dot and \\(X_j\\) is the x-value of any other dot in the neighborhood. The fitted-value of \\(\\hat{Y}_c\\) is obtained for the center point \\(X_c\\) of the current regression. This point is used as the Lowess (or Loess) curve’s value at that particular x-value. Well, almost. It’s a first guess at where this value will end up, but there’s a little more to the algorithm before we are done. Initial guesses for each of these fitted values are obtained for each point in the regression. Now each local regression for each neighborhood is re-run a few times in such a way the the effect of outliers is minimized. The final line for each neighborhood is obtained by the following steps. Compute all residuals for points in the neighborhood of the current regression, denoted by \\(r_i\\). Then compute the MAD, median absolute deviation, of the residuals \\(MAD = \\text{median} (|r_1|, |r_2|, \\ldots)\\). Divide all residuals by 6 times the MAD: \\(u_i = r_i/(6\\cdot MAD)\\) (If \\(r_i > 6\\cdot MAD\\) then set \\(u_i = 0\\).) Compute what are called bisquare weights using the formula: \\(b_i = (1 - u_i^2)^2\\) Perform a regression using the weights \\(w_i = w_i b_i\\) Repeat the above process with the new weights \\(w_i\\) until the weights stop changing very much. The final fitted values for each \\(X\\)-value in the regression are obtained from the final regression line for each neighborhood. These fitted values make up the Lowess (or loess) curve. Note that the default of the loess function in R is to use quadratic regressions in each neighborhood instead of linear regressions. This can be controlled with the loess option of “degree=2” (quadratic fits) or “degree = 1”. In the lowess function only a linear regression in each neighborhood is allowed. Examples: bodyweight, cars Quantitative Y | Multiple X Graphics Y is a single quantitative variable of interest, like height. While we could use an X1 of “shoe-size” to explain height, we might also want to use a second x-variable, X2, like “gender” to help explain height. Further x-variables could also be used. Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a student’s final exam grade? How do factors like age, weight, exercise level, and diet impact a person’s blood pressure? Examples: Is one Navy base more prone to heatwaves than the other? The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH) Tests Multiple Linear Regression Multiple regression is a way to understand how different factors affect something we’re interested in. It’s like having more than one clue to solve a mystery. Here’s what you need to know: We can use several different pieces of information (called variables) to predict an outcome. There are many ways to combine these pieces of information. We start with simple combinations and build up to more complex ones, kind of like building with Lego blocks. This helps us make better predictions and understand complex situations more clearly. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model is a bit like using the same ingredient twice in a recipe, but in different ways. It uses the same factor (\\(X_i^2\\)) in two different forms: Once in its regular form (\\(\\beta_1 X_i\\)) Once squared (\\(\\beta_2 X_i^2\\)) This squared part is what we call the “quadratic” term. It helps us describe relationships that aren’t just straight lines, but have curves to them. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and “steepness” of the Model: negative values face down, positive values face up; large values imply “steeper” parabolas and low values imply “flatter” parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)’s explanation. An Example Using the airquality data set, we run the following “quadratic” regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our “quadratic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. Temp Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These “Estimates” can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(Month^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of Month from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, )   Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will “open down” (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be “month zero.” Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the “cubic” term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the “base slope coefficient” and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following “cubic” regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our “cubic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. uptake Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^3) \\(X_{i}^3\\), where the function I(…) protects the cubing of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will “open up”. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The “two-lines” model is a way to compare two different groups in statistics using numbers. Here’s how it works: It uses two types of numbers: A regular number that can be any value (\\(X_{1i}\\)) A special number that’s either 0 or 1 (called an “indicator variable”, \\(X_{2i}\\)) The special number (\\(X_{2i}\\)) helps us turn group information (like “Group A” or “Group B”) into numbers we can use in calculations. This model creates two separate lines on a graph: One line for Group A One line for Group B By using this method, we can easily see and compare how the two groups are different from each other. Think of it like having two recipes for the same dish, but with slightly different ingredients. This model helps us see how those small changes affect the final result. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the “base-line” of the model, the “Group 0” line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the “base-line” line. \\(\\beta_3\\) Called the “interaction” term. Controls the change in the slope for the second line in the model as compared to the slope of the “base-line” line. An Example Using the mtcars data set, we run the following “two-lines” regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our “two-lines” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. mpg Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) (beta zero): This is like the starting point. It’s estimated by something called the “Intercept”. \\(\\beta_1\\) (beta one): This shows how one thing changes when another thing (called “qsec”) changes. It’s estimated to be 1.439. \\(\\beta_2\\) (beta two): This shows how something else (called “am”) affects the overall result. It’s estimated to be -14.51. \\(\\beta_3\\) (beta three): This shows how “qsec” and “am” work together to affect the result. It’s estimated to be 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called “3D” regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to “bend”. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the “Temp” direction is estimated to be 2.659 and the slope in the “Month” direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction term’s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the “slopes” in each direction to change, creating a “curved” surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called “HD”, or “High Dimensional”, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isn’t really even possible to “mentally connect” with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)’s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the “Wind” direction is estimated to be -3.334. The slope in the “Temp” direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the “Solar.R” direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here. R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the “Overview” section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the “column (c) bind” function and it joins together things as columns. Res =  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals.  panel=panel.smooth,  This places a lowess smoothing line on each scatterplot. col =  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(“color1”,“color2”, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for “linear model.” Y Y must be a “numeric” vector of the quantitative response variable.  ~  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats “numeric” variables as quantitative and “character” or “factor” variables as qualitative. R will automatcially recode qualitative variables to become “numeric” variables using a 0,1 encoding. See the Explanation tab for details.  + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details.  + …, * ... emphasizes that as many explanatory variables as are desired can be included in the model.  data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(…) function displays the results of an lm(…) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(…) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -4.3818 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -2.2696 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.1344 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   1.7058 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   5.8752 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero.   2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (26.6248479) by its standard error (2.1829432). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a “star”. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + …).   -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model.   0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” by its standard error. It gives the “number of standard errors” away from zero that the “estimate” has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + …).   5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\).   2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot “.” implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like.   0.0004029 This is the estimate of the coefficient of the interaction term.   0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data.  0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that “at least one is not.”  34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom.  on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero.  p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that “at least one” of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the “Overview” sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =… Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “prediction”) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “confidence”) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BIC… There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (“R-squared”). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The “simplest” but “best” model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it “an information criterion (AIC)” when he published the method and other people later began calling it the “Akaike Information Criterion.”) Model Selection (Expand) pairs plots, added variable plots, and pattern recognition… Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots {.tabset .tabset-pills} A useful visualization tool for model selection is the “pairs plot.” This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice that… the y-axis of each plot is found by locating the variable name (like “mpg”) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like “disp”) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model {.tabset .tabset-pills} Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldn’t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Let’s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a model’s ability to generalize to new data… The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesn’t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, let’s remind ourselves why we use regression models in the first place. The main goal is to capture the “essence” of the data. In other words, the general pattern is what we are after. We want a model that tells us how “all such” data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the “complicated model” is overfitting the original data. It does not capture the “essence” of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-value… The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the model… The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions{#check} The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read more…) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) Outlier Analysis (Expand) Cook’s Distances and Leverage Values… The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs. fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cook’s Distances The idea behind Cook’s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article “Detection of Influential Observation in Linear Regression” (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, let’s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2))   1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the “differences” in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cook’s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cook’s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cook’s Distances using the code cooks.distance(lmObject). Also, a graph of Cook’s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of “leverage” and is “pulling” the regression toward itself. A value near 0 implies the point is just “one of many” and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that “minimize” the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)’s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the “hat matrix” \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the “leverage values” and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a “lot of pull,” and values close to 0 represent “little pull.” In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with “lots of leverage” and a large “Cook’s Distance” are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regression… Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class. Examples: Civic Vs Corolla cadillacs Binomial Y | Quantitative X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t. We might use an explanatory variable X of “height” to see if taller students are more likely to get an A in Math 325 than shorter students. (They aren’t, if you were wondering.) Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1’s and 0’s (this does happen or this doesn’t happen): Does the width of a child’s foot predict whether or not they are right-handed? What is the probability the cat adopted is a male based on how many days it took them to get adopted? Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted? (Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\") Tests Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The “P” stands for “Probability that…” \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1… This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)… in other words, the “|” says “given” and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the “natural constant” number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14…) Part What it means \\(P(Y_i = 1| x_i)\\) - The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s) - The symbol used to represent the notation in logisitc regression is \\(\\pi_i\\) \\(Y_i\\) (Responsive Variable) Describes the probability of $Y_i = $ 1 or 0 for a given scenario. \\(x_i\\) (Explanatory variable) Describes the observed value and how it effects the \\(Y_i\\) \\(\\frac{e^{\\text{stuff}}}{1+e^{\\text{stuff}}}\\) The content for “stuff” being simple linear regression of the form \\(\\beta_0 + \\beta_{1x_i}\\) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -1.5651 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.6648 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -0.2460 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   0.7276 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   2.2691 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The test statistic is a regular old z-score. It is most reliable when the sample size is “large.” It is a measurement of the number of standard errors the estimate is from 0.   Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds.   1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, …).   -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds.   0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a “star”. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about.   Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\).  43.230  on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates.  29.732 This can be calculated by sum(log(myglm$res^2)).  on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, “A version of Akaike’s An Information Criterion…” The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model.  33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 ## disp -0.014604 0.005168 -2.826 0.00471 ## ## (Intercept) * ## disp ** ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0’s or 1’s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == “B” or height < 68. In other words, Y needs to be a collection of 0’s and 1’s.  ~  The tilde is read “Y on X.” X, X is some quantitative variable.  data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(…) function. curve( A function in R that draws a “curve” on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the “Intercept” estimate from your logistic regression summary output.  \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case “x” in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula.  exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator.  add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(…) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set.  aes( The aes(…) function stands for “aesthetics” and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The plus sign adds a new layer to the ggplot.   geom_point( Tells the plot to add the physical geometry of “points” to the plot. ) Closing parenthesis for the geom_point() function.  + Add another layer to the plot.   geom_smooth( Add a smoothing line to the graph. method=“glm”, This adds a general linear model to the graph. method.args = list(family=“binomial”), This tells the method=“glm” to choose specifically the “binomial” model, otherwise known as the “logistic regression” model.  se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function.  + Add another layer to the ggplot.   theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),  The xVariableName is the same one you used in your glm(y ~ x, …) statement for “x”. Input any desired value for the “someNumericValue” spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016 Explanation Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal). The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\). Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page. Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful. Checking Model Assumptions {#diagnostics} The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit. Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease. Binomial Y | Multiple X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t. We might use an explanatory variable X1 of “height” and a second explanatory variable X2 of “gender” to try to predict whether or not a student will get an A in Math 325. Test(s) used for this category: Multiple Logistic Regression Model Tests Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two. Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. …, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -2.9446 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.2364 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.0000 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q   0.2776 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   1.9968 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The Z-score testing the hypothesis that \\(\\beta_j=0\\)   Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, …).   0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\).   1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\).   4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\).   3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\).   0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\).   0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a “star”. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\).   471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option.   Null Deviance: The deviance of the null model.  800.44 In this case, the null deviance is 800.44  on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression.  256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit.  on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model.  272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities ## numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) -4.97603 0.65316 -7.618 ## Time 0.39111 0.04979 7.854 ## Diet2 0.58283 1.04061 0.560 ## Diet3 -8.44476 4.32324 -1.953 ## Diet4 -67.41995 3768.10023 -0.018 ## Time:Diet2 0.02391 0.08679 0.275 ## Time:Diet3 1.20955 0.51249 2.360 ## Time:Diet4 8.83168 471.01259 0.019 ## Pr(>|z|) ## (Intercept) 2.57e-14 *** ## Time 4.02e-15 *** ## Diet2 0.5754 ## Diet3 0.0508 . ## Diet4 0.9857 ## Time:Diet2 0.7830 ## Time:Diet3 0.0183 * ## Time:Diet4 0.9850 ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, …), X2 = c(Value 1, Value 2, …), …) command. You should see the GSS example file for an example of how to use this function. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the “b” vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(“SomeColor”,“DifferentColor”,…) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(…) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==“B”.  ~  The formula operator in R. X1, The first x-variable in your glm code.  data = YourDataSet, Specify the name of your data set.  pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(…) function computes e^(stuff) in R and stands for the “exponential function.” (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(…) function requires that you call the x-variable “x” although you can change this behavior using xname=“SomeOtherName” if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)).  col = palette()[1], Pull the first color from the color palette for the first curve.  add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters.  col = palette()[2], Set the color of this second curve to the second color in the palette.  add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. “topright”, Typically the legend is placed in the top-right corner of the graph. But it could be placed “top”, “topleft”, “left”, “bottomleft”, “bottom”, “center”, “right”, or “bottomright”.  legend = Why you have to write legend again, no one knows, but you do. c(“Lable 1”, “Label 2”), These are the values that will appear in the legend, one entry on each line of the legend.  col = palette(), This specifies the colors of the symbols in the legend. First color goes with “Label 1”, and second goes with “Label 2” and so on.  lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist.  bty = Specifies the “box type” (bty) that should be drawn around the legend. ‘n’) By placing a “no” option (‘n’) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot.  aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + Add a layer to the ggplot.   geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function.  + Add a layer to the ggplot.   geom_smooth(method=“glm”, method.args=list(family=“binomial”), se=FALSE, Add the logistic regression curve to the plot.   aes(color=“X2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function.  + Add a layer to the plot.   theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x' Explanation Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative. The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations. Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant. Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression. Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\). Categorical Y | Categorical X Graphics Y is a single categorical variable of interest, like gender. X is another categorical variable of interest, like “hair color”. This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender. Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year? Whats the probability that you have brown hair and blue eyes? Tests Add your own notes about appropriate inferential procedures for this type of data here.",
    "sentences": ["Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\).", "Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$\\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$$ \\(Y_i\\) The response variable.", "The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size.", "\\(=\\) This states that we are assuming \\(Y_i\\) was created, or is “equal to” the formula that will follow on the right-hand-side of the equation.", "\\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life.", "It can be likened to “God’s Law” or “Natural Law”.", "Something that governs the way the data behaves, but is unkown to us.", "\\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\).", "\\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\).", "The error terms are “random” and unique for each individual."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 97,
    "title": "📍 Quantitative Y | Multiple X",
    "url": "index.html#quantitativeymultiplex",
    "content": "Quantitative Y | Multiple X Graphics Y is a single quantitative variable of interest, like height. While we could use an X1 of “shoe-size” to explain height, we might also want to use a second x-variable, X2, like “gender” to help explain height. Further x-variables could also be used. Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a student’s final exam grade? How do factors like age, weight, exercise level, and diet impact a person’s blood pressure? Examples: Is one Navy base more prone to heatwaves than the other? The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH) Tests Multiple Linear Regression Multiple regression is a way to understand how different factors affect something we’re interested in. It’s like having more than one clue to solve a mystery. Here’s what you need to know: We can use several different pieces of information (called variables) to predict an outcome. There are many ways to combine these pieces of information. We start with simple combinations and build up to more complex ones, kind of like building with Lego blocks. This helps us make better predictions and understand complex situations more clearly. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model is a bit like using the same ingredient twice in a recipe, but in different ways. It uses the same factor (\\(X_i^2\\)) in two different forms: Once in its regular form (\\(\\beta_1 X_i\\)) Once squared (\\(\\beta_2 X_i^2\\)) This squared part is what we call the “quadratic” term. It helps us describe relationships that aren’t just straight lines, but have curves to them. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and “steepness” of the Model: negative values face down, positive values face up; large values imply “steeper” parabolas and low values imply “flatter” parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)’s explanation. An Example Using the airquality data set, we run the following “quadratic” regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our “quadratic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. Temp Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These “Estimates” can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(Month^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of Month from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, )   Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will “open down” (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be “month zero.” Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the “cubic” term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the “base slope coefficient” and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following “cubic” regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our “cubic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. uptake Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^3) \\(X_{i}^3\\), where the function I(…) protects the cubing of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will “open up”. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The “two-lines” model is a way to compare two different groups in statistics using numbers. Here’s how it works: It uses two types of numbers: A regular number that can be any value (\\(X_{1i}\\)) A special number that’s either 0 or 1 (called an “indicator variable”, \\(X_{2i}\\)) The special number (\\(X_{2i}\\)) helps us turn group information (like “Group A” or “Group B”) into numbers we can use in calculations. This model creates two separate lines on a graph: One line for Group A One line for Group B By using this method, we can easily see and compare how the two groups are different from each other. Think of it like having two recipes for the same dish, but with slightly different ingredients. This model helps us see how those small changes affect the final result. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the “base-line” of the model, the “Group 0” line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the “base-line” line. \\(\\beta_3\\) Called the “interaction” term. Controls the change in the slope for the second line in the model as compared to the slope of the “base-line” line. An Example Using the mtcars data set, we run the following “two-lines” regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our “two-lines” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. mpg Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) (beta zero): This is like the starting point. It’s estimated by something called the “Intercept”. \\(\\beta_1\\) (beta one): This shows how one thing changes when another thing (called “qsec”) changes. It’s estimated to be 1.439. \\(\\beta_2\\) (beta two): This shows how something else (called “am”) affects the overall result. It’s estimated to be -14.51. \\(\\beta_3\\) (beta three): This shows how “qsec” and “am” work together to affect the result. It’s estimated to be 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called “3D” regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to “bend”. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the “Temp” direction is estimated to be 2.659 and the slope in the “Month” direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction term’s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the “slopes” in each direction to change, creating a “curved” surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called “HD”, or “High Dimensional”, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isn’t really even possible to “mentally connect” with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)’s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the “Wind” direction is estimated to be -3.334. The slope in the “Temp” direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the “Solar.R” direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here. R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the “Overview” section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the “column (c) bind” function and it joins together things as columns. Res =  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals.  panel=panel.smooth,  This places a lowess smoothing line on each scatterplot. col =  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(“color1”,“color2”, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for “linear model.” Y Y must be a “numeric” vector of the quantitative response variable.  ~  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats “numeric” variables as quantitative and “character” or “factor” variables as qualitative. R will automatcially recode qualitative variables to become “numeric” variables using a 0,1 encoding. See the Explanation tab for details.  + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details.  + …, * ... emphasizes that as many explanatory variables as are desired can be included in the model.  data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(…) function displays the results of an lm(…) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(…) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -4.3818 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -2.2696 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.1344 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   1.7058 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   5.8752 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero.   2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (26.6248479) by its standard error (2.1829432). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a “star”. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + …).   -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model.   0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” by its standard error. It gives the “number of standard errors” away from zero that the “estimate” has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + …).   5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\).   2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot “.” implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like.   0.0004029 This is the estimate of the coefficient of the interaction term.   0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data.  0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that “at least one is not.”  34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom.  on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero.  p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that “at least one” of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the “Overview” sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =… Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “prediction”) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “confidence”) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BIC… There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (“R-squared”). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The “simplest” but “best” model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it “an information criterion (AIC)” when he published the method and other people later began calling it the “Akaike Information Criterion.”) Model Selection (Expand) pairs plots, added variable plots, and pattern recognition… Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots {.tabset .tabset-pills} A useful visualization tool for model selection is the “pairs plot.” This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice that… the y-axis of each plot is found by locating the variable name (like “mpg”) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like “disp”) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model {.tabset .tabset-pills} Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldn’t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Let’s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a model’s ability to generalize to new data… The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesn’t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, let’s remind ourselves why we use regression models in the first place. The main goal is to capture the “essence” of the data. In other words, the general pattern is what we are after. We want a model that tells us how “all such” data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the “complicated model” is overfitting the original data. It does not capture the “essence” of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-value… The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the model… The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions{#check} The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read more…) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) Outlier Analysis (Expand) Cook’s Distances and Leverage Values… The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs. fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cook’s Distances The idea behind Cook’s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article “Detection of Influential Observation in Linear Regression” (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, let’s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2))   1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the “differences” in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cook’s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cook’s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cook’s Distances using the code cooks.distance(lmObject). Also, a graph of Cook’s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of “leverage” and is “pulling” the regression toward itself. A value near 0 implies the point is just “one of many” and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that “minimize” the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)’s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the “hat matrix” \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the “leverage values” and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a “lot of pull,” and values close to 0 represent “little pull.” In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with “lots of leverage” and a large “Cook’s Distance” are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regression… Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class. Examples: Civic Vs Corolla cadillacs",
    "sentences": ["Graphics Y is a single quantitative variable of interest, like height.", "While we could use an X1 of “shoe-size” to explain height, we might also want to use a second x-variable, X2, like “gender” to help explain height.", "Further x-variables could also be used.", "Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a student’s final exam grade?", "How do factors like age, weight, exercise level, and diet impact a person’s blood pressure?", "Examples: Is one Navy base more prone to heatwaves than the other?", "The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH)", "Tests Multiple Linear Regression Multiple regression is a way to understand how different factors affect something we’re interested in.", "It’s like having more than one clue to solve a mystery.", "Here’s what you need to know: We can use several different pieces of information (called variables) to predict an outcome."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Quantitative Y | Multiple X"
  },
  {
    "id": 98,
    "title": "📍 Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single quantitative variable of interest, like height. While we could use an X1 of “shoe-size” to explain height, we might also want to use a second x-variable, X2, like “gender” to help explain height. Further x-variables could also be used. Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a student’s final exam grade? How do factors like age, weight, exercise level, and diet impact a person’s blood pressure? Examples: Is one Navy base more prone to heatwaves than the other? The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH)",
    "sentences": ["Y is a single quantitative variable of interest, like height.", "While we could use an X1 of “shoe-size” to explain height, we might also want to use a second x-variable, X2, like “gender” to help explain height.", "Further x-variables could also be used.", "Test(s) used for this category: Multiple Linear Regression Analysis : Heatwaves in Japanese Navy Air Bases Uses multiple factors (predictors) to see the effects on one quantitative Y variable (outcome): How does a combination of factors like study hours, prior test scores, and student engagement affect a student’s final exam grade?", "How do factors like age, weight, exercise level, and diet impact a person’s blood pressure?", "Examples: Is one Navy base more prone to heatwaves than the other?", "The graph below this one is just a subset by months of this graph weather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/weather.csv\") weathery <- weather %>% dplyr::select(NAME, MAX, RH, MONTH) %>% rename( HUMIDITY = RH, `AIR NAVY BASE` = NAME, MAXTEMP = MAX ) weathery.plot <- ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(aes( text = paste( \"Air Navy Base:\", `AIR NAVY BASE`, \"<br>\", \"Max Temp:\", round(MAXTEMP, 1), \"C<br>\", \"Humidity:\", round(HUMIDITY, 1), \"%<br>\", \"Month:\", MONTH ) ), size = 1) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() ggplotly(weathery.plot, tooltip=\"text\") ggplot(weathery, aes(y = HUMIDITY, x = MAXTEMP, color = factor(`AIR NAVY BASE`))) + geom_point(size = .5) + geom_smooth(method = \"lm\", aes(group = `AIR NAVY BASE`), se = FALSE) + scale_color_manual(name = \"Air Naval Bases\", values = c(\"red\", \"darkred\"), labels = c(\"Atsugi Air Naval Base\", \"Misawa Air Naval Base\")) + labs(title = \"Weather Patterns in Japan throughout the Year by Months\", x = \"Max Temperature (C)\", y=\"Relative Humidity (%)\" ) + theme_minimal() + facet_wrap(~MONTH)"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 99,
    "title": "📍 Tests",
    "url": "index.html#tests",
    "content": "Tests Multiple Linear Regression Multiple regression is a way to understand how different factors affect something we’re interested in. It’s like having more than one clue to solve a mystery. Here’s what you need to know: We can use several different pieces of information (called variables) to predict an outcome. There are many ways to combine these pieces of information. We start with simple combinations and build up to more complex ones, kind of like building with Lego blocks. This helps us make better predictions and understand complex situations more clearly. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model is a bit like using the same ingredient twice in a recipe, but in different ways. It uses the same factor (\\(X_i^2\\)) in two different forms: Once in its regular form (\\(\\beta_1 X_i\\)) Once squared (\\(\\beta_2 X_i^2\\)) This squared part is what we call the “quadratic” term. It helps us describe relationships that aren’t just straight lines, but have curves to them. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and “steepness” of the Model: negative values face down, positive values face up; large values imply “steeper” parabolas and low values imply “flatter” parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)’s explanation. An Example Using the airquality data set, we run the following “quadratic” regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our “quadratic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. Temp Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These “Estimates” can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(Month^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of Month from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, )   Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will “open down” (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be “month zero.” Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the “cubic” term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the “base slope coefficient” and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following “cubic” regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our “cubic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. uptake Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^3) \\(X_{i}^3\\), where the function I(…) protects the cubing of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will “open up”. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The “two-lines” model is a way to compare two different groups in statistics using numbers. Here’s how it works: It uses two types of numbers: A regular number that can be any value (\\(X_{1i}\\)) A special number that’s either 0 or 1 (called an “indicator variable”, \\(X_{2i}\\)) The special number (\\(X_{2i}\\)) helps us turn group information (like “Group A” or “Group B”) into numbers we can use in calculations. This model creates two separate lines on a graph: One line for Group A One line for Group B By using this method, we can easily see and compare how the two groups are different from each other. Think of it like having two recipes for the same dish, but with slightly different ingredients. This model helps us see how those small changes affect the final result. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the “base-line” of the model, the “Group 0” line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the “base-line” line. \\(\\beta_3\\) Called the “interaction” term. Controls the change in the slope for the second line in the model as compared to the slope of the “base-line” line. An Example Using the mtcars data set, we run the following “two-lines” regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our “two-lines” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. mpg Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) (beta zero): This is like the starting point. It’s estimated by something called the “Intercept”. \\(\\beta_1\\) (beta one): This shows how one thing changes when another thing (called “qsec”) changes. It’s estimated to be 1.439. \\(\\beta_2\\) (beta two): This shows how something else (called “am”) affects the overall result. It’s estimated to be -14.51. \\(\\beta_3\\) (beta three): This shows how “qsec” and “am” work together to affect the result. It’s estimated to be 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called “3D” regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to “bend”. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the “Temp” direction is estimated to be 2.659 and the slope in the “Month” direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction term’s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the “slopes” in each direction to change, creating a “curved” surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called “HD”, or “High Dimensional”, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isn’t really even possible to “mentally connect” with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)’s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the “Wind” direction is estimated to be -3.334. The slope in the “Temp” direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the “Solar.R” direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here. R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the “Overview” section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the “column (c) bind” function and it joins together things as columns. Res =  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals.  panel=panel.smooth,  This places a lowess smoothing line on each scatterplot. col =  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(“color1”,“color2”, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for “linear model.” Y Y must be a “numeric” vector of the quantitative response variable.  ~  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats “numeric” variables as quantitative and “character” or “factor” variables as qualitative. R will automatcially recode qualitative variables to become “numeric” variables using a 0,1 encoding. See the Explanation tab for details.  + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details.  + …, * ... emphasizes that as many explanatory variables as are desired can be included in the model.  data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(…) function displays the results of an lm(…) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(…) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -4.3818 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -2.2696 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.1344 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   1.7058 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   5.8752 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero.   2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (26.6248479) by its standard error (2.1829432). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a “star”. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + …).   -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model.   0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” by its standard error. It gives the “number of standard errors” away from zero that the “estimate” has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + …).   5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\).   2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot “.” implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like.   0.0004029 This is the estimate of the coefficient of the interaction term.   0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data.  0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that “at least one is not.”  34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom.  on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero.  p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that “at least one” of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the “Overview” sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =… Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “prediction”) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “confidence”) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BIC… There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (“R-squared”). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The “simplest” but “best” model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it “an information criterion (AIC)” when he published the method and other people later began calling it the “Akaike Information Criterion.”) Model Selection (Expand) pairs plots, added variable plots, and pattern recognition… Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots {.tabset .tabset-pills} A useful visualization tool for model selection is the “pairs plot.” This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice that… the y-axis of each plot is found by locating the variable name (like “mpg”) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like “disp”) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model {.tabset .tabset-pills} Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldn’t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Let’s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a model’s ability to generalize to new data… The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesn’t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, let’s remind ourselves why we use regression models in the first place. The main goal is to capture the “essence” of the data. In other words, the general pattern is what we are after. We want a model that tells us how “all such” data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the “complicated model” is overfitting the original data. It does not capture the “essence” of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-value… The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the model… The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions{#check} The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read more…) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) Outlier Analysis (Expand) Cook’s Distances and Leverage Values… The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs. fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cook’s Distances The idea behind Cook’s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article “Detection of Influential Observation in Linear Regression” (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, let’s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2))   1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the “differences” in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cook’s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cook’s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cook’s Distances using the code cooks.distance(lmObject). Also, a graph of Cook’s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of “leverage” and is “pulling” the regression toward itself. A value near 0 implies the point is just “one of many” and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that “minimize” the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)’s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the “hat matrix” \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the “leverage values” and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a “lot of pull,” and values close to 0 represent “little pull.” In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with “lots of leverage” and a large “Cook’s Distance” are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regression… Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class. Examples: Civic Vs Corolla cadillacs",
    "sentences": ["Multiple Linear Regression Multiple regression is a way to understand how different factors affect something we’re interested in.", "It’s like having more than one clue to solve a mystery.", "Here’s what you need to know: We can use several different pieces of information (called variables) to predict an outcome.", "There are many ways to combine these pieces of information.", "We start with simple combinations and build up to more complex ones, kind of like building with Lego blocks.", "This helps us make better predictions and understand complex situations more clearly.", "Overview Select a model to see interpretation details, an example, and R Code help.", "Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\).", "Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model is a bit like using the same ingredient twice in a recipe, but in different ways.", "It uses the same factor (\\(X_i^2\\)) in two different forms: Once in its regular form (\\(\\beta_1 X_i\\)) Once squared (\\(\\beta_2 X_i^2\\)) This squared part is what we call the “quadratic” term."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 100,
    "title": "📍 Binomial Y | Quantitative X",
    "url": "index.html#binomialyquantitativex",
    "content": "Binomial Y | Quantitative X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t. We might use an explanatory variable X of “height” to see if taller students are more likely to get an A in Math 325 than shorter students. (They aren’t, if you were wondering.) Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1’s and 0’s (this does happen or this doesn’t happen): Does the width of a child’s foot predict whether or not they are right-handed? What is the probability the cat adopted is a male based on how many days it took them to get adopted? Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted? (Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\") Tests Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The “P” stands for “Probability that…” \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1… This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)… in other words, the “|” says “given” and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the “natural constant” number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14…) Part What it means \\(P(Y_i = 1| x_i)\\) - The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s) - The symbol used to represent the notation in logisitc regression is \\(\\pi_i\\) \\(Y_i\\) (Responsive Variable) Describes the probability of $Y_i = $ 1 or 0 for a given scenario. \\(x_i\\) (Explanatory variable) Describes the observed value and how it effects the \\(Y_i\\) \\(\\frac{e^{\\text{stuff}}}{1+e^{\\text{stuff}}}\\) The content for “stuff” being simple linear regression of the form \\(\\beta_0 + \\beta_{1x_i}\\) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -1.5651 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.6648 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -0.2460 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   0.7276 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   2.2691 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The test statistic is a regular old z-score. It is most reliable when the sample size is “large.” It is a measurement of the number of standard errors the estimate is from 0.   Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds.   1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, …).   -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds.   0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a “star”. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about.   Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\).  43.230  on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates.  29.732 This can be calculated by sum(log(myglm$res^2)).  on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, “A version of Akaike’s An Information Criterion…” The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model.  33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 ## disp -0.014604 0.005168 -2.826 0.00471 ## ## (Intercept) * ## disp ** ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0’s or 1’s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == “B” or height < 68. In other words, Y needs to be a collection of 0’s and 1’s.  ~  The tilde is read “Y on X.” X, X is some quantitative variable.  data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(…) function. curve( A function in R that draws a “curve” on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the “Intercept” estimate from your logistic regression summary output.  \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case “x” in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula.  exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator.  add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(…) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set.  aes( The aes(…) function stands for “aesthetics” and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The plus sign adds a new layer to the ggplot.   geom_point( Tells the plot to add the physical geometry of “points” to the plot. ) Closing parenthesis for the geom_point() function.  + Add another layer to the plot.   geom_smooth( Add a smoothing line to the graph. method=“glm”, This adds a general linear model to the graph. method.args = list(family=“binomial”), This tells the method=“glm” to choose specifically the “binomial” model, otherwise known as the “logistic regression” model.  se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function.  + Add another layer to the ggplot.   theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),  The xVariableName is the same one you used in your glm(y ~ x, …) statement for “x”. Input any desired value for the “someNumericValue” spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016 Explanation Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal). The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\). Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page. Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful. Checking Model Assumptions {#diagnostics} The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit. Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease.",
    "sentences": ["Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y.", "This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t.", "We might use an explanatory variable X of “height” to see if taller students are more likely to get an A in Math 325 than shorter students.", "(They aren’t, if you were wondering.) Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1’s and 0’s (this does happen or this doesn’t happen): Does the width of a child’s foot predict whether or not they are right-handed?", "What is the probability the cat adopted is a male based on how many days it took them to get adopted?", "Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted?", "(Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\")", "Tests Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable.", "Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The “P” stands for “Probability that…” \\(Y_i\\) The response variable.", "The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Binomial Y | Quantitative X"
  },
  {
    "id": 101,
    "title": "📍 Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t. We might use an explanatory variable X of “height” to see if taller students are more likely to get an A in Math 325 than shorter students. (They aren’t, if you were wondering.) Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1’s and 0’s (this does happen or this doesn’t happen): Does the width of a child’s foot predict whether or not they are right-handed? What is the probability the cat adopted is a male based on how many days it took them to get adopted? Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted? (Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\")",
    "sentences": ["Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y.", "This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t.", "We might use an explanatory variable X of “height” to see if taller students are more likely to get an A in Math 325 than shorter students.", "(They aren’t, if you were wondering.)", "Test(s) used for this category: Simple Logistic Regression Model Analysis: Logistic Regression - Cat Adoptions Used for predicting something given a certain scenario with 1’s and 0’s (this does happen or this doesn’t happen): Does the width of a child’s foot predict whether or not they are right-handed?", "What is the probability the cat adopted is a male based on how many days it took them to get adopted?", "Examples: What is the probability the cat adopted is a male based on how many days it took them to get adopted?", "(Simple Logistic Regression) catadoptions <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/catadoptions.csv\") adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% dplyr::select(Age, Gender, Posted, Adopted, DaysListed) catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\")"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 102,
    "title": "📍 Tests",
    "url": "index.html#tests",
    "content": "Tests Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The “P” stands for “Probability that…” \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1… This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)… in other words, the “|” says “given” and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the “natural constant” number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14…) Part What it means \\(P(Y_i = 1| x_i)\\) - The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s) - The symbol used to represent the notation in logisitc regression is \\(\\pi_i\\) \\(Y_i\\) (Responsive Variable) Describes the probability of $Y_i = $ 1 or 0 for a given scenario. \\(x_i\\) (Explanatory variable) Describes the observed value and how it effects the \\(Y_i\\) \\(\\frac{e^{\\text{stuff}}}{1+e^{\\text{stuff}}}\\) The content for “stuff” being simple linear regression of the form \\(\\beta_0 + \\beta_{1x_i}\\) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -1.5651 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.6648 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -0.2460 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   0.7276 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   2.2691 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The test statistic is a regular old z-score. It is most reliable when the sample size is “large.” It is a measurement of the number of standard errors the estimate is from 0.   Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds.   1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, …).   -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds.   0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a “star”. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about.   Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\).  43.230  on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates.  29.732 This can be calculated by sum(log(myglm$res^2)).  on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, “A version of Akaike’s An Information Criterion…” The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model.  33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 ## disp -0.014604 0.005168 -2.826 0.00471 ## ## (Intercept) * ## disp ** ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0’s or 1’s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == “B” or height < 68. In other words, Y needs to be a collection of 0’s and 1’s.  ~  The tilde is read “Y on X.” X, X is some quantitative variable.  data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(…) function. curve( A function in R that draws a “curve” on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the “Intercept” estimate from your logistic regression summary output.  \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case “x” in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula.  exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator.  add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(…) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set.  aes( The aes(…) function stands for “aesthetics” and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The plus sign adds a new layer to the ggplot.   geom_point( Tells the plot to add the physical geometry of “points” to the plot. ) Closing parenthesis for the geom_point() function.  + Add another layer to the plot.   geom_smooth( Add a smoothing line to the graph. method=“glm”, This adds a general linear model to the graph. method.args = list(family=“binomial”), This tells the method=“glm” to choose specifically the “binomial” model, otherwise known as the “logistic regression” model.  se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function.  + Add another layer to the ggplot.   theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),  The xVariableName is the same one you used in your glm(y ~ x, …) statement for “x”. Input any desired value for the “someNumericValue” spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016 Explanation Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal). The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\). Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page. Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful. Checking Model Assumptions {#diagnostics} The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit. Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease.",
    "sentences": ["Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable.", "Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation: Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The “P” stands for “Probability that…” \\(Y_i\\) The response variable.", "The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size.", "\\(= 1\\) Equals 1… This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual.", "\\(| x_i)\\) Given \\(x_i\\)… in other words, the “|” says “given” and \\(x_i\\) means the x-value of the current individual.", "\\(=\\) Equals sign.", "\\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the “natural constant” number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope.", "\\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value.", "It is the short hand notation for \\(P(Y_i = 1 |x_i)\\).", "(It is NOT the number 3.14…) Part What it means \\(P(Y_i = 1| x_i)\\) - The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s) - The symbol used to represent the notation in logisitc regression is \\(\\pi_i\\) \\(Y_i\\) (Responsive Variable) Describes the probability of $Y_i = $ 1 or 0 for a given scenario."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 103,
    "title": "📍 Binomial Y | Multiple X",
    "url": "index.html#binomialymultiplex",
    "content": "Binomial Y | Multiple X Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t. We might use an explanatory variable X1 of “height” and a second explanatory variable X2 of “gender” to try to predict whether or not a student will get an A in Math 325. Test(s) used for this category: Multiple Logistic Regression Model Tests Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two. Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. …, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -2.9446 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.2364 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.0000 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q   0.2776 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   1.9968 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The Z-score testing the hypothesis that \\(\\beta_j=0\\)   Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, …).   0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\).   1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\).   4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\).   3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\).   0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\).   0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a “star”. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\).   471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option.   Null Deviance: The deviance of the null model.  800.44 In this case, the null deviance is 800.44  on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression.  256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit.  on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model.  272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities ## numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) -4.97603 0.65316 -7.618 ## Time 0.39111 0.04979 7.854 ## Diet2 0.58283 1.04061 0.560 ## Diet3 -8.44476 4.32324 -1.953 ## Diet4 -67.41995 3768.10023 -0.018 ## Time:Diet2 0.02391 0.08679 0.275 ## Time:Diet3 1.20955 0.51249 2.360 ## Time:Diet4 8.83168 471.01259 0.019 ## Pr(>|z|) ## (Intercept) 2.57e-14 *** ## Time 4.02e-15 *** ## Diet2 0.5754 ## Diet3 0.0508 . ## Diet4 0.9857 ## Time:Diet2 0.7830 ## Time:Diet3 0.0183 * ## Time:Diet4 0.9850 ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, …), X2 = c(Value 1, Value 2, …), …) command. You should see the GSS example file for an example of how to use this function. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the “b” vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(“SomeColor”,“DifferentColor”,…) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(…) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==“B”.  ~  The formula operator in R. X1, The first x-variable in your glm code.  data = YourDataSet, Specify the name of your data set.  pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(…) function computes e^(stuff) in R and stands for the “exponential function.” (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(…) function requires that you call the x-variable “x” although you can change this behavior using xname=“SomeOtherName” if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)).  col = palette()[1], Pull the first color from the color palette for the first curve.  add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters.  col = palette()[2], Set the color of this second curve to the second color in the palette.  add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. “topright”, Typically the legend is placed in the top-right corner of the graph. But it could be placed “top”, “topleft”, “left”, “bottomleft”, “bottom”, “center”, “right”, or “bottomright”.  legend = Why you have to write legend again, no one knows, but you do. c(“Lable 1”, “Label 2”), These are the values that will appear in the legend, one entry on each line of the legend.  col = palette(), This specifies the colors of the symbols in the legend. First color goes with “Label 1”, and second goes with “Label 2” and so on.  lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist.  bty = Specifies the “box type” (bty) that should be drawn around the legend. ‘n’) By placing a “no” option (‘n’) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot.  aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + Add a layer to the ggplot.   geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function.  + Add a layer to the ggplot.   geom_smooth(method=“glm”, method.args=list(family=“binomial”), se=FALSE, Add the logistic regression curve to the plot.   aes(color=“X2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function.  + Add a layer to the plot.   theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x' Explanation Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative. The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations. Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant. Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression. Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\).",
    "sentences": ["Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y.", "This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t.", "We might use an explanatory variable X1 of “height” and a second explanatory variable X2 of “gender” to try to predict whether or not a student will get an A in Math 325.", "Test(s) used for this category: Multiple Logistic Regression Model", "Tests Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two.", "Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly.", "Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead.", "The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\).", "The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\).", "Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Binomial Y | Multiple X"
  },
  {
    "id": 104,
    "title": "📍 Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y. This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t. We might use an explanatory variable X1 of “height” and a second explanatory variable X2 of “gender” to try to predict whether or not a student will get an A in Math 325. Test(s) used for this category: Multiple Logistic Regression Model",
    "sentences": ["Y is a single categorical (qualitative) variable of interest where 1 (success) or 0 (failure) are the only possible values for Y.", "This would be like “getting an A in Math 325” where 1 means you got an A and 0 means you didn’t.", "We might use an explanatory variable X1 of “height” and a second explanatory variable X2 of “gender” to try to predict whether or not a student will get an A in Math 325.", "Test(s) used for this category: Multiple Logistic Regression Model"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 105,
    "title": "📍 Tests",
    "url": "index.html#tests",
    "content": "Tests Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two. Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. …, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -2.9446 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.2364 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.0000 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q   0.2776 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   1.9968 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The Z-score testing the hypothesis that \\(\\beta_j=0\\)   Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, …).   0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\).   1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\).   4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\).   3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\).   0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\).   0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a “star”. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\).   471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option.   Null Deviance: The deviance of the null model.  800.44 In this case, the null deviance is 800.44  on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression.  256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit.  on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model.  272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities ## numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) -4.97603 0.65316 -7.618 ## Time 0.39111 0.04979 7.854 ## Diet2 0.58283 1.04061 0.560 ## Diet3 -8.44476 4.32324 -1.953 ## Diet4 -67.41995 3768.10023 -0.018 ## Time:Diet2 0.02391 0.08679 0.275 ## Time:Diet3 1.20955 0.51249 2.360 ## Time:Diet4 8.83168 471.01259 0.019 ## Pr(>|z|) ## (Intercept) 2.57e-14 *** ## Time 4.02e-15 *** ## Diet2 0.5754 ## Diet3 0.0508 . ## Diet4 0.9857 ## Time:Diet2 0.7830 ## Time:Diet3 0.0183 * ## Time:Diet4 0.9850 ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, …), X2 = c(Value 1, Value 2, …), …) command. You should see the GSS example file for an example of how to use this function. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the “b” vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(“SomeColor”,“DifferentColor”,…) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(…) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==“B”.  ~  The formula operator in R. X1, The first x-variable in your glm code.  data = YourDataSet, Specify the name of your data set.  pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(…) function computes e^(stuff) in R and stands for the “exponential function.” (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(…) function requires that you call the x-variable “x” although you can change this behavior using xname=“SomeOtherName” if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)).  col = palette()[1], Pull the first color from the color palette for the first curve.  add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters.  col = palette()[2], Set the color of this second curve to the second color in the palette.  add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. “topright”, Typically the legend is placed in the top-right corner of the graph. But it could be placed “top”, “topleft”, “left”, “bottomleft”, “bottom”, “center”, “right”, or “bottomright”.  legend = Why you have to write legend again, no one knows, but you do. c(“Lable 1”, “Label 2”), These are the values that will appear in the legend, one entry on each line of the legend.  col = palette(), This specifies the colors of the symbols in the legend. First color goes with “Label 1”, and second goes with “Label 2” and so on.  lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist.  bty = Specifies the “box type” (bty) that should be drawn around the legend. ‘n’) By placing a “no” option (‘n’) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot.  aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + Add a layer to the ggplot.   geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function.  + Add a layer to the ggplot.   geom_smooth(method=“glm”, method.args=list(family=“binomial”), se=FALSE, Add the logistic regression curve to the plot.   aes(color=“X2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function.  + Add a layer to the plot.   theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x' Explanation Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative. The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations. Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant. Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression. Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\).",
    "sentences": ["Multiple Logistic Regression Model Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two.", "Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly.", "Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead.", "The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\).", "The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\).", "Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.", " <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName.", "glm( glm( is an R function that stands for “General Linear Model”.", "It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command.", "Y  Y is your binary response variable."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 106,
    "title": "📍 Categorical Y | Categorical X",
    "url": "index.html#categoricalycategoricalx",
    "content": "Categorical Y | Categorical X Graphics Y is a single categorical variable of interest, like gender. X is another categorical variable of interest, like “hair color”. This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender. Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year? Whats the probability that you have brown hair and blue eyes? Tests Add your own notes about appropriate inferential procedures for this type of data here.",
    "sentences": ["Graphics Y is a single categorical variable of interest, like gender.", "X is another categorical variable of interest, like “hair color”.", "This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender.", "Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year?", "Whats the probability that you have brown hair and blue eyes?", "Tests Add your own notes about appropriate inferential procedures for this type of data here."],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Categorical Y | Categorical X"
  },
  {
    "id": 107,
    "title": "📍 Graphics",
    "url": "index.html#graphics",
    "content": "Graphics Y is a single categorical variable of interest, like gender. X is another categorical variable of interest, like “hair color”. This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender. Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year? Whats the probability that you have brown hair and blue eyes?",
    "sentences": ["Y is a single categorical variable of interest, like gender.", "X is another categorical variable of interest, like “hair color”.", "This type of data would help us understand if men or women are more likely to have certain hair colors than the other gender.", "Used to see if there is a independence or not independence on two factors: Is one gender more likely to be born in certain seasons of the year?", "Whats the probability that you have brown hair and blue eyes?"],
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Graphics"
  },
  {
    "id": 108,
    "title": "📍 Tests",
    "url": "index.html#tests",
    "content": "Tests Add your own notes about appropriate inferential procedures for this type of data here.",
    "sentences": "Add your own notes about appropriate inferential procedures for this type of data here.",
    "type": "section",
    "page_title": "Statistics Notebook",
    "section_title": "Tests"
  },
  {
    "id": 109,
    "title": "Kruskal-Wallis Test",
    "url": "Kruskal.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Kruskal-Wallis Test The nonparametric equivalent to one-way ANOVA. Kruskal-Wallis Rank Sum Test Allows for deciding if several samples come from the same population or if at least one sample comes from a different population. Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated. Can also be used when data is ordered (ordinal) but does not have an exact measurement. Best used when the distribution of the data is not normal. ANOVA is appropriate when the data is normal. Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, “is a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly. In many cases, this is practically equivalent to the mean of at least one population differing from the others.” Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable. g is a qualitative grouping variable defining which groups each value in x belongs to. It must either be a character vector or a factor vector. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. weight ‘weight’ is a numeric variable from the chickwts dataset that represents the quantatitive response variable.  ~  ‘~’ is the tilde symbol used to separate the left- and right-hand side in a model formula. feed, ‘feed’ is a qualitative grouping variable in the chickwts dataset.  data = chickwts) ‘chickwts’ is a dataset in R.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## ## Kruskal-Wallis rank sum test ## ## data: weight by feed ## Kruskal-Wallis chi-squared = 37.343, df = 5, ## p-value = 5.113e-07 Alternatively, you could use the following approach as well. kruskal.test(list(x, y, z)) x is a numeric vector of data values that represents the quantatitive response variable for the first group. y is a numeric vector of data values that represents the quantatitive response variable for the second group. z is a numeric vector of data values that represents the quantatitive response variable for the third group. Note that more than three vectors of data could be included inside of list(). Example Code Hover your mouse over the example codes to learn more. feed1 <- filter(chickwts, feed == “horsebean”) This splits out the first group of feed (horsebean) from the chickwts dataset. feed2 <- filter(chickwts, feed == “linseed”) This splits out the second group of feed (linseed) from the chickwts dataset. feed3 <- filter(chickwts, feed == “soybean”) This splits out the third group of feed (soybean) from the chickwts dataset. feed4 <- filter(chickwts, feed == “sunflower”) This splits out the fourth group of feed (sunflower) from the chickwts dataset. feed5 <- filter(chickwts, feed == “meatmeal”) This splits out the fifth group of feed (meatmeal) from the chickwts dataset. feed6 <- filter(chickwts, feed == “casein”) This splits out the sixth group of feed (casein) from the chickwts dataset. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. list(feed1$weight,feed2$weight,feed3$weight,feed4$weight,feed5$weight,feed6$weight)) A list of numeric vecotrs that represent the quantitative response variable for each group.  Click to View Output  Click to View Output. Load library(tidyverse) to run this code in R. ## ## Kruskal-Wallis rank sum test ## ## data: list(feed1$weight, feed2$weight, feed3$weight, feed4$weight, feed5$weight, feed6$weight) ## Kruskal-Wallis chi-squared = 37.343, df = 5, ## p-value = 5.113e-07 Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population. The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples. The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity. Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated. (Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\). Note that the notation \\(n_i\\) allows for each sample to be a different size. In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\). Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest. Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\). Award any tied values the average of the ranks of those values that are tied. In the bottle-cap data we have the following. Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample. Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\). In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\). In the bottle-cap data, \\(\\bar{R}_1 = \\frac{R_1}{n_1} = \\frac{24}{5} = 4.8\\) \\(\\bar{R}_2 = \\frac{R_2}{n_2} = \\frac{14}{3} = 4.667\\) \\(\\bar{R}_3 = \\frac{R_3}{n_3} = \\frac{40}{4} = 10.0\\) Step 5: If there are no ties, calculate the test statistic \\(H\\) by the formula \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] See the original article for the formula to use when there are ties present in the data. In the bottle-cap data, \\(H = 5.656\\). To understand the logic behind the test statistic \\(H\\), note that the null hypothesis assumes that each of the \\(C\\) samples are taken from the same population. Under this assumption, the ranks assigned to each sample should represent a uniform sample of the ranks \\(1,\\ldots,N\\). If the null hypothesis were true, we would expect the average of the ranks from each group, the \\(\\bar{R}_i\\), to be roughly about the same. More precisely, using the properties of the discrete uniform distribution (see wikipedia), we expect each \\(\\bar{R}_i\\) to be close to \\(\\frac{1}{2}(N+1)\\), which is the mean of the discrete uniform distribution. That \\(\\bar{R}_i\\) should be close to this value comes from the fact that the mean of a distribution of means is equal to the population mean. Further, the variance of \\(\\bar{R}_i\\) is given by the population variance, \\(\\frac{1}{12}(N^2-1)\\) in this case, divided by the sample size \\(n_i\\). Finally, the \\(\\frac{N-1}{N}\\) is a correction factor that will not be explained here. The full article explains the details about how a multivariate normal distribution is used to uncover the formula for \\(H\\). Thus, understanding \\(H\\) completely will require an advanced degree in statistical theory. However, the short story is that the distribution of the test statistic \\(H\\) is approximately a chi squared distribution with \\(C-1\\) degrees of freedom and has the form of a sum of values consisting of the (observed mean - expected mean)^2/(variance of the observed mean). Step 6: Calculate the p-value of \\(H\\) using a chi squared distribution with \\(C-1\\) degrees of freedom. \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] In the bottle-cap data, \\(p = 0.059\\), which represents the probability of \\(H\\) being as extreme or more extreme than the observed value of 5.656, assuming the null hypothesis is true. Examples: wages, pigweights Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) Good Example Analysis Poor Example Analysis High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Outlier Theory Assignment The nonparametric equivalent to one-way ANOVA. Kruskal-Wallis Rank Sum Test Allows for deciding if several samples come from the same population or if at least one sample comes from a different population. Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated. Can also be used when data is ordered (ordinal) but does not have an exact measurement. Best used when the distribution of the data is not normal. ANOVA is appropriate when the data is normal. Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, “is a tendency for observations in at leas",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Kruskal-Wallis Test The nonparametric equivalent to one-way ANOVA.", "Kruskal-Wallis Rank Sum Test Allows for deciding if several samples come from the same population or if at least one sample comes from a different population.", "Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated.", "Can also be used when data is ordered (ordinal) but does not have an exact measurement.", "Best used when the distribution of the data is not normal.", "ANOVA is appropriate when the data is normal.", "Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, “is a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly.", "In many cases, this is practically equivalent to the mean of at least one population differing from the others.” Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable.", "g is a qualitative grouping variable defining which groups each value in x belongs to.", "It must either be a character vector or a factor vector.", "YourDataSet is the name of your data set.", "Example Code Hover your mouse over the example codes to learn more.", "kruskal.test( The function that performs a Kruskal-Wallis rank sum test.", "weight ‘weight’ is a numeric variable from the chickwts dataset that represents the quantatitive response variable.", " ~  ‘~’ is the tilde symbol used to separate the left- and right-hand side in a model formula.", "feed, ‘feed’ is a qualitative grouping variable in the chickwts dataset.", " data = chickwts) ‘chickwts’ is a dataset in R.", "    Press Enter to run the code if you have typed it in yourself.", "You can also click here to view the output.", " Click to View Output  Click to View Output.", "## ## Kruskal-Wallis rank sum test ## ## data: weight by feed ## Kruskal-Wallis chi-squared = 37.343, df = 5, ## p-value = 5.113e-07 Alternatively, you could use the following approach as well.", "kruskal.test(list(x, y, z)) x is a numeric vector of data values that represents the quantatitive response variable for the first group.", "y is a numeric vector of data values that represents the quantatitive response variable for the second group.", "z is a numeric vector of data values that represents the quantatitive response variable for the third group.", "Note that more than three vectors of data could be included inside of list().", "Example Code Hover your mouse over the example codes to learn more.", "feed1 <- filter(chickwts, feed == “horsebean”) This splits out the first group of feed (horsebean) from the chickwts dataset.", "feed2 <- filter(chickwts, feed == “linseed”) This splits out the second group of feed (linseed) from the chickwts dataset.", "feed3 <- filter(chickwts, feed == “soybean”) This splits out the third group of feed (soybean) from the chickwts dataset.", "feed4 <- filter(chickwts, feed == “sunflower”) This splits out the fourth group of feed (sunflower) from the chickwts dataset.", "feed5 <- filter(chickwts, feed == “meatmeal”) This splits out the fifth group of feed (meatmeal) from the chickwts dataset.", "feed6 <- filter(chickwts, feed == “casein”) This splits out the sixth group of feed (casein) from the chickwts dataset.", "kruskal.test( The function that performs a Kruskal-Wallis rank sum test.", "list(feed1$weight,feed2$weight,feed3$weight,feed4$weight,feed5$weight,feed6$weight)) A list of numeric vecotrs that represent the quantitative response variable for each group.", " Click to View Output  Click to View Output.", "Load library(tidyverse) to run this code in R.", "## ## Kruskal-Wallis rank sum test ## ## data: list(feed1$weight, feed2$weight, feed3$weight, feed4$weight, feed5$weight, feed6$weight) ## Kruskal-Wallis chi-squared = 37.343, df = 5, ## p-value = 5.113e-07 Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population.", "The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples.", "The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity.", "Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated.", "(Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\).", "Note that the notation \\(n_i\\) allows for each sample to be a different size.", "In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\).", "Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest.", "Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\).", "Award any tied values the average of the ranks of those values that are tied.", "In the bottle-cap data we have the following.", "Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample.", "Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\).", "In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\)."],
    "type": "page",
    "page_title": "Kruskal-Wallis Test"
  },
  {
    "id": 110,
    "title": "📍 Kruskal-Wallis Rank Sum\r\nTest",
    "url": "Kruskal.html#kruskalwallisranksumtest",
    "content": "Kruskal-Wallis Rank Sum\r\nTest Allows for deciding if several samples come from the same population or if at least one sample comes from a different population. Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated. Can also be used when data is ordered (ordinal) but does not have an exact measurement. Best used when the distribution of the data is not normal. ANOVA is appropriate when the data is normal. Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, “is a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly. In many cases, this is practically equivalent to the mean of at least one population differing from the others.” Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable. g is a qualitative grouping variable defining which groups each value in x belongs to. It must either be a character vector or a factor vector. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. weight ‘weight’ is a numeric variable from the chickwts dataset that represents the quantatitive response variable.  ~  ‘~’ is the tilde symbol used to separate the left- and right-hand side in a model formula. feed, ‘feed’ is a qualitative grouping variable in the chickwts dataset.  data = chickwts) ‘chickwts’ is a dataset in R.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## ## Kruskal-Wallis rank sum test ## ## data: weight by feed ## Kruskal-Wallis chi-squared = 37.343, df = 5, ## p-value = 5.113e-07 Alternatively, you could use the following approach as well. kruskal.test(list(x, y, z)) x is a numeric vector of data values that represents the quantatitive response variable for the first group. y is a numeric vector of data values that represents the quantatitive response variable for the second group. z is a numeric vector of data values that represents the quantatitive response variable for the third group. Note that more than three vectors of data could be included inside of list(). Example Code Hover your mouse over the example codes to learn more. feed1 <- filter(chickwts, feed == “horsebean”) This splits out the first group of feed (horsebean) from the chickwts dataset. feed2 <- filter(chickwts, feed == “linseed”) This splits out the second group of feed (linseed) from the chickwts dataset. feed3 <- filter(chickwts, feed == “soybean”) This splits out the third group of feed (soybean) from the chickwts dataset. feed4 <- filter(chickwts, feed == “sunflower”) This splits out the fourth group of feed (sunflower) from the chickwts dataset. feed5 <- filter(chickwts, feed == “meatmeal”) This splits out the fifth group of feed (meatmeal) from the chickwts dataset. feed6 <- filter(chickwts, feed == “casein”) This splits out the sixth group of feed (casein) from the chickwts dataset. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. list(feed1$weight,feed2$weight,feed3$weight,feed4$weight,feed5$weight,feed6$weight)) A list of numeric vecotrs that represent the quantitative response variable for each group.  Click to View Output  Click to View Output. Load library(tidyverse) to run this code in R. ## ## Kruskal-Wallis rank sum test ## ## data: list(feed1$weight, feed2$weight, feed3$weight, feed4$weight, feed5$weight, feed6$weight) ## Kruskal-Wallis chi-squared = 37.343, df = 5, ## p-value = 5.113e-07 Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population. The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples. The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity. Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated. (Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\). Note that the notation \\(n_i\\) allows for each sample to be a different size. In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\). Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest. Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\). Award any tied values the average of the ranks of those values that are tied. In the bottle-cap data we have the following. Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample. Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\). In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\). In the bottle-cap data, \\(\\bar{R}_1 = \\frac{R_1}{n_1} = \\frac{24}{5} = 4.8\\) \\(\\bar{R}_2 = \\frac{R_2}{n_2} = \\frac{14}{3} = 4.667\\) \\(\\bar{R}_3 = \\frac{R_3}{n_3} = \\frac{40}{4} = 10.0\\) Step 5: If there are no ties, calculate the test statistic \\(H\\) by the formula \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] See the original article for the formula to use when there are ties present in the data. In the bottle-cap data, \\(H = 5.656\\). To understand the logic behind the test statistic \\(H\\), note that the null hypothesis assumes that each of the \\(C\\) samples are taken from the same population. Under this assumption, the ranks assigned to each sample should represent a uniform sample of the ranks \\(1,\\ldots,N\\). If the null hypothesis were true, we would expect the average of the ranks from each group, the \\(\\bar{R}_i\\), to be roughly about the same. More precisely, using the properties of the discrete uniform distribution (see wikipedia), we expect each \\(\\bar{R}_i\\) to be close to \\(\\frac{1}{2}(N+1)\\), which is the mean of the discrete uniform distribution. That \\(\\bar{R}_i\\) should be close to this value comes from the fact that the mean of a distribution of means is equal to the population mean. Further, the variance of \\(\\bar{R}_i\\) is given by the population variance, \\(\\frac{1}{12}(N^2-1)\\) in this case, divided by the sample size \\(n_i\\). Finally, the \\(\\frac{N-1}{N}\\) is a correction factor that will not be explained here. The full article explains the details about how a multivariate normal distribution is used to uncover the formula for \\(H\\). Thus, understanding \\(H\\) completely will require an advanced degree in statistical theory. However, the short story is that the distribution of the test statistic \\(H\\) is approximately a chi squared distribution with \\(C-1\\) degrees of freedom and has the form of a sum of values consisting of the (observed mean - expected mean)^2/(variance of the observed mean). Step 6: Calculate the p-value of \\(H\\) using a chi squared distribution with \\(C-1\\) degrees of freedom. \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] In the bottle-cap data, \\(p = 0.059\\), which represents the probability of \\(H\\) being as extreme or more extreme than the observed value of 5.656, assuming the null hypothesis is true. Examples: wages, pigweights",
    "sentences": ["Allows for deciding if several samples come from the same population or if at least one sample comes from a different population.", "Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated.", "Can also be used when data is ordered (ordinal) but does not have an exact measurement.", "Best used when the distribution of the data is not normal.", "ANOVA is appropriate when the data is normal.", "Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, “is a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly.", "In many cases, this is practically equivalent to the mean of at least one population differing from the others.” Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\]", "R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable.", "g is a qualitative grouping variable defining which groups each value in x belongs to.", "It must either be a character vector or a factor vector."],
    "type": "section",
    "page_title": "Kruskal-Wallis Test",
    "section_title": "Kruskal-Wallis Rank Sum\r\nTest"
  },
  {
    "id": 111,
    "title": "📍 Overview",
    "url": "Kruskal.html#overview",
    "content": "Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated. Can also be used when data is ordered (ordinal) but does not have an exact measurement. Best used when the distribution of the data is not normal. ANOVA is appropriate when the data is normal. Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, “is a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly. In many cases, this is practically equivalent to the mean of at least one population differing from the others.” Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\]",
    "sentences": ["It is assumed that the various populations are of approximately the same form, but are shifted or translated.", "Can also be used when data is ordered (ordinal) but does not have an exact measurement.", "Best used when the distribution of the data is not normal.", "ANOVA is appropriate when the data is normal.", "Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, “is a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly.", "In many cases, this is practically equivalent to the mean of at least one population differing from the others.” Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\]"],
    "type": "section",
    "page_title": "Kruskal-Wallis Test",
    "section_title": "Overview"
  },
  {
    "id": 112,
    "title": "📍 R Instructions",
    "url": "Kruskal.html#rinstructions",
    "content": "R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable. g is a qualitative grouping variable defining which groups each value in x belongs to. It must either be a character vector or a factor vector. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. weight ‘weight’ is a numeric variable from the chickwts dataset that represents the quantatitive response variable.  ~  ‘~’ is the tilde symbol used to separate the left- and right-hand side in a model formula. feed, ‘feed’ is a qualitative grouping variable in the chickwts dataset.  data = chickwts) ‘chickwts’ is a dataset in R.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## ## Kruskal-Wallis rank sum test ## ## data: weight by feed ## Kruskal-Wallis chi-squared = 37.343, df = 5, ## p-value = 5.113e-07 Alternatively, you could use the following approach as well. kruskal.test(list(x, y, z)) x is a numeric vector of data values that represents the quantatitive response variable for the first group. y is a numeric vector of data values that represents the quantatitive response variable for the second group. z is a numeric vector of data values that represents the quantatitive response variable for the third group. Note that more than three vectors of data could be included inside of list(). Example Code Hover your mouse over the example codes to learn more. feed1 <- filter(chickwts, feed == “horsebean”) This splits out the first group of feed (horsebean) from the chickwts dataset. feed2 <- filter(chickwts, feed == “linseed”) This splits out the second group of feed (linseed) from the chickwts dataset. feed3 <- filter(chickwts, feed == “soybean”) This splits out the third group of feed (soybean) from the chickwts dataset. feed4 <- filter(chickwts, feed == “sunflower”) This splits out the fourth group of feed (sunflower) from the chickwts dataset. feed5 <- filter(chickwts, feed == “meatmeal”) This splits out the fifth group of feed (meatmeal) from the chickwts dataset. feed6 <- filter(chickwts, feed == “casein”) This splits out the sixth group of feed (casein) from the chickwts dataset. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. list(feed1$weight,feed2$weight,feed3$weight,feed4$weight,feed5$weight,feed6$weight)) A list of numeric vecotrs that represent the quantitative response variable for each group.  Click to View Output  Click to View Output. Load library(tidyverse) to run this code in R. ## ## Kruskal-Wallis rank sum test ## ## data: list(feed1$weight, feed2$weight, feed3$weight, feed4$weight, feed5$weight, feed6$weight) ## Kruskal-Wallis chi-squared = 37.343, df = 5, ## p-value = 5.113e-07 Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population. The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples. The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity. Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated. (Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\). Note that the notation \\(n_i\\) allows for each sample to be a different size. In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\). Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest. Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\). Award any tied values the average of the ranks of those values that are tied. In the bottle-cap data we have the following. Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample. Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\). In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\). In the bottle-cap data, \\(\\bar{R}_1 = \\frac{R_1}{n_1} = \\frac{24}{5} = 4.8\\) \\(\\bar{R}_2 = \\frac{R_2}{n_2} = \\frac{14}{3} = 4.667\\) \\(\\bar{R}_3 = \\frac{R_3}{n_3} = \\frac{40}{4} = 10.0\\) Step 5: If there are no ties, calculate the test statistic \\(H\\) by the formula \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] See the original article for the formula to use when there are ties present in the data. In the bottle-cap data, \\(H = 5.656\\). To understand the logic behind the test statistic \\(H\\), note that the null hypothesis assumes that each of the \\(C\\) samples are taken from the same population. Under this assumption, the ranks assigned to each sample should represent a uniform sample of the ranks \\(1,\\ldots,N\\). If the null hypothesis were true, we would expect the average of the ranks from each group, the \\(\\bar{R}_i\\), to be roughly about the same. More precisely, using the properties of the discrete uniform distribution (see wikipedia), we expect each \\(\\bar{R}_i\\) to be close to \\(\\frac{1}{2}(N+1)\\), which is the mean of the discrete uniform distribution. That \\(\\bar{R}_i\\) should be close to this value comes from the fact that the mean of a distribution of means is equal to the population mean. Further, the variance of \\(\\bar{R}_i\\) is given by the population variance, \\(\\frac{1}{12}(N^2-1)\\) in this case, divided by the sample size \\(n_i\\). Finally, the \\(\\frac{N-1}{N}\\) is a correction factor that will not be explained here. The full article explains the details about how a multivariate normal distribution is used to uncover the formula for \\(H\\). Thus, understanding \\(H\\) completely will require an advanced degree in statistical theory. However, the short story is that the distribution of the test statistic \\(H\\) is approximately a chi squared distribution with \\(C-1\\) degrees of freedom and has the form of a sum of values consisting of the (observed mean - expected mean)^2/(variance of the observed mean). Step 6: Calculate the p-value of \\(H\\) using a chi squared distribution with \\(C-1\\) degrees of freedom. \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] In the bottle-cap data, \\(p = 0.059\\), which represents the probability of \\(H\\) being as extreme or more extreme than the observed value of 5.656, assuming the null hypothesis is true. Examples: wages, pigweights",
    "sentences": ["Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable.", "g is a qualitative grouping variable defining which groups each value in x belongs to.", "It must either be a character vector or a factor vector.", "YourDataSet is the name of your data set.", "Example Code Hover your mouse over the example codes to learn more.", "kruskal.test( The function that performs a Kruskal-Wallis rank sum test.", "weight ‘weight’ is a numeric variable from the chickwts dataset that represents the quantatitive response variable.", " ~  ‘~’ is the tilde symbol used to separate the left- and right-hand side in a model formula.", "feed, ‘feed’ is a qualitative grouping variable in the chickwts dataset.", " data = chickwts) ‘chickwts’ is a dataset in R."],
    "type": "section",
    "page_title": "Kruskal-Wallis Test",
    "section_title": "R Instructions"
  },
  {
    "id": 113,
    "title": "📍 Explanation",
    "url": "Kruskal.html#explanation",
    "content": "Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population. The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples. The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity. Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated. (Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\). Note that the notation \\(n_i\\) allows for each sample to be a different size. In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\). Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest. Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\). Award any tied values the average of the ranks of those values that are tied. In the bottle-cap data we have the following. Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample. Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\). In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\). In the bottle-cap data, \\(\\bar{R}_1 = \\frac{R_1}{n_1} = \\frac{24}{5} = 4.8\\) \\(\\bar{R}_2 = \\frac{R_2}{n_2} = \\frac{14}{3} = 4.667\\) \\(\\bar{R}_3 = \\frac{R_3}{n_3} = \\frac{40}{4} = 10.0\\) Step 5: If there are no ties, calculate the test statistic \\(H\\) by the formula \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] See the original article for the formula to use when there are ties present in the data. In the bottle-cap data, \\(H = 5.656\\). To understand the logic behind the test statistic \\(H\\), note that the null hypothesis assumes that each of the \\(C\\) samples are taken from the same population. Under this assumption, the ranks assigned to each sample should represent a uniform sample of the ranks \\(1,\\ldots,N\\). If the null hypothesis were true, we would expect the average of the ranks from each group, the \\(\\bar{R}_i\\), to be roughly about the same. More precisely, using the properties of the discrete uniform distribution (see wikipedia), we expect each \\(\\bar{R}_i\\) to be close to \\(\\frac{1}{2}(N+1)\\), which is the mean of the discrete uniform distribution. That \\(\\bar{R}_i\\) should be close to this value comes from the fact that the mean of a distribution of means is equal to the population mean. Further, the variance of \\(\\bar{R}_i\\) is given by the population variance, \\(\\frac{1}{12}(N^2-1)\\) in this case, divided by the sample size \\(n_i\\). Finally, the \\(\\frac{N-1}{N}\\) is a correction factor that will not be explained here. The full article explains the details about how a multivariate normal distribution is used to uncover the formula for \\(H\\). Thus, understanding \\(H\\) completely will require an advanced degree in statistical theory. However, the short story is that the distribution of the test statistic \\(H\\) is approximately a chi squared distribution with \\(C-1\\) degrees of freedom and has the form of a sum of values consisting of the (observed mean - expected mean)^2/(variance of the observed mean). Step 6: Calculate the p-value of \\(H\\) using a chi squared distribution with \\(C-1\\) degrees of freedom. \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] In the bottle-cap data, \\(p = 0.059\\), which represents the probability of \\(H\\) being as extreme or more extreme than the observed value of 5.656, assuming the null hypothesis is true.",
    "sentences": ["In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population.", "The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples.", "The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity.", "Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated.", "(Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\).", "Note that the notation \\(n_i\\) allows for each sample to be a different size.", "In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\).", "Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest.", "Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\).", "Award any tied values the average of the ranks of those values that are tied."],
    "type": "section",
    "page_title": "Kruskal-Wallis Test",
    "section_title": "Explanation"
  },
  {
    "id": 114,
    "title": "Linear Regression",
    "url": "LinearRegression.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Linear Regression Determine which explanatory variables have a significant effect on the mean of the quantitative response variable. Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\). Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$ \\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) $$ \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is “equal to” the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to “God’s Law” or “Natural Law”. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are “random” and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)… \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced “y-hat”, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what follows… \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, “the change in the average y-value for a one unit change in the x-value.” It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, “the average y-value when x is zero.” It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful. R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command.  <-  This is the “left arrow” assignment operator that stores the results of your lm() code into mylm name. lm( lm(…) is an R function that stands for “Linear Model”. It performs a linear regression analysis for Y ~ X. Y  Y is your quantitative response variable. It is the name of one of the columns in your data set. ~  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X. X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value.  data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X. ) Closing parenthesis for the lm(…) function. summary(mylm) The summary command allows you to print the results of your linear regression that were previously saved in mylm name.  Click to Show Output  Click to View Output. Example output from a regression. Hover each piece to learn more. Call: lm(formula = dist ~ speed, data = cars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -29.069 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -9.525 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -2.272 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -2.272 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-2.272) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   9.215 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   43.201 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -17.5791 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   6.7584 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -2.601 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (-17.5791) by its standard error (6.7584). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of -17.5791 is -2.601 standard errors (6.7584) from zero, which is a fairly surprising distance as shown by the p-value. 0.0123 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). speed This is always the name of your X-variable in your lm(Y ~ X, …).   3.9324 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.4155 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 9.464 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” of the slope (3.9324) by its standard error (0.4155). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from zero, which is a really surprising distance as shown by the smallness of the p-value. 1.49e-12 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  15.38 For this particular regression, the estimate of \\(\\sigma\\) is 15.38. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 48 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 50 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 50-2 = 48. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.6511, In this particular regression, 65.11% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitt",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Linear Regression Determine which explanatory variables have a significant effect on the mean of the quantitative response variable.", "Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\).", "Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$ \\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) $$ \\(Y_i\\) The response variable.", "The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size.", "\\(=\\) This states that we are assuming \\(Y_i\\) was created, or is “equal to” the formula that will follow on the right-hand-side of the equation.", "\\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life.", "It can be likened to “God’s Law” or “Natural Law”.", "Something that governs the way the data behaves, but is unkown to us.", "\\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\).", "\\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\).", "The error terms are “random” and unique for each individual.", "This provides the statistical relationship of the regression.", "It is what allows each dot to be different, while still coming from the same line, or underlying law.", "\\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)… \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance.", "Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual.", "In other words, the variance is constant.", "The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line.", "The estimated regression line obtained from a regression analysis, pronounced “y-hat”, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est.", "y-int} + \\overbrace{b_1}^\\text{est.", "slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\).", "It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value.", "\\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what follows… \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation.", "First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\).", "This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\).", "Second, this equation does not include \\(\\epsilon_i\\).", "In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values.", "Note: see the Explanation tab The Mathematical Model for details about these equations.", "Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line.", "This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true.", "If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin.", "This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true.", "Assumptions This regression model is appropriate for the data when five assumptions can be made.", "Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear.", "Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero.", "Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values.", "Fixed X: the \\(X_i\\) values can be considered fixed and measured without error.", "Independent Errors: the error terms \\(\\epsilon_i\\) are independent.", "Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions.", "Interpretation The slope is interpreted as, “the change in the average y-value for a one unit change in the x-value.” It is not the average change in y.", "It is the change in the average y-value.", "The y-intercept is interpreted as, “the average y-value when x is zero.” It is often not meaningful, but is sometimes useful.", "It just depends if x being zero is meaningful or not within the context of your analysis.", "For example, knowing the average price of a car with zero miles is useful.", "However, pretending to know the average height of adult males that weigh zero pounds, is not useful.", "R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command.", " <-  This is the “left arrow” assignment operator that stores the results of your lm() code into mylm name.", "lm( lm(…) is an R function that stands for “Linear Model”.", "It performs a linear regression analysis for Y ~ X.", "Y  Y is your quantitative response variable.", "It is the name of one of the columns in your data set."],
    "type": "page",
    "page_title": "Linear Regression"
  },
  {
    "id": 115,
    "title": "📍 Simple Linear\r\nRegression",
    "url": "LinearRegression.html#simplelinearregression",
    "content": "Simple Linear\r\nRegression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\). Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$ \\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) $$ \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is “equal to” the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to “God’s Law” or “Natural Law”. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are “random” and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)… \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced “y-hat”, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what follows… \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, “the change in the average y-value for a one unit change in the x-value.” It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, “the average y-value when x is zero.” It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful.",
    "sentences": ["Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\).", "Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$ \\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) $$ \\(Y_i\\) The response variable.", "The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size.", "\\(=\\) This states that we are assuming \\(Y_i\\) was created, or is “equal to” the formula that will follow on the right-hand-side of the equation.", "\\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life.", "It can be likened to “God’s Law” or “Natural Law”.", "Something that governs the way the data behaves, but is unkown to us.", "\\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\).", "\\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\).", "The error terms are “random” and unique for each individual."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Simple Linear\r\nRegression"
  },
  {
    "id": 116,
    "title": "📍 Overview",
    "url": "LinearRegression.html#overview",
    "content": "Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$ \\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) $$ \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is “equal to” the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to “God’s Law” or “Natural Law”. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are “random” and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)… \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced “y-hat”, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what follows… \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, “the change in the average y-value for a one unit change in the x-value.” It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, “the average y-value when x is zero.” It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful.",
    "sentences": ["Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$ \\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) $$ \\(Y_i\\) The response variable.", "The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size.", "\\(=\\) This states that we are assuming \\(Y_i\\) was created, or is “equal to” the formula that will follow on the right-hand-side of the equation.", "\\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life.", "It can be likened to “God’s Law” or “Natural Law”.", "Something that governs the way the data behaves, but is unkown to us.", "\\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\).", "\\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\).", "The error terms are “random” and unique for each individual.", "This provides the statistical relationship of the regression."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Overview"
  },
  {
    "id": 117,
    "title": "📍 R Instructions",
    "url": "LinearRegression.html#rinstructions",
    "content": "R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command.  <-  This is the “left arrow” assignment operator that stores the results of your lm() code into mylm name. lm( lm(…) is an R function that stands for “Linear Model”. It performs a linear regression analysis for Y ~ X. Y  Y is your quantitative response variable. It is the name of one of the columns in your data set. ~  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X. X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value.  data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X. ) Closing parenthesis for the lm(…) function. summary(mylm) The summary command allows you to print the results of your linear regression that were previously saved in mylm name.  Click to Show Output  Click to View Output. Example output from a regression. Hover each piece to learn more. Call: lm(formula = dist ~ speed, data = cars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -29.069 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -9.525 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -2.272 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -2.272 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-2.272) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   9.215 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   43.201 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -17.5791 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   6.7584 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -2.601 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (-17.5791) by its standard error (6.7584). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of -17.5791 is -2.601 standard errors (6.7584) from zero, which is a fairly surprising distance as shown by the p-value. 0.0123 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). speed This is always the name of your X-variable in your lm(Y ~ X, …).   3.9324 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.4155 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 9.464 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” of the slope (3.9324) by its standard error (0.4155). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from zero, which is a really surprising distance as shown by the smallness of the p-value. 1.49e-12 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  15.38 For this particular regression, the estimate of \\(\\sigma\\) is 15.38. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 48 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 50 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 50-2 = 48. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.6511, In this particular regression, 65.11% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data.  0.6438 In this case, the value of 0.6438 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is found as the ratio of the MSR/MSE where MSR = SSR/(p-1) and MSE = SSE/(n-p) where n is the sample size and p is the number of parameters in the regression model.  89.57 This is the value of the F-statistic for the lm(dist ~ speed, data=cars) regression. Note that SSE = sum( cars.lm$res^2 ) = 11353.52 with n - p = 50 - 2 = 48 degrees of freedom for this data. Further, SSR = sum( (cars.lm$fit - mean(cars$dist))^2 ) = 21185.46 with p - 1 = 1 degree of freedom. So MSR = 21185.46 and MSE = 11353.52 / 48 = 236.5317. So MSR / MSE = 21185.46 / 236.5317 = 89.56711.  on 1 and 48 DF, The 1 degree of freedom is the SSR degrees of freedom (p-1). The 48 is the SSE degrees of freedom (n-p).  p-value: 1.49e-12 The p-value for an F-statistic is found by the code pf(89.56711, 1, 48, lower.tail=FALSE), which gives the probability of being more extreme than the observed F-statistic in an F distribution with 1 and 48 degrees of freedom. Check Assumptions 1, 2, 3, and 5 par( The par(…) command stands for “Graphical PARameters”. It allows you to control various aspects of graphics in Base R. mfrow= This stands for “multiple frames filled by row”, which means, put lots of plots on the same row, starting with the plot on the left, then working towards the right as more plots are created. c( The combine function c(…) is used to specify how many rows and columns of graphics should be placed together. 1, This specifies that 1 row of graphics should be produced. 3 This states that 3 columns of graphics should be produced. ) Closing parenthesis for c(…) function. ) Closing parenthesis for par(…) function. plot( This version of plot(…) will actually create several regression diagnostic plots by default. mylm, This is the name of an lm object that you created previously. which= This allows you to select “which” regression diagnostic plots should be drawn. 1 Selecting 1, would give the residuals vs. fitted values plot only. : The colon allows you to select more than just one plot. 2 Selecting 2 also gives the Q-Q Plot of residuals. If you wanted to instead you could just use which=1 to get the residuals vs fitted values plot, then you could use qqPlot(mylm$residuals) to create a fancier Q-Q Plot of the residuals. ) Closing parenthesis for plot(…) function. plot( This version of plot(…) will be used to create a time-ordered plot of the residuals. The order of the residuals is the original order of the x-values in the original data set. If the original data set doesn’t have an order, then this plot is not interesting. mylm The lm object that you created previously. $ This allows you to access various elements from the regression that was performed. residuals This grabs the residuals for each observation in the regression. ) Closing parenthesis for plot(…) function.  Click to Show Output  Click to View Output. Plotting the Regression Line Base R ggplot2 To add the regression line to a scatterplot use the abline(...) command: plot( The plot(…) function is used to create a scatterplot with a y-axis (the vertical axis) and an x-axis (the horizontal axis). Y  This is the “response variable” of your regression. The thing you are interested in predicting. This is the name of a “numeric” column of data from the data set called YourDataSet. ~  The tilde “~” is used to relate Y to X and can be found on the top-left key of your keyboard. X,  This is the explanatory variable of your regression. It is the name of a “numeric” column of data from YourDataSet. data= The data= statement is used to specify the name of the data set where the columns of “X” and “Y” are located. YourDataSet This is the name of your data set, like KidsFeet or cars or airquality. ) Closing parenthesis for plot(…) function. abline( This stands for “a” (intercept) “b” (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm This is the name of an lm(…) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(…) function will locate these two values from within mylm and use them to add a line to your current plot(…). ) Closing parenthesis for abline(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm) You can customize the look of the regression line with abline( This stands for “a” (intercept) “b” (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm, This is the name of an lm(…) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(…) function will locate these two values from within mylm and use them to add a line to your current plot(…). lty= The lty= stands for “line type” and allows you to select between 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. 1, This creates a solid line. Remember, other options include: 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash. lwd= The lwd= allows you to specify the width of the line. The default width is 1. Using lwd=2 would double the thickness, and so on. Any positive value is allowed. 1, Default line width. To make a thicker line, us 2 or 3… To make a thinner line, try 0.5, but 1 is already pretty thin. col= This allows you to specify the color of the line using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). “someColor” Type colors() in R for options. ) Closing parenthesis for abline(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) abline(mylm, lty=1, lwd=1, col=\"firebrick\") You can add points to the regression with… points( This is like plot(…) but adds points to the current plot(…) instead of creating a new plot. newY  newY should be a column of values from some data set. Or, use points(newX, newY) to add a single point to a graph. ~  This links Y to X in the plot. newX,  newX should be a column of values from some data set. It should be the same length as newY. If just a single value, use points(newX, newY) instead. data=YourDataSet,  If newY and newX come from a dataset, then use data= to tell the points(…) function what data set they come from. If newY and newX are just single values, then data= is not needed. col=“skyblue”, This allows you to specify the color of the points using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque). pch=16 This allows you to specify the type of plotting symbol to be used for the points. Type ?pch and scroll half way down in the help file that appears to learn about other possible symbols. ) Closing parenthesis for points(…) function.  Click to Show Output  Click to View Output. mylm <- lm(dist ~ speed, data = cars) plot(dist ~ speed, data = cars) points(7,40, pch=16, col=\"skyblue\", cex=2) text(7,40, \"New Dot\", pos=3, cex=0.5) points(dist ~ speed, data=filter(cars, mylm$res > 2), cex=.8, col=\"red\") abline(mylm, lty=1, lwd=1, col=\"firebrick\") To add the regression line to a scatterplot using the ggplot2 approach, first ensure: library(ggplot2) or library(tidyverse) is loaded. Then, use the geom_smooth(method = lm) command: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. YourDataSet,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. X, This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet.  y =  “y= ” declares which variable will become the y-axis of the graphic. Y This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function. + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. + Here the + is used to add yet another layer to ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. There are several other methods that could be used here.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(…) to place more complicated models on the plot.  se = FALSE se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off. ) Closing parenthesis for the geom_smooth() function.  Click to Show Output  Click to View Output. ggplot(cars, aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = \"lm\", formula=y~x, se=FALSE) There are a number of ways to customize the appearance of the regression line: ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. speed,  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet. y =  “y= ” declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point() geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic.  + Here the + is used to add yet another layer to ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in the same way as lm(…) to place more complicated models on the plot.  se = FALSE, se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.  size = 2, Use size = 2 to adjust the thickness of the line to size 2.  color = “orange”, Use color = “orange” to change the color of the line to orange.   linetype = “dashed” Use linetype = “dashed” to change the solid line to a dashed line. Some linetype options include “dashed”, “dotted”, “longdash”, “dotdash”, etc. ) Closing parenthesis for the geom_smooth() function.  Click to Show Output  Click to View Output. In addition to customizing the regression line, you can customize the points, add points, add lines, and much more. ggplot( Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic. cars,  This is simply the name of your data set, like KidsFeet or starwars. aes( aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables. x =  “x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax. speed,  This is the explanatory variable of the regression: the variable used to explain the mean of y. It is the name of the “numeric” column of YourDataSet. y =  “y= ” declares which variable will become the y-axis of the grpahic. dist This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.   geom_point( geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic. size = 1.5, Use size = 1.5 to change the size of the points.  color = “skyblue” Use color = “skyblue” to change the color of the points to Brother Saunders’ favorite color.  alpha = 0.5 Use alpha = 0.5 to change the transparency of the points to 0.5. ) Closing parenthesis of geom_point() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_smooth( geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot. method =  Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve.. “lm”, lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic.  formula = y~x, This tells geom_smooth to place a simple linear regression line on the plot. Other formula statements can be used in ways similar to lm(…) to place more complicated models on the plot.  se = FALSE, se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.  color = “navy”, Use color = “navy” to change the color of the line to navy blue.  size = 1.5 Use size = 1.5 to adjust the thickness of the line to 1.5. ) Closing parenthesis of geom_smooth() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_hline( Use geom_hline() to add a horizontal line at a specified y-intercept. You can also use geom_vline(xintercept = some_number) to add a vertical line to the graph. yintercept = Use “yintercept =” to tell geom_hline() that you are going to declare a y intercept for the horizontal line.  75 75 is the value of the y-intercept. , color = “firebrick” Use color = “firebrick” to change the color of the horizontal line to firebrick red. , size = 1, Use size = 1 to adjust the thickness of the horizontal line to size 1.              linetype = “longdash” Use linetype = “longdash” to change the solid line to a dashed line with longer dashes. Some linetype options include “dashed”, “dotted”, “longdash”, “dotdash”, etc. , alpha = 0.5 Use alpha = 0.5 to change the transparency of the horizontal line to 0.5. ) Closing parenthesis of geom_hline function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_segment( geom_segment() allows you to add a line segment to ggplot() by using specified start and end points. x = “x =” tells geom_segment() that you are going to declare the x-coordinate for the starting point of the line segment.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the starting point of the line segment.  y =“y =” tells geom_segment() that you are going to declare the y-coordinate for the starting point of the line segment.  75, 75 is a number on the y-axis of your graph. It is the y-coordinate of the starting point of the line segment.  xend = “xend =” tells geom_segment() that you are going to declare the x-coordinate for the end point of the line segment.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the end point of the line segment.  yend = “yend =” tells geom_segment() that you are going to declare the y-coordinate for the end point of the line segment.  38, 38 is a number on the y-axis of your graph. It is the y-coordinate of the end point of the line segment.                size = 1 Use size = 1 to adjust the thickness of the line segment. , color = “lightgray” Use color = “lightgray” to change the color of the line segment to light gray. , linetype = “longdash” Use *linetype = “longdash* to change the solid line segment to a dashed one. Some linetype options include”dashed”, “dotted”, “longdash”, “dotdash”, etc. ) Closing parenthesis for geom_segment() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_point( geom_point() can also be used to add individual points to the graph. Simply declare the x and y coordinates of the point you want to plot. x = “x =” tells geom_point() that you are going to declare the x-coordinate for the point.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the point.  y = “y =” tells geom_point() that you are going to declare the y-coordinate for the point.  75 75 is a number on the y-axis of your graph. It is the y-coordinate of the point. , size = 3 Use size = 3 to make the point stand out more. , color = “firebrick” Use color = “firebrick” to change the color of the point to firebrick red. ) Closing parenthesis of the geom_point() function.  + The + allows you to add more layers to the framework provided by ggplot().   geom_text( geom_text() allows you to add customized text anywhere on the graph. It is very similar to the base R equivalent, text(…). x = “x =” tells geom_text() that you are going to declare the x-coordinate for the text.  14, 14 is a number on the x-axis of your graph. It is the x-coordinate of the text.  y = “y =” tells geom_text() that you are going to declare the y-coordinate for the text.  84, 84 is a number on the y-axis of your graph. It is the y-coordinate of the text.  label = “label =” tells geom_text() that you are going to give it the label.  “My Point (14, 75)”, “My Point (14, 75)” is the text that will appear on the graph.             color = “navy” Use color = “navy” to change the color of the text to navy blue. , size = 3 Use size = 3 to change the size of the text. ) Closing parenthesis of the geom_text() function.  + The + allows you to add more layers to the framework provided by ggplot().   theme_minimal() Add a minimalistic theme to the graph. There are many other themes that you can try out.  Click to Show Output  Click to View Output. ## `geom_smooth()` using formula = 'y ~ x' Accessing Parts of the Regression Finally, note that the mylm object contains the names(mylm) of mylm$coefficients Contains two values. The first is the estimated \\(y\\)-intercept. The second is the estimated slope. ## (Intercept) speed ## -17.579095 3.932409 mylm$residuals Contains the residuals from the regression in the same order as the actual dataset. ## 1 2 3 4 ## 3.849460 11.849460 -5.947766 12.052234 ## 5 6 7 8 ## 2.119825 -7.812584 -3.744993 4.255007 ## 9 10 11 12 ## 12.255007 -8.677401 2.322599 -15.609810 ## 13 14 15 16 ## -9.609810 -5.609810 -1.609810 -7.542219 ## 17 18 19 20 ## 0.457781 0.457781 12.457781 -11.474628 ## 21 22 23 24 ## -1.474628 22.525372 42.525372 -21.407036 ## 25 26 27 28 ## -15.407036 12.592964 -13.339445 -5.339445 ## 29 30 31 32 ## -17.271854 -9.271854 0.728146 -11.204263 ## 33 34 35 36 ## 2.795737 22.795737 30.795737 -21.136672 ## 37 38 39 40 ## -11.136672 10.863328 -29.069080 -13.069080 ## 41 42 43 44 ## -9.069080 -5.069080 2.930920 -2.933898 ## 45 46 47 48 ## -18.866307 -6.798715 15.201285 16.201285 ## 49 50 ## 43.201285 4.268876 mylm$fitted.values The values of \\(\\hat{Y}\\) in the same order as the original dataset. ## 1 2 3 4 5 ## -1.849460 -1.849460 9.947766 9.947766 13.880175 ## 6 7 8 9 10 ## 17.812584 21.744993 21.744993 21.744993 25.677401 ## 11 12 13 14 15 ## 25.677401 29.609810 29.609810 29.609810 29.609810 ## 16 17 18 19 20 ## 33.542219 33.542219 33.542219 33.542219 37.474628 ## 21 22 23 24 25 ## 37.474628 37.474628 37.474628 41.407036 41.407036 ## 26 27 28 29 30 ## 41.407036 45.339445 45.339445 49.271854 49.271854 ## 31 32 33 34 35 ## 49.271854 53.204263 53.204263 53.204263 53.204263 ## 36 37 38 39 40 ## 57.136672 57.136672 57.136672 61.069080 61.069080 ## 41 42 43 44 45 ## 61.069080 61.069080 61.069080 68.933898 72.866307 ## 46 47 48 49 50 ## 76.798715 76.798715 76.798715 76.798715 80.731124 mylm$… several other things that will not be explained here. Making Predictions predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ) Closing parenthesis for the data.frame(…) function. ) Closing parenthesis for the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12)) 1   29.60981 The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(…) function.  interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. “prediction” This specifies that a prediction interval will be included with the predicted value. A prediction interval gives you a 95% confidence interval that captures 95% of the data, or \\(Y_i\\) values for the specific \\(X\\)-value specified in the prediction. ) Closing parenthesis of the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"prediction\")   fit The “fit” is the predicted value.   lwr The “lwr” is the lower bound.   upr The “upr” is the upper bound. 1 29.60981 In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet. -1.749529 This is the lower bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us the stopping distance could be as quick as -1.749529 feet (or 0 because distance can’t go negative). 60.96915 This is the upper bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us that the actual stopping distance could be as high as 60.96915 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X=… Further, the value of \\(Xh\\) should be some specific number, like speed=12 for example. Xh The value of \\(Xh\\) should be some specific number, like 12, as in speed=12 for example. ), Closing parenthesis for the data.frame(…) function.  interval= This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval. “confidence” This specifies that a confidence interval for the prediction should be provided. This is of use whenever your interest is in just estimating the average y-value, not the actual y-values. ) Closing parenthesis of the predict(…) function. mylm <- lm(dist ~ speed, data = cars) predict(mylm, data.frame(speed = 12), interval = \"confidence\")   fit The “fit” is the predicted value.   lwr The “lwr” is the lower bound.   upr The “upr” is the upper bound. 1 29.60981 In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. 24.39514 This is the lower bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is greater than this value. 34.82448 This is the upper bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is less than this value. Finding Confidence Intervals for Model Parameters confint( The R function confint(…) allows you to use an lm(…) object to compute confidence intervals for one or more parameters (like \\(\\beta_0\\) or \\(\\beta_1\\)) in your model. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  level = “level =” tells the confint(…) function that you are going to declare at what level of confidence you want the interval. The default is “level = 0.95.” If you want to find 95% confidence intervals for your parameters, then just run confint(mylm).  someConfidenceLevel someConfidenceLevel is simply a confidence level you choose when you want something other than a 95% confidence interval. Some examples of appropriate levels include 0.90 and 0.99. ) Closing parenthesis for confint(..) function. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.90)     5 % The lower bound of a 90% confidence interval occurs at the 5th percentile. This is because at 90% confidence, 10% is left in the tails, with 5% on each end. The upper bound will thus end at the 95th percentile, hence the 5% and 95% as the column names.   95 % The upper bound of a 90% confidence interval ends at the 95th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -28.914514 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -6.243676 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.235501 and 4.629317. 3.235501 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. 4.629317 This is the upper bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317. mylm <- lm(dist ~ speed, data = cars) confint(mylm, level = 0.95)     2.5 % The lower bound of a 95% confidence interval occurs at the 2.5th percentile. This is because at 95% confidence, 5% is left in the tails, with 2.5% on each end. The upper bound will thus end at the 97.5th percentile, hence the 2.5% and 97.5% as the column names for the lower and upper bounds, respectively.   97.5 % The upper bound of a 95% confidence interval ends at the 97.5th percentile. (Intercept) This row of output specifies a confidence interval for \\(\\beta_0\\), the true y-intercept. -31.167850 This is the lower bound for the confidence interval of the y-intercept, \\(\\beta_0\\). In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. -3.990340 This is the upper bound for the confidence interval for \\(\\beta_0\\), the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance. speed This row of the output provides the upper and lower bound for the confidence interval for \\(\\beta_1\\), the true slope. In this case, you can be 90% confident that the true slope lies between 3.096964 and 4.767853. 3.096964 This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.096964 and 4.767853 4.767853 This is the upper bound of the confidence interval. In this case, you can be 95% confident that the slope lies between 3.096964 and 4.767853",
    "sentences": ["Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command.", " <-  This is the “left arrow” assignment operator that stores the results of your lm() code into mylm name.", "lm( lm(…) is an R function that stands for “Linear Model”.", "It performs a linear regression analysis for Y ~ X.", "Y  Y is your quantitative response variable.", "It is the name of one of the columns in your data set.", "~  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X.", "X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value.", " data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X.", "In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "R Instructions"
  },
  {
    "id": 118,
    "title": "📍 Explanation",
    "url": "LinearRegression.html#explanation",
    "content": "Explanation Linear regression has a rich mathematical theory behind it. This is because it uses a mathematical function and a random error term to describe the regression relation between a response variable \\(Y\\) and an explanatory variable called \\(X\\). Expand each element below to learn more. Regression Cheat Sheet (Expand) Term Pronunciation Meaning Math R Code \\(Y_i\\) $Y_i$ “why-eye” The data \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)\\) $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$ YourDataSet$YourYvariable \\(\\hat{Y}_i\\) $\\hat{Y}_i$ “why-hat-eye” The fitted line \\(\\hat{Y}_i = b_0 + b_1 X_i\\) $\\hat{Y}_i = b_0 + b_1 X_i$ lmObject$fitted.values \\(E\\{Y_i\\}\\) $E\\{Y_i\\}$ “expected value of why-eye” True mean y-value \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) $E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i$ <none> \\(\\beta_0\\) $\\beta_0$ “beta-zero” True y-intercept <none> <none> \\(\\beta_1\\) $\\beta_1$ “beta-one” True slope <none> <none> \\(b_0\\) $b_0$ “b-zero” Estimated y-intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) $b_0 = \\bar{Y} - b_1\\bar{X} b_0 <- mean(Y) - b_1*mean(X)$ \\(b_1\\) $b_1$ “b-one” Estimated slope \\(b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\) $b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})} {\\sum(X_i - \\bar{X})^2}$ b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) \\(\\epsilon_i\\) $\\epsilon_i$ “epsilon-eye” Distance of dot to true line \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) $\\epsilon_i = Y_i - E\\{Y_i\\}$ <none> \\(r_i\\) $r_i$ “r-eye” or “residual-eye” Distance of dot to estimated line \\(r_i = Y_i - \\hat{Y}_i\\) $r_i = Y_i - \\hat{Y}_i$ lmObject$residuals \\(\\sigma^2\\) $\\sigma^2$ “sigma-squared” Variance of the \\(\\epsilon_i\\) \\(Var\\{\\epsilon_i\\} = \\sigma^2\\)$Var\\{\\epsilon_i\\} = \\sigma^2$ <none> \\(MSE\\) $MSE$ “mean squared error” Estimate of \\(\\sigma^2\\) \\(MSE = \\frac{SSE}{n-p}\\)$MSE = \\frac{SSE}{n-p}$ sum( lmObject$res^2 ) / (n - p) \\(SSE\\) $SSE$ “sum of squared error” (residuals) Measure of dot’s total deviation from the line \\(SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\)$SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$ sum( lmObject$res^2 ) \\(SSR\\) $SSR$ “sum of squared regression error” Measure of line’s deviation from y-bar \\(SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\)$SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2$ sum( (lmObject$fit - mean(YourData$Y))^2 ) \\(SSTO\\) $SSTO$ “total sum of squares” Measure of total variation in Y \\(SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\)$SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2$ sum( (YourData$Y - mean(YourData$Y))^2 ) \\(R^2\\) $R^2$ “R-squared” Proportion of variation in Y explained by the regression \\(R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\)$R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}$ SSR/SSTO \\(r\\) $r$ “r” Correlation between X and Y. \\(r = \\sqrt{R^2}\\)$r = \\sqrt{R^2}$ sqrt(R^2) \\(\\hat{Y}_h\\) $\\hat{Y}_h$ “why-hat-aitch” Estimated mean y-value for some x-value called \\(X_h\\) \\(\\hat{Y}_h = b_0 + b_1 X_h\\)$\\hat{Y}_h = b_0 + b_1 X_h$ predict(lmObject, data.frame(XvarName=#)) \\(X_h\\) $X_h$ “ex-aitch” Some x-value, not necessarily one of the \\(X_i\\) values used in the regression \\(X_h =\\) some number$X_h = $ Xh = # Confidence Interval “confidence interval” Estimated bounds at a certain level of confidence for a parameter \\(b_0 \\pm t^* \\cdot s_{b_0}\\)b_0 \\pm t^* \\cdot s_{b_0} or \\(b_1 \\pm t^* \\cdot s_{b_1}\\)b_1 \\pm t^* \\cdot s_{b_1} confint(mylm, level = someConfidenceLevel) Parameter Estimate \\(\\beta_0\\) \\(b_0\\) \\(\\beta_1\\) \\(b_1\\) \\(\\epsilon_i\\) \\(r_i\\) \\(\\sigma^2\\) \\(MSE\\) \\(\\sigma\\) \\(\\sqrt{MSE}\\), the Residual standard error The Mathematical Model (Expand) \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)… There are three main elements to the mathematical model of regression. Each of these three elements is pictured below in the “Regression Relation Diagram.” Study both the three bullet points and their visual representations in the plot below for a clearer understanding. The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read more…) The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as “natural law” or “God’s law”. It is some true line that is unknown to us. The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line. The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\). Note: \\(E\\{Y\\}\\) is pronounced “the expected value of y” because, well… the mean is the typical, average, or “expected” value. The dots, i.e., the regression relation plus an error term: \\(Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}} + \\underbrace{\\epsilon_i}_\\text{error term} \\quad \\text{where} \\ \\epsilon_i\\sim N(0,\\sigma^2)\\) (Read more…) This is shown by the dots in the graph below. This is the data. In regression, the assumption is that the y-value for individual \\(i\\), denoted by \\(Y_i\\), was “created” by adding an error term \\(\\epsilon_i\\) to each individual’s “expected” value \\(\\beta_0 + \\beta_1 X_i\\). Note the “order of creation” would require first knowing an indivual’s x-value, \\(X_i\\), then their expected value from the regression relation \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) and then adding their \\(\\epsilon_i\\) value to the result. The \\(\\epsilon_i\\) allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance \\(\\epsilon_i\\) from the line. Note: \\(Y_i\\) is pronounced “why-eye” because it is the y-value for individual \\(i\\). Sometimes also called “why-sub-eye” because \\(i\\) is in the subscript of \\(Y\\). The estimated line, i.e., the line we get from a sample of data. \\(\\underbrace{\\hat{Y}_i}_{\\substack{\\text{estimated mean} \\\\ \\text{y-value}}} = \\underbrace{b_0 + b_1 X_i}_\\text{estimated regression equation}\\) (Read more…) The estimated line is shown by the solid line in the graph below. \\(\\hat{Y}\\) is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation \\(E\\{Y\\}\\). So \\(\\hat{Y}\\) is interpreted as the estimated average (or mean) \\(Y\\)-value for any given \\(X\\)-value. Thus, \\(b_0\\) is the estimated y-intercept and \\(b_1\\) is the estimated slope. The b’s are sample statistics, like \\(\\bar{x}\\) and the \\(\\beta\\)’s are population parameters like \\(\\mu\\). The \\(b\\)’s estimate the \\(\\beta\\)’s. Note: \\(\\hat{Y}_i\\) is pronounced “why-hat-eye” and is known as the “estimated y-value” or “fitted y-value” because it is the y-value you get from \\(b_0 + b_1 X_i\\). It is always different from \\(Y_i\\) because dots are rarely if ever exactly on the estimated regression line. This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line). Something to ponder: The true line, when coupled with the error terms, “creates” the data. The estimated (or fitted) line uses the sampled data to try to “re-create” the true line. We could loosely call this the “order of creation” as shown by the following diagram. par(mfrow=c(1,3), mai=c(.2,.2,.4,.1)) plot(y ~ x, col=\"white\", main=\"A Law is Given\", yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, main=\"Data is Created\", xaxt='n', yaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, xaxt='n', yaxt='n', main=\"The Law is Estimated\") curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) A Law is Given Data is Created The Law is Estimated \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) \\(Y_i = E\\{Y_i\\} + \\epsilon_i\\) \\(\\hat{Y}_i = b_0 + b_1 X_i\\) The true line is the “law”. The \\(Y_i\\) are created by adding \\(\\epsilon_i\\) to \\(E\\{Y_i\\}\\) where \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\). The law is estimated with \\(\\hat{Y}_i\\) which is given with lm(...). Click open the “Code” buttom below to the right to find code that runs a simulation demonstrating this “order of creation”. ## Simulating Data from a Regression Model ## This R-chunk is meant to be played in your R Console. ## It allows you to explore how the various elements ## of the regression model combine together to \"create\" ## data and then use the data to \"re-create\" the line. set.seed(101) #Allows us to always get the same \"random\" sample #Change to a new number to get a new sample n <- 153 #set the sample size X_i <- runif(n, 0, 22) #Gives n random values from a uniform distribution between 15 to 45. beta0 <- 89 #Our choice for the y-intercept. beta1 <- -1.25 #Our choice for the slope. sigma <- 8 #Our choice for the std. deviation of the error terms. epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data View(fabData) #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it. fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData. summary(fab.lm) #Summarize your model. plot(y ~ x, data=fabData) #Plot the data. abline(fab.lm) #Add the estimated regression line to your plot. # Now for something you can't do in real life... but since we created the data... abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). legend(\"topleft\", legend=c(\"True Line\", \"Estimated Line\"), lty=c(2,1), bty=\"n\") #Add a legend to your plot specifying which line is which. Interpreting the Model Parameters (Expand) \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted as… The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model. If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\). The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\). It is often misunderstood to be the “average change in y” or just “the change in y” but it is more correctly referred to as the “change in the average y”. To better see this, consider the three graphics shown below. par(mfrow=c(1,3)) hist(mtcars$mpg, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Number of Vehicles\", xlab=\"Gas Mileage (mpg)\", col=\"skyblue\") boxplot(mpg ~ cyl, data=mtcars, border=\"skyblue\", boxwex=0.5, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Number of Cylinders of Engine (cyl)\") plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=\"skyblue\", main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Quarter Mile Time (qsec)\") abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=\"darkgray\") mtext(side=3, text=\"Automatic Transmissions Only (am==0)\", cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=\"gray\") The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09. The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg. The scatterplot on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value. Residuals and Errors (Expand) \\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true error… Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\). The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\). We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summary… Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\). \\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms. Keep in mind the idea that the errors \\(\\epsilon_i\\) “created” the data and that the residuals \\(r_i\\) are computed after using the data to “re-create” the line. Residuals have many uses in regression analysis. They allow us to diagnose the regression assumptions, See the “Assumptions” section below for more details. estimate the regression relation, See the “Estimating the Model Parameters” section below for more details. estimate the variance of the error terms, See the “Estimating the Model Variance” section below for more details. and assess the fit of the regression relation. See the “Assessing the Fit of a Regression” section below for more details. Assessing the Fit of a Regression (Expand) \\(R^2\\), SSTO, SSR, and SSE… Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line. par(mfrow=c(1,3), mai=c(.1,.1,.5,.1)) set.seed(2) x <- runif(30,0,20) y1 <- 2 + 3.5*x + rnorm(30,0,2) y2 <- 2 + 3.5*x + rnorm(30,0,8) y3 <- 2 + 3.5*x + rnorm(30,0,27) plot(y1 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Excellent Fit\") abline(lm(y1 ~ x), col=\"gray\") plot(y2 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Good Fit\") abline(lm(y2 ~ x), col=\"gray\") plot(y3 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Poor Fit\") abline(lm(y3 ~ x), col=\"gray\") A common way to measure the fit of a regression is with correlation. While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\). (If you are a Math 325 student, just stick with correlation for now and skip on to the next section of this Explanation tab. If you are a Math 425 student, it is critical that you come to understand \\(R^2\\) deeply, so read on.) Before you can understand \\(R^2\\), you must understand three important “sums of squares”. (Read more about sums…) A sum is just a fancy word for adding things together. \\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand. So we use the symbol \\(\\Sigma\\) to denote the word “sum”. Further, we use a subscript \\(\\Sigma_{i=1}\\) to state what value the sum is beginning with, and a superscript \\(\\Sigma_{i=1}^6\\) to state the value we are ending at. This gives \\[ \\sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Test your knowledge, do you see why the answer is 6 to the sum below? \\[ \\sum_{i=1}^3 i = 6 \\] Computing sums in R is fairly easy. Type the following codes in your R Console. sum(1:6) #gives the answer of 21 sum(1:3) #gives the answer of 6 However, sums really become useful when used with a data set. Each row of a data set represents an “individual’s” data. We can reference each individual with a row number. In the data below, individual 3, denoted by \\(i=3\\), has a speed of 7 and a dist of 4. pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3) Individual speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 To compute the sum of the speed column, use sum(speed). If we divided this sum by 6, we would get the mean of speed mean(speed). In fact, the two most used statistics mean(...) and sd(...) both use sums. Take a moment to review the formulas for mean and standard deviation. It is strongly recommended that you study the Explanation tab for both as well. We’ll wait. See you back here shortly. … Welcome back. Suppose we let X = speed and Y = dist. Then \\(X_3 = 7\\) and \\(Y_3 = 4\\) because we are accessing row 3 of both the \\(X\\) (or speed) column and \\(Y\\) (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, sum(speed) would be written mathematically as \\(\\sum_{i=1}^6 X_i\\) and sum(dist) would be written as \\(\\sum_{i=1}^6 Y_i\\). Sum of Squared Errors Sum of Squares Regression Total Sum of Squares \\(\\text{SSE} = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\) \\(\\text{SSR} = \\sum_{i=1}^n \\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\) \\(\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\\) Measures how much the residuals deviate from the line. Measures how much the regression line deviates from the average y-value. Measures how much the y-values deviate from the average y-value. Equals SSTO - SSR Equals SSTO - SSE Equals SSE + SSR sum( (Y - mylm$fit)^2 ) sum( (mylm$fit - mean(Y))^2 ) sum( (Y - mean(Y))^2 ) It is important to remember that SSE and SSR split up SSTO, so that \\[ \\text{SSTO} = \\text{SSE} + \\text{SSR} \\] This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works. The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called \\(R^2\\) (“r-squared”). R-Squared (\\(R^2\\)) \\[ \\underbrace{R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}}_\\text{Interpretation: Proportion of variation in Y explained by the regression.} \\] The smallest \\(R^2\\) can be is zero, and the largest it can be is 1. This is because \\(SSR\\) must be between 0 and SSTO, inclusive. Residual Plots & Regression Assumptions (Expand) Residuals vs. fitted-values, Q-Q Plot of the residuals, and residuals vs. order plots… There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate. Each assumption is labeled in the regression equation below. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions. (Read more…) As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not. Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3 The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(\\hat{Y}_i\\). The residuals are the \\(r_i\\). This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable. | Show Examples | (Read more…) The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption. The linear relation is assumed to be satisfied if there are no apparent trends in the plot. The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values. The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance. set.seed(2) X <- rnorm(30,15,3) notLin <- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8)) notLin.lm <- lm(Y~X, data=notLin) set.seed(15) Lin <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) Lin.lm <- lm(Y~X, data=Lin) par(mfrow=c(3,3), mai=c(.25,.25,.25,.25), mgp=c(1,.75,0)) plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Not Linear\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(notLin.lm$fitted.values,notLin.lm$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Good: Linear, Constant Variance\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) set.seed(6) notCon <- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5)) notCon.lm <- lm(Y~X, data=notCon) LinO <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) LinO[1] <- LinO[1]^2 LinO.lm <- lm(Y~X, data=LinO) plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Unconstant Variance\", cex.main=0.95, yaxt='n', xaxt='n', col=\"firebrick\") polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) # plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Outliers\", cex.main=0.95) # abline(h=0) tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(Girth ~ Volume, data=trees[-31,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(Height ~ Volume, data=trees) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) abline(h=0) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals, f=.4) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(weight ~ repwt, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(weight ~ repht, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) abline(h=0) Q-Q Plot of the Residuals: Checks Assumption #2 The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption. | Show Examples | (Read more…) There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot. Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) set.seed(123) tmp <- rnorm(100) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"skyblue\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Normal\", col=\"skyblue\") tmp <- Davis$weight qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Right-skewed\", breaks=15, col=\"firebrick\") par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- rbeta(100, 5,1) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Left-skewed\", breaks=seq(min(tmp),max(tmp), length.out=13), col=\"firebrick\") tmp <- rbeta(100,2,2) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Heavy-tailed\", col=\"firebrick\") Residuals versus Order Plot: Checks Assumption #5 When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated. | Show Examples | (Read more…) Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Good: No Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: General Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ qsec, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: Interesting Patterns\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),]) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Bad: Obvious Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") Problems from Failed Assumptions There are various problems that can arise when certain of the regression assupmtions are not satisfied. Lack of Linearity When the linearity assumption is violated, pretty much everything we obtain from the regression summary is no longer meaningful. The y-intercept estimate can be drastically off from its actual true value. Important model information is lost by trying to use a simple slope term \\(\\beta_1\\) to describe the model with respect to \\(X\\). The residual standard error will be much higher than it otherwise would be because of curvature patterns in the data that the line cannot capture. Thus, R-squared will be lower than it otherwise should be. P-values can become non-significant, when in fact there is a strong pattern in the data, but that pattern just cannot be captured by a simple line. *Normality of the errors is often put into question as well when a simplified line is used to try to capture a more complicated curved model. The plot below demonstrate these difficulties. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 7.5 #True slope beta_2 <- -0.25 #True bend X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors Y_i <- beta_0 + beta_1*X_i + beta_2*X_i^2 + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Non-Linear Relation\") abline(mylm, col=\"gray\") #Add fitted line to plot curve(beta_0 + beta_1*x + beta_2*x^2, col=\"gray\", lty=2, add=TRUE) #Add True line to plot #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (True value:\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (True value:\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (True value:\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Non-normal Error Terms When the normality assumption for the error terms is violated, not all is lost. In fact, the estimate of the slope and intercept are still often fairly meaningful. However, it is unwise to put too much trust in the residual standard error as an estimate of the standard deviation \\(\\sigma\\). This is because the standard deviation in skewed distributions does not carry the same meaning it has in normal distributions. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rchisq(n, 1)*3 - 1 #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Unconstant Variance When variance of the error term changes across the regression, the regression approximates the “average variance” of the errors because the regression is still assuming the variance is constant across the regression. The estimates of the slope and intercept are still typically quite good, and can be used for interpretation. The residual standard error however should not be considered to be meaningful as it will be too large on one end of the regression and too small on the other end. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma + X_i) #normally distributed errors #with increasing variance Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Variance Varies (Non-Constant)\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 13.82 Slope 3.5 3.768 Sigma 2.5 13.02 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", round(mean(sigma + X_i), 2), \", mean)\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Normality Violated As silly as it sounds, if the only problem with the regression is the lack of normality of the error terms, it isn’t all that big of a problem. Depending on how non-normal the residuals appear, there could be some skewing to the residual standard error, but otherwise, the slope and intercept are still interpretable and meaningful. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- runif(n, -sqrt(12*sigma^2)/2, sqrt(12*sigma^2)/2) #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 14.7 Slope 3.5 3.449 Sigma 2.5 2.317 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Independence Assumption Violated While the slope and intercept are often still meaningful when the independence assumption is violated, the residual standard error is unnecessarily large in this case. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, 2.5) + (1:n -n/2)*.5 #normal, but correlated errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Independence Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 16.47 Slope 3.5 3.296 Sigma 2.5 4.819 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Outliers Present While outliers do not violate any of the regression assumptions, they do pose substantial difficulties for the least squares regression estimates of the slope and intercept. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors epsilon_i[3] <- ifelse(X_i[3] < 10, runif(1,25,35), -runif(1,25,35)) #create outlier Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"An Outlier Present\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 13.18 Slope 3.5 3.447 Sigma 2.5 6.389 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) The material below this section is meant for Math 425 students only. Estimating the Model Parameters (Expand) How to get \\(b_0\\) and \\(b_1\\): least squares & maximum likelihood… There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical. The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas. Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 <- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\). When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\). Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i. \\label{exp} \\end{equation}\\] Least Squares To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\). \\[ Q = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2 \\] Then we use the function Q as if it were a function of \\(\\beta_0\\) and \\(\\beta_1\\). Ironically, the values of \\(Y\\) and \\(X\\) are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we don’t know are the values of \\(\\beta_0\\) and \\(\\beta_1\\). This least squares applet is a good way to explore how various choices of the slope and intercept yield different values of the “sum of squared residuals”. But it turns out that there is one “best” choice of the slope and intercept that yields a “smallest” value of the “sum of squared residuals.” This best choice can actually be found using calculus by taking the partial derivatives of \\(Q\\) with respect to both \\(\\beta_0\\) and \\(\\beta_1\\). \\[ \\frac{\\partial Q}{\\partial \\beta_0} = -2\\sum (Y_i - \\beta_0 - \\beta_1X_i) \\] \\[ \\frac{\\partial Q}{\\partial \\beta_1} = -2\\sum X_i(Y_i-\\beta_0-\\beta_1X_i) \\] Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize \\(Q\\) for a given set of data. After all the calculations are completed we find the values of the parameter estimators \\(b_0\\) and \\(b_1\\) (of \\(\\beta_0\\) and \\(\\beta_1\\), respectively) are as stated previously. Maximum Likelihood The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of \\(\\beta_0\\) and \\(\\beta_1\\) which minime the least squares \\(Q\\) function, we choose the values of \\(\\beta_0\\) and \\(\\beta_1\\) which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the \\(Y_i\\) for all observations \\(i=1,\\ldots,n\\). We can do this rather simply by using the assumption that the errors, \\(\\epsilon_i\\) are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if \\(f(Y_i)\\) denotes the probability density function for \\(Y_i\\), then the joint probability density for all \\(Y_i\\), \\(f(Y_1,\\ldots,Y_n)\\) is given by \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) \\] Since each \\(Y_i\\) is assumed to be normally distributed with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\) (see model (\\(\\ref{model}\\))) we have that \\[ f(Y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{Y_i-\\beta_0-\\beta_1X_i}{\\sigma}\\right)^2\\right]} \\] which provides the joint probability as \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] The likelihood function \\(L\\) is then given by consider the \\(Y_i\\) and \\(X_i\\) fixed and the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) as the variables in the function. \\[ L(\\beta_0,\\beta_1,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] Instead of taking partial derivatives of \\(L\\) directly (with respect to all parameters) we take the partial derivatives of the \\(\\log\\) of \\(L\\), which is easier to work with. In a similar, but more difficult calculation, to that of minimizing \\(Q\\), we obtain the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) which maximize the log of \\(L\\), and which therefore maximize \\(L\\). (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate \\(\\hat{\\sigma}^2\\) of \\(\\sigma^2\\). \\[ \\hat{\\sigma}^2 = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n} \\] Estimating the Model Variance (Expand) Estimating \\(\\sigma^2\\) with MSE… As shown previously in the “Estimating Model Parameters” section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation. Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\). This estimate turns out to be a biased estimator. This means that it is consistently wrong in its estimates of \\(\\sigma^2\\). If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong. This is bad. Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\). Without going into all the details, to “fix” the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently. To find the correct degrees of freedom, we have to notice that the \\(\\hat{Y}_i\\) in the numerator of \\(\\widehat{\\sigma}^2\\) is defined by \\[\\begin{equation} \\widehat{Y}_i = b_0 + b_1X_i \\label{hatY} \\end{equation}\\] From this equation, we notice that two means, \\(\\bar{X}\\) and \\(\\bar{Y}\\), were estimated from the data in order to obtain \\(\\hat{Y}_i\\). (See the formulas for \\(b_0\\) and \\(b_1\\) above, and note how they use both \\(\\bar{X}\\) and \\(\\bar{Y}\\) in their calculation.) Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for \\(\\hat{\\sigma}^2\\) should be \\(n-2\\) instead of \\(n\\). Some incredibly long calculations will show that the “fixed” estimator \\[\\begin{equation} s^2 = MSE = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n-2} \\quad \\text{(Unbiased Estimator of $\\sigma^2$)} \\end{equation}\\] is an unbiased estimator of \\(\\sigma^2\\). Here \\(MSE\\) stands for mean squared error, which is the most obvious name for a formula that squares the errors \\(Y_i-\\hat{Y}_i\\) then adds them up and divides by their degrees of freedom. Similarly, we call the numerator \\(\\sum(Y_i-\\hat{Y}_i)^2\\) the sum of the squared errors, denoted by \\(SSE\\). It is also important to note that the errors are often denoted by \\(r_i = Y_i-\\hat{Y}_i\\), the residuals. Putting this all together we get the following equivalent statements for \\(MSE\\). \\[\\begin{equation} s^2 = MSE = \\frac{SSE}{n-2} = \\frac{\\sum(Y_i-\\widehat{Y}_i)^2}{n-2} = \\frac{\\sum r_i^2}{n-2} \\end{equation}\\] As a final note, even though the expected value \\(E\\{MSE\\} = \\sigma^2\\), which shows \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\), it unfortunately isn’t true that \\(\\sqrt{MSE}\\) is an unbiased estimator of \\(\\sigma\\). This presents a few problems later on, but these are minimal enough that we can overlook the issue and move forward. Transformations (Expand) \\(Y'\\), \\(X'\\), and returning to the original space… Y transformations are denoted by y-prime, written \\(Y'\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\). \\[ Y' = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y' = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y' = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y' = \\log(Y)\\) lm(log(Y) ~ X) 0.25 \\(Y' = \\sqrt(\\sqrt(Y))\\) lm(sqrt(sqrt(Y)) ~ X) 0.5 \\(Y' = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y' = Y\\) lm(Y ~ X) 2 \\(Y' = Y^2\\) lm(Y^2 ~ X) Using “maximum-likelihood” estimation, the Box-Cox procedure can actually automatically detect the “optimal” value of \\(\\lambda\\) to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise. Scatterplot Recognition Box-Cox Suggestion An Example Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\). set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Ya <- 1/sqrt(Y) #1/Y^2 Lam = -2 Yb <- 1/Y #1/Y Lam = -1 Yc <- exp(.02*Y) #log(Y) Lam = 0 Yd <- Y^2 #sqrt(Y) Lam = 0.5 Ye <- Y #Y Lam = 1 Yf <- sqrt(Y) #Y^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Ya ~ X, main=expression(paste(\"Use \", lambda == -2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ya^-2 ~ X)) curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yb ~ X, main=expression(paste(\"Use \", lambda == -1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yb^-1 ~ X)) curve(1/(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yc ~ X, main=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(log(Yc) ~ X)) curve(exp(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yd ~ X, main=expression(paste(\"Use \", lambda == 0.5)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(sqrt(Yd) ~ X)) curve((b[1] + b[2]*x)^2, add=TRUE, col=\"green\", lwd=2) plot(Ye ~ X, main=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ye ~ X)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yf ~ X, main=expression(paste(\"Use \", lambda == 2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yf^2 ~ X)) curve(sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try. par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0)) boxCox(lm(Ya ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -2)), line=.5) boxCox(lm(Yb ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -1)), line=.5) boxCox(lm(Yc ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), line=.5) boxCox(lm(Yd ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0.5)), line=.5) boxCox(lm(Ye ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), line=.5) boxCox(lm(Yf ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 2)), line=.5) An Example Suppose we were running a simple linear regression on the cars dataset. This would be done with the code cars.lm <- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesn’t quite fit the data as well as we would hope. Instead, the data looks a little curved. cars.lm <-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") abline(cars.lm, col=\"gray\") Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful. library(car) boxCox(cars.lm) The output from the boxCox(...) function looks as follows. This plot tells use to use the \\(\\lambda = 0.5\\) transformation, so that \\(Y' = Y^0.5 = \\sqrt{Y}\\). (To see this yourself, click on the “Box-Cox Suggestion” tab above, as well as on the “Scatterplot Recognition” tab.) Now, a transformation regression is performed using sqrt(Y) in place of Y as follows: cars.lm.t <- lm(sqrt(dist) ~ speed, data=cars) summary(cars.lm.t)   Estimate Std. Error t value Pr(> (Intercept) 1.277 0.4844 2.636 0.01126 speed 0.3224 0.02978 10.83 1.773e-14 Then, \\[ \\widehat{Y}_i' = 1.277 + 0.3224 X_i \\] And replacing \\(\\hat{Y}_i' = \\sqrt{\\hat{Y}_i}\\) we have \\[ \\sqrt{\\widehat{Y}_i} = 1.277 + 0.3224 X_i \\] Solving for \\(\\hat{Y}_i\\) gives \\[ \\widehat{Y}_i = (1.277 + 0.3224 X_i)^2 \\] Which, using curve((1.277 + 0.3224*x)^2, add=TRUE) (see code for details) looks like this: plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") curve( (1.277 + 0.3224*x)^2, add=TRUE, col=\"firebrick\") X-Transformations X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them. The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated. The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation. set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Xa <- 1/sqrt(X) #1/X^2 Lam = -2 Xb <- 1/X #1/X Lam = -1 Xc <- exp(.02*X) #log(X) Lam = 0 Xd <- X^2 #sqrt(X) Lam = 0.5 Xe <- X #X Lam = 1 Xf <- sqrt(X) #X^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Y ~ Xa, main=expression(paste(\"Use \", X*minute == X^-2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xa^-2))) curve(b[1] + b[2]*x^-2, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xb, main=expression(paste(\"Use \", X*minute == X^-1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xb^-1))) curve(b[1] + b[2]*x^-1, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xc, main=expression(paste(\"Use \", X*minute == log(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ log(Xc))) curve(b[1] + b[2]*log(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xd, main=expression(paste(\"Use \", X*minute == sqrt(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ sqrt(Xd))) curve(b[1] + b[2]*sqrt(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xe, main=expression(paste(\"Use \", X*minute == X, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ Xe)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xf, main=expression(paste(\"Use \", X*minute == X^2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xf^2))) curve(b[1] + b[2]*x^2, add=TRUE, col=\"green\", lwd=2) Inference for the Model Parameters (Expand) t test formulas, sampling distributions, confidence intervals, and F tests… When fitting the regression model given by the equation \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] to a sample of data, we typically test hypotheses about the parameters \\(\\beta_0\\), \\(\\beta_1\\), or both. Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_0 - \\overbrace{0}^\\text{a number}}{s_{b_0}}\\] This is the formula for the test statistic. It measures how far the estimated y-intercept \\(b_0\\) is from the null hypothesis for \\(\\beta_0\\) in units of “standard errors of \\(b_0\\)”. Thus the division by \\(s_{b_0}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0, it could be any number. \\(H_0: \\beta_1 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_1\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_1 - \\overbrace{0}^\\text{a number}}{s_{b_1}}\\] This is the formula for the test statistic. It measures how far the estimated slope \\(b_1\\) is from the null hypothesis for \\(\\beta_1\\) in units of “standard errors of \\(b_1\\)”. Thus the division by \\(s_{b_1}\\). Though the hypothesized value of \\(\\beta_1\\) is typically 0, it could be any number. Left-tailed p-value = pt(-abs(tvalue), degrees of freedom). Double it to get the two-sided p-value. In R, these values correspond to the output summary of an lm as follows. (Show Example) Consider the cars data in R. Suppose we used the regression model given by \\[ \\underbrace{Y_i}_\\text{Feet to Stop} = \\beta_0 + \\beta_1 \\underbrace{X_{i}}_\\text{mph} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\quad \\sim N(0,\\sigma^2) \\] to model the feet a vehicle (from the 1920’s) takes to stop when traveling at a certain speed (in miles per hour, mph) prior to stopping. When the regression is performed and summarized in R, it is always testing the following two hypotheses: \\[ H_0: \\beta_0 = 0 \\quad\\quad H_0: \\beta_1 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\quad\\quad H_a: \\beta_1 \\neq 0 \\] To perform the test of these hypotheses for the regression stated above, we would run the following codes in R. cars.lm <- lm(dist ~ speed, data=cars) pander(summary(cars.lm)$coefficients) These would produce summary output like the following, but the following output has been labeled with the math notation corresponding to each value. Let’s emphasize what is happening in this summary output table. First, here is how the “t value” is calculated for the “(Intercept)” in the summary table above. \\[ t = \\frac{b_0-0}{s_{b_0}} = \\frac{-17.58 - 0}{6.758} = -2.601 \\] Second, here is a visual representation of how the P-value, the “Pr(>|t|)” as it is called in the summary table above, is calculated for this test statistic. (Click the graph to view an interactive applet showing this calculation.) Notice both ends of the t-distribution are being shaded to compute the P-value because the alternative hypothesis was \\(H_a: \\beta_0 \\neq 0\\). To compute the P-value in R, we use the “percentile function for the t-distribution” called pt( ). This function requires two things, the t-value and the degrees of freedom, in our case pt(-2.601, 48). Note the degrees of freedom (df) are 48 because the sample size is \\(n=50\\) and there are two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) in our regression model. Running this code in R gives: pt(-2.601, 48) = 0.00616 However, note that this value is only half of the actual P-value of 0.0123. To get the “two-sided” P-value (note that our alternative hypothesis used a \\(\\neq\\) symbol) we need to double this left-tailed P-value. 2*pt(-2.601, 48)) = 0.0123 Finally, note that the same procedure can be used to test hypotheses that use a value other than 0 in the null and alternative. For example, to test the hypotheses: \\[ H_0: \\beta_1 = 3 \\\\ H_a: \\beta_1 \\neq 3 \\] Use the t-formula \\[ t = \\frac{b_1 - 3}{s_{b_1}} = \\frac{\\overbrace{3.932}^{b_1} - \\overbrace{3}^{H_0}}{\\underbrace{0.4155}_{s_{b_1}}} = 2.243 \\] then the P-value is calculated in R by 2*pt(-abs(2.243), 48) = 0.0295495 To obtain confidence intervals in R use confint(mylm). Confidence Interval Formula Standard Error \\(\\beta_0\\) \\(b_0 \\pm\\) \\(t^*\\) This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\). Use qt(0.975, df) to get \\(t*\\) in R. \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_0\\). \\(s_{b_0}\\) The standard error of \\(b_0\\), denoted by \\(s_{b_0}\\) is provided in the regression summary output under the column header called “Std. Error” for the “(Intercept)” row of the output. It is calculated using the formula shown below. \\[s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\] This is called the “estimated variance of \\(b_0\\)”. Taking the square root of this number gives the “standard error of \\(b_0\\)”. \\(\\beta_1\\) \\(b_1 \\pm\\) \\(t^*\\) This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\) (sample size - number of parameters in the regression model). Use qt(0.975, df) to get \\(t*\\) in R \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_1\\). \\(s_{b_1}\\) The standard error of \\(b_1\\), denoted by \\(s_{b_1}\\) is provided in the regression summary output under the column header called “Std. It is calculated using the formula shown below. \\[s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\] This is called the “estimated variance of \\(b_1\\)”. Taking the square root of this number gives the “standard error of \\(b_1\\)”. To be more exact, the types of inference we are interested in are the following. Determine if there is evidence of a meaningful linear relationship in the data. If \\(\\beta_1 = 0\\), then there is no relation between \\(X\\) and \\(E\\{Y\\}\\). Hence we might be interested in testing the hypotheses \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) is some hypothesized number. To provide a confidence interval for the true value of \\(\\beta_1\\). Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the sampling distribution of the estimate \\(b_1\\) of the parameter \\(\\beta_1\\). And, while we are at it, we may as well come to understand the sampling distribution of the estimate \\(b_0\\) of the parameter \\(\\beta_0\\). Review sampling distributions from Math 221. Since \\(b_1\\) is an estimate, it will vary from sample to sample, even though the truth, \\(\\beta_1\\), remains fixed. (The same holds for \\(b_0\\) and \\(\\beta_0\\).) It turns out that the sampling distribution of \\(b_1\\) (where the \\(X\\) values remain fixed from study to study) is normal with mean and variance: \\[ \\mu_{b_1} = \\beta_1 \\] \\[ \\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} \\] ## Simulation to Show relationship between Standard Errors ##----------------------------------------------- ## Edit anything in this area... n <- 100 #sample size Xstart <- 30 #lower-bound for x-axis Xstop <- 100 #upper-bound for x-axis beta_0 <- 2 #choice of true y-intercept beta_1 <- 3.5 #choice of true slope sigma <- 13.8 #choice of st. deviation of error terms ## End of Editable area. ##----------------------------------------------- # Create X, which will be used in the next R-chunk. X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) ## After playing this chunk, play the next chunk as well. To see that this is true, consider the regression model with values specified for each parameter as follows. \\[ Y_i = \\overbrace{\\beta_0}^{2} + \\overbrace{\\beta_1}^{3.5} X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\overbrace{\\sigma^2}^{\\sigma=13.8}) \\] Using the equations above for \\(\\mu_{b_1}\\) and \\(\\sigma^2_{b_1}\\) we obtain that the mean of the sampling distribution of \\(b_1\\) will be \\(\\mu_{b_1} = \\beta_1 = 3.5\\) Further, we see that the variance of the sampling distribution of \\(b_1\\) will be \\(\\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} = \\frac{13.8^2}{4.25\\times 10^{4}}\\) Taking the square root of the variance, the standard deviation of the sampling distribution of \\(b_1\\) will be \\(\\sigma_{b_1} = 0.067\\). That’s very nice. But to really believe it, let’s run a simulation ourselves. The “Code” below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of \\(b_1\\) and \\(b_0\\) for that regression. After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of \\(b_0\\) is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of \\(b_1\\) is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of \\(\\beta_0\\) or \\(\\beta_1\\), respectively. Also, the “Std. Error” of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of \\(\\sigma_{b_0}\\) and \\(\\sigma_{b_1}\\), respectively. N <- 5000 #number of times to pull a random sample storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N) for (i in 1:N){ Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model mylm <- lm(Y ~ X) storage_b0[i] <- coef(mylm)[1] storage_b1[i] <- coef(mylm)[2] storage_rmse[i] <- summary(mylm)$sigma } layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3)) Ystart <- 0 #min(0,min(Y)) Ystop <- 500 #max(max(Y), 0) Yrange <- Ystop - Ystart plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), ylim=c(Ystart, Ystop), pch=16, col=\"gray\", main=\"Regression Lines from many Samples (gray lines) \\n Plus Residual Standard Deviation Lines (green lines)\") text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1) text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1) text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1) for (i in 1:N){ abline(storage_b0[i], storage_b1[i], col=\"darkgray\") } abline(beta_0, beta_1, col=\"green\", lwd=3) abline(beta_0+sigma, beta_1, col=\"green\", lwd=2) abline(beta_0-sigma, beta_1, col=\"green\", lwd=2) abline(beta_0+2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0-2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0+3*sigma, beta_1, col=\"green\", lwd=.5) abline(beta_0-3*sigma, beta_1, col=\"green\", lwd=.5) par(mai=c(1,.6,.5,.01)) addnorm <- function(m,s, col=\"firebrick\"){ curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2) lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col) lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col) lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col) lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col) lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col) lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col) lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col) legend(\"topleft\", legend=paste(\"Std. Error = \", round(s,3)), cex=0.7, bty=\"n\") } h0 <- hist(storage_b0, col=\"skyblue3\", main=\"Sampling Distribution\\n Y-intercept\", xlab=expression(paste(\"Estimates of \", beta[0], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m0 <- mean(storage_b0) s0 <- sd(storage_b0) addnorm(m0,s0, col=\"green\") h1 <- hist(storage_b1, col=\"skyblue3\", main=\"Sampling Distribution\\n Slope\", xlab=expression(paste(\"Estimates of \", beta[1], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m1 <- mean(storage_b1) s1 <- sd(storage_b1) addnorm(m1,s1, col=\"green\") t Tests Using the information above about the sampling distributions of \\(b_1\\) and \\(b_0\\), an immediate choice of statistical test to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) can be zero, or any other value, is a t test given by \\[ t = \\frac{b_1 - \\beta_{10}}{s_{b_1}} \\] where \\(s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\). (You may want to review the section “Estimating the Model Variance” of this file to know where MSE came from.) With quite a bit of work it has been shown that \\(t\\) is distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. The nearly identical test statistic for testing \\[ H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_0 \\neq \\beta_{00} \\] is given by \\[ t = \\frac{b_0 - \\beta_{00}}{s_{b_0}} \\] where \\(s^2_{b_0} = MSE\\left[\\frac{1}{n}+\\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\). This version of \\(t\\) has also been shown to be distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. Confidence Intervals Creating a confidence interval for either \\(\\beta_1\\) or \\(\\beta_0\\) follows immediately from these results using the formulas \\[ b_1 \\pm t^*_{n-2}\\cdot s_{b_1} \\] \\[ b_0 \\pm t^*_{n-2}\\cdot s_{b_0} \\] where \\(t^*_{n-2}\\) is the critical value from a t distribution with \\(n-2\\) degrees of freedom corresponding to the chosen confidence level. F tests Another way to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\quad\\quad \\text{or} \\quad\\quad H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\quad\\quad \\ \\ \\quad \\quad H_a: \\beta_0 \\neq \\beta_{00} \\] is with an \\(F\\) Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an \\(F\\) test is very general and can be used in many places that a t test cannot. In its most general form, the \\(F\\) test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that \\(H_0:\\beta_1=0\\) against the alternative that \\(H_a: \\beta_1\\neq 0\\), we are essentially comparing two models against each other. If \\(\\beta_1=0\\), then the corresponding model would be \\(E\\{Y_i\\} = \\beta_0\\). If \\(\\beta_1\\neq0\\), then the model remains \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the \\(F\\) Test, that the null model (reduced model) have fewer “free” parameters than the alternative model (full model). To demonstrate what we mean by “free” parameters, consider the following example. Say we wanted to test the hypothesis that \\(H_0:\\beta_1 = 2.5\\) against the alternative that \\(\\beta_1\\neq2.5\\). Then the null, or reduced model, would be \\(E\\{Y_i\\}=\\beta_0+2.5X_i\\). The alternative, or full model, would be \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). Thus, the null (reduced) model contains only one “free” parameter because \\(\\beta_1\\) has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two “free” parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model. Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing \\(H_0: \\beta_1=0\\) against \\(H_a:\\beta_1\\neq0\\) we have the partition \\[ \\underbrace{Y_i-\\bar{Y}}_{Total} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{Regression} + \\underbrace{Y_i-\\hat{Y}_i}_{Error} \\] The reason we use \\(\\bar{Y}\\) for the null model is that \\(\\bar{Y}\\) is the unbiased estimator of \\(\\beta_0\\) for the null model, \\(E\\{Y_i\\} = \\beta_0\\). Thus we would compute the following sums of squares: \\[ SSTO = \\sum(Y_i-\\bar{Y})^2 \\] \\[ SSR = \\sum(\\hat{Y}_i-\\bar{Y})^2 \\] \\[ SSE = \\sum(Y_i-\\hat{Y}_i)^2 \\] and note that \\(SSTO = SSR + SSE\\). Important to note is that \\(SSTO\\) uses the difference between the observations \\(Y_i\\) and the null (reduced) model. The \\(SSR\\) uses the diffences between the alternative (full) and null (reduced) model. The \\(SSE\\) uses the differences between the observations \\(Y_i\\) and the alternative (full) model. From these we could set up a General \\(F\\) table of the form   Sum Sq Df Mean Sq F Value Model Error \\(SSR\\) \\(df_R-df_F\\) \\(\\frac{SSR}{df_R-df_F}\\) \\(\\frac{SSR}{df_R-df_F}\\cdot\\frac{df_F}{SSE}\\) Residual Error \\(SSE\\) \\(df_F\\) \\(\\frac{SSE}{df_F}\\) Total Error \\(SSTO\\) \\(df_R\\) Prediction and Confidence Intervals for \\(\\hat{Y}_h\\) (Expand) predict(…, interval=“prediction”)… It is a common mistake to assume that averages (means) describe individuals. They do not. So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line. Interval R Code Math Equation When to Use Prediction predict(..., interval=\"prediction\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\text{Pred}\\ Y}\\) Predict an individual’s value. Confidence predict(..., interval=\"confidence\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\hat{Y}}\\) Estimate location of the mean y-value. predict(mylm, data.frame(XvarName = number), interval=...) For example, consider this graph. Then click here to read about the graph. Notice the three dots above 15 mph in the graph. Each of these dots show a car that was going 15 mph when it applied the brakes. However, stopping distances of the three individual cars differ with one at 20 feet, one at 26 feet and one at 54 feet. The regression line represents the average stopping distance of cars. In this case, cars going 15 mph are estimated to have an average stopping distance of about 40 feet, as shown by the line. But individual vehicles, all going the same speed of 15 mph, varied from stopping distances of 20 feet up to 54 feet! So, to predict that a car going 15 mph will take 41.4 feet to stop, doesn’t tell the whole story. Far more revealing is the complete statement, “Cars going 15 mph are predicted to take anywhere from 10.2 to 72.6 feet to stop, with an average stopping distance of 41.4 feet.” This is called the “prediction interval” and is shown in the graph in blue. It is obtained in R with the codes: cars.lm <- lm(dist ~ speed, data=cars) predict(cars.lm, data.frame(speed=15), interval=\"prediction\") cars.lm <- lm(dist ~ speed, data=cars) pander(predict(cars.lm, data.frame(speed=15), interval=\"prediction\")) fit lwr upr 41.41 10.17 72.64 plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=\"firebrick2\", cex=1.5) cars.lm <- lm(dist ~ speed, data=cars) abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3)) abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2)) abline(v=15, lty=2, col=\"firebrick\") preds <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12) lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8)) lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8)) Now, for the details behind prediction intervals and confidence intervals. Let’s begin by recalling some details (from the section “Inference for the Model Parameters”) about the standard error of the y-intercept, \\(b_0\\). Recall that the y-intercept is the average y-value for the given x-value of \\(x=0\\). Recall further that the formula for the standard error of \\(b_0\\) is given by the formula \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] If we wanted to be more exact with this formula, we would write it as \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{(0-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice the addition of \\((0 - \\bar{X})^2\\) instead of just \\(\\bar{X}^2\\) in the numerator of the right-most part of the equation? This more complete statement obviously would reduce to just \\(\\bar{X}^2\\), but that is only because \\(X=0\\) when we are working with the y-intercept, \\(b_0\\). We could be working with other values of \\(X\\) than just zero. Let’s take a quick detour and talk notation for a second. Typically, \\(X_i\\) and \\(Y_i\\) are used to denote the x-value and y-value of points that are contained in our data set. When we want to reference a point that wasn’t within our original data set, we use the notation \\(X_h\\) and \\(Y_h\\). (The letter h is close to i, but different from i, so why not. There is really no other reason to use h.) Thus, \\(Y_h\\) is the y-value for the \\(X_h\\) x-value, neither of which were included in our original regression of \\(X_i\\)’s and \\(Y_i\\)’s. Now, back to the previous discussion. If \\(X_h = 0\\), then \\(\\hat{Y}_h\\) is the y-intercept, so \\(\\hat{Y}_h = b_0\\) when \\(X_h=0\\). So, we could write, \\[ s^2_{\\hat{Y}_h} = MSE\\left[\\frac{1}{n} + \\frac{(X_h-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice how the \\(b_0\\) in \\(s_{b_0}\\) was replaced with \\(\\hat{Y}_h\\) to get \\(s_{\\hat{Y}_h}\\) and the 0 in \\((0 - \\bar{X})^2\\) was replaced with \\(X_h\\) to get \\((X_h - \\bar{X})^2\\)? Interesting. We now have a formula that would give us the standard error of \\(\\hat{Y}_h\\) for any \\(X_h\\) value, not just \\(X_h = 0\\), or the y-intercept, \\(b_0\\). That is fantastic. It would look like this if plotted. Notice how the gray region is showing the standard error for each \\(\\hat{Y}_h\\) value? (It is technically showing the confidence interval for \\(E\\{Y_h\\}\\) at every possible \\(X_h\\) value, but that is just \\(\\hat{Y}_h \\pm t^* \\cdot s_{\\hat{Y}_h}\\).) ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + theme_bw() ## `geom_smooth()` using formula = 'y ~ x' Confidence Interval for \\(\\hat{Y}_h\\) \\[ \\hat{Y}_h \\pm t^* s_{\\hat{Y}_h} \\quad \\text{where} \\ s_{\\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] The confidence interval is a wonderful tool for estimating \\(E\\{Y_h\\}\\), the “true” average y-value for a given x-value of \\(X_h\\). However, it is not valuable for predicting an individual dot, or \\(Y_h\\) value. Notice how few of the dots of the regression are actually contained within the confidence interval band in the plot? The confidence interval does not really predict where the dots will land, just where the average y-value is located for each x-value. Remember the 68-95-99.7 Rule of the normal distribution? If not, here is a link back to that concept in the Math 221 textbook. This rule states that roughly 95% of data, when normally distributed, will be between \\(z=-2\\) and \\(z=2\\) standard deviations from the mean. So, is going two “residual standard errors” to both sides of the regression line enough to capture 95% of the data? The answer is, not quite. The reason for this is because our knowledge of where the true mean lies is uncertain. (Notice the confidence interval band shown in the plot.) However, adding two standard errors to the edges of the confidence band would get us in the right place. In other words, there are two sources of variability at play here, (1) our uncertaintity in where the regression line is sitting, and (2) the natural variability of the data points around the line. Thus, the “prediction interval” requires accounting for both of these sources of variability to produce the following equation. Prediction Interval for \\(Y_h\\) \\[ \\hat{Y}_h \\pm t^* s_{Pred \\hat{Y}_h} \\quad \\text{where} \\ s_{Pred \\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + 1 + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] This formula provides a useful band for identifying a region where we are 95% confident that a new observation for \\(Y_h\\) will land, given the value of \\(X_h\\). It looks as follows. Notice the prediction interval is much wider than the confidence interval. This is because data varies far more than do means. Prediction is for where the individual data points will land, confidence is for where the mean will land. cars.lm <- lm(dist ~ speed, data=cars) predy <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + geom_segment(aes(x=15, xend=15, y=predy[2], yend=predy[3]), lwd=4, color=rgb(.5,.7,.5,.01)) + geom_point(aes(x=15, y=predy[1]), cex=2, color=\"skyblue\", pch=15) + theme_bw() ## Warning in geom_segment(aes(x = 15, xend = 15, y = predy[2], yend = predy[3]), : All aesthetics have length 1, but the data has 50 ## rows. ## ℹ Please consider using `annotate()` or provide ## this layer with data containing a single row. ## Warning in geom_point(aes(x = 15, y = predy[1]), cex = 2, color = \"skyblue\", : All aesthetics have length 1, but the data has 50 ## rows. ## ℹ Please consider using `annotate()` or provide ## this layer with data containing a single row. ## `geom_smooth()` using formula = 'y ~ x' Lowess (and Loess) Curves (Expand) A non-parametric approach to estimating \\(E\\{Y_i\\}\\)… Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value. Using Base R air2 <- airquality %>% na.omit(select(Temp, Ozone)) # Just quickly draw the lowess curve: plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") lines(lowess(air2$Ozone, air2$Temp), col=\"firebrick\") ## OR optionally, ## allow for predictions as well as the graph: # plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # lines(mylo$fit ~ Ozone, data=air2) Using ggplot2 air2 <- airquality %>% na.omit(select(Temp, Ozone)) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=\"darkgray\") + geom_smooth(se=F, method=\"loess\", method.args = list(degree=1)) + #Note, degree=2 by default. theme_bw() ## `geom_smooth()` using formula = 'y ~ x' ## OR optionally, ## allow for predictions as well as the graph: # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fit, x=Ozone)) Advantages Disadvantages Quick. Good at ignoring outliers. Good at capturing the general pattern in the data. Good for making predictions within the scope of the data. No mathematical model. Not interpretable. No p-values. No adjusted R-squared. How it Works The Lowess curve localizes the regression model to a “neighborhood” of points, and then joins these localized regressions together into a smooth line. It minimizes the effect of outliers, and let’s the data “speak for itself”. As a downside, it is not interpretable, and has no final way to write the model mathematically. All the same, it is a very powerful tool for identifying an appropriate model, or verifying the fit of a model, or making predictions when no reasonable model does an adequate job. Study this graphic and the explanations below to learn how it works. Recommendation: run the code in this “Code” chunk to the right in your Console, and flip through the resulting graphics. X <- cars$speed Y <- cars$dist X <- X[!is.na(X) & !is.na(Y)] Y <- Y[!is.na(X) & !is.na(Y)] f <- 1/2 n <- length(X) lfit <- rep(NA,n) for (xh in 1:n){ xdists <- X - X[xh] nn <- floor(n*f) r <- sort(abs(xdists))[nn] xdists.nbrhd <- which(abs(xdists) < r) w <- rep(0, length(xdists)) w[xdists.nbrhd] <- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w), col=rgb(.2,.2,.2,.3), cex=1.5, yaxt='n', xaxt='n', xlab=\"\", ylab=\"\") points(Y[xh] ~ X[xh], pch=16, col=\"orange\") lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2)) cat(\"\\n\\n\") readline(prompt=paste0(\"Center point is point #\", xh, \"... Press [enter] to continue...\")) MADnotThereYet <- TRUE count <- 0 while(MADnotThereYet){ readline(prompt=paste0(\"\\n Adjusting line to account for outliers in the y-direction... Press [enter] to continue...\")) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"wheat\", add=TRUE) MAD <- median(abs(lmc$res)) resm <- lmc$res/(6*MAD) resm[resm>1] <- 1 bisq <- (1-resm^2)^2 w <- w*bisq obs <- coef(lmc) lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) count <- count + 1 if ( (sum(abs(obs-lmc$coef))<.1) | (count > 3)) MADnotThereYet <- FALSE } curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"green\", add=TRUE) points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=\"green\") readline(prompt=paste0(\"\\n Use final line to get fitted value for this point... Press [enter] to continue to next point...\")) lfit[xh] <- predict(lmc, data.frame(X=X[xh])) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") if (xh == n){ readline(prompt=paste0(\"\\n Press [enter] to see actual Lowess curve...\")) lines(lowess(X,Y, f=f), col=\"firebrick\") legend(\"topleft\", bty=\"n\", legend=\"Actual lowess Curve using lowess(...)\", col=\"firebrick\", lty=1) } } Select a fraction of the data to use for the “neighborhood” of points (shown in blue in the graph above). The lowess function in R uses “f=2/3” and the loess function uses “span=0.75” for this value, which selects the nearest two-thirds or 75% of the data, respectively, depending on which function you use. For this example, we set the fraction of points at 50%. Both functions can be set to whatever you want. Pick any point in the regression, eventually selecting all points one at a time. The selected point becomes the “center” of a “neighborhood” of points surrounding it. In this example, the center point is in orange, and the neighboring points are in blue. Use the points within the neighborhood to fit a regression line. However, make the regression depend most on points closest to “center” and least on points furthest from “center.” This is called a weighted regression. Weights are decided according to what is called the tricubic weight function, so that the weight \\(w\\) given to point \\(j\\) of the neighborhood of points is defined by \\[ w_j = \\left(1- \\left( \\frac{|X_c - X_j|}{\\max_k |X_c - X_k|}\\right)^3\\right)^3 \\] where \\(X_c\\) is the x-value of the “center” dot and \\(X_j\\) is the x-value of any other dot in the neighborhood. The fitted-value of \\(\\hat{Y}_c\\) is obtained for the center point \\(X_c\\) of the current regression. This point is used as the Lowess (or Loess) curve’s value at that particular x-value. Well, almost. It’s a first guess at where this value will end up, but there’s a little more to the algorithm before we are done. Initial guesses for each of these fitted values are obtained for each point in the regression. Now each local regression for each neighborhood is re-run a few times in such a way the the effect of outliers is minimized. The final line for each neighborhood is obtained by the following steps. Compute all residuals for points in the neighborhood of the current regression, denoted by \\(r_i\\). Then compute the MAD, median absolute deviation, of the residuals \\(MAD = \\text{median} (|r_1|, |r_2|, \\ldots)\\). Divide all residuals by 6 times the MAD: \\(u_i = r_i/(6\\cdot MAD)\\) (If \\(r_i > 6\\cdot MAD\\) then set \\(u_i = 0\\).) Compute what are called bisquare weights using the formula: \\(b_i = (1 - u_i^2)^2\\) Perform a regression using the weights \\(w_i = w_i b_i\\) Repeat the above process with the new weights \\(w_i\\) until the weights stop changing very much. The final fitted values for each \\(X\\)-value in the regression are obtained from the final regression line for each neighborhood. These fitted values make up the Lowess (or loess) curve. Note that the default of the loess function in R is to use quadratic regressions in each neighborhood instead of linear regressions. This can be controlled with the loess option of “degree=2” (quadratic fits) or “degree = 1”. In the lowess function only a linear regression in each neighborhood is allowed. Examples: bodyweight, cars Multiple Linear Regression Multiple regression allows for more than one explanatory variable to be included in the modeling of the expected value of the quantitative response variable \\(Y_i\\). There are infinitely many possible multiple regression models to choose from. Here are a few “basic” models that work as building blocks to more complicated models. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model uses the same \\(X\\)-variable twice, once with a \\(\\beta_1 X_i\\) term and once with a \\(\\beta_2 X_i^2\\) term. The \\(X_i^2\\) term is called the “quadratic” term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and “steepness” of the Model: negative values face down, positive values face up; large values imply “steeper” parabolas and low values imply “flatter” parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)’s explanation. An Example Using the airquality data set, we run the following “quadratic” regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our “quadratic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. Temp Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These “Estimates” can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(Month^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of Month from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, )   Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will “open down” (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be “month zero.” Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the “cubic” term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the “base slope coefficient” and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following “cubic” regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our “cubic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. uptake Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^3) \\(X_{i}^3\\), where the function I(…) protects the cubing of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will “open up”. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The “two-lines” model is a simple way to compare two groups in statistics. It uses two main parts: A regular number (\\(X_{1i}\\)) that can be any value. A special number (\\(X_{2i}\\)) that’s either 0 or 1. This special number (also called a “dummy variable” or “indicator variable”) helps turn information about groups (like “Group A” or “Group B”) into numbers we can use in math. By using these two parts, we can create two separate lines on a graph: One line for Group A One line for Group B This helps us see how the two groups are different from each other. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the “base-line” of the model, the “Group 0” line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the “base-line” line. \\(\\beta_3\\) Called the “interaction” term. Controls the change in the slope for the second line in the model as compared to the slope of the “base-line” line. An Example Using the mtcars data set, we run the following “two-lines” regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our “two-lines” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. mpg Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept), \\(\\beta_1\\) is estimated by the qsec value of 1.439, \\(\\beta_2\\) is estimated by the am value of -14.51, and \\(\\beta_3\\) is estimated by the qsec:am value of 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called “3D” regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to “bend”. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the “Temp” direction is estimated to be 2.659 and the slope in the “Month” direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction term’s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the “slopes” in each direction to change, creating a “curved” surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called “HD”, or “High Dimensional”, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isn’t really even possible to “mentally connect” with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)’s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the “Wind” direction is estimated to be -3.334. The slope in the “Temp” direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the “Solar.R” direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here. R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the “Overview” section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the “column (c) bind” function and it joins together things as columns. Res =  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals.  panel=panel.smooth,  This places a lowess smoothing line on each scatterplot. col =  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(“color1”,“color2”, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for “linear model.” Y Y must be a “numeric” vector of the quantitative response variable.  ~  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats “numeric” variables as quantitative and “character” or “factor” variables as qualitative. R will automatcially recode qualitative variables to become “numeric” variables using a 0,1 encoding. See the Explanation tab for details.  + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details.  + …, * ... emphasizes that as many explanatory variables as are desired can be included in the model.  data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(…) function displays the results of an lm(…) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(…) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -4.3818 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -2.2696 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.1344 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   1.7058 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   5.8752 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero.   2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (26.6248479) by its standard error (2.1829432). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a “star”. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + …).   -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model.   0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” by its standard error. It gives the “number of standard errors” away from zero that the “estimate” has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + …).   5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\).   2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot “.” implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like.   0.0004029 This is the estimate of the coefficient of the interaction term.   0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data.  0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that “at least one is not.”  34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom.  on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero.  p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that “at least one” of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the “Overview” sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =… Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “prediction”) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “confidence”) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet. Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BIC… There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (“R-squared”). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The “simplest” but “best” model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it “an information criterion (AIC)” when he published the method and other people later began calling it the “Akaike Information Criterion.”) Model Selection (Expand) pairs plots, added variable plots, and pattern recognition… Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots A useful visualization tool for model selection is the “pairs plot.” This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice that… the y-axis of each plot is found by locating the variable name (like “mpg”) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like “disp”) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldn’t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Let’s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a model’s ability to generalize to new data… The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesn’t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, let’s remind ourselves why we use regression models in the first place. The main goal is to capture the “essence” of the data. In other words, the general pattern is what we are after. We want a model that tells us how “all such” data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the “complicated model” is overfitting the original data. It does not capture the “essence” of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-value… The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the model… The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read more…) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) Outlier Analysis (Expand) Cook’s Distances and Leverage Values… The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs. fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cook’s Distances The idea behind Cook’s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article “Detection of Influential Observation in Linear Regression” (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, let’s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2))   1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the “differences” in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cook’s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cook’s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cook’s Distances using the code cooks.distance(lmObject). Also, a graph of Cook’s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of “leverage” and is “pulling” the regression toward itself. A value near 0 implies the point is just “one of many” and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that “minimize” the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)’s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the “hat matrix” \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the “leverage values” and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a “lot of pull,” and values close to 0 represent “little pull.” In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with “lots of leverage” and a large “Cook’s Distance” are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regression… Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class. Examples: Civic Vs Corolla cadillacs",
    "sentences": ["Linear regression has a rich mathematical theory behind it.", "This is because it uses a mathematical function and a random error term to describe the regression relation between a response variable \\(Y\\) and an explanatory variable called \\(X\\).", "Expand each element below to learn more.", "Regression Cheat Sheet (Expand) Term Pronunciation Meaning Math R Code \\(Y_i\\) $Y_i$ “why-eye” The data \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)\\) $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)$ YourDataSet$YourYvariable \\(\\hat{Y}_i\\) $\\hat{Y}_i$ “why-hat-eye” The fitted line \\(\\hat{Y}_i = b_0 + b_1 X_i\\) $\\hat{Y}_i = b_0 + b_1 X_i$ lmObject$fitted.values \\(E\\{Y_i\\}\\) $E\\{Y_i\\}$ “expected value of why-eye” True mean y-value \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) $E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i$ <none> \\(\\beta_0\\) $\\beta_0$ “beta-zero” True y-intercept <none> <none> \\(\\beta_1\\) $\\beta_1$ “beta-one” True slope <none> <none> \\(b_0\\) $b_0$ “b-zero” Estimated y-intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) $b_0 = \\bar{Y} - b_1\\bar{X} b_0 <- mean(Y) - b_1*mean(X)$ \\(b_1\\) $b_1$ “b-one” Estimated slope \\(b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\) $b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})} {\\sum(X_i - \\bar{X})^2}$ b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) \\(\\epsilon_i\\) $\\epsilon_i$ “epsilon-eye” Distance of dot to true line \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) $\\epsilon_i = Y_i - E\\{Y_i\\}$ <none> \\(r_i\\) $r_i$ “r-eye” or “residual-eye” Distance of dot to estimated line \\(r_i = Y_i - \\hat{Y}_i\\) $r_i = Y_i - \\hat{Y}_i$ lmObject$residuals \\(\\sigma^2\\) $\\sigma^2$ “sigma-squared” Variance of the \\(\\epsilon_i\\) \\(Var\\{\\epsilon_i\\} = \\sigma^2\\)$Var\\{\\epsilon_i\\} = \\sigma^2$ <none> \\(MSE\\) $MSE$ “mean squared error” Estimate of \\(\\sigma^2\\) \\(MSE = \\frac{SSE}{n-p}\\)$MSE = \\frac{SSE}{n-p}$ sum( lmObject$res^2 ) / (n - p) \\(SSE\\) $SSE$ “sum of squared error” (residuals) Measure of dot’s total deviation from the line \\(SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\)$SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$ sum( lmObject$res^2 ) \\(SSR\\) $SSR$ “sum of squared regression error” Measure of line’s deviation from y-bar \\(SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\)$SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2$ sum( (lmObject$fit - mean(YourData$Y))^2 ) \\(SSTO\\) $SSTO$ “total sum of squares” Measure of total variation in Y \\(SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\)$SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2$ sum( (YourData$Y - mean(YourData$Y))^2 ) \\(R^2\\) $R^2$ “R-squared” Proportion of variation in Y explained by the regression \\(R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\)$R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}$ SSR/SSTO \\(r\\) $r$ “r” Correlation between X and Y.", "\\(r = \\sqrt{R^2}\\)$r = \\sqrt{R^2}$ sqrt(R^2) \\(\\hat{Y}_h\\) $\\hat{Y}_h$ “why-hat-aitch” Estimated mean y-value for some x-value called \\(X_h\\) \\(\\hat{Y}_h = b_0 + b_1 X_h\\)$\\hat{Y}_h = b_0 + b_1 X_h$ predict(lmObject, data.frame(XvarName=#)) \\(X_h\\) $X_h$ “ex-aitch” Some x-value, not necessarily one of the \\(X_i\\) values used in the regression \\(X_h =\\) some number$X_h = $ Xh = # Confidence Interval “confidence interval” Estimated bounds at a certain level of confidence for a parameter \\(b_0 \\pm t^* \\cdot s_{b_0}\\)b_0 \\pm t^* \\cdot s_{b_0} or \\(b_1 \\pm t^* \\cdot s_{b_1}\\)b_1 \\pm t^* \\cdot s_{b_1} confint(mylm, level = someConfidenceLevel) Parameter Estimate \\(\\beta_0\\) \\(b_0\\) \\(\\beta_1\\) \\(b_1\\) \\(\\epsilon_i\\) \\(r_i\\) \\(\\sigma^2\\) \\(MSE\\) \\(\\sigma\\) \\(\\sqrt{MSE}\\), the Residual standard error The Mathematical Model (Expand) \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)… There are three main elements to the mathematical model of regression.", "Each of these three elements is pictured below in the “Regression Relation Diagram.” Study both the three bullet points and their visual representations in the plot below for a clearer understanding.", "The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read more…) The true line is shown by the dotted line in the graph pictured below.", "This is typically unobservable.", "Think of it as “natural law” or “God’s law”.", "It is some true line that is unknown to us."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Explanation"
  },
  {
    "id": 119,
    "title": "📍 The Mathematical Model\r\n(Expand)",
    "url": "LinearRegression.html#themathematicalmodelexpand",
    "content": "The Mathematical Model\r\n(Expand) \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)… There are three main elements to the mathematical model of regression. Each of these three elements is pictured below in the “Regression Relation Diagram.” Study both the three bullet points and their visual representations in the plot below for a clearer understanding. The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read more…) The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as “natural law” or “God’s law”. It is some true line that is unknown to us. The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line. The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\). Note: \\(E\\{Y\\}\\) is pronounced “the expected value of y” because, well… the mean is the typical, average, or “expected” value. The dots, i.e., the regression relation plus an error term: \\(Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}} + \\underbrace{\\epsilon_i}_\\text{error term} \\quad \\text{where} \\ \\epsilon_i\\sim N(0,\\sigma^2)\\) (Read more…) This is shown by the dots in the graph below. This is the data. In regression, the assumption is that the y-value for individual \\(i\\), denoted by \\(Y_i\\), was “created” by adding an error term \\(\\epsilon_i\\) to each individual’s “expected” value \\(\\beta_0 + \\beta_1 X_i\\). Note the “order of creation” would require first knowing an indivual’s x-value, \\(X_i\\), then their expected value from the regression relation \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) and then adding their \\(\\epsilon_i\\) value to the result. The \\(\\epsilon_i\\) allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance \\(\\epsilon_i\\) from the line. Note: \\(Y_i\\) is pronounced “why-eye” because it is the y-value for individual \\(i\\). Sometimes also called “why-sub-eye” because \\(i\\) is in the subscript of \\(Y\\). The estimated line, i.e., the line we get from a sample of data. \\(\\underbrace{\\hat{Y}_i}_{\\substack{\\text{estimated mean} \\\\ \\text{y-value}}} = \\underbrace{b_0 + b_1 X_i}_\\text{estimated regression equation}\\) (Read more…) The estimated line is shown by the solid line in the graph below. \\(\\hat{Y}\\) is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation \\(E\\{Y\\}\\). So \\(\\hat{Y}\\) is interpreted as the estimated average (or mean) \\(Y\\)-value for any given \\(X\\)-value. Thus, \\(b_0\\) is the estimated y-intercept and \\(b_1\\) is the estimated slope. The b’s are sample statistics, like \\(\\bar{x}\\) and the \\(\\beta\\)’s are population parameters like \\(\\mu\\). The \\(b\\)’s estimate the \\(\\beta\\)’s. Note: \\(\\hat{Y}_i\\) is pronounced “why-hat-eye” and is known as the “estimated y-value” or “fitted y-value” because it is the y-value you get from \\(b_0 + b_1 X_i\\). It is always different from \\(Y_i\\) because dots are rarely if ever exactly on the estimated regression line. This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line). Something to ponder: The true line, when coupled with the error terms, “creates” the data. The estimated (or fitted) line uses the sampled data to try to “re-create” the true line. We could loosely call this the “order of creation” as shown by the following diagram. par(mfrow=c(1,3), mai=c(.2,.2,.4,.1)) plot(y ~ x, col=\"white\", main=\"A Law is Given\", yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, main=\"Data is Created\", xaxt='n', yaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, xaxt='n', yaxt='n', main=\"The Law is Estimated\") curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt='n', xaxt='n') curve(beta0 + beta1*x, add=TRUE, lty=2) A Law is Given Data is Created The Law is Estimated \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) \\(Y_i = E\\{Y_i\\} + \\epsilon_i\\) \\(\\hat{Y}_i = b_0 + b_1 X_i\\) The true line is the “law”. The \\(Y_i\\) are created by adding \\(\\epsilon_i\\) to \\(E\\{Y_i\\}\\) where \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\). The law is estimated with \\(\\hat{Y}_i\\) which is given with lm(...). Click open the “Code” buttom below to the right to find code that runs a simulation demonstrating this “order of creation”. ## Simulating Data from a Regression Model ## This R-chunk is meant to be played in your R Console. ## It allows you to explore how the various elements ## of the regression model combine together to \"create\" ## data and then use the data to \"re-create\" the line. set.seed(101) #Allows us to always get the same \"random\" sample #Change to a new number to get a new sample n <- 153 #set the sample size X_i <- runif(n, 0, 22) #Gives n random values from a uniform distribution between 15 to 45. beta0 <- 89 #Our choice for the y-intercept. beta1 <- -1.25 #Our choice for the slope. sigma <- 8 #Our choice for the std. deviation of the error terms. epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data View(fabData) #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it. fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData. summary(fab.lm) #Summarize your model. plot(y ~ x, data=fabData) #Plot the data. abline(fab.lm) #Add the estimated regression line to your plot. # Now for something you can't do in real life... but since we created the data... abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). legend(\"topleft\", legend=c(\"True Line\", \"Estimated Line\"), lty=c(2,1), bty=\"n\") #Add a legend to your plot specifying which line is which.",
    "sentences": ["\\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)…", "There are three main elements to the mathematical model of regression.", "Each of these three elements is pictured below in the “Regression Relation Diagram.” Study both the three bullet points and their visual representations in the plot below for a clearer understanding.", "The true line, i.e., the regression relation: \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) (Read more…) The true line is shown by the dotted line in the graph pictured below.", "This is typically unobservable.", "Think of it as “natural law” or “God’s law”.", "It is some true line that is unknown to us.", "The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line.", "The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\).", "Note: \\(E\\{Y\\}\\) is pronounced “the expected value of y” because, well… the mean is the typical, average, or “expected” value."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "The Mathematical Model\r\n(Expand)"
  },
  {
    "id": 120,
    "title": "📍 Interpreting the Model Parameters\r\n(Expand)",
    "url": "LinearRegression.html#interpretingthemodelparametersexpand",
    "content": "Interpreting the Model Parameters\r\n(Expand) \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted as… The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model. If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\). The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\). It is often misunderstood to be the “average change in y” or just “the change in y” but it is more correctly referred to as the “change in the average y”. To better see this, consider the three graphics shown below. par(mfrow=c(1,3)) hist(mtcars$mpg, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Number of Vehicles\", xlab=\"Gas Mileage (mpg)\", col=\"skyblue\") boxplot(mpg ~ cyl, data=mtcars, border=\"skyblue\", boxwex=0.5, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Number of Cylinders of Engine (cyl)\") plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=\"skyblue\", main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Quarter Mile Time (qsec)\") abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=\"darkgray\") mtext(side=3, text=\"Automatic Transmissions Only (am==0)\", cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=\"gray\") The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09. The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg. The scatterplot on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value.",
    "sentences": ["\\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted as…", "The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model.", "If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\).", "The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\).", "It is often misunderstood to be the “average change in y” or just “the change in y” but it is more correctly referred to as the “change in the average y”.", "To better see this, consider the three graphics shown below.", "par(mfrow=c(1,3)) hist(mtcars$mpg, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Number of Vehicles\", xlab=\"Gas Mileage (mpg)\", col=\"skyblue\") boxplot(mpg ~ cyl, data=mtcars, border=\"skyblue\", boxwex=0.5, main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Number of Cylinders of Engine (cyl)\") plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=\"skyblue\", main=\"Gas Mileage of mtcars Vehicles\", ylab=\"Gas Mileage (mpg)\", xlab=\"Quarter Mile Time (qsec)\") abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=\"darkgray\") mtext(side=3, text=\"Automatic Transmissions Only (am==0)\", cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=\"gray\") The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set.", "The average gas mileage is 20.09.", "The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively.", "If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Interpreting the Model Parameters\r\n(Expand)"
  },
  {
    "id": 121,
    "title": "📍 Residuals and Errors\r\n(Expand)",
    "url": "LinearRegression.html#residualsanderrorsexpand",
    "content": "Residuals and Errors\r\n(Expand) \\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true error… Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\). The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\). We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summary… Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\). \\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms. Keep in mind the idea that the errors \\(\\epsilon_i\\) “created” the data and that the residuals \\(r_i\\) are computed after using the data to “re-create” the line. Residuals have many uses in regression analysis. They allow us to diagnose the regression assumptions, See the “Assumptions” section below for more details. estimate the regression relation, See the “Estimating the Model Parameters” section below for more details. estimate the variance of the error terms, See the “Estimating the Model Variance” section below for more details. and assess the fit of the regression relation. See the “Assessing the Fit of a Regression” section below for more details.",
    "sentences": ["\\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true error…", "Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\).", "The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\).", "We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summary… Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\).", "\\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms.", "Keep in mind the idea that the errors \\(\\epsilon_i\\) “created” the data and that the residuals \\(r_i\\) are computed after using the data to “re-create” the line.", "Residuals have many uses in regression analysis.", "They allow us to diagnose the regression assumptions, See the “Assumptions” section below for more details.", "estimate the regression relation, See the “Estimating the Model Parameters” section below for more details.", "estimate the variance of the error terms, See the “Estimating the Model Variance” section below for more details."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Residuals and Errors\r\n(Expand)"
  },
  {
    "id": 122,
    "title": "📍 Assessing the Fit of a Regression\r\n(Expand)",
    "url": "LinearRegression.html#assessingthefitofaregressionexpand",
    "content": "Assessing the Fit of a Regression\r\n(Expand) \\(R^2\\), SSTO, SSR, and SSE… Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line. par(mfrow=c(1,3), mai=c(.1,.1,.5,.1)) set.seed(2) x <- runif(30,0,20) y1 <- 2 + 3.5*x + rnorm(30,0,2) y2 <- 2 + 3.5*x + rnorm(30,0,8) y3 <- 2 + 3.5*x + rnorm(30,0,27) plot(y1 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Excellent Fit\") abline(lm(y1 ~ x), col=\"gray\") plot(y2 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Good Fit\") abline(lm(y2 ~ x), col=\"gray\") plot(y3 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Poor Fit\") abline(lm(y3 ~ x), col=\"gray\") A common way to measure the fit of a regression is with correlation. While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\). (If you are a Math 325 student, just stick with correlation for now and skip on to the next section of this Explanation tab. If you are a Math 425 student, it is critical that you come to understand \\(R^2\\) deeply, so read on.) Before you can understand \\(R^2\\), you must understand three important “sums of squares”. (Read more about sums…) A sum is just a fancy word for adding things together. \\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand. So we use the symbol \\(\\Sigma\\) to denote the word “sum”. Further, we use a subscript \\(\\Sigma_{i=1}\\) to state what value the sum is beginning with, and a superscript \\(\\Sigma_{i=1}^6\\) to state the value we are ending at. This gives \\[ \\sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Test your knowledge, do you see why the answer is 6 to the sum below? \\[ \\sum_{i=1}^3 i = 6 \\] Computing sums in R is fairly easy. Type the following codes in your R Console. sum(1:6) #gives the answer of 21 sum(1:3) #gives the answer of 6 However, sums really become useful when used with a data set. Each row of a data set represents an “individual’s” data. We can reference each individual with a row number. In the data below, individual 3, denoted by \\(i=3\\), has a speed of 7 and a dist of 4. pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3) Individual speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 To compute the sum of the speed column, use sum(speed). If we divided this sum by 6, we would get the mean of speed mean(speed). In fact, the two most used statistics mean(...) and sd(...) both use sums. Take a moment to review the formulas for mean and standard deviation. It is strongly recommended that you study the Explanation tab for both as well. We’ll wait. See you back here shortly. … Welcome back. Suppose we let X = speed and Y = dist. Then \\(X_3 = 7\\) and \\(Y_3 = 4\\) because we are accessing row 3 of both the \\(X\\) (or speed) column and \\(Y\\) (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, sum(speed) would be written mathematically as \\(\\sum_{i=1}^6 X_i\\) and sum(dist) would be written as \\(\\sum_{i=1}^6 Y_i\\). Sum of Squared Errors Sum of Squares Regression Total Sum of Squares \\(\\text{SSE} = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\) \\(\\text{SSR} = \\sum_{i=1}^n \\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\) \\(\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\\) Measures how much the residuals deviate from the line. Measures how much the regression line deviates from the average y-value. Measures how much the y-values deviate from the average y-value. Equals SSTO - SSR Equals SSTO - SSE Equals SSE + SSR sum( (Y - mylm$fit)^2 ) sum( (mylm$fit - mean(Y))^2 ) sum( (Y - mean(Y))^2 ) It is important to remember that SSE and SSR split up SSTO, so that \\[ \\text{SSTO} = \\text{SSE} + \\text{SSR} \\] This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works. The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called \\(R^2\\) (“r-squared”). R-Squared (\\(R^2\\)) \\[ \\underbrace{R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}}_\\text{Interpretation: Proportion of variation in Y explained by the regression.} \\] The smallest \\(R^2\\) can be is zero, and the largest it can be is 1. This is because \\(SSR\\) must be between 0 and SSTO, inclusive.",
    "sentences": ["\\(R^2\\), SSTO, SSR, and SSE…", "Not all regressions are created equally as the three plots below show.", "Sometimes the dots are a clustered very tightly to the line.", "At other times, the dots spread out fairly dramatically from the line.", "par(mfrow=c(1,3), mai=c(.1,.1,.5,.1)) set.seed(2) x <- runif(30,0,20) y1 <- 2 + 3.5*x + rnorm(30,0,2) y2 <- 2 + 3.5*x + rnorm(30,0,8) y3 <- 2 + 3.5*x + rnorm(30,0,27) plot(y1 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Excellent Fit\") abline(lm(y1 ~ x), col=\"gray\") plot(y2 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Good Fit\") abline(lm(y2 ~ x), col=\"gray\") plot(y3 ~ x, pch=16, col=\"darkgray\", xlim=c(-1,21), yaxt='n', xaxt='n', ylim=c(-10,100), main=\"Poor Fit\") abline(lm(y3 ~ x), col=\"gray\") A common way to measure the fit of a regression is with correlation.", "While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\).", "(If you are a Math 325 student, just stick with correlation for now and skip on to the next section of this Explanation tab.", "If you are a Math 425 student, it is critical that you come to understand \\(R^2\\) deeply, so read on.) Before you can understand \\(R^2\\), you must understand three important “sums of squares”.", "(Read more about sums…) A sum is just a fancy word for adding things together.", "\\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Assessing the Fit of a Regression\r\n(Expand)"
  },
  {
    "id": 123,
    "title": "📍 Residual Plots & Regression Assumptions\r\n(Expand)",
    "url": "LinearRegression.html#residualplotsregressionassumptionsexpand",
    "content": "Residual Plots & Regression Assumptions\r\n(Expand) Residuals vs. fitted-values, Q-Q Plot of the residuals, and residuals vs. order plots… There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate. Each assumption is labeled in the regression equation below. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions. (Read more…) As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not. Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3 The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(\\hat{Y}_i\\). The residuals are the \\(r_i\\). This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable. | Show Examples | (Read more…) The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption. The linear relation is assumed to be satisfied if there are no apparent trends in the plot. The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values. The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance. set.seed(2) X <- rnorm(30,15,3) notLin <- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8)) notLin.lm <- lm(Y~X, data=notLin) set.seed(15) Lin <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) Lin.lm <- lm(Y~X, data=Lin) par(mfrow=c(3,3), mai=c(.25,.25,.25,.25), mgp=c(1,.75,0)) plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Not Linear\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(notLin.lm$fitted.values,notLin.lm$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Good: Linear, Constant Variance\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) set.seed(6) notCon <- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5)) notCon.lm <- lm(Y~X, data=notCon) LinO <- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3)) LinO[1] <- LinO[1]^2 LinO.lm <- lm(Y~X, data=LinO) plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Unconstant Variance\", cex.main=0.95, yaxt='n', xaxt='n', col=\"firebrick\") polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) # plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"Outliers\", cex.main=0.95) # abline(h=0) tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(Girth ~ Volume, data=trees[-31,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(Height ~ Volume, data=trees) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) abline(h=0) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") mycurve <- lowess(tmp$fitted.values,tmp$residuals, f=.4) mycurveOrder <- order(mycurve$x) mycurve$x <- mycurve$x[mycurveOrder] mycurve$y <- mycurve$y[mycurveOrder] polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) abline(h=0) tmp <- lm(weight ~ repwt, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") abline(h=0) tmp <- lm(weight ~ repht, data=Davis[-12,]) plot(tmp$residuals ~ tmp$fitted.values, pch=20, xlab=\"Fitted Values\", ylab=\"Residuals\", main=\"\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) abline(h=0) Q-Q Plot of the Residuals: Checks Assumption #2 The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption. | Show Examples | (Read more…) There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot. Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) set.seed(123) tmp <- rnorm(100) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"skyblue\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Normal\", col=\"skyblue\") tmp <- Davis$weight qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Right-skewed\", breaks=15, col=\"firebrick\") par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- rbeta(100, 5,1) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Left-skewed\", breaks=seq(min(tmp),max(tmp), length.out=13), col=\"firebrick\") tmp <- rbeta(100,2,2) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n', col=\"firebrick\") qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Heavy-tailed\", col=\"firebrick\") Residuals versus Order Plot: Checks Assumption #5 When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated. | Show Examples | (Read more…) Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- lm(mpg ~ disp, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Good: No Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"skyblue\") tmp <- lm(height ~ age, data=Loblolly) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: General Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ qsec, data=mtcars) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Questionable: Interesting Patterns\", cex.main=0.95, xaxt='n', yaxt='n', col=\"orangered\") tmp <- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),]) plot(tmp$residuals, pch=20, xlab=\"Order\", ylab=\"Residuals\", main=\"Bad: Obvious Trend\", cex.main=0.95, xaxt='n', yaxt='n', col=\"firebrick\") Problems from Failed Assumptions There are various problems that can arise when certain of the regression assupmtions are not satisfied. Lack of Linearity When the linearity assumption is violated, pretty much everything we obtain from the regression summary is no longer meaningful. The y-intercept estimate can be drastically off from its actual true value. Important model information is lost by trying to use a simple slope term \\(\\beta_1\\) to describe the model with respect to \\(X\\). The residual standard error will be much higher than it otherwise would be because of curvature patterns in the data that the line cannot capture. Thus, R-squared will be lower than it otherwise should be. P-values can become non-significant, when in fact there is a strong pattern in the data, but that pattern just cannot be captured by a simple line. *Normality of the errors is often put into question as well when a simplified line is used to try to capture a more complicated curved model. The plot below demonstrate these difficulties. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 7.5 #True slope beta_2 <- -0.25 #True bend X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors Y_i <- beta_0 + beta_1*X_i + beta_2*X_i^2 + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Non-Linear Relation\") abline(mylm, col=\"gray\") #Add fitted line to plot curve(beta_0 + beta_1*x + beta_2*x^2, col=\"gray\", lty=2, add=TRUE) #Add True line to plot #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (True value:\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (True value:\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (True value:\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Non-normal Error Terms When the normality assumption for the error terms is violated, not all is lost. In fact, the estimate of the slope and intercept are still often fairly meaningful. However, it is unwise to put too much trust in the residual standard error as an estimate of the standard deviation \\(\\sigma\\). This is because the standard deviation in skewed distributions does not carry the same meaning it has in normal distributions. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rchisq(n, 1)*3 - 1 #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Unconstant Variance When variance of the error term changes across the regression, the regression approximates the “average variance” of the errors because the regression is still assuming the variance is constant across the regression. The estimates of the slope and intercept are still typically quite good, and can be used for interpretation. The residual standard error however should not be considered to be meaningful as it will be too large on one end of the regression and too small on the other end. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma + X_i) #normally distributed errors #with increasing variance Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Variance Varies (Non-Constant)\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 13.82 Slope 3.5 3.768 Sigma 2.5 13.02 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", round(mean(sigma + X_i), 2), \", mean)\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Normality Violated As silly as it sounds, if the only problem with the regression is the lack of normality of the error terms, it isn’t all that big of a problem. Depending on how non-normal the residuals appear, there could be some skewing to the residual standard error, but otherwise, the slope and intercept are still interpretable and meaningful. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- runif(n, -sqrt(12*sigma^2)/2, sqrt(12*sigma^2)/2) #non-normally distributed errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Normality Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 14.7 Slope 3.5 3.449 Sigma 2.5 2.317 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Independence Assumption Violated While the slope and intercept are often still meaningful when the independence assumption is violated, the residual standard error is unnecessarily large in this case. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, 2.5) + (1:n -n/2)*.5 #normal, but correlated errors Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"Independence Assumption Violated\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 16.47 Slope 3.5 3.296 Sigma 2.5 4.819 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3) Outliers Present While outliers do not violate any of the regression assumptions, they do pose substantial difficulties for the least squares regression estimates of the slope and intercept. # Create Data from a True Model n <- 30 #sample size beta_0 <- 14.2 #True y-intercept beta_1 <- 3.5 #True slope X_i <- runif(n, 0, 20) #Sample of X-values sigma <- 2.5 #True standard deviation epsilon_i <- rnorm(n, 0, sigma) #normally distributed errors epsilon_i[3] <- ifelse(X_i[3] < 10, runif(1,25,35), -runif(1,25,35)) #create outlier Y_i <- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm <- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=\"darkgray\", xlim=c(0,20), ylim=c(0,100), main=\"An Outlier Present\") abline(mylm, col=\"gray\") #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=\"gray\", lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))   True Estimated Y-Intercept 14.2 13.18 Slope 3.5 3.447 Sigma 2.5 6.389 #Add summary to plot legend(\"topleft\", legend=c(paste(\"Y-Intercept:\", round(mylm$coef[[1]], 3), \" (\", beta_0, \")\"), paste(\"Slope:\", round(mylm$coef[[2]], 3), \" (\", beta_1, \")\"), paste(\"Sigma:\", round(summary(mylm)$sigma, 3), \" (\", sigma, \")\")), bty='n') #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=\"Residuals\") mtext(\"Residuals vs Order\", side=3)",
    "sentences": ["Residuals vs. fitted-values, Q-Q Plot of the residuals, and residuals vs. order plots…", "There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate.", "Each assumption is labeled in the regression equation below.", "The regression relation between \\(Y\\) and \\(X\\) is linear.", "The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\).", "The variance of the error terms is constant over all \\(X\\) values.", "The \\(X\\) values can be considered fixed and measured without error.", "The error terms are independent.", "Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions.", "(Read more…) As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Residual Plots & Regression Assumptions\r\n(Expand)"
  },
  {
    "id": 124,
    "title": "📍 Estimating the Model Parameters\r\n(Expand)",
    "url": "LinearRegression.html#estimatingthemodelparametersexpand",
    "content": "Estimating the Model Parameters\r\n(Expand) How to get \\(b_0\\) and \\(b_1\\): least squares & maximum likelihood… There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical. The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas. Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 <- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\). When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\). Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i. \\label{exp} \\end{equation}\\] Least Squares To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\). \\[ Q = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2 \\] Then we use the function Q as if it were a function of \\(\\beta_0\\) and \\(\\beta_1\\). Ironically, the values of \\(Y\\) and \\(X\\) are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we don’t know are the values of \\(\\beta_0\\) and \\(\\beta_1\\). This least squares applet is a good way to explore how various choices of the slope and intercept yield different values of the “sum of squared residuals”. But it turns out that there is one “best” choice of the slope and intercept that yields a “smallest” value of the “sum of squared residuals.” This best choice can actually be found using calculus by taking the partial derivatives of \\(Q\\) with respect to both \\(\\beta_0\\) and \\(\\beta_1\\). \\[ \\frac{\\partial Q}{\\partial \\beta_0} = -2\\sum (Y_i - \\beta_0 - \\beta_1X_i) \\] \\[ \\frac{\\partial Q}{\\partial \\beta_1} = -2\\sum X_i(Y_i-\\beta_0-\\beta_1X_i) \\] Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize \\(Q\\) for a given set of data. After all the calculations are completed we find the values of the parameter estimators \\(b_0\\) and \\(b_1\\) (of \\(\\beta_0\\) and \\(\\beta_1\\), respectively) are as stated previously. Maximum Likelihood The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of \\(\\beta_0\\) and \\(\\beta_1\\) which minime the least squares \\(Q\\) function, we choose the values of \\(\\beta_0\\) and \\(\\beta_1\\) which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the \\(Y_i\\) for all observations \\(i=1,\\ldots,n\\). We can do this rather simply by using the assumption that the errors, \\(\\epsilon_i\\) are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if \\(f(Y_i)\\) denotes the probability density function for \\(Y_i\\), then the joint probability density for all \\(Y_i\\), \\(f(Y_1,\\ldots,Y_n)\\) is given by \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) \\] Since each \\(Y_i\\) is assumed to be normally distributed with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\) (see model (\\(\\ref{model}\\))) we have that \\[ f(Y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{Y_i-\\beta_0-\\beta_1X_i}{\\sigma}\\right)^2\\right]} \\] which provides the joint probability as \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] The likelihood function \\(L\\) is then given by consider the \\(Y_i\\) and \\(X_i\\) fixed and the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) as the variables in the function. \\[ L(\\beta_0,\\beta_1,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] Instead of taking partial derivatives of \\(L\\) directly (with respect to all parameters) we take the partial derivatives of the \\(\\log\\) of \\(L\\), which is easier to work with. In a similar, but more difficult calculation, to that of minimizing \\(Q\\), we obtain the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) which maximize the log of \\(L\\), and which therefore maximize \\(L\\). (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate \\(\\hat{\\sigma}^2\\) of \\(\\sigma^2\\). \\[ \\hat{\\sigma}^2 = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n} \\]",
    "sentences": ["How to get \\(b_0\\) and \\(b_1\\): least squares & maximum likelihood…", "There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model.", "The oldest and most tradiational approach is using the idea of least squares.", "A more general approach uses the idea of maximum likelihood (see below).", "Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical.", "The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas.", "Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 <- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\).", "When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\).", "Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i.", "\\label{exp} \\end{equation}\\] Least Squares To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\)."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Estimating the Model Parameters\r\n(Expand)"
  },
  {
    "id": 125,
    "title": "📍 Estimating the Model Variance\r\n(Expand)",
    "url": "LinearRegression.html#estimatingthemodelvarianceexpand",
    "content": "Estimating the Model Variance\r\n(Expand) Estimating \\(\\sigma^2\\) with MSE… As shown previously in the “Estimating Model Parameters” section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation. Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\). This estimate turns out to be a biased estimator. This means that it is consistently wrong in its estimates of \\(\\sigma^2\\). If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong. This is bad. Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\). Without going into all the details, to “fix” the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently. To find the correct degrees of freedom, we have to notice that the \\(\\hat{Y}_i\\) in the numerator of \\(\\widehat{\\sigma}^2\\) is defined by \\[\\begin{equation} \\widehat{Y}_i = b_0 + b_1X_i \\label{hatY} \\end{equation}\\] From this equation, we notice that two means, \\(\\bar{X}\\) and \\(\\bar{Y}\\), were estimated from the data in order to obtain \\(\\hat{Y}_i\\). (See the formulas for \\(b_0\\) and \\(b_1\\) above, and note how they use both \\(\\bar{X}\\) and \\(\\bar{Y}\\) in their calculation.) Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for \\(\\hat{\\sigma}^2\\) should be \\(n-2\\) instead of \\(n\\). Some incredibly long calculations will show that the “fixed” estimator \\[\\begin{equation} s^2 = MSE = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n-2} \\quad \\text{(Unbiased Estimator of $\\sigma^2$)} \\end{equation}\\] is an unbiased estimator of \\(\\sigma^2\\). Here \\(MSE\\) stands for mean squared error, which is the most obvious name for a formula that squares the errors \\(Y_i-\\hat{Y}_i\\) then adds them up and divides by their degrees of freedom. Similarly, we call the numerator \\(\\sum(Y_i-\\hat{Y}_i)^2\\) the sum of the squared errors, denoted by \\(SSE\\). It is also important to note that the errors are often denoted by \\(r_i = Y_i-\\hat{Y}_i\\), the residuals. Putting this all together we get the following equivalent statements for \\(MSE\\). \\[\\begin{equation} s^2 = MSE = \\frac{SSE}{n-2} = \\frac{\\sum(Y_i-\\widehat{Y}_i)^2}{n-2} = \\frac{\\sum r_i^2}{n-2} \\end{equation}\\] As a final note, even though the expected value \\(E\\{MSE\\} = \\sigma^2\\), which shows \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\), it unfortunately isn’t true that \\(\\sqrt{MSE}\\) is an unbiased estimator of \\(\\sigma\\). This presents a few problems later on, but these are minimal enough that we can overlook the issue and move forward.",
    "sentences": ["Estimating \\(\\sigma^2\\) with MSE…", "As shown previously in the “Estimating Model Parameters” section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation.", "Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators.", "Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\).", "This estimate turns out to be a biased estimator.", "This means that it is consistently wrong in its estimates of \\(\\sigma^2\\).", "If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong.", "This is bad.", "Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\).", "Without going into all the details, to “fix” the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Estimating the Model Variance\r\n(Expand)"
  },
  {
    "id": 126,
    "title": "📍 Transformations\r\n(Expand)",
    "url": "LinearRegression.html#transformationsexpand",
    "content": "Transformations\r\n(Expand) \\(Y'\\), \\(X'\\), and returning to the original space… Y transformations are denoted by y-prime, written \\(Y'\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\). \\[ Y' = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y' = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y' = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y' = \\log(Y)\\) lm(log(Y) ~ X) 0.25 \\(Y' = \\sqrt(\\sqrt(Y))\\) lm(sqrt(sqrt(Y)) ~ X) 0.5 \\(Y' = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y' = Y\\) lm(Y ~ X) 2 \\(Y' = Y^2\\) lm(Y^2 ~ X) Using “maximum-likelihood” estimation, the Box-Cox procedure can actually automatically detect the “optimal” value of \\(\\lambda\\) to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise. Scatterplot Recognition Box-Cox Suggestion An Example Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\). set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Ya <- 1/sqrt(Y) #1/Y^2 Lam = -2 Yb <- 1/Y #1/Y Lam = -1 Yc <- exp(.02*Y) #log(Y) Lam = 0 Yd <- Y^2 #sqrt(Y) Lam = 0.5 Ye <- Y #Y Lam = 1 Yf <- sqrt(Y) #Y^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Ya ~ X, main=expression(paste(\"Use \", lambda == -2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ya^-2 ~ X)) curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yb ~ X, main=expression(paste(\"Use \", lambda == -1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yb^-1 ~ X)) curve(1/(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yc ~ X, main=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(log(Yc) ~ X)) curve(exp(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yd ~ X, main=expression(paste(\"Use \", lambda == 0.5)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(sqrt(Yd) ~ X)) curve((b[1] + b[2]*x)^2, add=TRUE, col=\"green\", lwd=2) plot(Ye ~ X, main=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ye ~ X)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yf ~ X, main=expression(paste(\"Use \", lambda == 2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yf^2 ~ X)) curve(sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try. par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0)) boxCox(lm(Ya ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -2)), line=.5) boxCox(lm(Yb ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -1)), line=.5) boxCox(lm(Yc ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), line=.5) boxCox(lm(Yd ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0.5)), line=.5) boxCox(lm(Ye ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), line=.5) boxCox(lm(Yf ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 2)), line=.5) An Example Suppose we were running a simple linear regression on the cars dataset. This would be done with the code cars.lm <- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesn’t quite fit the data as well as we would hope. Instead, the data looks a little curved. cars.lm <-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") abline(cars.lm, col=\"gray\") Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful. library(car) boxCox(cars.lm) The output from the boxCox(...) function looks as follows. This plot tells use to use the \\(\\lambda = 0.5\\) transformation, so that \\(Y' = Y^0.5 = \\sqrt{Y}\\). (To see this yourself, click on the “Box-Cox Suggestion” tab above, as well as on the “Scatterplot Recognition” tab.) Now, a transformation regression is performed using sqrt(Y) in place of Y as follows: cars.lm.t <- lm(sqrt(dist) ~ speed, data=cars) summary(cars.lm.t)   Estimate Std. Error t value Pr(> (Intercept) 1.277 0.4844 2.636 0.01126 speed 0.3224 0.02978 10.83 1.773e-14 Then, \\[ \\widehat{Y}_i' = 1.277 + 0.3224 X_i \\] And replacing \\(\\hat{Y}_i' = \\sqrt{\\hat{Y}_i}\\) we have \\[ \\sqrt{\\widehat{Y}_i} = 1.277 + 0.3224 X_i \\] Solving for \\(\\hat{Y}_i\\) gives \\[ \\widehat{Y}_i = (1.277 + 0.3224 X_i)^2 \\] Which, using curve((1.277 + 0.3224*x)^2, add=TRUE) (see code for details) looks like this: plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") curve( (1.277 + 0.3224*x)^2, add=TRUE, col=\"firebrick\") X-Transformations X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them. The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated. The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation. set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Xa <- 1/sqrt(X) #1/X^2 Lam = -2 Xb <- 1/X #1/X Lam = -1 Xc <- exp(.02*X) #log(X) Lam = 0 Xd <- X^2 #sqrt(X) Lam = 0.5 Xe <- X #X Lam = 1 Xf <- sqrt(X) #X^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Y ~ Xa, main=expression(paste(\"Use \", X*minute == X^-2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xa^-2))) curve(b[1] + b[2]*x^-2, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xb, main=expression(paste(\"Use \", X*minute == X^-1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xb^-1))) curve(b[1] + b[2]*x^-1, add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xc, main=expression(paste(\"Use \", X*minute == log(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ log(Xc))) curve(b[1] + b[2]*log(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xd, main=expression(paste(\"Use \", X*minute == sqrt(X))), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ sqrt(Xd))) curve(b[1] + b[2]*sqrt(x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xe, main=expression(paste(\"Use \", X*minute == X, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ Xe)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Y ~ Xf, main=expression(paste(\"Use \", X*minute == X^2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Y ~ I(Xf^2))) curve(b[1] + b[2]*x^2, add=TRUE, col=\"green\", lwd=2)",
    "sentences": ["\\(Y'\\), \\(X'\\), and returning to the original space…", "Y transformations are denoted by y-prime, written \\(Y'\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\).", "\\[ Y' = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y' = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y' = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y' = \\log(Y)\\) lm(log(Y) ~ X) 0.25 \\(Y' = \\sqrt(\\sqrt(Y))\\) lm(sqrt(sqrt(Y)) ~ X) 0.5 \\(Y' = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y' = Y\\) lm(Y ~ X) 2 \\(Y' = Y^2\\) lm(Y^2 ~ X) Using “maximum-likelihood” estimation, the Box-Cox procedure can actually automatically detect the “optimal” value of \\(\\lambda\\) to consider for a Y-transformation.", "Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise.", "Scatterplot Recognition Box-Cox Suggestion An Example Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\).", "set.seed(15) N <- 300 X <- runif(N, 5, 50) Y <- 25 + 3.5*X + rnorm(N, 0, 20) Ya <- 1/sqrt(Y) #1/Y^2 Lam = -2 Yb <- 1/Y #1/Y Lam = -1 Yc <- exp(.02*Y) #log(Y) Lam = 0 Yd <- Y^2 #sqrt(Y) Lam = 0.5 Ye <- Y #Y Lam = 1 Yf <- sqrt(Y) #Y^2 Lam = 2 par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0)) plot(Ya ~ X, main=expression(paste(\"Use \", lambda == -2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ya^-2 ~ X)) curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yb ~ X, main=expression(paste(\"Use \", lambda == -1)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yb^-1 ~ X)) curve(1/(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yc ~ X, main=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(log(Yc) ~ X)) curve(exp(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yd ~ X, main=expression(paste(\"Use \", lambda == 0.5)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(sqrt(Yd) ~ X)) curve((b[1] + b[2]*x)^2, add=TRUE, col=\"green\", lwd=2) plot(Ye ~ X, main=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Ye ~ X)) curve((b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) plot(Yf ~ X, main=expression(paste(\"Use \", lambda == 2)), ylab=\"Y in Original Units\", pch=16, col=\"gray45\", cex=0.9, yaxt='n', xaxt='n', xlab=\"X in Original Units\") b <- coef(lm(Yf^2 ~ X)) curve(sqrt(b[1] + b[2]*x), add=TRUE, col=\"green\", lwd=2) Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try.", "par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0)) boxCox(lm(Ya ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -2)), line=.5) boxCox(lm(Yb ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == -1)), line=.5) boxCox(lm(Yc ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0, \" i.e., log(...)\")), line=.5) boxCox(lm(Yd ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 0.5)), line=.5) boxCox(lm(Ye ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 1, \" (No Transformation)\")), line=.5) boxCox(lm(Yf ~ X)) mtext(side=3, text=expression(paste(\"Use \", lambda == 2)), line=.5) An Example Suppose we were running a simple linear regression on the cars dataset.", "This would be done with the code cars.lm <- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesn’t quite fit the data as well as we would hope.", "Instead, the data looks a little curved.", "cars.lm <-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") abline(cars.lm, col=\"gray\") Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Transformations\r\n(Expand)"
  },
  {
    "id": 127,
    "title": "📍 Inference for the Model Parameters\r\n(Expand)",
    "url": "LinearRegression.html#inferenceforthemodelparametersexpand",
    "content": "Inference for the Model Parameters\r\n(Expand) t test formulas, sampling distributions, confidence intervals, and F tests… When fitting the regression model given by the equation \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] to a sample of data, we typically test hypotheses about the parameters \\(\\beta_0\\), \\(\\beta_1\\), or both. Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_0 - \\overbrace{0}^\\text{a number}}{s_{b_0}}\\] This is the formula for the test statistic. It measures how far the estimated y-intercept \\(b_0\\) is from the null hypothesis for \\(\\beta_0\\) in units of “standard errors of \\(b_0\\)”. Thus the division by \\(s_{b_0}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0, it could be any number. \\(H_0: \\beta_1 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_1\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_1 - \\overbrace{0}^\\text{a number}}{s_{b_1}}\\] This is the formula for the test statistic. It measures how far the estimated slope \\(b_1\\) is from the null hypothesis for \\(\\beta_1\\) in units of “standard errors of \\(b_1\\)”. Thus the division by \\(s_{b_1}\\). Though the hypothesized value of \\(\\beta_1\\) is typically 0, it could be any number. Left-tailed p-value = pt(-abs(tvalue), degrees of freedom). Double it to get the two-sided p-value. In R, these values correspond to the output summary of an lm as follows. (Show Example) Consider the cars data in R. Suppose we used the regression model given by \\[ \\underbrace{Y_i}_\\text{Feet to Stop} = \\beta_0 + \\beta_1 \\underbrace{X_{i}}_\\text{mph} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\quad \\sim N(0,\\sigma^2) \\] to model the feet a vehicle (from the 1920’s) takes to stop when traveling at a certain speed (in miles per hour, mph) prior to stopping. When the regression is performed and summarized in R, it is always testing the following two hypotheses: \\[ H_0: \\beta_0 = 0 \\quad\\quad H_0: \\beta_1 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\quad\\quad H_a: \\beta_1 \\neq 0 \\] To perform the test of these hypotheses for the regression stated above, we would run the following codes in R. cars.lm <- lm(dist ~ speed, data=cars) pander(summary(cars.lm)$coefficients) These would produce summary output like the following, but the following output has been labeled with the math notation corresponding to each value. Let’s emphasize what is happening in this summary output table. First, here is how the “t value” is calculated for the “(Intercept)” in the summary table above. \\[ t = \\frac{b_0-0}{s_{b_0}} = \\frac{-17.58 - 0}{6.758} = -2.601 \\] Second, here is a visual representation of how the P-value, the “Pr(>|t|)” as it is called in the summary table above, is calculated for this test statistic. (Click the graph to view an interactive applet showing this calculation.) Notice both ends of the t-distribution are being shaded to compute the P-value because the alternative hypothesis was \\(H_a: \\beta_0 \\neq 0\\). To compute the P-value in R, we use the “percentile function for the t-distribution” called pt( ). This function requires two things, the t-value and the degrees of freedom, in our case pt(-2.601, 48). Note the degrees of freedom (df) are 48 because the sample size is \\(n=50\\) and there are two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) in our regression model. Running this code in R gives: pt(-2.601, 48) = 0.00616 However, note that this value is only half of the actual P-value of 0.0123. To get the “two-sided” P-value (note that our alternative hypothesis used a \\(\\neq\\) symbol) we need to double this left-tailed P-value. 2*pt(-2.601, 48)) = 0.0123 Finally, note that the same procedure can be used to test hypotheses that use a value other than 0 in the null and alternative. For example, to test the hypotheses: \\[ H_0: \\beta_1 = 3 \\\\ H_a: \\beta_1 \\neq 3 \\] Use the t-formula \\[ t = \\frac{b_1 - 3}{s_{b_1}} = \\frac{\\overbrace{3.932}^{b_1} - \\overbrace{3}^{H_0}}{\\underbrace{0.4155}_{s_{b_1}}} = 2.243 \\] then the P-value is calculated in R by 2*pt(-abs(2.243), 48) = 0.0295495 To obtain confidence intervals in R use confint(mylm). Confidence Interval Formula Standard Error \\(\\beta_0\\) \\(b_0 \\pm\\) \\(t^*\\) This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\). Use qt(0.975, df) to get \\(t*\\) in R. \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_0\\). \\(s_{b_0}\\) The standard error of \\(b_0\\), denoted by \\(s_{b_0}\\) is provided in the regression summary output under the column header called “Std. Error” for the “(Intercept)” row of the output. It is calculated using the formula shown below. \\[s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\] This is called the “estimated variance of \\(b_0\\)”. Taking the square root of this number gives the “standard error of \\(b_0\\)”. \\(\\beta_1\\) \\(b_1 \\pm\\) \\(t^*\\) This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\) (sample size - number of parameters in the regression model). Use qt(0.975, df) to get \\(t*\\) in R \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_1\\). \\(s_{b_1}\\) The standard error of \\(b_1\\), denoted by \\(s_{b_1}\\) is provided in the regression summary output under the column header called “Std. It is calculated using the formula shown below. \\[s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\] This is called the “estimated variance of \\(b_1\\)”. Taking the square root of this number gives the “standard error of \\(b_1\\)”. To be more exact, the types of inference we are interested in are the following. Determine if there is evidence of a meaningful linear relationship in the data. If \\(\\beta_1 = 0\\), then there is no relation between \\(X\\) and \\(E\\{Y\\}\\). Hence we might be interested in testing the hypotheses \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) is some hypothesized number. To provide a confidence interval for the true value of \\(\\beta_1\\). Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the sampling distribution of the estimate \\(b_1\\) of the parameter \\(\\beta_1\\). And, while we are at it, we may as well come to understand the sampling distribution of the estimate \\(b_0\\) of the parameter \\(\\beta_0\\). Review sampling distributions from Math 221. Since \\(b_1\\) is an estimate, it will vary from sample to sample, even though the truth, \\(\\beta_1\\), remains fixed. (The same holds for \\(b_0\\) and \\(\\beta_0\\).) It turns out that the sampling distribution of \\(b_1\\) (where the \\(X\\) values remain fixed from study to study) is normal with mean and variance: \\[ \\mu_{b_1} = \\beta_1 \\] \\[ \\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} \\] ## Simulation to Show relationship between Standard Errors ##----------------------------------------------- ## Edit anything in this area... n <- 100 #sample size Xstart <- 30 #lower-bound for x-axis Xstop <- 100 #upper-bound for x-axis beta_0 <- 2 #choice of true y-intercept beta_1 <- 3.5 #choice of true slope sigma <- 13.8 #choice of st. deviation of error terms ## End of Editable area. ##----------------------------------------------- # Create X, which will be used in the next R-chunk. X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) ## After playing this chunk, play the next chunk as well. To see that this is true, consider the regression model with values specified for each parameter as follows. \\[ Y_i = \\overbrace{\\beta_0}^{2} + \\overbrace{\\beta_1}^{3.5} X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\overbrace{\\sigma^2}^{\\sigma=13.8}) \\] Using the equations above for \\(\\mu_{b_1}\\) and \\(\\sigma^2_{b_1}\\) we obtain that the mean of the sampling distribution of \\(b_1\\) will be \\(\\mu_{b_1} = \\beta_1 = 3.5\\) Further, we see that the variance of the sampling distribution of \\(b_1\\) will be \\(\\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} = \\frac{13.8^2}{4.25\\times 10^{4}}\\) Taking the square root of the variance, the standard deviation of the sampling distribution of \\(b_1\\) will be \\(\\sigma_{b_1} = 0.067\\). That’s very nice. But to really believe it, let’s run a simulation ourselves. The “Code” below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of \\(b_1\\) and \\(b_0\\) for that regression. After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of \\(b_0\\) is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of \\(b_1\\) is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of \\(\\beta_0\\) or \\(\\beta_1\\), respectively. Also, the “Std. Error” of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of \\(\\sigma_{b_0}\\) and \\(\\sigma_{b_1}\\), respectively. N <- 5000 #number of times to pull a random sample storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N) for (i in 1:N){ Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model mylm <- lm(Y ~ X) storage_b0[i] <- coef(mylm)[1] storage_b1[i] <- coef(mylm)[2] storage_rmse[i] <- summary(mylm)$sigma } layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3)) Ystart <- 0 #min(0,min(Y)) Ystop <- 500 #max(max(Y), 0) Yrange <- Ystop - Ystart plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), ylim=c(Ystart, Ystop), pch=16, col=\"gray\", main=\"Regression Lines from many Samples (gray lines) \\n Plus Residual Standard Deviation Lines (green lines)\") text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1) text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1) text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1) for (i in 1:N){ abline(storage_b0[i], storage_b1[i], col=\"darkgray\") } abline(beta_0, beta_1, col=\"green\", lwd=3) abline(beta_0+sigma, beta_1, col=\"green\", lwd=2) abline(beta_0-sigma, beta_1, col=\"green\", lwd=2) abline(beta_0+2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0-2*sigma, beta_1, col=\"green\", lwd=1) abline(beta_0+3*sigma, beta_1, col=\"green\", lwd=.5) abline(beta_0-3*sigma, beta_1, col=\"green\", lwd=.5) par(mai=c(1,.6,.5,.01)) addnorm <- function(m,s, col=\"firebrick\"){ curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2) lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col) lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col) lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col) lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col) lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col) lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col) lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col) legend(\"topleft\", legend=paste(\"Std. Error = \", round(s,3)), cex=0.7, bty=\"n\") } h0 <- hist(storage_b0, col=\"skyblue3\", main=\"Sampling Distribution\\n Y-intercept\", xlab=expression(paste(\"Estimates of \", beta[0], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m0 <- mean(storage_b0) s0 <- sd(storage_b0) addnorm(m0,s0, col=\"green\") h1 <- hist(storage_b1, col=\"skyblue3\", main=\"Sampling Distribution\\n Slope\", xlab=expression(paste(\"Estimates of \", beta[1], \" from each Sample\")), freq=FALSE, yaxt='n', ylab=\"\") m1 <- mean(storage_b1) s1 <- sd(storage_b1) addnorm(m1,s1, col=\"green\") t Tests Using the information above about the sampling distributions of \\(b_1\\) and \\(b_0\\), an immediate choice of statistical test to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) can be zero, or any other value, is a t test given by \\[ t = \\frac{b_1 - \\beta_{10}}{s_{b_1}} \\] where \\(s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\). (You may want to review the section “Estimating the Model Variance” of this file to know where MSE came from.) With quite a bit of work it has been shown that \\(t\\) is distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. The nearly identical test statistic for testing \\[ H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_0 \\neq \\beta_{00} \\] is given by \\[ t = \\frac{b_0 - \\beta_{00}}{s_{b_0}} \\] where \\(s^2_{b_0} = MSE\\left[\\frac{1}{n}+\\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\). This version of \\(t\\) has also been shown to be distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. Confidence Intervals Creating a confidence interval for either \\(\\beta_1\\) or \\(\\beta_0\\) follows immediately from these results using the formulas \\[ b_1 \\pm t^*_{n-2}\\cdot s_{b_1} \\] \\[ b_0 \\pm t^*_{n-2}\\cdot s_{b_0} \\] where \\(t^*_{n-2}\\) is the critical value from a t distribution with \\(n-2\\) degrees of freedom corresponding to the chosen confidence level. F tests Another way to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\quad\\quad \\text{or} \\quad\\quad H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\quad\\quad \\ \\ \\quad \\quad H_a: \\beta_0 \\neq \\beta_{00} \\] is with an \\(F\\) Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an \\(F\\) test is very general and can be used in many places that a t test cannot. In its most general form, the \\(F\\) test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that \\(H_0:\\beta_1=0\\) against the alternative that \\(H_a: \\beta_1\\neq 0\\), we are essentially comparing two models against each other. If \\(\\beta_1=0\\), then the corresponding model would be \\(E\\{Y_i\\} = \\beta_0\\). If \\(\\beta_1\\neq0\\), then the model remains \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the \\(F\\) Test, that the null model (reduced model) have fewer “free” parameters than the alternative model (full model). To demonstrate what we mean by “free” parameters, consider the following example. Say we wanted to test the hypothesis that \\(H_0:\\beta_1 = 2.5\\) against the alternative that \\(\\beta_1\\neq2.5\\). Then the null, or reduced model, would be \\(E\\{Y_i\\}=\\beta_0+2.5X_i\\). The alternative, or full model, would be \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). Thus, the null (reduced) model contains only one “free” parameter because \\(\\beta_1\\) has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two “free” parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model. Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing \\(H_0: \\beta_1=0\\) against \\(H_a:\\beta_1\\neq0\\) we have the partition \\[ \\underbrace{Y_i-\\bar{Y}}_{Total} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{Regression} + \\underbrace{Y_i-\\hat{Y}_i}_{Error} \\] The reason we use \\(\\bar{Y}\\) for the null model is that \\(\\bar{Y}\\) is the unbiased estimator of \\(\\beta_0\\) for the null model, \\(E\\{Y_i\\} = \\beta_0\\). Thus we would compute the following sums of squares: \\[ SSTO = \\sum(Y_i-\\bar{Y})^2 \\] \\[ SSR = \\sum(\\hat{Y}_i-\\bar{Y})^2 \\] \\[ SSE = \\sum(Y_i-\\hat{Y}_i)^2 \\] and note that \\(SSTO = SSR + SSE\\). Important to note is that \\(SSTO\\) uses the difference between the observations \\(Y_i\\) and the null (reduced) model. The \\(SSR\\) uses the diffences between the alternative (full) and null (reduced) model. The \\(SSE\\) uses the differences between the observations \\(Y_i\\) and the alternative (full) model. From these we could set up a General \\(F\\) table of the form   Sum Sq Df Mean Sq F Value Model Error \\(SSR\\) \\(df_R-df_F\\) \\(\\frac{SSR}{df_R-df_F}\\) \\(\\frac{SSR}{df_R-df_F}\\cdot\\frac{df_F}{SSE}\\) Residual Error \\(SSE\\) \\(df_F\\) \\(\\frac{SSE}{df_F}\\) Total Error \\(SSTO\\) \\(df_R\\)",
    "sentences": ["t test formulas, sampling distributions, confidence intervals, and F tests…", "When fitting the regression model given by the equation \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] to a sample of data, we typically test hypotheses about the parameters \\(\\beta_0\\), \\(\\beta_1\\), or both.", "Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0.", "However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0.", "To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown.", "\\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(>\\) or \\(<\\) instead of \\(\\neq\\) for the alternative hypothesis.", "By default, the p-value from summary(mylm) in R uses \\(\\neq\\).", "\\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0.", "However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0.", "To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Inference for the Model Parameters\r\n(Expand)"
  },
  {
    "id": 128,
    "title": "📍 Prediction and Confidence Intervals for \\(\\hat{Y}_h\\)\r\n(Expand)",
    "url": "LinearRegression.html#predictionandconfidenceintervalsfor\\\\hatyh\\expand",
    "content": "Prediction and Confidence Intervals for \\(\\hat{Y}_h\\)\r\n(Expand) predict(…, interval=“prediction”)… It is a common mistake to assume that averages (means) describe individuals. They do not. So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line. Interval R Code Math Equation When to Use Prediction predict(..., interval=\"prediction\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\text{Pred}\\ Y}\\) Predict an individual’s value. Confidence predict(..., interval=\"confidence\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\hat{Y}}\\) Estimate location of the mean y-value. predict(mylm, data.frame(XvarName = number), interval=...) For example, consider this graph. Then click here to read about the graph. Notice the three dots above 15 mph in the graph. Each of these dots show a car that was going 15 mph when it applied the brakes. However, stopping distances of the three individual cars differ with one at 20 feet, one at 26 feet and one at 54 feet. The regression line represents the average stopping distance of cars. In this case, cars going 15 mph are estimated to have an average stopping distance of about 40 feet, as shown by the line. But individual vehicles, all going the same speed of 15 mph, varied from stopping distances of 20 feet up to 54 feet! So, to predict that a car going 15 mph will take 41.4 feet to stop, doesn’t tell the whole story. Far more revealing is the complete statement, “Cars going 15 mph are predicted to take anywhere from 10.2 to 72.6 feet to stop, with an average stopping distance of 41.4 feet.” This is called the “prediction interval” and is shown in the graph in blue. It is obtained in R with the codes: cars.lm <- lm(dist ~ speed, data=cars) predict(cars.lm, data.frame(speed=15), interval=\"prediction\") cars.lm <- lm(dist ~ speed, data=cars) pander(predict(cars.lm, data.frame(speed=15), interval=\"prediction\")) fit lwr upr 41.41 10.17 72.64 plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=\"firebrick2\", cex=1.5) cars.lm <- lm(dist ~ speed, data=cars) abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3)) abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2)) abline(v=15, lty=2, col=\"firebrick\") preds <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12) lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8)) lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8)) Now, for the details behind prediction intervals and confidence intervals. Let’s begin by recalling some details (from the section “Inference for the Model Parameters”) about the standard error of the y-intercept, \\(b_0\\). Recall that the y-intercept is the average y-value for the given x-value of \\(x=0\\). Recall further that the formula for the standard error of \\(b_0\\) is given by the formula \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] If we wanted to be more exact with this formula, we would write it as \\[ s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{(0-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice the addition of \\((0 - \\bar{X})^2\\) instead of just \\(\\bar{X}^2\\) in the numerator of the right-most part of the equation? This more complete statement obviously would reduce to just \\(\\bar{X}^2\\), but that is only because \\(X=0\\) when we are working with the y-intercept, \\(b_0\\). We could be working with other values of \\(X\\) than just zero. Let’s take a quick detour and talk notation for a second. Typically, \\(X_i\\) and \\(Y_i\\) are used to denote the x-value and y-value of points that are contained in our data set. When we want to reference a point that wasn’t within our original data set, we use the notation \\(X_h\\) and \\(Y_h\\). (The letter h is close to i, but different from i, so why not. There is really no other reason to use h.) Thus, \\(Y_h\\) is the y-value for the \\(X_h\\) x-value, neither of which were included in our original regression of \\(X_i\\)’s and \\(Y_i\\)’s. Now, back to the previous discussion. If \\(X_h = 0\\), then \\(\\hat{Y}_h\\) is the y-intercept, so \\(\\hat{Y}_h = b_0\\) when \\(X_h=0\\). So, we could write, \\[ s^2_{\\hat{Y}_h} = MSE\\left[\\frac{1}{n} + \\frac{(X_h-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2}\\right] \\] Did you notice how the \\(b_0\\) in \\(s_{b_0}\\) was replaced with \\(\\hat{Y}_h\\) to get \\(s_{\\hat{Y}_h}\\) and the 0 in \\((0 - \\bar{X})^2\\) was replaced with \\(X_h\\) to get \\((X_h - \\bar{X})^2\\)? Interesting. We now have a formula that would give us the standard error of \\(\\hat{Y}_h\\) for any \\(X_h\\) value, not just \\(X_h = 0\\), or the y-intercept, \\(b_0\\). That is fantastic. It would look like this if plotted. Notice how the gray region is showing the standard error for each \\(\\hat{Y}_h\\) value? (It is technically showing the confidence interval for \\(E\\{Y_h\\}\\) at every possible \\(X_h\\) value, but that is just \\(\\hat{Y}_h \\pm t^* \\cdot s_{\\hat{Y}_h}\\).) ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + theme_bw() ## `geom_smooth()` using formula = 'y ~ x' Confidence Interval for \\(\\hat{Y}_h\\) \\[ \\hat{Y}_h \\pm t^* s_{\\hat{Y}_h} \\quad \\text{where} \\ s_{\\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] The confidence interval is a wonderful tool for estimating \\(E\\{Y_h\\}\\), the “true” average y-value for a given x-value of \\(X_h\\). However, it is not valuable for predicting an individual dot, or \\(Y_h\\) value. Notice how few of the dots of the regression are actually contained within the confidence interval band in the plot? The confidence interval does not really predict where the dots will land, just where the average y-value is located for each x-value. Remember the 68-95-99.7 Rule of the normal distribution? If not, here is a link back to that concept in the Math 221 textbook. This rule states that roughly 95% of data, when normally distributed, will be between \\(z=-2\\) and \\(z=2\\) standard deviations from the mean. So, is going two “residual standard errors” to both sides of the regression line enough to capture 95% of the data? The answer is, not quite. The reason for this is because our knowledge of where the true mean lies is uncertain. (Notice the confidence interval band shown in the plot.) However, adding two standard errors to the edges of the confidence band would get us in the right place. In other words, there are two sources of variability at play here, (1) our uncertaintity in where the regression line is sitting, and (2) the natural variability of the data points around the line. Thus, the “prediction interval” requires accounting for both of these sources of variability to produce the following equation. Prediction Interval for \\(Y_h\\) \\[ \\hat{Y}_h \\pm t^* s_{Pred \\hat{Y}_h} \\quad \\text{where} \\ s_{Pred \\hat{Y}_h}^2 = MSE\\left[\\frac{1}{n} + 1 + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\right] \\] This formula provides a useful band for identifying a region where we are 95% confident that a new observation for \\(Y_h\\) will land, given the value of \\(X_h\\). It looks as follows. Notice the prediction interval is much wider than the confidence interval. This is because data varies far more than do means. Prediction is for where the individual data points will land, confidence is for where the mean will land. cars.lm <- lm(dist ~ speed, data=cars) predy <- predict(cars.lm, data.frame(speed=15), interval=\"prediction\") ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth(method=\"lm\", color=\"skyblue\") + geom_segment(aes(x=15, xend=15, y=predy[2], yend=predy[3]), lwd=4, color=rgb(.5,.7,.5,.01)) + geom_point(aes(x=15, y=predy[1]), cex=2, color=\"skyblue\", pch=15) + theme_bw() ## Warning in geom_segment(aes(x = 15, xend = 15, y = predy[2], yend = predy[3]), : All aesthetics have length 1, but the data has 50 ## rows. ## ℹ Please consider using `annotate()` or provide ## this layer with data containing a single row. ## Warning in geom_point(aes(x = 15, y = predy[1]), cex = 2, color = \"skyblue\", : All aesthetics have length 1, but the data has 50 ## rows. ## ℹ Please consider using `annotate()` or provide ## this layer with data containing a single row. ## `geom_smooth()` using formula = 'y ~ x'",
    "sentences": ["predict(…, interval=“prediction”)…", "It is a common mistake to assume that averages (means) describe individuals.", "They do not.", "So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line.", "Interval R Code Math Equation When to Use Prediction predict(..., interval=\"prediction\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\text{Pred}\\ Y}\\) Predict an individual’s value.", "Confidence predict(..., interval=\"confidence\") \\(\\hat{Y}_i \\pm t^* \\cdot s_{\\hat{Y}}\\) Estimate location of the mean y-value.", "predict(mylm, data.frame(XvarName = number), interval=...) For example, consider this graph.", "Then click here to read about the graph.", "Notice the three dots above 15 mph in the graph.", "Each of these dots show a car that was going 15 mph when it applied the brakes."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Prediction and Confidence Intervals for \\(\\hat{Y}_h\\)\r\n(Expand)"
  },
  {
    "id": 129,
    "title": "📍 Lowess (and Loess) Curves\r\n(Expand)",
    "url": "LinearRegression.html#lowessandloesscurvesexpand",
    "content": "Lowess (and Loess) Curves\r\n(Expand) A non-parametric approach to estimating \\(E\\{Y_i\\}\\)… Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value. Using Base R air2 <- airquality %>% na.omit(select(Temp, Ozone)) # Just quickly draw the lowess curve: plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") lines(lowess(air2$Ozone, air2$Temp), col=\"firebrick\") ## OR optionally, ## allow for predictions as well as the graph: # plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # lines(mylo$fit ~ Ozone, data=air2) Using ggplot2 air2 <- airquality %>% na.omit(select(Temp, Ozone)) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=\"darkgray\") + geom_smooth(se=F, method=\"loess\", method.args = list(degree=1)) + #Note, degree=2 by default. theme_bw() ## `geom_smooth()` using formula = 'y ~ x' ## OR optionally, ## allow for predictions as well as the graph: # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fit, x=Ozone)) Advantages Disadvantages Quick. Good at ignoring outliers. Good at capturing the general pattern in the data. Good for making predictions within the scope of the data. No mathematical model. Not interpretable. No p-values. No adjusted R-squared. How it Works The Lowess curve localizes the regression model to a “neighborhood” of points, and then joins these localized regressions together into a smooth line. It minimizes the effect of outliers, and let’s the data “speak for itself”. As a downside, it is not interpretable, and has no final way to write the model mathematically. All the same, it is a very powerful tool for identifying an appropriate model, or verifying the fit of a model, or making predictions when no reasonable model does an adequate job. Study this graphic and the explanations below to learn how it works. Recommendation: run the code in this “Code” chunk to the right in your Console, and flip through the resulting graphics. X <- cars$speed Y <- cars$dist X <- X[!is.na(X) & !is.na(Y)] Y <- Y[!is.na(X) & !is.na(Y)] f <- 1/2 n <- length(X) lfit <- rep(NA,n) for (xh in 1:n){ xdists <- X - X[xh] nn <- floor(n*f) r <- sort(abs(xdists))[nn] xdists.nbrhd <- which(abs(xdists) < r) w <- rep(0, length(xdists)) w[xdists.nbrhd] <- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w), col=rgb(.2,.2,.2,.3), cex=1.5, yaxt='n', xaxt='n', xlab=\"\", ylab=\"\") points(Y[xh] ~ X[xh], pch=16, col=\"orange\") lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2)) cat(\"\\n\\n\") readline(prompt=paste0(\"Center point is point #\", xh, \"... Press [enter] to continue...\")) MADnotThereYet <- TRUE count <- 0 while(MADnotThereYet){ readline(prompt=paste0(\"\\n Adjusting line to account for outliers in the y-direction... Press [enter] to continue...\")) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"wheat\", add=TRUE) MAD <- median(abs(lmc$res)) resm <- lmc$res/(6*MAD) resm[resm>1] <- 1 bisq <- (1-resm^2)^2 w <- w*bisq obs <- coef(lmc) lmc <- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"orange\", add=TRUE) count <- count + 1 if ( (sum(abs(obs-lmc$coef))<.1) | (count > 3)) MADnotThereYet <- FALSE } curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=\"green\", add=TRUE) points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=\"green\") readline(prompt=paste0(\"\\n Use final line to get fitted value for this point... Press [enter] to continue to next point...\")) lfit[xh] <- predict(lmc, data.frame(X=X[xh])) lines(lfit[1:xh] ~ X[1:xh], col=\"gray\") if (xh == n){ readline(prompt=paste0(\"\\n Press [enter] to see actual Lowess curve...\")) lines(lowess(X,Y, f=f), col=\"firebrick\") legend(\"topleft\", bty=\"n\", legend=\"Actual lowess Curve using lowess(...)\", col=\"firebrick\", lty=1) } } Select a fraction of the data to use for the “neighborhood” of points (shown in blue in the graph above). The lowess function in R uses “f=2/3” and the loess function uses “span=0.75” for this value, which selects the nearest two-thirds or 75% of the data, respectively, depending on which function you use. For this example, we set the fraction of points at 50%. Both functions can be set to whatever you want. Pick any point in the regression, eventually selecting all points one at a time. The selected point becomes the “center” of a “neighborhood” of points surrounding it. In this example, the center point is in orange, and the neighboring points are in blue. Use the points within the neighborhood to fit a regression line. However, make the regression depend most on points closest to “center” and least on points furthest from “center.” This is called a weighted regression. Weights are decided according to what is called the tricubic weight function, so that the weight \\(w\\) given to point \\(j\\) of the neighborhood of points is defined by \\[ w_j = \\left(1- \\left( \\frac{|X_c - X_j|}{\\max_k |X_c - X_k|}\\right)^3\\right)^3 \\] where \\(X_c\\) is the x-value of the “center” dot and \\(X_j\\) is the x-value of any other dot in the neighborhood. The fitted-value of \\(\\hat{Y}_c\\) is obtained for the center point \\(X_c\\) of the current regression. This point is used as the Lowess (or Loess) curve’s value at that particular x-value. Well, almost. It’s a first guess at where this value will end up, but there’s a little more to the algorithm before we are done. Initial guesses for each of these fitted values are obtained for each point in the regression. Now each local regression for each neighborhood is re-run a few times in such a way the the effect of outliers is minimized. The final line for each neighborhood is obtained by the following steps. Compute all residuals for points in the neighborhood of the current regression, denoted by \\(r_i\\). Then compute the MAD, median absolute deviation, of the residuals \\(MAD = \\text{median} (|r_1|, |r_2|, \\ldots)\\). Divide all residuals by 6 times the MAD: \\(u_i = r_i/(6\\cdot MAD)\\) (If \\(r_i > 6\\cdot MAD\\) then set \\(u_i = 0\\).) Compute what are called bisquare weights using the formula: \\(b_i = (1 - u_i^2)^2\\) Perform a regression using the weights \\(w_i = w_i b_i\\) Repeat the above process with the new weights \\(w_i\\) until the weights stop changing very much. The final fitted values for each \\(X\\)-value in the regression are obtained from the final regression line for each neighborhood. These fitted values make up the Lowess (or loess) curve. Note that the default of the loess function in R is to use quadratic regressions in each neighborhood instead of linear regressions. This can be controlled with the loess option of “degree=2” (quadratic fits) or “degree = 1”. In the lowess function only a linear regression in each neighborhood is allowed.",
    "sentences": ["A non-parametric approach to estimating \\(E\\{Y_i\\}\\)…", "Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value.", "Using Base R air2 <- airquality %>% na.omit(select(Temp, Ozone)) # Just quickly draw the lowess curve: plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") lines(lowess(air2$Ozone, air2$Temp), col=\"firebrick\") ## OR optionally, ## allow for predictions as well as the graph: # plot(Temp ~ Ozone, data=air2, pch=16, col=\"darkgray\") # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # lines(mylo$fit ~ Ozone, data=air2) Using ggplot2 air2 <- airquality %>% na.omit(select(Temp, Ozone)) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=\"darkgray\") + geom_smooth(se=F, method=\"loess\", method.args = list(degree=1)) + #Note, degree=2 by default.", "theme_bw() ## `geom_smooth()` using formula = 'y ~ x' ## OR optionally, ## allow for predictions as well as the graph: # air2 <- arrange(air2, desc(Ozone)) # mylo <- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fit, x=Ozone)) Advantages Disadvantages Quick.", "Good at ignoring outliers.", "Good at capturing the general pattern in the data.", "Good for making predictions within the scope of the data.", "No mathematical model.", "Not interpretable.", "No p-values."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Lowess (and Loess) Curves\r\n(Expand)"
  },
  {
    "id": 130,
    "title": "📍 Multiple Linear\r\nRegression",
    "url": "LinearRegression.html#multiplelinearregression",
    "content": "Multiple Linear\r\nRegression Multiple regression allows for more than one explanatory variable to be included in the modeling of the expected value of the quantitative response variable \\(Y_i\\). There are infinitely many possible multiple regression models to choose from. Here are a few “basic” models that work as building blocks to more complicated models. Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model uses the same \\(X\\)-variable twice, once with a \\(\\beta_1 X_i\\) term and once with a \\(\\beta_2 X_i^2\\) term. The \\(X_i^2\\) term is called the “quadratic” term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and “steepness” of the Model: negative values face down, positive values face up; large values imply “steeper” parabolas and low values imply “flatter” parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)’s explanation. An Example Using the airquality data set, we run the following “quadratic” regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our “quadratic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. Temp Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These “Estimates” can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(Month^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of Month from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, )   Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will “open down” (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be “month zero.” Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the “cubic” term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the “base slope coefficient” and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following “cubic” regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our “cubic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. uptake Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^3) \\(X_{i}^3\\), where the function I(…) protects the cubing of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will “open up”. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The “two-lines” model is a simple way to compare two groups in statistics. It uses two main parts: A regular number (\\(X_{1i}\\)) that can be any value. A special number (\\(X_{2i}\\)) that’s either 0 or 1. This special number (also called a “dummy variable” or “indicator variable”) helps turn information about groups (like “Group A” or “Group B”) into numbers we can use in math. By using these two parts, we can create two separate lines on a graph: One line for Group A One line for Group B This helps us see how the two groups are different from each other. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the “base-line” of the model, the “Group 0” line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the “base-line” line. \\(\\beta_3\\) Called the “interaction” term. Controls the change in the slope for the second line in the model as compared to the slope of the “base-line” line. An Example Using the mtcars data set, we run the following “two-lines” regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our “two-lines” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. mpg Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept), \\(\\beta_1\\) is estimated by the qsec value of 1.439, \\(\\beta_2\\) is estimated by the am value of -14.51, and \\(\\beta_3\\) is estimated by the qsec:am value of 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called “3D” regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to “bend”. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the “Temp” direction is estimated to be 2.659 and the slope in the “Month” direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction term’s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the “slopes” in each direction to change, creating a “curved” surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called “HD”, or “High Dimensional”, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isn’t really even possible to “mentally connect” with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)’s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the “Wind” direction is estimated to be -3.334. The slope in the “Temp” direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the “Solar.R” direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here.",
    "sentences": ["Multiple regression allows for more than one explanatory variable to be included in the modeling of the expected value of the quantitative response variable \\(Y_i\\).", "There are infinitely many possible multiple regression models to choose from.", "Here are a few “basic” models that work as building blocks to more complicated models.", "Overview Select a model to see interpretation details, an example, and R Code help.", "Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\).", "Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model uses the same \\(X\\)-variable twice, once with a \\(\\beta_1 X_i\\) term and once with a \\(\\beta_2 X_i^2\\) term.", "The \\(X_i^2\\) term is called the “quadratic” term.", "Parameter Effect \\(\\beta_0\\) Y-intercept of the Model.", "\\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\).", "\\(\\beta_2\\) Controls the concavity and “steepness” of the Model: negative values face down, positive values face up; large values imply “steeper” parabolas and low values imply “flatter” parabolas."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Multiple Linear\r\nRegression"
  },
  {
    "id": 131,
    "title": "📍 Overview",
    "url": "LinearRegression.html#overview",
    "content": "Overview Select a model to see interpretation details, an example, and R Code help. Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\). Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model uses the same \\(X\\)-variable twice, once with a \\(\\beta_1 X_i\\) term and once with a \\(\\beta_2 X_i^2\\) term. The \\(X_i^2\\) term is called the “quadratic” term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\). \\(\\beta_2\\) Controls the concavity and “steepness” of the Model: negative values face down, positive values face up; large values imply “steeper” parabolas and low values imply “flatter” parabolas. Also involved in the position of the vertex, see \\(\\beta_1\\)’s explanation. An Example Using the airquality data set, we run the following “quadratic” regression. Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...). \\[ \\underbrace{Y_i}_\\text{Temp} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{Month} \\underbrace{+}_{+} \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(Month^2)} + \\epsilon_i \\] lm.quad <- A name we made up for our “quadratic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. Temp Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{i}\\) (Month in this case) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These “Estimates” can be found using summary(lmObject) and looking at the Estimates column in the output. Month \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(Month^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of Month from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=airquality This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.quad <- lm(Temp ~ Month + I(Month^2), data=airquality) emphasize.strong.cols(1) pander(summary(lm.quad)$coefficients, )   Estimate Std. Error t value Pr(>|t|) (Intercept) -95.73 15.24 -6.281 3.458e-09 Month 48.72 4.489 10.85 1.29e-20 I(Month^2) -3.283 0.3199 -10.26 4.737e-19 The estimates shown in the summary output table above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -95.73, \\(\\beta_1\\) is estimated by the Month value of 48.72, and \\(\\beta_2\\) is estimated by the I(Month^2) value of -3.283. Because the estimate of the \\(\\beta_2\\) term is negative (-3.283), this parabola will “open down” (concave). This tells us that average temperatures will increase to a point, then decrease again. The vertex of this parabola will be at \\(-b_1/(2b_2) = -(48.72)/(2\\cdot (-3.283)) = 7.420043\\) months, which tells us that the highest average temperature will occur around mid July (7.42 months to be exact). The y-intercept is -95.73, which would be awfully cold if it were possible for the month to be “month zero.” Since this is not possible, the y-intercept is not meaningful for this model. Note that interpreting either \\(\\beta_1\\) or \\(\\beta_2\\) by themselves is quite difficult because they both work with together with \\(X_{i}\\). \\[ \\hat{Y}_i = \\overbrace{-95.73}^\\text{y-int} + \\overbrace{48.72}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-3.283}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(Temp ~ Month, data=airquality, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Quadratic Model using airquality data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 numbers stored inside: # b[1] is the estimate of beta_0: -95.73 # b[2] is the estimate of beta_1: 48.72 # b[3] is the estimate of beta_2: -3.28 curve(b[1] + b[2]*x + b[3]*x^2, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.quad) # Then b will have 3 estimates: # b[1] is the estimate of beta_0: 35.38 # b[2] is the estimate of beta_1: -7.099 # b[3] is the estimate of beta_2: 0.4759 ggplot(airquality, aes(y=Temp, x=Month)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=\"skyblue\") + labs(title=\"Quadratic Model using airquality data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3}_{E\\{Y_i\\}}}^\\text{Cubic Model} + \\epsilon_i \\] The Cubic model uses the same \\(X\\)-variable thrice, once with a \\(\\beta_1 X_i\\) term, once with a \\(\\beta_2 X_i^2\\) term, and once with a \\(\\beta_3 X_i^3\\) term. The \\(X_i^3\\) term is called the “cubic” term. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) No clear interpretation, but could be called the “base slope coefficient” and contributes to the position of the inflection points of the cubic function. \\(\\beta_2\\) No clear interpretation, but it also contributes to the location of the inflection points. \\(\\beta_3\\) This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign. An Example Using the CO2 data set, we run the following “cubic” regression. \\[ \\underbrace{Y_i}_\\text{uptake} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\text{y-int}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{term}}} \\underbrace{X_{i}}_\\text{conc} + \\overbrace{\\beta_2}^{\\stackrel{\\text{quadratic}}{\\text{term}}} \\underbrace{X_{i}^2}_\\text{I(conc^2)} + \\overbrace{\\beta_3}^{\\stackrel{\\text{cubic}}{\\text{term}}} \\underbrace{X_{i}^3}_\\text{I(conc^3)} + \\epsilon_i \\] lm.cubic <- A name we made up for our “cubic” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. uptake Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_i\\) is the first term following ~. This is because the \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). conc \\(X_{i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^2) \\(X_{i}^2\\), where the function I(…) protects the squaring of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. I(conc^3) \\(X_{i}^3\\), where the function I(…) protects the cubing of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement. , data=CO2 This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{i} + X_{i}^2\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lmObject$residuals. lm.cubic <- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2) pander(summary(lm.cubic)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -1.483 5.043 -0.2941 0.7694 conc 0.1814 0.0416 4.36 3.83e-05 I(conc^2) -0.0003063 9.067e-05 -3.378 0.00113 I(conc^3) 1.601e-07 5.512e-08 2.905 0.004745 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept) value of -1.483, \\(\\beta_1\\) is estimated by the conc value of 0.1814, \\(\\beta_2\\) is estimated by the I(conc^2) value of -0.0003063, and \\(\\beta_3\\) is estimated by the I(conc^3) value of 1.601e-07, which translates to 0.0000001601. Because the estimate of the \\(\\beta_3\\) term is positive, this cubic model will “open up”. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead. \\[ \\hat{Y}_i = \\overbrace{-1.483}^\\text{y-int} + \\overbrace{0.1814}^{\\stackrel{\\text{slope}}{\\text{term}}} X_{i} + \\overbrace{-0.0003063}^{\\stackrel{\\text{quadratic}}{\\text{term}}} X_{i}^2 + \\overbrace{1.601e-07}^{\\stackrel{\\text{cubic}}{\\text{term}}} X_{i}^3 \\] The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above. Using Base R plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 ggplot(CO2, aes(y=uptake, x=conc)) + geom_point(pch=21, bg=\"gray83\", color=\"skyblue\") + #geom_smooth(method=\"lm\", se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic. stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=\"skyblue\") + labs(title=\"Cubic Model using CO2 data set\") It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below. plot(uptake ~ conc, data=CO2, col=\"skyblue\", pch=21, bg=\"gray83\", main=\"Cubic Model using CO2 data set\", cex.main=1) #get the \"Estimates\" automatically: b <- coef(lm.cubic) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -1.483 # b[2] is the estimate of beta_1: 0.1814 # b[3] is the estimate of beta_2: -0.0003063 # b[4] is the estimate of beta_3: 1.601e-07 curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=\"skyblue\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc + I(conc^2), data=CO2)) curve(b[1] + b[2]*x + b[3]*x^2, col=\"firebrick\", lwd=2, add=TRUE) b <- coef(lm(uptake ~ conc, data=CO2)) curve(b[1] + b[2]*x, col=\"orange\", lwd=2, add=TRUE) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i}}_{E\\{Y_i\\}}}^\\text{Two-lines Model} + \\epsilon_i \\] \\[ X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Group B} \\\\ 0, & \\text{Group A} \\end{array}\\right. \\] The “two-lines” model is a simple way to compare two groups in statistics. It uses two main parts: A regular number (\\(X_{1i}\\)) that can be any value. A special number (\\(X_{2i}\\)) that’s either 0 or 1. This special number (also called a “dummy variable” or “indicator variable”) helps turn information about groups (like “Group A” or “Group B”) into numbers we can use in math. By using these two parts, we can create two separate lines on a graph: One line for Group A One line for Group B This helps us see how the two groups are different from each other. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model. \\(\\beta_1\\) Controls the slope of the “base-line” of the model, the “Group 0” line. \\(\\beta_2\\) Controls the change in y-intercept for the second line in the model as compared to the y-intercept of the “base-line” line. \\(\\beta_3\\) Called the “interaction” term. Controls the change in the slope for the second line in the model as compared to the slope of the “base-line” line. An Example Using the mtcars data set, we run the following “two-lines” regression. Note that am has only 0 or 1 values: View(mtcars). \\[ \\underbrace{Y_i}_\\text{mpg} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{qsec} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{am} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{qsec:am} + \\epsilon_i \\] lm.2lines <- A name we made up for our “two-lines” regression. lm( R function lm used to perform linear regressions in R. The lm stands for “linear model”. mpg Y-variable, should be quantitative.  ~  The tilde ~ is what lm(…) uses to state the regression equation \\(Y_i = ...\\). Notice that the ~ is not followed by \\(\\beta_0 + \\beta_1\\) like \\(Y_i = ...\\). Instead, \\(X_{1i}\\) is the first term following ~. This is because \\(\\beta\\)’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject). qsec \\(X_{1i}\\), should be quantitative.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. am \\(X_{2i}\\), an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ.  +  The plus + is used between each term in the model. Note that only the x-variables are included in the lm(…) from the \\(Y_i = ...\\) model. No beta’s are included. qsec:am \\(X_{1i}X_{2i}\\) the interaction term. This allows the slopes of the two lines to differ. , data=mtcars This is the data set we are using for the regression. )Closing parenthsis for the lm(…) function.     Press Enter to run the code.  …  Click to View Output. Pay special attention to how the lm(…) code uses \\(Y_i \\sim X_{1i} + X_{2i} + X_{1i}X_{2i}\\) and drops all \\(\\beta\\)’s and \\(\\epsilon\\) from the model statement. This is because the estimates for the \\(\\beta\\)’s and \\(\\epsilon\\) are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in lm.2lines$residuals. lm.2lines <- lm(mpg ~ qsec + am + qsec:am, data=mtcars) pander(summary(lm.2lines)$coefficients)   Estimate Std. Error t value Pr(>|t|) (Intercept) -9.01 8.218 -1.096 0.2823 qsec 1.439 0.45 3.197 0.003432 am -14.51 12.48 -1.163 0.2548 qsec:am 1.321 0.7017 1.883 0.07012 The estimates shown above approximate the \\(\\beta\\)’s in the regression model: \\(\\beta_0\\) is estimated by the (Intercept), \\(\\beta_1\\) is estimated by the qsec value of 1.439, \\(\\beta_2\\) is estimated by the am value of -14.51, and \\(\\beta_3\\) is estimated by the qsec:am value of 1.321. This gives two separate equations of lines. Automatic Transmission (am==0, \\(X_{2i} = 0\\)) Line \\[ \\hat{Y}_i = \\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} X_{1i} \\] Manual Transmission (am==1 , \\(X_{2i} = 1\\)) Line \\[ \\hat{Y}_i = \\underbrace{(\\overbrace{-9.01}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{-14.51}^{\\stackrel{\\text{change in}}{\\text{y-int}}})}_{\\stackrel{\\text{y-intercept}}{-23.52}} + \\underbrace{(\\overbrace{1.439}^{\\stackrel{\\text{slope}}{\\text{baseline}}} +\\overbrace{1.321}^{\\stackrel{\\text{change in}}{\\text{slope}}})}_{\\stackrel{\\text{slope}}{2.76}} X_{1i} \\] These lines are drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the two equations above. Using Base R plot(mpg ~ qsec, data=mtcars, col=c(\"skyblue\",\"orange\")[as.factor(am)], pch=21, bg=\"gray83\", main=\"Two-lines Model using mtcars data set\", cex.main=1) legend(\"topleft\", legend=c(\"Baseline (am==0)\", \"Changed-line (am==1)\"), bty=\"n\", lty=1, col=c(\"skyblue\",\"orange\"), cex=0.8) #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 curve(b[1] + b[2]*x, col=\"skyblue\", lwd=2, add=TRUE) #baseline (in blue) curve((b[1] + b[3]) + (b[2] + b[4])*x, col=\"orange\", lwd=2, add=TRUE) #changed line (in orange) Using ggplot2 #get the \"Estimates\" automatically: b <- coef(lm.2lines) # Then b will have 4 estimates: # b[1] is the estimate of beta_0: -9.0099 # b[2] is the estimate of beta_1: 1.4385 # b[3] is the estimate of beta_2: -14.5107 # b[4] is the estimate of beta_3: 1.3214 ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) + geom_point(pch=21, bg=\"gray83\") + #geom_smooth(method=\"lm\", se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic. stat_function(fun = function(x) b[1] + b[2]*x, color=\"skyblue\") + #am==0 line stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=\"orange\") + #am==1 line scale_color_manual(name=\"Transmission (am)\", values=c(\"skyblue\",\"orange\")) + labs(title=\"Two-lines Model using mtcars data set\") \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i}X_{2i}}_{E\\{Y_i\\}}}^\\text{3D Model} + \\epsilon_i \\] The so called “3D” regression model uses two different quantitative x-variables, an \\(X_{1i}\\) and an \\(X_{2i}\\). Unlike the two-lines model where \\(X_{2i}\\) could only be a 0 or a 1, this \\(X_{2i}\\) variable is quantitative, and can take on any quantitative value. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(\\beta_3\\) Interaction term that allows the model, which is a plane in three-dimensional space, to “bend”. If this term is zero, then the regression surface is just a flat plane. An Example Here is what a 3D regression looks like when there is no interaction term. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month -139.6 2.659 -3.522 Notice how the slope, \\(\\beta_1\\), in the “Temp” direction is estimated to be 2.659 and the slope in the “Month” direction, \\(\\beta_2\\), is estimated to be -3.522. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -139.6. ## Hint: library(car) has a scatterplot 3d function which is simple to use # but the code should only be run in your console, not knit. ## library(car) ## scatter3d(Y ~ X1 + X2, data=yourdata) ## To embed the 3d-scatterplot inside of your html document is harder. #library(plotly) #library(reshape2) #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") Here is a second view of this same regression with what is called a contour plot, contour map, or density plot. mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26)) Including the Interaction Term Here is what a 3D regression looks like when the interaction term is present. The two x-variables of Month and Temp are being used to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope}}{\\text{baseline}}} \\underbrace{X_{1i}}_\\text{Temp} + \\overbrace{\\beta_2}^{\\stackrel{\\text{change in}}{\\text{y-int}}} \\underbrace{X_{2i}}_\\text{Month} + \\overbrace{\\beta_3}^{\\stackrel{\\text{change in}}{\\text{slope}}} \\underbrace{X_{1i}X_{2i}}_\\text{Temp:Month} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) pander(air_lm$coefficients) (Intercept) Temp Month Temp:Month -3.915 0.77 -23.01 0.2678 Notice how all coefficient estimates have changed. The y-intercept, \\(\\beta_0\\) is now estimated to be \\(-3.915\\). The slope term, \\(\\beta_1\\), in the Temp-direction is estimated as \\(0.77\\), while the slope term, \\(\\beta_2\\), in the Month-direction is estimated to be \\(-23.01\\). This change in estimated coefficiets is due to the presence of the interaction term’s coefficient, \\(\\beta_3\\), which is estimated to be \\(0.2678\\). As you should notice in the graphic, the interaction model allows the “slopes” in each direction to change, creating a “curved” surface for the regression surface instead of a flat surface. #Perform the multiple regression air_lm <- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality) #Graph Resolution (more important for more complex shapes) graph_reso <- 0.5 #Setup Axis axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso) axis_y <- seq(min(airquality$Month), max(airquality$Month), by = graph_reso) #Sample points air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface)) air_surface <- acast(air_surface, Month ~ Temp, value.var = \"Z\") #y ~ x #Create scatterplot plot_ly(airquality, x = ~Temp, y = ~Month, z = ~Ozone, text = rownames(airquality), type = \"scatter3d\", mode = \"markers\") %>% add_trace(z = air_surface, x = axis_x, y = axis_y, type = \"surface\") And here is that same plot as a contour plot. air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F) air_surface$Z <- predict.lm(air_lm, newdata = air_surface) mycolorpalette <- colorRampPalette(c(\"skyblue2\", \"orange\")) filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27)) \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p-1}X_{p-1,i}}_{E\\{Y_i\\}}}^\\text{\"High Dimensional Models\"} + \\epsilon_i \\] The so called “HD”, or “High Dimensional”, regression model uses three or more different quantitative x-variables, an \\(X_{1i}\\), an \\(X_{2i}\\), and at least an \\(X_{3i}\\), but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isn’t really even possible to “mentally connect” with this type of model is it exists beyond what our 3D minds can really comprehend. Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line in the \\(X_1\\) direction. \\(\\beta_2\\) Slope of the line in the \\(X_2\\) direction. \\(...\\) Slopes in other directions depending on how many other variables are included in the model. \\(\\beta_{p-1}\\) Final term in the model where there are \\(p\\) total \\(\\beta\\)’s. The reason for the \\(p-1\\) on the last term is because we started with \\(\\beta_0\\) for the first term, leaving \\(\\beta_{p-1}\\) as the last term. An Example Suppose we used three x-variables of Wind, Temp, and Solar.R to predict the y-variable of Ozone. \\[ \\underbrace{Y_i}_\\text{Ozone} \\underbrace{=}_{\\sim} \\overbrace{\\beta_0}^{\\stackrel{\\text{y-int}}{\\text{baseline}}} + \\overbrace{\\beta_1}^{\\stackrel{\\text{slope in}}{\\text{Wind Direction}}} \\underbrace{X_{1i}}_\\text{Wind} + \\overbrace{\\beta_2}^{\\stackrel{\\text{slope in}}{\\text{Temp Direction}}} \\underbrace{X_{2i}}_\\text{Temp} + \\overbrace{\\beta_3}^{\\stackrel{\\text{slope in}}{\\text{Solar.R Direction}}} \\underbrace{X_{3i}}_\\text{Solar.R} + \\epsilon_i \\] air_lm <- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality) pander(air_lm$coefficients) (Intercept) Wind Temp Solar.R -64.34 -3.334 1.652 0.05982 Notice how the slope, \\(\\beta_1\\), in the “Wind” direction is estimated to be -3.334. The slope in the “Temp” direction, \\(\\beta_2\\), is estimated to be 1.652. The slope in the “Solar.R” direction, \\(\\beta_3\\), is estimated to be 0.05982. Also, the y-intercept, \\(\\beta_0\\), is estimated to be -64.34. Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw \\(Y\\) against each \\(X\\)-variable in separate scatterplots, one for each \\(X\\)-variable used in our model. b <- coef(air_lm) par(mfrow=c(1,3)) plot(Ozone ~ Wind, data=airquality) curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=\"skyblue\") # The x-variable of this plot is \"Wind\" # The values of Temp=79 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Temp, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=\"orange\") # The x-variable of this plot is \"Temp\" # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value, # in this case, their respective medians. plot(Ozone ~ Solar.R, data=airquality) curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=\"firebrick\") # The x-variable of this plot is \"Solar.R\" # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value, # in this case, their respective medians. The coefficient \\(\\beta_j\\) is interpreted as the change in the expected value of \\(Y\\) for a unit increase in \\(X_{j}\\), holding all other variables constant, for \\(j=1,\\ldots,p-1\\). However, this interpretation breaks down when higher order terms (like \\(X^2\\)) or interaction terms (like \\(X1:X2\\)) are included in the model. See the Explanation tab for details about possible hypotheses here.",
    "sentences": ["Select a model to see interpretation details, an example, and R Code help.", "Simple Quadratic Cubic Two-Lines 3D HD \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}}}^\\text{Simple Model} + \\epsilon_i \\] The Simple Linear Regression model uses a single x-variable once: \\(X_i\\).", "Parameter Effect \\(\\beta_0\\) Y-intercept of the Model \\(\\beta_1\\) Slope of the line \\[ Y_i = \\overbrace{\\underbrace{\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2}_{E\\{Y_i\\}}}^\\text{Quadratic Model} + \\epsilon_i \\] The Quadratic model uses the same \\(X\\)-variable twice, once with a \\(\\beta_1 X_i\\) term and once with a \\(\\beta_2 X_i^2\\) term.", "The \\(X_i^2\\) term is called the “quadratic” term.", "Parameter Effect \\(\\beta_0\\) Y-intercept of the Model.", "\\(\\beta_1\\) Controls the x-position of the vertex of the parabola by \\(\\frac{-\\beta_1}{2\\cdot\\beta_2}\\).", "\\(\\beta_2\\) Controls the concavity and “steepness” of the Model: negative values face down, positive values face up; large values imply “steeper” parabolas and low values imply “flatter” parabolas.", "Also involved in the position of the vertex, see \\(\\beta_1\\)’s explanation.", "An Example Using the airquality data set, we run the following “quadratic” regression.", "Pay careful attention to how the mathematical model for \\(Y_i = \\ldots\\) is translated to R-Code inside of lm(...)."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Overview"
  },
  {
    "id": 132,
    "title": "📍 R Instructions",
    "url": "LinearRegression.html#rinstructions",
    "content": "R Instructions NOTE: These are general R Commands for all types of multiple linear regressions. See the “Overview” section for R Commands details about a specific multiple linear regression model. Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set. It requires that all columns of the data set be either numeric or factor classes. (Character classes will throw an error.) cbind( This is the “column (c) bind” function and it joins together things as columns. Res =  This is just any name you come up with, but Res is a good abbreviation for Residuals. mylm$residuals,  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set. YourDataSet), This puts the original data set along side the residuals.  panel=panel.smooth,  This places a lowess smoothing line on each scatterplot. col =  specifies the colors of the dots. as.factor(YourDataSet$Xvar) This causes the coloring of the points in the plot to be colored according to the groups found in Xvar. Using palette(c(“color1”,“color2”, and so on)) prior to the plotting code allows you to specify the colors pairs will pick from when choosing colors. ) Closing parenthesis for the pairs function. Perform the Regression Everything is the same as in simple linear regression except that more variables are allowed in the call to lm(). mylm <- lm( mylm is some name you come up with to store the results of the lm() test. Note that lm() stands for “linear model.” Y Y must be a “numeric” vector of the quantitative response variable.  ~  Formula operator in R. X1 + X2 X1 and X2 are the explanatory variables. These can either be quantitative or qualitative. Note that R treats “numeric” variables as quantitative and “character” or “factor” variables as qualitative. R will automatcially recode qualitative variables to become “numeric” variables using a 0,1 encoding. See the Explanation tab for details.  + X1:X2 X1:X2 is called the interaction term. See the Explanation tab for details.  + …, * ... emphasizes that as many explanatory variables as are desired can be included in the model.  data = YourDataSet) YourDataSet is the name of your data set. summary( The summary(…) function displays the results of an lm(…) in R. mylm The name of your lm that was performed earlier. ) Closing parenthesis for summary(…) function. Example output from a regression. Hover each piece to learn more. Call: lm(formula = mpg ~ hp + am + hp:am, data = mtcars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -4.3818 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -2.2696 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.1344 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   1.7058 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   5.8752 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   26.6248479 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when all X-variables are zero.   2.1829432 This is the standard error of \\(b_0\\). It estimates how much \\(b_0\\) varies from sample to sample. The closer to zero, the more reliable the estimate of the intercept. 12.197 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (26.6248479) by its standard error (2.1829432). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 26.6248479 is t=12.197 standard errors away from zero, which is a fairly surprising distance as shown by the p-value. 1.01e-12 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. *** This is called a “star”. Three stars means significant at the 0 level of \\(\\alpha\\). hp This is always the name of your first X-variable in your lm(Y ~ X1 + …).   -0.0591370 This is the estimate of \\(\\beta_1\\) in the regression model. It is called \\(b_1\\). Interpreting this value depends on your choice of regression model.   0.0129449 This is the standard error of \\(b_1\\). It estimates how much \\(b_1\\) varies from sample to sample. The closer to zero, the more precise the estimate. -4.568 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” by its standard error. It gives the “number of standard errors” away from zero that the “estimate” has landed. 9.02e-05 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). am This is the second X-variable of your regression model in lm(Y ~ X1 + X2 + …).   5.2176534 This is the estimated value for \\(\\beta_2\\) and is called \\(b_2\\).   2.6650931 This is the standard error of \\(b_2\\). It estimates how much \\(b_2\\) will vary from sample to sample. 1.958 Test statistic (t) for the test of \\(\\beta_2 = 0\\). It represents the number of standard errors that \\(b_2\\) is from 0. 0.0603 The p-value for the test of \\(\\beta_2 = 0\\). The dot “.” implies the result is significant at the 0.1 level. hp:am This is the interaction of \\(X1\\) and \\(X2\\). Not all regression models require an interaction term, and they can include more than one interaction term. This is just an example of what an interaction term would look like.   0.0004029 This is the estimate of the coefficient of the interaction term.   0.0164602 Estimated standard error of the interaction term. 0.024 Test statistic for the test that \\(\\beta_3 = 0\\). 0.9806 P-value for the test that \\(\\beta_3 = 0\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  2.939 For this particular regression, the estimate of \\(\\sigma\\) is 2.939. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 28 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-4 = 28. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.7852, In this particular regression, 78.52% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data.  0.7621 In this case, the value of 0.7621 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is the test statistic for the test of \\(\\beta_1 = \\beta_2 = \\beta_3 = \\ldots = 0\\). In other words, it tests that ALL coefficients are zero against the alternative that “at least one is not.”  34.11 This is the value of the F-statistic that should be compared to an F-distribution with 3 and 28 degrees of freedom.  on 3 and 28 DF, These two numbers give the two parameters (degrees of freedom 1 and degrees of freedom 2) of the F-distribution. Knowing these parameters and the value of the F-statistic allows the computation of the p-value for the test that all regression coefficients are zero.  p-value: 1.73e-09 The p-value of the test that all regression coefficients are zero. If this p-value is significant, then it can be determined that “at least one” of the variables included in the regression gives significant insight about the average y-value. Plotting the Regression Lines See each of the “Overview” sections for details on how to plot the various types of multiple linear regression models. Making Predictions predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). newdata = data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). \\(X_1\\)= The value for X= should be whatever x-variable name was used in the original regression. For example, if mylm <- lm(mpg ~ hp + am + hp:am, data=mtcars) was the original regression, then this code would read hp = instead of X1 =… Further, the value of \\(X_{1h}\\) should be some specific number, like hp=123 for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 123, as in hp=123 for example. \\(X_2\\)= This is the value of the second x-variable, say am. \\(X_{2h}\\)) Since the am column can only be a 1 or 0, we would try am=1 for example, or am=0. ) Closing parenthesis. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = \"response\") ## 1 ## 24.79441 The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...).  newdata=data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “prediction”) This causes the prediction to include the lower bound and upper bound of the prediction interval for \\(Y_i\\) for the given X1, X2, and so on values that have been specified. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"prediction\") ## fit lwr upr ## 1 24.79441 18.49923 31.08959 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet. predict( The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values. mylm, This is the name of a previously performed lm(…) that was saved into the name mylm <- lm(...). data.frame( To specify the values of \\(x\\) that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…). X1= The X1= should be replaced with whatever x-variable name was used in the original regression. For example, if mylm <- lm(dist ~ speed, data=cars) was the original regression, then this code would read speed = instead of X1=… Further, the value of \\(X_{1h}\\) should be some specific number, like 12 so that it reads speed=12, for example. \\(X_{1h}\\), The value of \\(X_{1h}\\) should be some specific number, like 12, as in speed=12 for example. X2= If a regression of lm(Y ~ X1 + X2 + …) was performed, then X2 is the name of the second x-variable used in the regression. \\(X_{2h}\\)), A number should be specified for \\(X_{2h}\\), something that would be meaningful for X2 to be equal to. interval = “confidence”) This causes the prediction to include the lower and upper bound of a confidence interval for \\(E{Y_i}\\) for the given \\(X\\)-values. mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = \"confidence\") mylm <- lm(mpg ~ hp + am + hp:am, data = mtcars) predict(mylm, data.frame(hp = 120, am = 1), interval = \"confidence\") ## fit lwr upr ## 1 24.79441 23.10635 26.48247 The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound. In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet.",
    "sentences": ["NOTE: These are general R Commands for all types of multiple linear regressions.", "See the “Overview” section for R Commands details about a specific multiple linear regression model.", "Console Help Command: ?lm() Finding Variables pairs( A function in R that creates all possible two-variable scatterplots from a data set.", "It requires that all columns of the data set be either numeric or factor classes.", "(Character classes will throw an error.) cbind( This is the “column (c) bind” function and it joins together things as columns.", "Res =  This is just any name you come up with, but Res is a good abbreviation for Residuals.", "mylm$residuals,  This pulls out the residuals from the current regression and adds them as a new column inside the cbind data set.", "YourDataSet), This puts the original data set along side the residuals.", " panel=panel.smooth,  This places a lowess smoothing line on each scatterplot.", "col =  specifies the colors of the dots."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "R Instructions"
  },
  {
    "id": 133,
    "title": "📍 Explanation",
    "url": "LinearRegression.html#explanation",
    "content": "Explanation Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BIC… There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (“R-squared”). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The “simplest” but “best” model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it “an information criterion (AIC)” when he published the method and other people later began calling it the “Akaike Information Criterion.”) Model Selection (Expand) pairs plots, added variable plots, and pattern recognition… Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots A useful visualization tool for model selection is the “pairs plot.” This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice that… the y-axis of each plot is found by locating the variable name (like “mpg”) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like “disp”) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldn’t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Let’s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Model Validation (Expand) Verifying a model’s ability to generalize to new data… The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesn’t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, let’s remind ourselves why we use regression models in the first place. The main goal is to capture the “essence” of the data. In other words, the general pattern is what we are after. We want a model that tells us how “all such” data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the “complicated model” is overfitting the original data. It does not capture the “essence” of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data. Interpretation (Expand) \\(\\beta_j\\) is the change in the average y-value… The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant. Added Variable Plots (Expand) When to add another \\(X\\)-variable to the model… The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read more…) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) Outlier Analysis (Expand) Cook’s Distances and Leverage Values… The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs. fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cook’s Distances The idea behind Cook’s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article “Detection of Influential Observation in Linear Regression” (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, let’s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2))   1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the “differences” in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cook’s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cook’s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cook’s Distances using the code cooks.distance(lmObject). Also, a graph of Cook’s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of “leverage” and is “pulling” the regression toward itself. A value near 0 implies the point is just “one of many” and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that “minimize” the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)’s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the “hat matrix” \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the “leverage values” and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a “lot of pull,” and values close to 0 represent “little pull.” In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with “lots of leverage” and a large “Cook’s Distance” are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression. Inference for the Model Parameters (Expand) t Tests and F tests in multiple regression… Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class.",
    "sentences": ["Assessing the Model Fit (Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BIC… There are many measures of the quality of a regression model.", "One of the most popular measurements is the \\(R^2\\) value (“R-squared”).", "The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model.", "Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1.", "Values close to 1 imply a very good model.", "Values close to 0 imply a very poor model.", "One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model.", "Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty.", "The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\).", "Consider the models below."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Explanation"
  },
  {
    "id": 134,
    "title": "📍 Assessing the Model Fit\r\n(Expand)",
    "url": "LinearRegression.html#assessingthemodelfitexpand",
    "content": "Assessing the Model Fit\r\n(Expand) \\(R^2\\), adjusted \\(R^2\\), AIC, BIC… There are many measures of the quality of a regression model. One of the most popular measurements is the \\(R^2\\) value (“R-squared”). The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model. Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model. One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty. The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\). Consider the models below. The value of \\(R^2\\) always gets higher as the model adds more parameters. However, the value of \\(R^2_{adj}\\) sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y. par(mfrow=c(1,5), mai=c(0,.1,.4,.1)) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Simple Linear\") lm1 <- lm(dist ~ speed, data=cars) b <- coef(lm1) curve(b[1] + b[2]*x, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quadratic\") lm1 <- lm(dist ~ speed + I(speed^2), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Cubic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quartic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) plot(dist ~ speed, data=cars, pch=16, col=\"skyblue\", yaxt='n', xaxt='n', cex=2, xlim=c(0,27), main=\"Quintic\") lm1 <- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars) b <- coef(lm1) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=\"orange\", lwd=2) text(1,110,bquote(R^2 == .(round(summary(lm1)$r.squared,3))),pos=4) text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4) The “simplest” but “best” model of those shown above would be the Quadratic. This is because it has the best \\(R^2_{adj}\\) (0.653) other than the far more complicated Quartic model (0.655). But the \\(R^2_{adj}\\) for the Quadratic model is a good improvement over that of the \\(R^2_{adj}\\) for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the \\(R^2_{adj}\\) to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model. \\[ \\text{\\emph{Quadratic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\epsilon_i \\] \\[ \\text{\\emph{Quartic Model}:}\\quad Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\underbrace{\\beta_3 X_i^3 + \\beta_4 X_i^4}_\\text{Cubic and Quartic Terms} + \\epsilon_i \\] AIC and BIC Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using AIC(yourlm) and BIC(yourlm). The formula for each are given in different, but equivalent ways depending on which source you obtain the equation. Perhaps the easiest formulation to understand is that given by Kutner, Nachtsheim, and Neter in their book Applied Linear Regression Models (4th edition, page 360) \\[ \\text{AIC:} \\quad n \\ln(SSE) - n \\ln(n) + 2p \\] where SSE is the usual \\(\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) of the current regression model under consideration, \\(n\\) is the sample size, and \\(p\\) is the number of parameters in the current regression model. \\[ \\text{BIC:} \\quad n \\ln(SSE) - n \\ln(n) + p\\ln(n) \\] This shows how the BIC differs only from the AIC in the final term, where AIC uses \\(2p\\) and BIC uses \\(p\\ln(n)\\). Since \\(\\ln(n) \\geq 2\\) for \\(n\\geq8\\), then BIC enforces a larger penalty than the AIC for extra model parameters (\\(p\\)) when the sample size is 8 or larger, i.e., most data sets. The AIC was formulated by Hirotugu Akaike in 1971. (Here is a short commentary by Akaike about how he developed this information criterion. Note that he named it “an information criterion (AIC)” when he published the method and other people later began calling it the “Akaike Information Criterion.”)",
    "sentences": ["\\(R^2\\), adjusted \\(R^2\\), AIC, BIC…", "There are many measures of the quality of a regression model.", "One of the most popular measurements is the \\(R^2\\) value (“R-squared”).", "The \\(R^2\\) value is a measure of the proportion of variation of the \\(Y\\)-variable that is explained by the model.", "Specifically, \\[ R^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = 1-\\frac{\\text{SSE}}{\\text{SSTO}} \\] The range of \\(R^2\\) is between 0 and 1.", "Values close to 1 imply a very good model.", "Values close to 0 imply a very poor model.", "One difficulty of \\(R^2\\) in multiple regression is that it will always get larger when more variables are included in the regression model.", "Thus, in multiple linear regression, it is best to make an adjustment to the \\(R^2\\) value to protect against this difficulty.", "The value of the adjusted \\(R^2\\) is given by \\[ R^2_{adj} = 1 - \\frac{(n-1)}{(n-p)}\\frac{\\text{SSE}}{\\text{SSTO}} \\] The interpretation of \\(R^2_{adj}\\) is essentially the same as the interpretation of \\(R^2\\), with the understanding that a correction has been made for the number of parameters included in the model, \\((n-p)\\)."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Assessing the Model Fit\r\n(Expand)"
  },
  {
    "id": 135,
    "title": "📍 Model Selection\r\n(Expand)",
    "url": "LinearRegression.html#modelselectionexpand",
    "content": "Model Selection\r\n(Expand) pairs plots, added variable plots, and pattern recognition… Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\). They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model. However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model. Pairs Plots A useful visualization tool for model selection is the “pairs plot.” This plot shows all possible 2D scatterplots that can be created from a given dataset. Here is a pairs plot of the mtcars data set in R. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice that… the y-axis of each plot is found by locating the variable name (like “mpg”) that is found to the left or right of the current plot. the x-axis of each plot is found by locating the variable name (like “disp”) that is found above or below each plot. the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot. Selecting a Model Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg. \\[ \\underbrace{Y_i}_\\text{mpg} = \\underbrace{?}_\\text{Our model} + \\ \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] To find meaningful x-variables that could predict our chosen y-variable of mpg, we look at all plots that have mpg as the y-axis of the plot. This happens to be the first row of the pairs plot. When looking at the graph, we are looking for variables that show a strong change in the average y-value (i.e., the LOWESS curve should show steep slope or a meaningful trend). While all variables in the mtcars data set seem to have some relationship with mpg, the strongest relationships appear to e with cyl, disp, hp, wt, vs, am, and gear. Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Also worth noting is that the relationship of mpg with each of disp, hp, and wt are all similar, they each look to be an exponential decay type of model. This tells us that we had better check to see if disp, hp, and wt are related to each other. If they are, then we should only use one of them in the regression model as the other two likely wouldn’t give any new information about mpg. Sure enough, the pairs plot shows that there is a fairly strong relationship between disp and hp, hp and wt, and disp and wt. Now, with all of this in mind, we could start looking at a few possible regression models. Let’s start with perhaps the simplest and strongest trend we saw with mpg and any of the x-variables, wt. plot(mpg ~ wt, data=mtcars) lm.wt <- lm(mpg ~ wt, data=mtcars) summary(lm.wt) %>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446",
    "sentences": ["pairs plots, added variable plots, and pattern recognition…", "Model selection is an exploratory analysis tool that is useful for proposing possible regression models for a given response variable \\(Y\\).", "They should always be followed up by confirmatory analysis that tests the theories proposed by the selected model.", "However, when confirmatory studies are not possible, model validation is a meaningful tool that can be used to attempt to confirm the utility of a model.", "Pairs Plots A useful visualization tool for model selection is the “pairs plot.” This plot shows all possible 2D scatterplots that can be created from a given dataset.", "Here is a pairs plot of the mtcars data set in R.", "Basic View pairs(mtcars, panel=panel.smooth) More Detailed View pairs(mtcars, panel=panel.smooth) Notice that… the y-axis of each plot is found by locating the variable name (like “mpg”) that is found to the left or right of the current plot.", "the x-axis of each plot is found by locating the variable name (like “disp”) that is found above or below each plot.", "the LOWESS curves have been added to each plot to visualize the type of regression model that would best fit each plot.", "Selecting a Model Suppose now that we are trying to come up with a good regression model for predicting the gas mileage of a car, \\(Y=\\)mpg."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Model Selection\r\n(Expand)"
  },
  {
    "id": 136,
    "title": "📍 Model Validation\r\n(Expand)",
    "url": "LinearRegression.html#modelvalidationexpand",
    "content": "Model Validation\r\n(Expand) Verifying a model’s ability to generalize to new data… The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesn’t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model. set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, let’s remind ourselves why we use regression models in the first place. The main goal is to capture the “essence” of the data. In other words, the general pattern is what we are after. We want a model that tells us how “all such” data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data. This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data. set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data. yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num. parameters in model ps <- length(coef(lms)) #num. parameters in model pc <- length(coef(lmc)) #num. parameters in model rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO my_output_table2 <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca)) colnames(my_output_table2) <- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\") knitr::kable(my_output_table2, escape=TRUE, digits=4) Model Original \\(R^2\\) Original Adj. \\(R^2\\) Validation \\(R^2\\) Validation Adj. \\(R^2\\) True 0.9959 0.9948 0.9928 0.9908 Simple 0.8115 0.8010 0.8002 0.7891 Complicated 0.9985 0.9941 0.8686 0.5008 Notice how the \\(R^2\\) for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted \\(R^2\\) dropped from 0.994 to 0.501! On the other hand, the \\(R^2\\) and adjusted \\(R^2\\) values for the True and Simple model were relatively unchanged. This is clear evidence that the “complicated model” is overfitting the original data. It does not capture the “essence” of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by over fitting a model to a particular sample of data.",
    "sentences": ["Verifying a model’s ability to generalize to new data…", "The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesn’t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model.", "set.seed(123) #gives us the same randomness n <- 20 #sample size x <- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8 # Coefficients for the true model: beta0 <- 2 beta1 <- -2.5 beta2 <- 1 beta3 <- 3 beta4 <- -0.8 # Get y-value using a true model y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors thedata <- data.frame(y, x) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=\"lightgray\", bg=\"steelblue\", cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"Original Data (Training Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmo <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmo) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') my_output_table <- data.frame(Model = c(\"True\", \"Simple\", \"Complicated\"), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared)) colnames(my_output_table) <- c(\"Model\", \"$R^2$\", \"Adjusted $R^2$\") knitr::kable(my_output_table) Model \\(R^2\\) Adjusted \\(R^2\\) True 0.9958725 0.9947718 Simple 0.8114836 0.8010105 Complicated 0.9984527 0.9941204 Now, let’s remind ourselves why we use regression models in the first place.", "The main goal is to capture the “essence” of the data.", "In other words, the general pattern is what we are after.", "We want a model that tells us how “all such” data is created, not just the specific data we have sampled.", "So, the great test of a model is to see how well it works on a new sample of data.", "This is precisely model validation, the verification that a model fit on one sample of data, continues to perform well on a new sample of data.", "set.seed(14551) #get same random sample # Get a new sample of data from the true model Xnew <- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8 Ynew <- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors thedata2 <- data.frame(y=Ynew, x=Xnew) # Plot it par(mai=c(.1,.5,.2,.1)) plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt='n', xaxt='n', ylab=\"\", xlab=\"\") mtext(side=3, text=\"New Data (Testing Data)\", cex=0.7, at=-.8, line=.1) # Draw true model curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4) lmt <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later # Draw simple linear model lms <- lm(y ~ x, data=thedata) b <- coef(lms) curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2) # Draw overly complicated model lmc <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata) b <- coef(lmc) curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2) # Add new data to plot points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=\"orange\", cex=1.3) # Add legend legend(\"topleft\", legend=c(\"True Model\", \"Simple Model\", \"Complicated Model\"), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty='n') # Add dot legend legend(\"bottomright\", legend=c(\"Original Sample\", \"New Sample\"), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),\"orange\"), bty='n') # Compute R-squared for each validation # Get y-hat for each model on new data.", "yht <- predict(lmt, newdata=thedata2) yhs <- predict(lms, newdata=thedata2) yhc <- predict(lmc, newdata=thedata2) # Compute y-bar ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data # Compute SSTO SSTO <- sum( (thedata2$y - ybar)^2 ) # Compute SSE for each model using y - yhat SSEt <- sum( (thedata2$y - yht)^2 ) SSEs <- sum( (thedata2$y - yhs)^2 ) SSEc <- sum( (thedata2$y - yhc)^2 ) # Compute R-squared for each rst <- 1 - SSEt/SSTO rss <- 1 - SSEs/SSTO rsc <- 1 - SSEc/SSTO # Compute adjusted R-squared for each n <- length(thedata2$y) #sample size pt <- length(coef(lmt)) #num."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Model Validation\r\n(Expand)"
  },
  {
    "id": 137,
    "title": "📍 Interpretation\r\n(Expand)",
    "url": "LinearRegression.html#interpretationexpand",
    "content": "Interpretation\r\n(Expand) \\(\\beta_j\\) is the change in the average y-value… The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant.",
    "sentences": ["\\(\\beta_j\\) is the change in the average y-value…", "The only change to interpretation from the simple linear regression model is that each coefficient, \\(\\beta_j\\) \\(j=1,\\ldots,p\\), represents the change in the \\(E\\{Y\\}\\) for a unit change in \\(X_j\\), holding all other variables constant."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Interpretation\r\n(Expand)"
  },
  {
    "id": 138,
    "title": "📍 Added Variable Plots\r\n(Expand)",
    "url": "LinearRegression.html#addedvariableplotsexpand",
    "content": "Added Variable Plots\r\n(Expand) When to add another \\(X\\)-variable to the model… The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. All important variables are included in the model. Checking the Assumptions The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. Let \\(X_{new}\\) be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against \\(X_{new}\\) allows us to determine if \\(X_{new}\\) has any information to add to the current model. If there is a trend in the plot, then \\(X_{new}\\) should be added to the model. If there is no trend in the plot, then the \\(X_{new}\\) should be left out. | Show Examples | (Read more…) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0)",
    "sentences": ["When to add another \\(X\\)-variable to the model…", "The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption.", "The regression relation between \\(Y\\) and \\(X\\) is linear.", "The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\).", "The variance of the error terms is constant over all \\(X\\) values.", "The \\(X\\) values can be considered fixed and measured without error.", "The error terms are independent.", "All important variables are included in the model.", "Checking the Assumptions The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot.", "Added variable plots can be used to determine if a new variable should be included in the model."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Added Variable Plots\r\n(Expand)"
  },
  {
    "id": 139,
    "title": "📍 Checking the Assumptions",
    "url": "LinearRegression.html#checkingtheassumptions",
    "content": "Checking the Assumptions The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model. (Read more…) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model. The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis). If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model. If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model. The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model. tmp <- lm(circumference ~ age, data=Orange) plot(tmp$residuals ~ age, data=Orange, pch=20, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0) tmp <- lm(salary ~ yrs.since.phd, data=Salaries) plot(tmp$residuals ~ yrs.service, data=Salaries, pch=20, cex=0.8, xlab=expression(X[new]), ylab=“Residuals”, main=““, cex.main=0.95, xaxt=‘n’, yaxt=‘n’, col=”firebrick”) abline(h=0)",
    "sentences": ["The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot.", "Added variable plots can be used to determine if a new variable should be included in the model.", "(Read more…) An added variable plot checks to see if a new variable has any information to add to the current multiple regression model.", "The plot is made by taking the residuals from the current multiple regression model (\\(y\\)-axis) and plotting them against the new explanatory variable (\\(x\\)-axis).", "If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression.", "The new variable should be included in the model.", "If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model.", "The new variable should continue to be left out of the model.", "The left column of plots below show scenarios where the new explanatory variable should be included in the model.", "The right column of plots show scenarios where the new explanatory variable should not be included in the model."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Checking the Assumptions"
  },
  {
    "id": 140,
    "title": "📍 Outlier Analysis\r\n(Expand)",
    "url": "LinearRegression.html#outlieranalysisexpand",
    "content": "Outlier Analysis\r\n(Expand) Cook’s Distances and Leverage Values… The presence of outlying points in a regression can bias the regression estimates substantially. In simple linear regressions, the outlier are usually quite visible in a residuals vs. fitted-values plot. However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression. Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model. Cook’s Distances The idea behind Cook’s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\). As found in the original article “Detection of Influential Observation in Linear Regression” (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression. To understand this formula, let’s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\). Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression. See the image below for a visual explanation. X <- c(2,3,5,6,8,13) Y <- c(3,5,7,9,8,12) plot(Y ~ X, pch=16, col=\"skyblue\", ylim=c(0,14)) points(X[4],Y[4], pch=16, cex=1.1, col=\"orange\") lm1 <- lm(Y ~ X) lm2 <- lm(Y ~ X, w=c(1,1,1,0,1,1)) abline(lm1, col=\"skyblue\", lwd=2) abline(lm2, col=\"orange\", lwd=2) legend(\"topleft\", legend=c(\"All Points Included\", \"Orange Point Removed\"), lty=1, col=c(\"skyblue\",\"orange\"), bty=\"n\") for (i in 1:6){ lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=\"skyblue\") lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=\"orange\") } pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2))   1 2 3 4 5 6 Original Residuals -1.23 0.02 0.53 1.79 -0.7 -0.42 Orange Point Removed -0.86 0.4 0.9 2.15 -0.35 -0.09 Difference -0.38 -0.37 -0.36 -0.36 -0.35 -0.33 Squaring the sum of the “differences” in the residuals from the original regression and the one where point \\(i\\) (the orange dot) has been removed gives \\(0.77186\\). Then, noting that the MSE for the original regression was \\(1.418605\\), and that \\(p=2\\) because there were two parameters, we find the Cook’s Distance for Point #4 comes out to be \\[ D_4 = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(4)})^2}{p\\cdot MSE} \\approx \\frac{0.77186}{2\\cdot 1.418605} \\approx 0.272 \\] Similar calculations show the Cook’s Distances for each point to be pander(round(cooks.distance(lm1),3), caption=\"Cook's Distances for each Point 1, ..., 6\") 1 2 3 4 5 6 0.551 0 0.028 0.272 0.057 0.807 In R, it is simple to calculate Cook’s Distances using the code cooks.distance(lmObject). Also, a graph of Cook’s Distances can be obtained using plot(lmObject, which=4) as shown here: plot(lm1, which=4) Leverage Values The leverage value of a point is a measurement that lives between 0 and 1 where values close to 1 imply the point has a lot of “leverage” and is “pulling” the regression toward itself. A value near 0 implies the point is just “one of many” and that it is not unduly influencing the regression line. It is difficult to understand leverage values mathematically unless we look at regression from a linear algebra (matrix) perspective. To do this, first recall the simple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] This could be expanded to explicity list out each value of \\(i\\) in the model using vector notation: \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] We could then rewrite this in matrix notation using \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] Or, more concisely as \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] The goal of regression is to choose values for \\(\\beta_0\\) and \\(\\beta_1\\) that “minimize” the sum of the squared errors. Mathematically this would be written as \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] If you are familiar with vectors then you would see that this could be written with the notation \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] And since we can also write \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] then we have \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] To choose the values of \\(\\vec{\\beta}\\) that minimize the above equation, we will take the derivative with respect to \\(\\vec{\\beta}\\) which turns out to give \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] Setting the derivative equal to the zero vector \\(\\vec{0}\\) and solving, we obtain \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\] Since \\(\\mathbf{X}^t\\mathbf{X}\\) is a square matrix, it is invertible. This allows us to solve for \\(\\vec{\\beta}\\) by \\[ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] However, at this point istead of pretending we have found the true \\(\\beta\\)’s, we change the equation to \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] Then, if we use the equation for \\(\\hat{Y}_i\\) in vector notation, we get \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] and substituting into \\(\\vec{b}\\) gives \\[ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] This shows the \\(\\hat{Y}\\) values are a matrix transformation of the \\(Y\\) values, often called a projection of \\(Y\\) onto the \\(\\hat{Y}\\) surface. But now we have arrived at the thing we wanted to look at in order to talk about leverage, the “hat matrix” \\(\\mathbf{H}\\): \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] This allows us to write \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] The diagonal elements of \\(\\mathbf{H}\\) are the “leverage values” and are notated as the \\(h_{ii}\\) values. Essentially each of these values explain how much \\(\\hat{Y}_i\\) is being pulled towards \\(Y_i\\) by each \\(Y_i\\), where values of \\(h_{ii}\\) close to 1 represent a “lot of pull,” and values close to 0 represent “little pull.” In R these values are obtained by the hatvalues(...) function: hatvalues(lm1) %>% pander() 1 2 3 4 5 6 0.3869 0.2939 0.1839 0.167 0.2093 0.759 Or, graphically depicted by plot(lmObject, which=5) plot(lm1, which=5) Points with “lots of leverage” and a large “Cook’s Distance” are points that should be investigated for accuracy and possibly removed (or downweighted) in the regression.",
    "sentences": ["Cook’s Distances and Leverage Values…", "The presence of outlying points in a regression can bias the regression estimates substantially.", "In simple linear regressions, the outlier are usually quite visible in a residuals vs. fitted-values plot.", "However, in higher dimensional regression models, it can become very difficult to locate points that are negatively effecting the regression.", "Here are two measurements that are helpful in identifying points that are negatively impacting an estimated regression model.", "Cook’s Distances The idea behind Cook’s Distance is to measure the impact each individual point has on the regression estimates \\(b_i\\) for each \\(\\beta_i\\).", "As found in the original article “Detection of Influential Observation in Linear Regression” (Dennis Cook, 1977) the formula Cook developed for measuring this effect is given by (when adapted to fit the notation of this book) \\[ D_i = \\frac{\\sum_{j=1}^n (\\widehat{Y}_{j} - \\widehat{Y}_{j(i)})^2}{p\\cdot MSE} \\] where \\(p\\) is the number of parameters in the regression model, \\(MSE\\) is the estimate of \\(\\sigma^2\\) (the mean squared error), and \\(\\hat{Y}_{j(i)}\\) represents the residual for point \\(j\\) when the \\(i\\)th point was removed from the regression.", "To understand this formula, let’s focus first on the numerator: \\(\\sum_{j=1}^n \\widehat{Y}_j - \\widehat{Y}_{j(i)}\\).", "Here, we are comparing the residual from the original regression for point \\(j\\), \\(\\widehat{Y}_j\\) to the modified value of that same residual when point \\(i\\) is removed from the regression.", "See the image below for a visual explanation."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Outlier Analysis\r\n(Expand)"
  },
  {
    "id": 141,
    "title": "📍 Inference for the Model Parameters\r\n(Expand)",
    "url": "LinearRegression.html#inferenceforthemodelparametersexpand",
    "content": "Inference for the Model Parameters\r\n(Expand) t Tests and F tests in multiple regression… Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously. t Tests The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance. F Tests Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously. The most commonly used F Test is the one given by the hypotheses \\[ H_0: \\beta_1 = \\cdots = \\beta_p = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j \\in \\{1,\\ldots,p\\} \\] However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class.",
    "sentences": ["t Tests and F tests in multiple regression…", "Inference in the multiple regression model can be for any of the model coefficients, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_p\\) or for several coefficients simultaneously.", "t Tests The most typical tests for multiple regression are t Tests for a single coefficient.", "The hypotheses for these t Tests are written as \\[ H_0: \\beta_j = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\] Note that these hypotheses assume that all other variables (and coefficients) are already in the model.", "The significance of the single variable is thus assessed after accounting for the effect of all other variables.", "If a t Test of a single coefficient is significant, then that variable should remain in the model.", "If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides.", "Removing it from the model may be appropriate.", "However, whenever a single variable is removed from the model the other variables can change in their significance.", "F Tests Another approach to testing hypotheses about coefficients is to use an F Test."],
    "type": "section",
    "page_title": "Linear Regression",
    "section_title": "Inference for the Model Parameters\r\n(Expand)"
  },
  {
    "id": 142,
    "title": "Logistic Regression",
    "url": "LogisticRegression.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Logistic Regression Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\). The explanatory variables can be either quantitative or qualitative. Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The “P” stands for “Probability that…” \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1… This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)… in other words, the “|” says “given” and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the “natural constant” number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14…) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -1.5651 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.6648 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -0.2460 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   0.7276 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   2.2691 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The test statistic is a regular old z-score. It is most reliable when the sample size is “large.” It is a measurement of the number of standard errors the estimate is from 0.   Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds.   1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, …).   -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds.   0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a “star”. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about.   Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\).  43.230  on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates.  29.732 This can be calculated by sum(log(myglm$res^2)).  on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, “A version of Akaike’s An Information Criterion…” The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model.  33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 ## disp -0.014604 0.005168 -2.826 0.00471 ## ## (Intercept) * ## disp ** ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the lo",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Logistic Regression Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\).", "The explanatory variables can be either quantitative or qualitative.", "Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable.", "Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The “P” stands for “Probability that…” \\(Y_i\\) The response variable.", "The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size.", "\\(= 1\\) Equals 1… This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual.", "\\(| x_i)\\) Given \\(x_i\\)… in other words, the “|” says “given” and \\(x_i\\) means the x-value of the current individual.", "\\(=\\) Equals sign.", "\\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the “natural constant” number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope.", "\\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value.", "It is the short hand notation for \\(P(Y_i = 1 |x_i)\\).", "(It is NOT the number 3.14…) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly.", "Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead.", "The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\).", "The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\).", "Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.", " <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName.", "glm( glm( is an R function that stands for “General Linear Model”.", "It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command.", "Y  Y is your binary response variable.", "It must consist of only 0’s and 1’s.", "Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s.", "~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X.", "X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1.", " data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X.", "In other words, one column of your dataset would be called Y and another column would be called X.", " family=binomial) The family=binomial command tells the glm( function to perform a logistic regression.", "It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course.", "summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName.", "Example output from a regression.", "Hover each piece to learn more.", "Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(…) “call” that you made when performing your regression.", "It allows you to verify that you ran what you thought you ran in the glm(…).", "Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space.", "(This is a fairly complicated idea.) Min   -1.5651 “min” gives the value of the residual that is furthest below the regression line.", "Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line.", "1Q   -0.6648 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile.", "Median   -0.2460 “Median” gives the median of the residuals, which would ideally would be about equal to zero.", "Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary.", "This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed.", "This can also be seen in the maximum being much larger in magnitude than the minimum.", "3Q   0.7276 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile.", "In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally.", "Max   2.2691 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual.", "In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed.", "Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\).", "You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model.", "These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.", "  Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details.", "Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section."],
    "type": "page",
    "page_title": "Logistic Regression"
  },
  {
    "id": 143,
    "title": "📍 Simple Logistic Regression\r\nModel",
    "url": "LogisticRegression.html#simplelogisticregressionmodel",
    "content": "Simple Logistic Regression\r\nModel Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The “P” stands for “Probability that…” \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1… This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)… in other words, the “|” says “given” and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the “natural constant” number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14…) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse",
    "sentences": ["Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable.", "Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The “P” stands for “Probability that…” \\(Y_i\\) The response variable.", "The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size.", "\\(= 1\\) Equals 1… This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual.", "\\(| x_i)\\) Given \\(x_i\\)… in other words, the “|” says “given” and \\(x_i\\) means the x-value of the current individual.", "\\(=\\) Equals sign.", "\\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the “natural constant” number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope.", "\\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value.", "It is the short hand notation for \\(P(Y_i = 1 |x_i)\\).", "(It is NOT the number 3.14…) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Simple Logistic Regression\r\nModel"
  },
  {
    "id": 144,
    "title": "📍 Overview",
    "url": "LogisticRegression.html#overview",
    "content": "Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The “P” stands for “Probability that…” \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1… This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)… in other words, the “|” says “given” and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the “natural constant” number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14…) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse",
    "sentences": ["The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The “P” stands for “Probability that…” \\(Y_i\\) The response variable.", "The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size.", "\\(= 1\\) Equals 1… This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual.", "\\(| x_i)\\) Given \\(x_i\\)… in other words, the “|” says “given” and \\(x_i\\) means the x-value of the current individual.", "\\(=\\) Equals sign.", "\\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the “natural constant” number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope.", "\\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value.", "It is the short hand notation for \\(P(Y_i = 1 |x_i)\\).", "(It is NOT the number 3.14…) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly.", "Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Overview"
  },
  {
    "id": 145,
    "title": "📍 R Instructions",
    "url": "LogisticRegression.html#rinstructions",
    "content": "R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -1.5651 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.6648 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -0.2460 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   0.7276 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   2.2691 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The test statistic is a regular old z-score. It is most reliable when the sample size is “large.” It is a measurement of the number of standard errors the estimate is from 0.   Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds.   1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, …).   -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds.   0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a “star”. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about.   Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\).  43.230  on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates.  29.732 This can be calculated by sum(log(myglm$res^2)).  on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, “A version of Akaike’s An Information Criterion…” The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model.  33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 ## disp -0.014604 0.005168 -2.826 0.00471 ## ## (Intercept) * ## disp ** ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0’s or 1’s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == “B” or height < 68. In other words, Y needs to be a collection of 0’s and 1’s.  ~  The tilde is read “Y on X.” X, X is some quantitative variable.  data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(…) function. curve( A function in R that draws a “curve” on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the “Intercept” estimate from your logistic regression summary output.  \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case “x” in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula.  exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator.  add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(…) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set.  aes( The aes(…) function stands for “aesthetics” and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The plus sign adds a new layer to the ggplot.   geom_point( Tells the plot to add the physical geometry of “points” to the plot. ) Closing parenthesis for the geom_point() function.  + Add another layer to the plot.   geom_smooth( Add a smoothing line to the graph. method=“glm”, This adds a general linear model to the graph. method.args = list(family=“binomial”), This tells the method=“glm” to choose specifically the “binomial” model, otherwise known as the “logistic regression” model.  se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function.  + Add another layer to the ggplot.   theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),  The xVariableName is the same one you used in your glm(y ~ x, …) statement for “x”. Input any desired value for the “someNumericValue” spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016",
    "sentences": ["Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.", " <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName.", "glm( glm( is an R function that stands for “General Linear Model”.", "It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command.", "Y  Y is your binary response variable.", "It must consist of only 0’s and 1’s.", "Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s.", "~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X.", "X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1.", " data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "R Instructions"
  },
  {
    "id": 146,
    "title": "📍 Perform a Logistic Regression",
    "url": "LogisticRegression.html#performalogisticregression",
    "content": "Perform a Logistic Regression Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -1.5651 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.6648 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -0.2460 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   0.7276 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   2.2691 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The test statistic is a regular old z-score. It is most reliable when the sample size is “large.” It is a measurement of the number of standard errors the estimate is from 0.   Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds.   1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, …).   -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds.   0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a “star”. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about.   Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\).  43.230  on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates.  29.732 This can be calculated by sum(log(myglm$res^2)).  on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, “A version of Akaike’s An Information Criterion…” The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model.  33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required.",
    "sentences": ["Example output from a regression.", "Hover each piece to learn more.", "Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(…) “call” that you made when performing your regression.", "It allows you to verify that you ran what you thought you ran in the glm(…).", "Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space.", "(This is a fairly complicated idea.) Min   -1.5651 “min” gives the value of the residual that is furthest below the regression line.", "Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line.", "1Q   -0.6648 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile.", "Median   -0.2460 “Median” gives the median of the residuals, which would ideally would be about equal to zero.", "Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Perform a Logistic Regression"
  },
  {
    "id": 147,
    "title": "📍 Diagnose the Goodness-of-Fit",
    "url": "LogisticRegression.html#diagnosethegoodnessoffit",
    "content": "Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 ## disp -0.014604 0.005168 -2.826 0.00471 ## ## (Intercept) * ## disp ** ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression.",
    "sentences": ["There are two ways to check the goodness of fit of a logistic regression model.", "Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test.", "library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function.", "You may need to run the code: install.packages(“ResourceSelection”) first.", "hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test.", "See the “Explanation” file to learn about this test.", "YourGlmName YourGlmName is the name of your glm(…) code that you created previously.", "$y,  ALWAYS type a “y” here.", "This gives you the actual binary (0,1) y-values of your logistic regression.", "The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Diagnose the Goodness-of-Fit"
  },
  {
    "id": 148,
    "title": "📍 Plot the Regression",
    "url": "LogisticRegression.html#plottheregression",
    "content": "Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0’s or 1’s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == “B” or height < 68. In other words, Y needs to be a collection of 0’s and 1’s.  ~  The tilde is read “Y on X.” X, X is some quantitative variable.  data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(…) function. curve( A function in R that draws a “curve” on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the “Intercept” estimate from your logistic regression summary output.  \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case “x” in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula.  exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator.  add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot.",
    "sentences": ["Base R ggplot2", "plot( The plot function allows us to draw a scatterplot where the y-axis is only 0’s or 1’s and the x-axis is quantitative.", "Y Y should be some logical statement like Fail > 0 or sex == “B” or height < 68.", "In other words, Y needs to be a collection of 0’s and 1’s.", " ~  The tilde is read “Y on X.” X, X is some quantitative variable.", " data= Tell the plot which data set to use.", "X and Y are columns of that data set.", "YourDataSet The name of your data set.", ") Closing parenthesis for plot(…) function.", "curve( A function in R that draws a “curve” on a plot."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Plot the Regression"
  },
  {
    "id": 149,
    "title": "📍 Predict Probabilities",
    "url": "LogisticRegression.html#predictprobabilities",
    "content": "Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016",
    "sentences": ["To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code", "myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response\") ## 1 ## 0.4280016"],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Predict Probabilities"
  },
  {
    "id": 150,
    "title": "📍 Explanation",
    "url": "LogisticRegression.html#explanation",
    "content": "Explanation Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal). The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\). Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page. Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful. Checking Model Assumptions The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit. Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease.",
    "sentences": ["Simple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal).", "The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario.", "The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\).", "Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation.", "The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\).", "It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation.", "The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\).", "Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page.", "Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression.", "If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\)."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Explanation"
  },
  {
    "id": 151,
    "title": "📍 The Model",
    "url": "LogisticRegression.html#themodel",
    "content": "The Model Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario. The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\).",
    "sentences": ["Since \\(Y_i\\) is binary (can only be 0 or 1) the model focuses on describing the probability that \\(Y_i=1\\) for a given scenario.", "The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation", "\\[ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\]", "The assumption is that for certain values of \\(X\\) the probability that \\(Y_i=1\\) is higher than for other values of \\(X\\)."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "The Model"
  },
  {
    "id": 152,
    "title": "📍 Interpretation",
    "url": "LogisticRegression.html#interpretation",
    "content": "Interpretation This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation. The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\). It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation. The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\). Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page.",
    "sentences": ["This model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_i}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_i}^{\\text{linear regression}}} = e^{\\beta_0}e^{\\beta_1 x_i} \\] Thus, while the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly, \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) have a valuable interpretation.", "The value of \\(e^{\\beta_0}\\) is interpreted as the odds for \\(Y_i=1\\) when \\(x_i = 0\\).", "It may not be possible for a given model to have \\(x_i=0\\), in which case \\(e^{\\beta_0}\\) has no interpretation.", "The value of \\(e^{\\beta_1}\\) denotes the proportional change in the odds that \\(Y_i=1\\) for every one unit increase in \\(x_i\\).", "Notice that solving the last equation for \\(\\pi_i\\) results in the logistic regression model presented at the beginning of this page."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Interpretation"
  },
  {
    "id": 153,
    "title": "📍 Hypothesis Testing",
    "url": "LogisticRegression.html#hypothesistesting",
    "content": "Hypothesis Testing Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression. If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\). In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\). If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful.",
    "sentences": ["Similar to linear regression, the hypothesis that \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] can be tested with a logistic regression.", "If \\(\\beta_1 = 0\\), then there is no relationship between \\(x_i\\) and the log of the odds that \\(Y_i = 1\\).", "In other words, \\(x_i\\) is not useful in predicting the probability that \\(Y_i = 1\\).", "If \\(\\beta_1 \\neq 0\\), then there is information in \\(x_i\\) that can be utilized to predict the probability that \\(Y_i = 1\\), i.e., the logistic regression is meaningful."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Hypothesis Testing"
  },
  {
    "id": 154,
    "title": "📍 Checking Model Assumptions",
    "url": "LogisticRegression.html#checkingmodelassumptions",
    "content": "Checking Model Assumptions The model assumptions are not as clear in logistic regression as they are in linear regression. For our purposes we will focus only on considering the goodness of fit of the logistic regression model. If the model appears to fit the data well, then it will be assumed to be appropriate. Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses. In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit.",
    "sentences": ["The model assumptions are not as clear in logistic regression as they are in linear regression.", "For our purposes we will focus only on considering the goodness of fit of the logistic regression model.", "If the model appears to fit the data well, then it will be assumed to be appropriate.", "Deviance Goodness of Fit Test If there are replicated values of each \\(x_i\\), then the deviance goodness of fit test tests the hypotheses \\[ H_0: \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] \\[ H_a: \\pi_i \\neq \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\] Hosmer-Lemeshow Goodness of Fit Test If there are very few or no replicated values of each \\(x_i\\), then the Hosmer-Lemeshow goodness of fit test can be used to test these same hypotheses.", "In each case, the null assumes that logistic regression is a good fit for the data while the alternative is that logistic regression is not a good fit."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Checking Model Assumptions"
  },
  {
    "id": 155,
    "title": "📍 Prediction",
    "url": "LogisticRegression.html#prediction",
    "content": "Prediction One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\). This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual. For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease.",
    "sentences": ["One of the great uses of Logistic Regression is that it provides an estimate of the probability that \\(Y_i=1\\) for a given value of \\(x_i\\).", "This probability is often referred to as the risk that \\(Y_i=1\\) for a certain individual.", "For example, if \\(Y_i=1\\) implies a person has a disease, then \\(\\pi_i=P(Y_i=1)\\) represents the risk of individual \\(i\\) having the disease based on their value of \\(x_i\\), perhaps a measure of their cholesterol or some other predictor of the disease."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Prediction"
  },
  {
    "id": 156,
    "title": "📍 Multiple Logistic Regression\r\nModel",
    "url": "LogisticRegression.html#multiplelogisticregressionmodel",
    "content": "Multiple Logistic Regression\r\nModel Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two. Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. …, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -2.9446 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.2364 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.0000 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q   0.2776 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   1.9968 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The Z-score testing the hypothesis that \\(\\beta_j=0\\)   Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, …).   0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\).   1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\).   4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\).   3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\).   0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\).   0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a “star”. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\).   471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option.   Null Deviance: The deviance of the null model.  800.44 In this case, the null deviance is 800.44  on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression.  256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit.  on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model.  272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities ## numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) -4.97603 0.65316 -7.618 ## Time 0.39111 0.04979 7.854 ## Diet2 0.58283 1.04061 0.560 ## Diet3 -8.44476 4.32324 -1.953 ## Diet4 -67.41995 3768.10023 -0.018 ## Time:Diet2 0.02391 0.08679 0.275 ## Time:Diet3 1.20955 0.51249 2.360 ## Time:Diet4 8.83168 471.01259 0.019 ## Pr(>|z|) ## (Intercept) 2.57e-14 *** ## Time 4.02e-15 *** ## Diet2 0.5754 ## Diet3 0.0508 . ## Diet4 0.9857 ## Time:Diet2 0.7830 ## Time:Diet3 0.0183 * ## Time:Diet4 0.9850 ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, …), X2 = c(Value 1, Value 2, …), …) command. You should see the GSS example file for an example of how to use this function. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the “b” vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(“SomeColor”,“DifferentColor”,…) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(…) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==“B”.  ~  The formula operator in R. X1, The first x-variable in your glm code.  data = YourDataSet, Specify the name of your data set.  pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(…) function computes e^(stuff) in R and stands for the “exponential function.” (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(…) function requires that you call the x-variable “x” although you can change this behavior using xname=“SomeOtherName” if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)).  col = palette()[1], Pull the first color from the color palette for the first curve.  add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters.  col = palette()[2], Set the color of this second curve to the second color in the palette.  add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. “topright”, Typically the legend is placed in the top-right corner of the graph. But it could be placed “top”, “topleft”, “left”, “bottomleft”, “bottom”, “center”, “right”, or “bottomright”.  legend = Why you have to write legend again, no one knows, but you do. c(“Lable 1”, “Label 2”), These are the values that will appear in the legend, one entry on each line of the legend.  col = palette(), This specifies the colors of the symbols in the legend. First color goes with “Label 1”, and second goes with “Label 2” and so on.  lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist.  bty = Specifies the “box type” (bty) that should be drawn around the legend. ‘n’) By placing a “no” option (‘n’) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot.  aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + Add a layer to the ggplot.   geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function.  + Add a layer to the ggplot.   geom_smooth(method=“glm”, method.args=list(family=“binomial”), se=FALSE, Add the logistic regression curve to the plot.   aes(color=“X2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function.  + Add a layer to the plot.   theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x'",
    "sentences": ["Logistic regression for multiple explanatory variables that can either be quantitative or qualitative or a mixture of the two.", "Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly.", "Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead.", "The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\).", "The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\).", "Examples: GSS", "R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.", " <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName.", "glm( glm( is an R function that stands for “General Linear Model”.", "It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Multiple Logistic Regression\r\nModel"
  },
  {
    "id": 157,
    "title": "📍 Overview",
    "url": "LogisticRegression.html#overview",
    "content": "Overview The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly. Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead. The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: GSS",
    "sentences": ["The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The coefficents \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\) are difficult to interpret directly.", "Typically \\(e^{\\beta_k}\\) for \\(k=0,1,\\ldots,p\\) is interpreted instead.", "The value of \\(e^{\\beta_k}\\) denotes the relative change in the odds that \\(Y_i=1\\).", "The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\).", "Examples: GSS"],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Overview"
  },
  {
    "id": 158,
    "title": "📍 R Instructions",
    "url": "LogisticRegression.html#rinstructions",
    "content": "R Instructions Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1. * The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2. X2  X2 is second the explanatory variable either quantitative or qualitative that will be used to explain the probability that the response variable Y is a 1. …, In theory, you could have many other explanatory variables, interaction terms, or even squared, cubed, or other transformations of terms added to this model.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -2.9446 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.2364 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.0000 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q   0.2776 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   1.9968 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The Z-score testing the hypothesis that \\(\\beta_j=0\\)   Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, …).   0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\).   1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\).   4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\).   3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\).   0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\).   0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a “star”. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\).   471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option.   Null Deviance: The deviance of the null model.  800.44 In this case, the null deviance is 800.44  on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression.  256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit.  on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model.  272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities ## numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) -4.97603 0.65316 -7.618 ## Time 0.39111 0.04979 7.854 ## Diet2 0.58283 1.04061 0.560 ## Diet3 -8.44476 4.32324 -1.953 ## Diet4 -67.41995 3768.10023 -0.018 ## Time:Diet2 0.02391 0.08679 0.275 ## Time:Diet3 1.20955 0.51249 2.360 ## Time:Diet4 8.83168 471.01259 0.019 ## Pr(>|z|) ## (Intercept) 2.57e-14 *** ## Time 4.02e-15 *** ## Diet2 0.5754 ## Diet3 0.0508 . ## Diet4 0.9857 ## Time:Diet2 0.7830 ## Time:Diet3 0.0183 * ## Time:Diet4 0.9850 ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). NewDataFrame,  Typically, NewDataFrame is created in real time using the data.frame( X1 = c(Value 1, Value 2, …), X2 = c(Value 1, Value 2, …), …) command. You should see the GSS example file for an example of how to use this function. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. ## 1 ## 0.7314498 Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the “b” vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(“SomeColor”,“DifferentColor”,…) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(…) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==“B”.  ~  The formula operator in R. X1, The first x-variable in your glm code.  data = YourDataSet, Specify the name of your data set.  pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(…) function computes e^(stuff) in R and stands for the “exponential function.” (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(…) function requires that you call the x-variable “x” although you can change this behavior using xname=“SomeOtherName” if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)).  col = palette()[1], Pull the first color from the color palette for the first curve.  add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters.  col = palette()[2], Set the color of this second curve to the second color in the palette.  add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. “topright”, Typically the legend is placed in the top-right corner of the graph. But it could be placed “top”, “topleft”, “left”, “bottomleft”, “bottom”, “center”, “right”, or “bottomright”.  legend = Why you have to write legend again, no one knows, but you do. c(“Lable 1”, “Label 2”), These are the values that will appear in the legend, one entry on each line of the legend.  col = palette(), This specifies the colors of the symbols in the legend. First color goes with “Label 1”, and second goes with “Label 2” and so on.  lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist.  bty = Specifies the “box type” (bty) that should be drawn around the legend. ‘n’) By placing a “no” option (‘n’) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n') ggplot(data=YourDataName, Initialize a ggplot.  aes( Declare the aesthetics of the graph. x=X1, Set the x-axis to be your X1 variable. y=Y Set the y-axis to be your Y-variable. ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + Add a layer to the ggplot.   geom_point( Add dots to the ggplot. ) Closing parenthesis for the geom_point() function.  + Add a layer to the ggplot.   geom_smooth(method=“glm”, method.args=list(family=“binomial”), se=FALSE, Add the logistic regression curve to the plot.   aes(color=“X2) Create a different logistic regression curve for each group found in X2, and give each curve a different color. ) Closing parenthesis for the geom_smooth() function.  + Add a layer to the plot.   theme_bw() Add a black and white them to the plot. ggplot(ChickWeight, aes(Time, as.numeric(weight>100))) + geom_point() + stat_smooth(aes(color = Diet), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + theme_bw() + labs(y = \"weight>100\") ## `geom_smooth()` using formula = 'y ~ x'",
    "sentences": ["Console Help Command: ?glm() Perform the Logistic Regression To perform a logistic regression in R use the commands YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.", " <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName.", "glm( glm( is an R function that stands for “General Linear Model”.", "It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command.", "Y  Y is your binary response variable.", "It must consist of only 0’s and 1’s.", "Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s.", "~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X.", "X1 X1 is the first explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1.", "* The times symbol allows a shortcut for writing X1 + X2 + X1:X2 = X1*X2."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "R Instructions"
  },
  {
    "id": 159,
    "title": "📍 Perform the Logistic Regression",
    "url": "LogisticRegression.html#performthelogisticregression",
    "content": "Perform the Logistic Regression To perform a logistic regression in R use the commands Example output from a regression. Hover each piece to learn more. Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -2.9446 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.2364 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   0.0000 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. 3Q   0.2776 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   1.9968 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The Z-score testing the hypothesis that \\(\\beta_j=0\\)   Pr(>|z|) The p-value for the corresponding Z-score. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -4.97603 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   0.65316 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -7.618 Z-score for \\(b_0\\). 2.57e-14 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a Z-value as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Time This is always the name of your X-variable in your glm(Y ~ X, …).   0.39111 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.04979 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 7.854 Z-score for \\(b_1\\). 4.02e-15 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.58283 This is the estimate of the change in the intercept, \\(\\beta_2\\). It is called \\(b_2\\).   1.04061 This is the standard error of \\(b_2\\). It tells you how much \\(b_2\\) varies from sample to sample. The closer to zero, the better. 0.560 Z-score for \\(b_2\\). 0.5754 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   -8.44476 This is the estimate of the change in the intercept, \\(\\beta_3\\). It is called \\(b_3\\).   4.32324 This is the standard error of \\(b_3\\). It tells you how much \\(b_3\\) varies from sample to sample. The closer to zero, the better. -1.953 Z-score for \\(b_3\\). 0.0508 This is the p-value of the test of the hypothesis that \\(\\beta_3 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 . This is called a period. It means significant at the 0.1 level of \\(\\alpha\\). Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   -67.41995 This is the estimate of the change in the intercept, \\(\\beta_4\\). It is called \\(b_4\\).   3768.10023 This is the standard error of \\(b_4\\). It tells you how much \\(b_4\\) varies from sample to sample. The closer to zero, the better. -0.018 Z-score for \\(b_4\\). 0.9857 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet2 This is always the name of your X-variable in your glm(Y ~ X, …).   0.02391 This is the estimate of the change in the slope, \\(\\beta_5\\). It is called \\(b_5\\).   0.08679 This is the standard error of \\(b_5\\). It tells you how much \\(b_5\\) varies from sample to sample. The closer to zero, the better. 0.275 Z-score for \\(b_5\\). 0.7830 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 Time:Diet3 This is always the name of your X-variable in your glm(Y ~ X, …).   1.20955 This is the estimate of the change in the slope, \\(\\beta_6\\). It is called \\(b_6\\).   0.51249 This is the standard error of \\(b_6\\). It tells you how much \\(b_6\\) varies from sample to sample. The closer to zero, the better. 2.360 Z-score for \\(b_6\\). 0.0183 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 * This is called a “star”. One star means significant at the 0.05 level of \\(\\alpha\\). Time:Diet4 This is always the name of your X-variable in your glm(Y ~ X, …).   8.83168 This is the estimate of the change in the slope, \\(\\beta_7\\). It is called \\(b_7\\).   471.01259 This is the standard error of \\(b_7\\). It tells you how much \\(b_7\\) varies from sample to sample. The closer to zero, the better. 0.019 Z-score for \\(b_7\\). 0.9850 This is the p-value of the test of the hypothesis that \\(\\beta_2 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) A simplifying assumption of the logistic regression. This can be changed if you know what you are doing. See ?summary.glm and look at the dispersion option for details. Unless you are pursuing an advanced degree in statistics, it is not recommended that you explore this option.   Null Deviance: The deviance of the null model.  800.44 In this case, the null deviance is 800.44  on 577 degrees of freedom The residual degrees of freedom for the null model. Residual deviance: The test statistic from the chi-squared goodness of fit test of the logistic regression.  256.30 This is computed by the sum(log(myglm$res^2)). As long as this value is similar in size to the degrees of freedom, then the model is a good fit.  on 570 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 578 and five parameters, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), \\(\\beta_5\\),\\(\\beta_6\\), and \\(\\beta_7\\) so 578-8 = 570. AIC: A form of the Akiake Information Criterion. Smaller values of the AIC indicate a better fitting model.  272.3 If a model for the same Y-variable can be found with an AIC value lower than 272.3, then it is a better fit of the data. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  20 This implementation of glm required 20 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required.",
    "sentences": ["To perform a logistic regression in R use the commands", "Example output from a regression.", "Hover each piece to learn more.", "Call: glm(formula = weight > 100 ~ Time*Diet, family = binomial, data = ChickWeight) This is simply a statement of your original glm(…) “call” that you made when performing your regression.", "It allows you to verify that you ran what you thought you ran in the glm(…).", "Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space.", "(This is a fairly complicated idea.) Min   -2.9446 “min” gives the value of the residual that is furthest below the regression line.", "Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line.", "1Q   -0.2364 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile.", "Median   0.0000 “Median” gives the median of the residuals, which would ideally would be about equal to zero."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Perform the Logistic Regression"
  },
  {
    "id": 160,
    "title": "📍 Diagnose the Goodness-of-Fit",
    "url": "LogisticRegression.html#diagnosethegoodnessoffit",
    "content": "Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## Warning: glm.fit: fitted probabilities ## numerically 0 or 1 occurred ## ## Hosmer and Lemeshow goodness of fit (GOF) ## test ## ## data: myglm$y, myglm$fitted ## X-squared = 18.212, df = 8, p-value = 0.0197 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test In some cases, there are many replicated \\(x\\)-values for all x-values. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) summary(myglm) ## ## Call: ## glm(formula = weight > 100 ~ Time * Diet, family = binomial, ## data = ChickWeight) ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) -4.97603 0.65316 -7.618 ## Time 0.39111 0.04979 7.854 ## Diet2 0.58283 1.04061 0.560 ## Diet3 -8.44476 4.32324 -1.953 ## Diet4 -67.41995 3768.10023 -0.018 ## Time:Diet2 0.02391 0.08679 0.275 ## Time:Diet3 1.20955 0.51249 2.360 ## Time:Diet4 8.83168 471.01259 0.019 ## Pr(>|z|) ## (Intercept) 2.57e-14 *** ## Time 4.02e-15 *** ## Diet2 0.5754 ## Diet3 0.0508 . ## Diet4 0.9857 ## Time:Diet2 0.7830 ## Time:Diet3 0.0183 * ## Time:Diet4 0.9850 ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 800.44 on 577 degrees of freedom ## Residual deviance: 256.30 on 570 degrees of freedom ## AIC: 272.3 ## ## Number of Fisher Scoring iterations: 20 pchisq(256.30, 570, lower.tail = TRUE) ## [1] 6.585816e-33 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression.",
    "sentences": ["There are two ways to check the goodness of fit of a logistic regression model.", "Option 1: Hosmer-Lemeshow Goodness-of-Fit Test To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test.", "library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function.", "You may need to run the code: install.packages(“ResourceSelection”) first.", "hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test.", "See the “Explanation” file to learn about this test.", "YourGlmName YourGlmName is the name of your glm(…) code that you created previously.", "$y,  ALWAYS type a “y” here.", "This gives you the actual binary (0,1) y-values of your logistic regression.", "The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Diagnose the Goodness-of-Fit"
  },
  {
    "id": 161,
    "title": "📍 Predict Probabilities",
    "url": "LogisticRegression.html#predictprobabilities",
    "content": "Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code ## 1 ## 0.7314498",
    "sentences": ["To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code", "## 1 ## 0.7314498"],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Predict Probabilities"
  },
  {
    "id": 162,
    "title": "📍 Plot the Regression",
    "url": "LogisticRegression.html#plottheregression",
    "content": "Plot the Regression Base R ggplot2 b <- coef(myglm) This stores the estimated coefficients from the regression into the “b” vector. palette( The palette() function allows you to specify the colors R chooses for the plot. c(“SomeColor”,“DifferentColor”,…) Specify as many colors for the palette as you wish. R will choose a color for each group that you specify later on in your plot(…) code. )) Closing parentheses. plot( Create the binary scatterplot for the logistic regression. Y Note that Y must be binary (0,1) values. If it is not, use a logical statement to make it binary, like height>60 or sex==“B”.  ~  The formula operator in R. X1, The first x-variable in your glm code.  data = YourDataSet, Specify the name of your data set.  pch = 16) Select the type of plotting characters to use for the plot. curve( Use the curve function to add the logistic regression curve to the plot. exp( The exp(…) function computes e^(stuff) in R and stands for the “exponential function.” (b[1]+ The first coefficient found in b is the y-intercept. b[2] The second coefficient found in b is the slope term. *x) The curve(…) function requires that you call the x-variable “x” although you can change this behavior using xname=“SomeOtherName” if you want. / The logistic model is e^(stuff) / (1 + e^(stuff)). (1+exp(b[1]+b[2]x)) The denominator of the logistic regression. Be careful to group the entire denominator (1+exp(stuff)).  col = palette()[1], Pull the first color from the color palette for the first curve.  add = TRUE) Add the curve to the current plot. curve(exp((b[1]+b[3])+(b[2]+b[4])x) Note how the numerator of this second logistic curve uses and adjusted y-intercept (b[1]+b[3]) and an adjusted slope (b[2]+b[4])x. / Begin the denominator. (1+exp((b[1]+b[3])+(b[2]+b[4])x)) The denominator consists of (1+exp(stuff)). , Begin optional parameters.  col = palette()[2], Set the color of this second curve to the second color in the palette.  add = TRUE) Add the curve to the current plot. legend( The legend function adds a legend to the current plot. “topright”, Typically the legend is placed in the top-right corner of the graph. But it could be placed “top”, “topleft”, “left”, “bottomleft”, “bottom”, “center”, “right”, or “bottomright”.  legend = Why you have to write legend again, no one knows, but you do. c(“Lable 1”, “Label 2”), These are the values that will appear in the legend, one entry on each line of the legend.  col = palette(), This specifies the colors of the symbols in the legend. First color goes with “Label 1”, and second goes with “Label 2” and so on.  lty = 1, Specifies that solid lines should be used in the legend. If you wanted dots instead, use pch=16. If you wanted dashed lines, use lty=2. Many other options exist.  bty = Specifies the “box type” (bty) that should be drawn around the legend. ‘n’) By placing a “no” option (‘n’) here, no box is drawn around the legend. myglm <- glm(weight>100 ~ Time*Diet, data = ChickWeight, family = binomial) palette(c(\"firebrick\", \"green\", \"skyblue\", \"darkorchid4\")) b <- myglm$coefficients plot(weight>100 ~ Time, data = ChickWeight, pch=16) curve(exp(b[1]+b[2]*x)/(1+exp(b[1]+b[2]*x)), add=TRUE, col = palette()[1]) curve(exp((b[1]+b[3])+(b[2]+b[6])*x)/(1+exp((b[1]+b[3])+(b[2]+b[6])*x)), add=TRUE, col = palette()[2]) curve(exp((b[1]+b[4])+(b[2]+b[7])*x)/(1+exp((b[1]+b[4])+(b[2]+b[7])*x)), add=TRUE, col = palette()[3]) curve(exp((b[1]+b[5])+(b[2]+b[8])*x)/(1+exp((b[1]+b[5])+(b[2]+b[8])*x)), add=TRUE, col = palette()[4]) legend(\"topleft\", legend = c(\"Diet 1\", \"Diet 2\", \"Diet 3\", \"Diet 4\"), col = palette(), lty = 1, bty = 'n')",
    "sentences": ["Base R ggplot2", "b <- coef(myglm) This stores the estimated coefficients from the regression into the “b” vector.", "palette( The palette() function allows you to specify the colors R chooses for the plot.", "c(“SomeColor”,“DifferentColor”,…) Specify as many colors for the palette as you wish.", "R will choose a color for each group that you specify later on in your plot(…) code.", ")) Closing parentheses.", "plot( Create the binary scatterplot for the logistic regression.", "Y Note that Y must be binary (0,1) values.", "If it is not, use a logical statement to make it binary, like height>60 or sex==“B”.", " ~  The formula operator in R."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Plot the Regression"
  },
  {
    "id": 163,
    "title": "📍 Explanation",
    "url": "LogisticRegression.html#explanation",
    "content": "Explanation Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative. The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations. Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant. Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression. Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\).",
    "sentences": ["Multiple Logistic Regression is used when the response variable is binary \\((Y_i=0\\) or \\(1)\\), and there are multiple explanatory variables \\(X_1,\\ldots,X_p\\) that can be either quantitative or qualitative.", "The Model Very little changes in multiple logistic regression from Simple Logistic Regression.", "The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations.", "Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant.", "Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression.", "Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression.", "The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\)."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Explanation"
  },
  {
    "id": 164,
    "title": "📍 The Model",
    "url": "LogisticRegression.html#themodel",
    "content": "The Model Very little changes in multiple logistic regression from Simple Logistic Regression. The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation \\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\] The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations.",
    "sentences": ["Very little changes in multiple logistic regression from Simple Logistic Regression.", "The probability that \\(Y_i = 1\\) given the observed data \\((x_{i1},\\ldots,x_{ip})\\) is called \\(\\pi_i\\) and is modeled by the expanded equation", "\\[ P(Y_i = 1|\\, x_{i1},\\ldots,x_{ip}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} }} = \\pi_i \\]", "The assumption is that for certain combinations of \\(X_1,\\ldots,X_p\\) the probability that \\(Y_i=1\\) is higher than for other combinations."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "The Model"
  },
  {
    "id": 165,
    "title": "📍 Interpretation",
    "url": "LogisticRegression.html#interpretation",
    "content": "Interpretation The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant.",
    "sentences": "The model for \\(\\pi_i\\) comes from modeling the log of the odds that \\(Y_i=1\\) using a linear regression, i.e., \\[ \\log\\underbrace{\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)}_{\\text{Odds for}\\ Y_i=1} = \\underbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}_{\\text{linear regression}} \\] Beginning to solve this equation for \\(\\pi_i\\) leads to the intermediate, but important result that \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{\\overbrace{\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}}^{\\text{liear regression}}} = e^{\\beta_0}e^{\\beta_1 x_{i1}}\\cdots e^{\\beta_p x_{ip}} \\] As in Simple Linear Regression, the values of \\(e^{\\beta_0}\\), \\(e^{\\beta_1}\\), \\(\\ldots\\), \\(e^{\\beta_p}\\) are interpreted as the proportional change in odds for \\(Y_i=1\\) when a given \\(x\\)-variable experiences a unit change, all other variables being held constant.",
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Interpretation"
  },
  {
    "id": 166,
    "title": "📍 Checking the Model Assumptions",
    "url": "LogisticRegression.html#checkingthemodelassumptions",
    "content": "Checking the Model Assumptions Diagnostics are the same in multiple logistic regression as they are in simple logistic regression.",
    "sentences": "Diagnostics are the same in multiple logistic regression as they are in simple logistic regression.",
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Checking the Model Assumptions"
  },
  {
    "id": 167,
    "title": "📍 Prediction",
    "url": "LogisticRegression.html#prediction",
    "content": "Prediction The idea behind prediction in multiple logistic regression is the same as in simple logistic regression. The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\).",
    "sentences": ["The idea behind prediction in multiple logistic regression is the same as in simple logistic regression.", "The only difference is that more than one explanatory variable is used to make the prediction of the risk that \\(Y_i=1\\)."],
    "type": "section",
    "page_title": "Logistic Regression",
    "section_title": "Prediction"
  },
  {
    "id": 168,
    "title": "Making Inference",
    "url": "MakingInference.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Making Inference It is common to only have a sample of data from some population of interest. Using the information from the sample to reach conclusions about the population is called making inference. When statistical inference is performed properly, the conclusions about the population are almost always correct. Hypothesis Testing One of the great focal points of statistics concerns hypothesis testing. Science generally agrees upon the principle that truth must be uncovered by the process of elimination. The process begins by establishing a starting assumption, or null hypothesis (\\(H_0\\)). Data is then collected and the evidence against the null hypothesis is measured, typically with the \\(p\\)-value. The \\(p\\)-value becomes small (gets close to zero) when the evidence is extremely different from what would be expected if the null hypothesis were true. When the \\(p\\)-value is below the significance level \\(\\alpha\\) (typically \\(\\alpha=0.05\\)) the null hypothesis is abandoned (rejected) in favor of a competing alternative hypothesis (\\(H_a\\)). Click for an Example The current hypothesis may be that the world is flat. Then someone who thinks otherwise sets sail in a boat, gathers some evidence, and when there is sufficient evidence in the data to disbelieve the current hypothesis, we conclude the world is not flat. In light of this new knowledge, we shift our belief to the next working hypothesis, that the world is round. After a while, someone gathers more evidence and shows that the world is not round, and we move to the next working hypothesis, that it is oblate spheroid, i.e., a sphere that is squashed at its poles and swollen at the equator. This process of elimination is called hypothesis testing. The process begins by establishing a null hypothesis (denoted symbolically by \\(H_0\\)) which represents the current opinion, status quo, or what we will believe if the evidence is not sufficient to suggest otherwise. The alternative hypothesis (denoted symbolically by \\(H_a\\)) designates what we will believe if there is sufficient evidence in the data to discredit, or “reject,” the null hypothesis. See the BYU-I Math 221 Stats Wiki for another example. Managing Decision Errors When the \\(p\\)-value approaches zero, one of two things must be occurring. Either an extremely rare event has happened or the null hypothesis is incorrect. Since the second option, that the null hypothesis is incorrect, is the more plausible option, we reject the null hypothesis in favor of the alternative whenever the \\(p\\)-value is close to zero. It is important to remember that rejecting the null hypothesis could however be a mistake.   \\(H_0\\) True \\(H_0\\) False Reject \\(H_0\\) Type I Error Correct Decision Accept \\(H_0\\) Correct Decision Type II Error Type I Error, Significance Level, Confidence and \\(\\alpha\\) A Type I Error is defined as rejecting the null hypothesis when it is actually true. (Throwing away truth.) The significance level, \\(\\alpha\\), of a hypothesis test controls the probability of a Type I Error. The typical value of \\(\\alpha = 0.05\\) came from tradition and is a somewhat arbitrary value. Any value from 0 to 1 could be used for \\(\\alpha\\). When deciding on the level of \\(\\alpha\\) for a particular study it is important to remember that as \\(\\alpha\\) increases, the probability of a Type I Error increases, and the probability of a Type II Error decreases. When \\(\\alpha\\) gets smaller, the probability of a Type I Error gets smaller, while the probability of a Type II Error increases. Confidence is defined as \\(1-\\alpha\\) or the opposite of a Type I error. That is the probability of accepting the NULL when it is in fact true. Type II Errors, \\(\\beta\\), and Power It is also possible to make a Type II Error, which is defined as failing to reject the null hypothesis when it is actually false. (Failing to move to truth.) The probability of a Type II Error, \\(\\beta\\), is often unknown. However, practitioners often make an assumption about a detectable difference that is desired which then allows \\(\\beta\\) to be prescribed much like \\(\\alpha\\). In essence, the detectable difference prescribes a fixed value for \\(H_a\\). We can then talk about the power of of a hypothesis test, which is 1 minus the probability of a Type II Error, \\(\\beta\\). See Statistical Power in Wikipedia for a starting source if your are interested. This website provides a novel interactive visualization to help you understand power. It does require a little background on Cohen’s D. Sufficient Evidence Statistics comes in to play with hypothesis testing by defining the phrase “sufficient evidence.” When there is “sufficient evidence” in the data, the null hypothesis is rejected and the alternative hypothesis becomes the working hypothesis. There are many statistical approaches to this problem of measuring the significance of evidence, but in almost all cases, the final measurement of evidence is given by the \\(p\\)-value of the hypothesis test. The \\(p\\)-value of a test is defined as the probability of the evidence being as extreme or more extreme than what was observed assuming the null hypothesis is true. This is an interesting phrase that is at first difficult to understand. The “as extreme or more extreme” part of the definition of the \\(p\\)-value comes from the idea that the null hypothesis will be rejected when the evidence in the data is extremely inconsistent with the null hypothesis. If the data is not extremely different from what we would expect under the null hypothesis, then we will continue to believe the null hypothesis. Although, it is worth emphasizing that this does not prove the null hypothesis to be true. Evidence not Proof Hypothesis testing allows us a formal way to decide if we should “conclude the alternative” or “continue to accept the null.” It is important to remember that statistics (and science) cannot prove anything, just show evidence towards. Thus we never really prove a hypothesis is true, we simply show evidence towards or against a hypothesis. Calculating the \\(p\\)-Value Recall that the \\(p\\)-value measures how extremely the data (the evidence) differs from what is expected under the null hypothesis. Small \\(p\\)-values lead us to discard (reject) the null hypothesis. A \\(p\\)-value can be calculated whenever we have two things. A test statistic, which is a way of measuring how “far” the observed data is from what is expected under the null hypothesis. The sampling distribution of the test statistic, which is the theoretical distribution of the test statistic over all possible samples, assuming the null hypothesis was true. Visit the Math 221 textbook for an explanation. A distribution describes how data is spread out. When we know the shape of a distribution, we know which values are possible, but more importantly which values are most plausible (likely) and which are the least plausible (unlikely). The \\(p\\)-value uses the sampling distribution of the test statistic to measure the probability of the observed test statistic being as extreme or more extreme than the one observed. All \\(p\\)-value computation methods can be classified into two broad categories, parametric methods and nonparametric methods. Parametric Methods Parametric methods assume that, under the null hypothesis, the test statistic follows a specific theoretical parametric distribution. Parametric methods are typically more statistically powerful than nonparametric methods, but necessarily force more assumptions on the data. Parametric distributions are theoretical distributions that can be described by a mathematical function. There are many theoretical distributions. (See the List of Probability Distributions in Wikipedia for details.) Four of the most widely used parametric distributions are: The Normal Distribution Click for Details One of the most important distributions in statistics is the normal distribution. It is a theoretical distribution that approximates the distributions of many real life data distributions, like heights of people, heights of corn plants, baseball batting averages, lengths of gestational periods for many species including humans, and so on. More importantly, the sampling distribution of the sample mean \\(\\bar{x}\\) is normally distributed in two important scenarios. The parent population is normally distributed. The sample size is sufficiently large. (Often \\(n\\geq 30\\) is sufficient, but this is a general rule of thumb that is sometimes insufficient.) Mathematical Formula \\[ f(x | \\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} \\] The symbols \\(\\mu\\) and \\(\\sigma\\) are the two parameters of this distribution. The parameter \\(\\mu\\) controls the center, or mean of the distribution. The parameter \\(\\sigma\\) controls the spread, or standard deviation of the distribution. Graphical Form Comments The usefulness of the normal distribution is that we know which values of data are likely and which are unlikely by just knowing three things: that the data is normally distributed, \\(\\mu\\), the mean of the distribution, and \\(\\sigma\\), the standard deviation of the distribution. For example, as shown in the plot above, a value of \\(x=-8\\) would be very probable for the normal distribution with \\(\\mu=-5\\) and \\(\\sigma=2\\) (light blue curve). However, the value of \\(x=-8\\) would be very unlikely to occur in the normal distribution with \\(\\mu=3\\) and \\(\\sigma=3\\) (gray curve). In fact, \\(x=-8\\) would be even more unlikely an occurance for the \\(\\mu=0\\) and \\(\\sigma=1\\) distribution (dark blue curve). The Chi Squared Distribution Click for Details The chi squared distribution only allows for values that are greater than or equal to zero. While it has a few real life applications, by far its greatest use is theoretical. The test statistic of the chi squared test is distributed according to a chi squared distribution. Mathematical Formula \\[ f(x|p) = \\frac{1}{\\Gamma(p/2)2^{p/2}}x^{(p/2)-1}e^{-x/2} \\] The only parameter of the chi squared distribution is \\(p\\), which is known as the degrees of freedom. Larger values of the parameter \\(p\\) move the center of the chi squared distribution farther to the right. As \\(p\\) goes to infinity, the chi squared distribution begins to look more and more normal in shape. Note that the symbol in the denominator of the chi squared distribution, \\(\\Gamma(p/2)\\), is the Gamma function of \\(p/2\\). (See Gamma Function in Wikipedia for details.) Graphical Form Comments It is important to remember that the chi squared distribution is only defined for \\(x\\geq 0\\) and for positive values of the parameter \\(p\\). This is unlike the normal distribution which is defined for all numbers \\(x\\) from negative infinity to positive infinity as well as for all values of \\(\\mu\\) from negative infinity to positive infinity. The t Distribution Click for Details A close friend of the normal distribution is the t distribution. Although the t distribution is seldom used to model real life data, the distribution is used extensively in hypothesis testing. For example, it is the sampling distribution of the one sample t statistic. It also shows up in many other places, like in regression, in the independent samples t test, and in the paired samples t test. Mathematical Formula \\[ f(x|p) = \\frac{\\Gamma\\left(\\frac{p+1}{2}\\right)}{\\Gamma\\left(\\frac{p}{2}\\right)}\\frac{1}{\\sqrt{p\\pi}}\\frac{1}{\\left(1 + \\left(\\frac{x^2}{p}\\right)\\right)^{(p+1)/2}} \\] Notice that, similar to the chi squared distribution, the t distribution has only one parameter, the degrees of freedom \\(p\\). As the single parameter \\(p\\) is varied from \\(p=1\\), to \\(p=2\\), …, \\(p=5\\), and larger and larger numbers, the resulting distribution becomes more and more normal in shape. Note that the expressions \\(\\Gamma\\left(\\frac{p+1}{2}\\right)\\) and \\(\\Gamma(p/2)\\), refer to the Gamma function. (See Gamma Function in Wikipedia for details.) Graphical Form Comments When the degrees of freedom \\(p=30\\), the resulting t distribution is almost indistinguishable visually from the normal distribution. This is one of the reasons that a sample size of 30 is often used as a rule of thumb for the sample size being “large enough” to assume the sampling distribution of the sample mean is approximately normal. The F Distribution Click for Details Another commonly used distribution for test statistics, like in ANOVA and regression, is the F distribution. Technically speaking, the F distribution is the ratio of two chi squared random variables that are each divided by their respective degrees of freedom. Mathematical Formula \\[ f(x|p_1,p_2) = \\frac{\\Gamma\\left(\\frac{p_1+p_2}{2}\\right)}{\\Gamma\\left(\\frac{p_1}{2}\\right)\\Gamma\\left(\\frac{p_2}{2}\\right)}\\frac{\\left(\\frac{p_1}{p_2}\\right)^{p_1/2}x^{(p_1-2)/2}}{\\left(1+\\left(\\frac{p_1}{p_2}\\right)x\\right)^{(p_1+p_2)/2}} \\] where \\(x\\geq 0\\) and the parameters \\(p_1\\) and \\(p_2\\) are the “numerator” and “denominator” degrees of freedom, respectively. Graphical Form Comments The effects of the parameters \\(p_1\\) and \\(p_2\\) on the F distribution are complicated, but generally speaking, as they both increase the distribution becomes more and more normal in shape. Nonparametric Methods Nonparametric methods place minimal assumptions on the distribution of data. They allow the data to “speak for itself.” They are typically less powerful than the parametric alternatives, but are more broadly applicable because fewer assumptions need to be satisfied. Nonparametric methods include Rank Sum Tests and Permutation Tests. Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Making Inference It is common to only have a sample of data from some population of interest.", "Using the information from the sample to reach conclusions about the population is called making inference.", "When statistical inference is performed properly, the conclusions about the population are almost always correct.", "Hypothesis Testing One of the great focal points of statistics concerns hypothesis testing.", "Science generally agrees upon the principle that truth must be uncovered by the process of elimination.", "The process begins by establishing a starting assumption, or null hypothesis (\\(H_0\\)).", "Data is then collected and the evidence against the null hypothesis is measured, typically with the \\(p\\)-value.", "The \\(p\\)-value becomes small (gets close to zero) when the evidence is extremely different from what would be expected if the null hypothesis were true.", "When the \\(p\\)-value is below the significance level \\(\\alpha\\) (typically \\(\\alpha=0.05\\)) the null hypothesis is abandoned (rejected) in favor of a competing alternative hypothesis (\\(H_a\\)).", "Click for an Example The current hypothesis may be that the world is flat.", "Then someone who thinks otherwise sets sail in a boat, gathers some evidence, and when there is sufficient evidence in the data to disbelieve the current hypothesis, we conclude the world is not flat.", "In light of this new knowledge, we shift our belief to the next working hypothesis, that the world is round.", "After a while, someone gathers more evidence and shows that the world is not round, and we move to the next working hypothesis, that it is oblate spheroid, i.e., a sphere that is squashed at its poles and swollen at the equator.", "This process of elimination is called hypothesis testing.", "The process begins by establishing a null hypothesis (denoted symbolically by \\(H_0\\)) which represents the current opinion, status quo, or what we will believe if the evidence is not sufficient to suggest otherwise.", "The alternative hypothesis (denoted symbolically by \\(H_a\\)) designates what we will believe if there is sufficient evidence in the data to discredit, or “reject,” the null hypothesis.", "See the BYU-I Math 221 Stats Wiki for another example.", "Managing Decision Errors When the \\(p\\)-value approaches zero, one of two things must be occurring.", "Either an extremely rare event has happened or the null hypothesis is incorrect.", "Since the second option, that the null hypothesis is incorrect, is the more plausible option, we reject the null hypothesis in favor of the alternative whenever the \\(p\\)-value is close to zero.", "It is important to remember that rejecting the null hypothesis could however be a mistake.", "  \\(H_0\\) True \\(H_0\\) False Reject \\(H_0\\) Type I Error Correct Decision Accept \\(H_0\\) Correct Decision Type II Error Type I Error, Significance Level, Confidence and \\(\\alpha\\) A Type I Error is defined as rejecting the null hypothesis when it is actually true.", "(Throwing away truth.) The significance level, \\(\\alpha\\), of a hypothesis test controls the probability of a Type I Error.", "The typical value of \\(\\alpha = 0.05\\) came from tradition and is a somewhat arbitrary value.", "Any value from 0 to 1 could be used for \\(\\alpha\\).", "When deciding on the level of \\(\\alpha\\) for a particular study it is important to remember that as \\(\\alpha\\) increases, the probability of a Type I Error increases, and the probability of a Type II Error decreases.", "When \\(\\alpha\\) gets smaller, the probability of a Type I Error gets smaller, while the probability of a Type II Error increases.", "Confidence is defined as \\(1-\\alpha\\) or the opposite of a Type I error.", "That is the probability of accepting the NULL when it is in fact true.", "Type II Errors, \\(\\beta\\), and Power It is also possible to make a Type II Error, which is defined as failing to reject the null hypothesis when it is actually false.", "(Failing to move to truth.) The probability of a Type II Error, \\(\\beta\\), is often unknown.", "However, practitioners often make an assumption about a detectable difference that is desired which then allows \\(\\beta\\) to be prescribed much like \\(\\alpha\\).", "In essence, the detectable difference prescribes a fixed value for \\(H_a\\).", "We can then talk about the power of of a hypothesis test, which is 1 minus the probability of a Type II Error, \\(\\beta\\).", "See Statistical Power in Wikipedia for a starting source if your are interested.", "This website provides a novel interactive visualization to help you understand power.", "It does require a little background on Cohen’s D.", "Sufficient Evidence Statistics comes in to play with hypothesis testing by defining the phrase “sufficient evidence.” When there is “sufficient evidence” in the data, the null hypothesis is rejected and the alternative hypothesis becomes the working hypothesis.", "There are many statistical approaches to this problem of measuring the significance of evidence, but in almost all cases, the final measurement of evidence is given by the \\(p\\)-value of the hypothesis test.", "The \\(p\\)-value of a test is defined as the probability of the evidence being as extreme or more extreme than what was observed assuming the null hypothesis is true.", "This is an interesting phrase that is at first difficult to understand.", "The “as extreme or more extreme” part of the definition of the \\(p\\)-value comes from the idea that the null hypothesis will be rejected when the evidence in the data is extremely inconsistent with the null hypothesis.", "If the data is not extremely different from what we would expect under the null hypothesis, then we will continue to believe the null hypothesis.", "Although, it is worth emphasizing that this does not prove the null hypothesis to be true.", "Evidence not Proof Hypothesis testing allows us a formal way to decide if we should “conclude the alternative” or “continue to accept the null.” It is important to remember that statistics (and science) cannot prove anything, just show evidence towards.", "Thus we never really prove a hypothesis is true, we simply show evidence towards or against a hypothesis.", "Calculating the \\(p\\)-Value Recall that the \\(p\\)-value measures how extremely the data (the evidence) differs from what is expected under the null hypothesis.", "Small \\(p\\)-values lead us to discard (reject) the null hypothesis.", "A \\(p\\)-value can be calculated whenever we have two things.", "A test statistic, which is a way of measuring how “far” the observed data is from what is expected under the null hypothesis."],
    "type": "page",
    "page_title": "Making Inference"
  },
  {
    "id": 169,
    "title": "📍 Hypothesis Testing",
    "url": "MakingInference.html#hypothesistesting",
    "content": "Hypothesis Testing One of the great focal points of statistics concerns hypothesis testing. Science generally agrees upon the principle that truth must be uncovered by the process of elimination. The process begins by establishing a starting assumption, or null hypothesis (\\(H_0\\)). Data is then collected and the evidence against the null hypothesis is measured, typically with the \\(p\\)-value. The \\(p\\)-value becomes small (gets close to zero) when the evidence is extremely different from what would be expected if the null hypothesis were true. When the \\(p\\)-value is below the significance level \\(\\alpha\\) (typically \\(\\alpha=0.05\\)) the null hypothesis is abandoned (rejected) in favor of a competing alternative hypothesis (\\(H_a\\)). Click for an Example The current hypothesis may be that the world is flat. Then someone who thinks otherwise sets sail in a boat, gathers some evidence, and when there is sufficient evidence in the data to disbelieve the current hypothesis, we conclude the world is not flat. In light of this new knowledge, we shift our belief to the next working hypothesis, that the world is round. After a while, someone gathers more evidence and shows that the world is not round, and we move to the next working hypothesis, that it is oblate spheroid, i.e., a sphere that is squashed at its poles and swollen at the equator. This process of elimination is called hypothesis testing. The process begins by establishing a null hypothesis (denoted symbolically by \\(H_0\\)) which represents the current opinion, status quo, or what we will believe if the evidence is not sufficient to suggest otherwise. The alternative hypothesis (denoted symbolically by \\(H_a\\)) designates what we will believe if there is sufficient evidence in the data to discredit, or “reject,” the null hypothesis. See the BYU-I Math 221 Stats Wiki for another example. Managing Decision Errors When the \\(p\\)-value approaches zero, one of two things must be occurring. Either an extremely rare event has happened or the null hypothesis is incorrect. Since the second option, that the null hypothesis is incorrect, is the more plausible option, we reject the null hypothesis in favor of the alternative whenever the \\(p\\)-value is close to zero. It is important to remember that rejecting the null hypothesis could however be a mistake.   \\(H_0\\) True \\(H_0\\) False Reject \\(H_0\\) Type I Error Correct Decision Accept \\(H_0\\) Correct Decision Type II Error Type I Error, Significance Level, Confidence and \\(\\alpha\\) A Type I Error is defined as rejecting the null hypothesis when it is actually true. (Throwing away truth.) The significance level, \\(\\alpha\\), of a hypothesis test controls the probability of a Type I Error. The typical value of \\(\\alpha = 0.05\\) came from tradition and is a somewhat arbitrary value. Any value from 0 to 1 could be used for \\(\\alpha\\). When deciding on the level of \\(\\alpha\\) for a particular study it is important to remember that as \\(\\alpha\\) increases, the probability of a Type I Error increases, and the probability of a Type II Error decreases. When \\(\\alpha\\) gets smaller, the probability of a Type I Error gets smaller, while the probability of a Type II Error increases. Confidence is defined as \\(1-\\alpha\\) or the opposite of a Type I error. That is the probability of accepting the NULL when it is in fact true. Type II Errors, \\(\\beta\\), and Power It is also possible to make a Type II Error, which is defined as failing to reject the null hypothesis when it is actually false. (Failing to move to truth.) The probability of a Type II Error, \\(\\beta\\), is often unknown. However, practitioners often make an assumption about a detectable difference that is desired which then allows \\(\\beta\\) to be prescribed much like \\(\\alpha\\). In essence, the detectable difference prescribes a fixed value for \\(H_a\\). We can then talk about the power of of a hypothesis test, which is 1 minus the probability of a Type II Error, \\(\\beta\\). See Statistical Power in Wikipedia for a starting source if your are interested. This website provides a novel interactive visualization to help you understand power. It does require a little background on Cohen’s D. Sufficient Evidence Statistics comes in to play with hypothesis testing by defining the phrase “sufficient evidence.” When there is “sufficient evidence” in the data, the null hypothesis is rejected and the alternative hypothesis becomes the working hypothesis. There are many statistical approaches to this problem of measuring the significance of evidence, but in almost all cases, the final measurement of evidence is given by the \\(p\\)-value of the hypothesis test. The \\(p\\)-value of a test is defined as the probability of the evidence being as extreme or more extreme than what was observed assuming the null hypothesis is true. This is an interesting phrase that is at first difficult to understand. The “as extreme or more extreme” part of the definition of the \\(p\\)-value comes from the idea that the null hypothesis will be rejected when the evidence in the data is extremely inconsistent with the null hypothesis. If the data is not extremely different from what we would expect under the null hypothesis, then we will continue to believe the null hypothesis. Although, it is worth emphasizing that this does not prove the null hypothesis to be true. Evidence not Proof Hypothesis testing allows us a formal way to decide if we should “conclude the alternative” or “continue to accept the null.” It is important to remember that statistics (and science) cannot prove anything, just show evidence towards. Thus we never really prove a hypothesis is true, we simply show evidence towards or against a hypothesis.",
    "sentences": ["One of the great focal points of statistics concerns hypothesis testing.", "Science generally agrees upon the principle that truth must be uncovered by the process of elimination.", "The process begins by establishing a starting assumption, or null hypothesis (\\(H_0\\)).", "Data is then collected and the evidence against the null hypothesis is measured, typically with the \\(p\\)-value.", "The \\(p\\)-value becomes small (gets close to zero) when the evidence is extremely different from what would be expected if the null hypothesis were true.", "When the \\(p\\)-value is below the significance level \\(\\alpha\\) (typically \\(\\alpha=0.05\\)) the null hypothesis is abandoned (rejected) in favor of a competing alternative hypothesis (\\(H_a\\)).", "Click for an Example The current hypothesis may be that the world is flat.", "Then someone who thinks otherwise sets sail in a boat, gathers some evidence, and when there is sufficient evidence in the data to disbelieve the current hypothesis, we conclude the world is not flat.", "In light of this new knowledge, we shift our belief to the next working hypothesis, that the world is round.", "After a while, someone gathers more evidence and shows that the world is not round, and we move to the next working hypothesis, that it is oblate spheroid, i.e., a sphere that is squashed at its poles and swollen at the equator."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Hypothesis Testing"
  },
  {
    "id": 170,
    "title": "📍 Managing Decision Errors",
    "url": "MakingInference.html#managingdecisionerrors",
    "content": "Managing Decision Errors When the \\(p\\)-value approaches zero, one of two things must be occurring. Either an extremely rare event has happened or the null hypothesis is incorrect. Since the second option, that the null hypothesis is incorrect, is the more plausible option, we reject the null hypothesis in favor of the alternative whenever the \\(p\\)-value is close to zero. It is important to remember that rejecting the null hypothesis could however be a mistake.   \\(H_0\\) True \\(H_0\\) False Reject \\(H_0\\) Type I Error Correct Decision Accept \\(H_0\\) Correct Decision Type II Error",
    "sentences": ["When the \\(p\\)-value approaches zero, one of two things must be occurring.", "Either an extremely rare event has happened or the null hypothesis is incorrect.", "Since the second option, that the null hypothesis is incorrect, is the more plausible option, we reject the null hypothesis in favor of the alternative whenever the \\(p\\)-value is close to zero.", "It is important to remember that rejecting the null hypothesis could however be a mistake.", "  \\(H_0\\) True \\(H_0\\) False Reject \\(H_0\\) Type I Error Correct Decision Accept \\(H_0\\) Correct Decision Type II Error"],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Managing Decision Errors"
  },
  {
    "id": 171,
    "title": "📍 Type I Error, Significance Level, Confidence and \\(\\alpha\\)",
    "url": "MakingInference.html#typeierrorsignificancelevelconfidenceand\\\\alpha\\",
    "content": "Type I Error, Significance Level, Confidence and \\(\\alpha\\) A Type I Error is defined as rejecting the null hypothesis when it is actually true. (Throwing away truth.) The significance level, \\(\\alpha\\), of a hypothesis test controls the probability of a Type I Error. The typical value of \\(\\alpha = 0.05\\) came from tradition and is a somewhat arbitrary value. Any value from 0 to 1 could be used for \\(\\alpha\\). When deciding on the level of \\(\\alpha\\) for a particular study it is important to remember that as \\(\\alpha\\) increases, the probability of a Type I Error increases, and the probability of a Type II Error decreases. When \\(\\alpha\\) gets smaller, the probability of a Type I Error gets smaller, while the probability of a Type II Error increases. Confidence is defined as \\(1-\\alpha\\) or the opposite of a Type I error. That is the probability of accepting the NULL when it is in fact true.",
    "sentences": ["A Type I Error is defined as rejecting the null hypothesis when it is actually true.", "(Throwing away truth.) The significance level, \\(\\alpha\\), of a hypothesis test controls the probability of a Type I Error.", "The typical value of \\(\\alpha = 0.05\\) came from tradition and is a somewhat arbitrary value.", "Any value from 0 to 1 could be used for \\(\\alpha\\).", "When deciding on the level of \\(\\alpha\\) for a particular study it is important to remember that as \\(\\alpha\\) increases, the probability of a Type I Error increases, and the probability of a Type II Error decreases.", "When \\(\\alpha\\) gets smaller, the probability of a Type I Error gets smaller, while the probability of a Type II Error increases.", "Confidence is defined as \\(1-\\alpha\\) or the opposite of a Type I error.", "That is the probability of accepting the NULL when it is in fact true."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Type I Error, Significance Level, Confidence and \\(\\alpha\\)"
  },
  {
    "id": 172,
    "title": "📍 Type II Errors, \\(\\beta\\), and\r\nPower",
    "url": "MakingInference.html#typeiierrors\\\\beta\\andpower",
    "content": "Type II Errors, \\(\\beta\\), and\r\nPower It is also possible to make a Type II Error, which is defined as failing to reject the null hypothesis when it is actually false. (Failing to move to truth.) The probability of a Type II Error, \\(\\beta\\), is often unknown. However, practitioners often make an assumption about a detectable difference that is desired which then allows \\(\\beta\\) to be prescribed much like \\(\\alpha\\). In essence, the detectable difference prescribes a fixed value for \\(H_a\\). We can then talk about the power of of a hypothesis test, which is 1 minus the probability of a Type II Error, \\(\\beta\\). See Statistical Power in Wikipedia for a starting source if your are interested. This website provides a novel interactive visualization to help you understand power. It does require a little background on Cohen’s D.",
    "sentences": ["It is also possible to make a Type II Error, which is defined as failing to reject the null hypothesis when it is actually false.", "(Failing to move to truth.) The probability of a Type II Error, \\(\\beta\\), is often unknown.", "However, practitioners often make an assumption about a detectable difference that is desired which then allows \\(\\beta\\) to be prescribed much like \\(\\alpha\\).", "In essence, the detectable difference prescribes a fixed value for \\(H_a\\).", "We can then talk about the power of of a hypothesis test, which is 1 minus the probability of a Type II Error, \\(\\beta\\).", "See Statistical Power in Wikipedia for a starting source if your are interested.", "This website provides a novel interactive visualization to help you understand power.", "It does require a little background on Cohen’s D."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Type II Errors, \\(\\beta\\), and\r\nPower"
  },
  {
    "id": 173,
    "title": "📍 Sufficient Evidence",
    "url": "MakingInference.html#sufficientevidence",
    "content": "Sufficient Evidence Statistics comes in to play with hypothesis testing by defining the phrase “sufficient evidence.” When there is “sufficient evidence” in the data, the null hypothesis is rejected and the alternative hypothesis becomes the working hypothesis. There are many statistical approaches to this problem of measuring the significance of evidence, but in almost all cases, the final measurement of evidence is given by the \\(p\\)-value of the hypothesis test. The \\(p\\)-value of a test is defined as the probability of the evidence being as extreme or more extreme than what was observed assuming the null hypothesis is true. This is an interesting phrase that is at first difficult to understand. The “as extreme or more extreme” part of the definition of the \\(p\\)-value comes from the idea that the null hypothesis will be rejected when the evidence in the data is extremely inconsistent with the null hypothesis. If the data is not extremely different from what we would expect under the null hypothesis, then we will continue to believe the null hypothesis. Although, it is worth emphasizing that this does not prove the null hypothesis to be true.",
    "sentences": ["Statistics comes in to play with hypothesis testing by defining the phrase “sufficient evidence.” When there is “sufficient evidence” in the data, the null hypothesis is rejected and the alternative hypothesis becomes the working hypothesis.", "There are many statistical approaches to this problem of measuring the significance of evidence, but in almost all cases, the final measurement of evidence is given by the \\(p\\)-value of the hypothesis test.", "The \\(p\\)-value of a test is defined as the probability of the evidence being as extreme or more extreme than what was observed assuming the null hypothesis is true.", "This is an interesting phrase that is at first difficult to understand.", "The “as extreme or more extreme” part of the definition of the \\(p\\)-value comes from the idea that the null hypothesis will be rejected when the evidence in the data is extremely inconsistent with the null hypothesis.", "If the data is not extremely different from what we would expect under the null hypothesis, then we will continue to believe the null hypothesis.", "Although, it is worth emphasizing that this does not prove the null hypothesis to be true."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Sufficient Evidence"
  },
  {
    "id": 174,
    "title": "📍 Evidence not Proof",
    "url": "MakingInference.html#evidencenotproof",
    "content": "Evidence not Proof Hypothesis testing allows us a formal way to decide if we should “conclude the alternative” or “continue to accept the null.” It is important to remember that statistics (and science) cannot prove anything, just show evidence towards. Thus we never really prove a hypothesis is true, we simply show evidence towards or against a hypothesis.",
    "sentences": ["Hypothesis testing allows us a formal way to decide if we should “conclude the alternative” or “continue to accept the null.” It is important to remember that statistics (and science) cannot prove anything, just show evidence towards.", "Thus we never really prove a hypothesis is true, we simply show evidence towards or against a hypothesis."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Evidence not Proof"
  },
  {
    "id": 175,
    "title": "📍 Calculating the \\(p\\)-Value",
    "url": "MakingInference.html#calculatingthe\\p\\value",
    "content": "Calculating the \\(p\\)-Value Recall that the \\(p\\)-value measures how extremely the data (the evidence) differs from what is expected under the null hypothesis. Small \\(p\\)-values lead us to discard (reject) the null hypothesis. A \\(p\\)-value can be calculated whenever we have two things. A test statistic, which is a way of measuring how “far” the observed data is from what is expected under the null hypothesis. The sampling distribution of the test statistic, which is the theoretical distribution of the test statistic over all possible samples, assuming the null hypothesis was true. Visit the Math 221 textbook for an explanation. A distribution describes how data is spread out. When we know the shape of a distribution, we know which values are possible, but more importantly which values are most plausible (likely) and which are the least plausible (unlikely). The \\(p\\)-value uses the sampling distribution of the test statistic to measure the probability of the observed test statistic being as extreme or more extreme than the one observed. All \\(p\\)-value computation methods can be classified into two broad categories, parametric methods and nonparametric methods. Parametric Methods Parametric methods assume that, under the null hypothesis, the test statistic follows a specific theoretical parametric distribution. Parametric methods are typically more statistically powerful than nonparametric methods, but necessarily force more assumptions on the data. Parametric distributions are theoretical distributions that can be described by a mathematical function. There are many theoretical distributions. (See the List of Probability Distributions in Wikipedia for details.) Four of the most widely used parametric distributions are: The Normal Distribution Click for Details One of the most important distributions in statistics is the normal distribution. It is a theoretical distribution that approximates the distributions of many real life data distributions, like heights of people, heights of corn plants, baseball batting averages, lengths of gestational periods for many species including humans, and so on. More importantly, the sampling distribution of the sample mean \\(\\bar{x}\\) is normally distributed in two important scenarios. The parent population is normally distributed. The sample size is sufficiently large. (Often \\(n\\geq 30\\) is sufficient, but this is a general rule of thumb that is sometimes insufficient.) Mathematical Formula \\[ f(x | \\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} \\] The symbols \\(\\mu\\) and \\(\\sigma\\) are the two parameters of this distribution. The parameter \\(\\mu\\) controls the center, or mean of the distribution. The parameter \\(\\sigma\\) controls the spread, or standard deviation of the distribution. Graphical Form Comments The usefulness of the normal distribution is that we know which values of data are likely and which are unlikely by just knowing three things: that the data is normally distributed, \\(\\mu\\), the mean of the distribution, and \\(\\sigma\\), the standard deviation of the distribution. For example, as shown in the plot above, a value of \\(x=-8\\) would be very probable for the normal distribution with \\(\\mu=-5\\) and \\(\\sigma=2\\) (light blue curve). However, the value of \\(x=-8\\) would be very unlikely to occur in the normal distribution with \\(\\mu=3\\) and \\(\\sigma=3\\) (gray curve). In fact, \\(x=-8\\) would be even more unlikely an occurance for the \\(\\mu=0\\) and \\(\\sigma=1\\) distribution (dark blue curve). The Chi Squared Distribution Click for Details The chi squared distribution only allows for values that are greater than or equal to zero. While it has a few real life applications, by far its greatest use is theoretical. The test statistic of the chi squared test is distributed according to a chi squared distribution. Mathematical Formula \\[ f(x|p) = \\frac{1}{\\Gamma(p/2)2^{p/2}}x^{(p/2)-1}e^{-x/2} \\] The only parameter of the chi squared distribution is \\(p\\), which is known as the degrees of freedom. Larger values of the parameter \\(p\\) move the center of the chi squared distribution farther to the right. As \\(p\\) goes to infinity, the chi squared distribution begins to look more and more normal in shape. Note that the symbol in the denominator of the chi squared distribution, \\(\\Gamma(p/2)\\), is the Gamma function of \\(p/2\\). (See Gamma Function in Wikipedia for details.) Graphical Form Comments It is important to remember that the chi squared distribution is only defined for \\(x\\geq 0\\) and for positive values of the parameter \\(p\\). This is unlike the normal distribution which is defined for all numbers \\(x\\) from negative infinity to positive infinity as well as for all values of \\(\\mu\\) from negative infinity to positive infinity. The t Distribution Click for Details A close friend of the normal distribution is the t distribution. Although the t distribution is seldom used to model real life data, the distribution is used extensively in hypothesis testing. For example, it is the sampling distribution of the one sample t statistic. It also shows up in many other places, like in regression, in the independent samples t test, and in the paired samples t test. Mathematical Formula \\[ f(x|p) = \\frac{\\Gamma\\left(\\frac{p+1}{2}\\right)}{\\Gamma\\left(\\frac{p}{2}\\right)}\\frac{1}{\\sqrt{p\\pi}}\\frac{1}{\\left(1 + \\left(\\frac{x^2}{p}\\right)\\right)^{(p+1)/2}} \\] Notice that, similar to the chi squared distribution, the t distribution has only one parameter, the degrees of freedom \\(p\\). As the single parameter \\(p\\) is varied from \\(p=1\\), to \\(p=2\\), …, \\(p=5\\), and larger and larger numbers, the resulting distribution becomes more and more normal in shape. Note that the expressions \\(\\Gamma\\left(\\frac{p+1}{2}\\right)\\) and \\(\\Gamma(p/2)\\), refer to the Gamma function. (See Gamma Function in Wikipedia for details.) Graphical Form Comments When the degrees of freedom \\(p=30\\), the resulting t distribution is almost indistinguishable visually from the normal distribution. This is one of the reasons that a sample size of 30 is often used as a rule of thumb for the sample size being “large enough” to assume the sampling distribution of the sample mean is approximately normal. The F Distribution Click for Details Another commonly used distribution for test statistics, like in ANOVA and regression, is the F distribution. Technically speaking, the F distribution is the ratio of two chi squared random variables that are each divided by their respective degrees of freedom. Mathematical Formula \\[ f(x|p_1,p_2) = \\frac{\\Gamma\\left(\\frac{p_1+p_2}{2}\\right)}{\\Gamma\\left(\\frac{p_1}{2}\\right)\\Gamma\\left(\\frac{p_2}{2}\\right)}\\frac{\\left(\\frac{p_1}{p_2}\\right)^{p_1/2}x^{(p_1-2)/2}}{\\left(1+\\left(\\frac{p_1}{p_2}\\right)x\\right)^{(p_1+p_2)/2}} \\] where \\(x\\geq 0\\) and the parameters \\(p_1\\) and \\(p_2\\) are the “numerator” and “denominator” degrees of freedom, respectively. Graphical Form Comments The effects of the parameters \\(p_1\\) and \\(p_2\\) on the F distribution are complicated, but generally speaking, as they both increase the distribution becomes more and more normal in shape. Nonparametric Methods Nonparametric methods place minimal assumptions on the distribution of data. They allow the data to “speak for itself.” They are typically less powerful than the parametric alternatives, but are more broadly applicable because fewer assumptions need to be satisfied. Nonparametric methods include Rank Sum Tests and Permutation Tests.",
    "sentences": ["Recall that the \\(p\\)-value measures how extremely the data (the evidence) differs from what is expected under the null hypothesis.", "Small \\(p\\)-values lead us to discard (reject) the null hypothesis.", "A \\(p\\)-value can be calculated whenever we have two things.", "A test statistic, which is a way of measuring how “far” the observed data is from what is expected under the null hypothesis.", "The sampling distribution of the test statistic, which is the theoretical distribution of the test statistic over all possible samples, assuming the null hypothesis was true.", "Visit the Math 221 textbook for an explanation.", "A distribution describes how data is spread out.", "When we know the shape of a distribution, we know which values are possible, but more importantly which values are most plausible (likely) and which are the least plausible (unlikely).", "The \\(p\\)-value uses the sampling distribution of the test statistic to measure the probability of the observed test statistic being as extreme or more extreme than the one observed.", "All \\(p\\)-value computation methods can be classified into two broad categories, parametric methods and nonparametric methods."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Calculating the \\(p\\)-Value"
  },
  {
    "id": 176,
    "title": "📍 Parametric Methods",
    "url": "MakingInference.html#parametricmethods",
    "content": "Parametric Methods Parametric methods assume that, under the null hypothesis, the test statistic follows a specific theoretical parametric distribution. Parametric methods are typically more statistically powerful than nonparametric methods, but necessarily force more assumptions on the data. Parametric distributions are theoretical distributions that can be described by a mathematical function. There are many theoretical distributions. (See the List of Probability Distributions in Wikipedia for details.) Four of the most widely used parametric distributions are: The Normal Distribution Click for Details One of the most important distributions in statistics is the normal distribution. It is a theoretical distribution that approximates the distributions of many real life data distributions, like heights of people, heights of corn plants, baseball batting averages, lengths of gestational periods for many species including humans, and so on. More importantly, the sampling distribution of the sample mean \\(\\bar{x}\\) is normally distributed in two important scenarios. The parent population is normally distributed. The sample size is sufficiently large. (Often \\(n\\geq 30\\) is sufficient, but this is a general rule of thumb that is sometimes insufficient.) Mathematical Formula \\[ f(x | \\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} \\] The symbols \\(\\mu\\) and \\(\\sigma\\) are the two parameters of this distribution. The parameter \\(\\mu\\) controls the center, or mean of the distribution. The parameter \\(\\sigma\\) controls the spread, or standard deviation of the distribution. Graphical Form Comments The usefulness of the normal distribution is that we know which values of data are likely and which are unlikely by just knowing three things: that the data is normally distributed, \\(\\mu\\), the mean of the distribution, and \\(\\sigma\\), the standard deviation of the distribution. For example, as shown in the plot above, a value of \\(x=-8\\) would be very probable for the normal distribution with \\(\\mu=-5\\) and \\(\\sigma=2\\) (light blue curve). However, the value of \\(x=-8\\) would be very unlikely to occur in the normal distribution with \\(\\mu=3\\) and \\(\\sigma=3\\) (gray curve). In fact, \\(x=-8\\) would be even more unlikely an occurance for the \\(\\mu=0\\) and \\(\\sigma=1\\) distribution (dark blue curve). The Chi Squared Distribution Click for Details The chi squared distribution only allows for values that are greater than or equal to zero. While it has a few real life applications, by far its greatest use is theoretical. The test statistic of the chi squared test is distributed according to a chi squared distribution. Mathematical Formula \\[ f(x|p) = \\frac{1}{\\Gamma(p/2)2^{p/2}}x^{(p/2)-1}e^{-x/2} \\] The only parameter of the chi squared distribution is \\(p\\), which is known as the degrees of freedom. Larger values of the parameter \\(p\\) move the center of the chi squared distribution farther to the right. As \\(p\\) goes to infinity, the chi squared distribution begins to look more and more normal in shape. Note that the symbol in the denominator of the chi squared distribution, \\(\\Gamma(p/2)\\), is the Gamma function of \\(p/2\\). (See Gamma Function in Wikipedia for details.) Graphical Form Comments It is important to remember that the chi squared distribution is only defined for \\(x\\geq 0\\) and for positive values of the parameter \\(p\\). This is unlike the normal distribution which is defined for all numbers \\(x\\) from negative infinity to positive infinity as well as for all values of \\(\\mu\\) from negative infinity to positive infinity. The t Distribution Click for Details A close friend of the normal distribution is the t distribution. Although the t distribution is seldom used to model real life data, the distribution is used extensively in hypothesis testing. For example, it is the sampling distribution of the one sample t statistic. It also shows up in many other places, like in regression, in the independent samples t test, and in the paired samples t test. Mathematical Formula \\[ f(x|p) = \\frac{\\Gamma\\left(\\frac{p+1}{2}\\right)}{\\Gamma\\left(\\frac{p}{2}\\right)}\\frac{1}{\\sqrt{p\\pi}}\\frac{1}{\\left(1 + \\left(\\frac{x^2}{p}\\right)\\right)^{(p+1)/2}} \\] Notice that, similar to the chi squared distribution, the t distribution has only one parameter, the degrees of freedom \\(p\\). As the single parameter \\(p\\) is varied from \\(p=1\\), to \\(p=2\\), …, \\(p=5\\), and larger and larger numbers, the resulting distribution becomes more and more normal in shape. Note that the expressions \\(\\Gamma\\left(\\frac{p+1}{2}\\right)\\) and \\(\\Gamma(p/2)\\), refer to the Gamma function. (See Gamma Function in Wikipedia for details.) Graphical Form Comments When the degrees of freedom \\(p=30\\), the resulting t distribution is almost indistinguishable visually from the normal distribution. This is one of the reasons that a sample size of 30 is often used as a rule of thumb for the sample size being “large enough” to assume the sampling distribution of the sample mean is approximately normal. The F Distribution Click for Details Another commonly used distribution for test statistics, like in ANOVA and regression, is the F distribution. Technically speaking, the F distribution is the ratio of two chi squared random variables that are each divided by their respective degrees of freedom. Mathematical Formula \\[ f(x|p_1,p_2) = \\frac{\\Gamma\\left(\\frac{p_1+p_2}{2}\\right)}{\\Gamma\\left(\\frac{p_1}{2}\\right)\\Gamma\\left(\\frac{p_2}{2}\\right)}\\frac{\\left(\\frac{p_1}{p_2}\\right)^{p_1/2}x^{(p_1-2)/2}}{\\left(1+\\left(\\frac{p_1}{p_2}\\right)x\\right)^{(p_1+p_2)/2}} \\] where \\(x\\geq 0\\) and the parameters \\(p_1\\) and \\(p_2\\) are the “numerator” and “denominator” degrees of freedom, respectively. Graphical Form Comments The effects of the parameters \\(p_1\\) and \\(p_2\\) on the F distribution are complicated, but generally speaking, as they both increase the distribution becomes more and more normal in shape.",
    "sentences": ["Parametric methods assume that, under the null hypothesis, the test statistic follows a specific theoretical parametric distribution.", "Parametric methods are typically more statistically powerful than nonparametric methods, but necessarily force more assumptions on the data.", "Parametric distributions are theoretical distributions that can be described by a mathematical function.", "There are many theoretical distributions.", "(See the List of Probability Distributions in Wikipedia for details.) Four of the most widely used parametric distributions are: The Normal Distribution Click for Details One of the most important distributions in statistics is the normal distribution.", "It is a theoretical distribution that approximates the distributions of many real life data distributions, like heights of people, heights of corn plants, baseball batting averages, lengths of gestational periods for many species including humans, and so on.", "More importantly, the sampling distribution of the sample mean \\(\\bar{x}\\) is normally distributed in two important scenarios.", "The parent population is normally distributed.", "The sample size is sufficiently large.", "(Often \\(n\\geq 30\\) is sufficient, but this is a general rule of thumb that is sometimes insufficient.) Mathematical Formula \\[ f(x | \\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} \\] The symbols \\(\\mu\\) and \\(\\sigma\\) are the two parameters of this distribution."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Parametric Methods"
  },
  {
    "id": 177,
    "title": "📍 The Normal Distribution",
    "url": "MakingInference.html#thenormaldistribution",
    "content": "The Normal Distribution One of the most important distributions in statistics is the normal distribution. It is a theoretical distribution that approximates the distributions of many real life data distributions, like heights of people, heights of corn plants, baseball batting averages, lengths of gestational periods for many species including humans, and so on. More importantly, the sampling distribution of the sample mean \\(\\bar{x}\\) is normally distributed in two important scenarios. The parent population is normally distributed. The sample size is sufficiently large. (Often \\(n\\geq 30\\) is sufficient, but this is a general rule of thumb that is sometimes insufficient.) Mathematical Formula \\[ f(x | \\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} \\] The symbols \\(\\mu\\) and \\(\\sigma\\) are the two parameters of this distribution. The parameter \\(\\mu\\) controls the center, or mean of the distribution. The parameter \\(\\sigma\\) controls the spread, or standard deviation of the distribution. Graphical Form Comments The usefulness of the normal distribution is that we know which values of data are likely and which are unlikely by just knowing three things: that the data is normally distributed, \\(\\mu\\), the mean of the distribution, and \\(\\sigma\\), the standard deviation of the distribution. For example, as shown in the plot above, a value of \\(x=-8\\) would be very probable for the normal distribution with \\(\\mu=-5\\) and \\(\\sigma=2\\) (light blue curve). However, the value of \\(x=-8\\) would be very unlikely to occur in the normal distribution with \\(\\mu=3\\) and \\(\\sigma=3\\) (gray curve). In fact, \\(x=-8\\) would be even more unlikely an occurance for the \\(\\mu=0\\) and \\(\\sigma=1\\) distribution (dark blue curve).",
    "sentences": ["One of the most important distributions in statistics is the normal distribution.", "It is a theoretical distribution that approximates the distributions of many real life data distributions, like heights of people, heights of corn plants, baseball batting averages, lengths of gestational periods for many species including humans, and so on.", "More importantly, the sampling distribution of the sample mean \\(\\bar{x}\\) is normally distributed in two important scenarios.", "The parent population is normally distributed.", "The sample size is sufficiently large.", "(Often \\(n\\geq 30\\) is sufficient, but this is a general rule of thumb that is sometimes insufficient.) Mathematical Formula \\[ f(x | \\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} \\] The symbols \\(\\mu\\) and \\(\\sigma\\) are the two parameters of this distribution.", "The parameter \\(\\mu\\) controls the center, or mean of the distribution.", "The parameter \\(\\sigma\\) controls the spread, or standard deviation of the distribution.", "Graphical Form Comments The usefulness of the normal distribution is that we know which values of data are likely and which are unlikely by just knowing three things: that the data is normally distributed, \\(\\mu\\), the mean of the distribution, and \\(\\sigma\\), the standard deviation of the distribution.", "For example, as shown in the plot above, a value of \\(x=-8\\) would be very probable for the normal distribution with \\(\\mu=-5\\) and \\(\\sigma=2\\) (light blue curve)."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "The Normal Distribution"
  },
  {
    "id": 178,
    "title": "📍 The Chi Squared Distribution",
    "url": "MakingInference.html#thechisquareddistribution",
    "content": "The Chi Squared Distribution The chi squared distribution only allows for values that are greater than or equal to zero. While it has a few real life applications, by far its greatest use is theoretical. The test statistic of the chi squared test is distributed according to a chi squared distribution. Mathematical Formula \\[ f(x|p) = \\frac{1}{\\Gamma(p/2)2^{p/2}}x^{(p/2)-1}e^{-x/2} \\] The only parameter of the chi squared distribution is \\(p\\), which is known as the degrees of freedom. Larger values of the parameter \\(p\\) move the center of the chi squared distribution farther to the right. As \\(p\\) goes to infinity, the chi squared distribution begins to look more and more normal in shape. Note that the symbol in the denominator of the chi squared distribution, \\(\\Gamma(p/2)\\), is the Gamma function of \\(p/2\\). (See Gamma Function in Wikipedia for details.) Graphical Form Comments It is important to remember that the chi squared distribution is only defined for \\(x\\geq 0\\) and for positive values of the parameter \\(p\\). This is unlike the normal distribution which is defined for all numbers \\(x\\) from negative infinity to positive infinity as well as for all values of \\(\\mu\\) from negative infinity to positive infinity.",
    "sentences": ["The chi squared distribution only allows for values that are greater than or equal to zero.", "While it has a few real life applications, by far its greatest use is theoretical.", "The test statistic of the chi squared test is distributed according to a chi squared distribution.", "Mathematical Formula \\[ f(x|p) = \\frac{1}{\\Gamma(p/2)2^{p/2}}x^{(p/2)-1}e^{-x/2} \\] The only parameter of the chi squared distribution is \\(p\\), which is known as the degrees of freedom.", "Larger values of the parameter \\(p\\) move the center of the chi squared distribution farther to the right.", "As \\(p\\) goes to infinity, the chi squared distribution begins to look more and more normal in shape.", "Note that the symbol in the denominator of the chi squared distribution, \\(\\Gamma(p/2)\\), is the Gamma function of \\(p/2\\).", "(See Gamma Function in Wikipedia for details.) Graphical Form Comments It is important to remember that the chi squared distribution is only defined for \\(x\\geq 0\\) and for positive values of the parameter \\(p\\).", "This is unlike the normal distribution which is defined for all numbers \\(x\\) from negative infinity to positive infinity as well as for all values of \\(\\mu\\) from negative infinity to positive infinity."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "The Chi Squared Distribution"
  },
  {
    "id": 179,
    "title": "📍 The t Distribution",
    "url": "MakingInference.html#thetdistribution",
    "content": "The t Distribution A close friend of the normal distribution is the t distribution. Although the t distribution is seldom used to model real life data, the distribution is used extensively in hypothesis testing. For example, it is the sampling distribution of the one sample t statistic. It also shows up in many other places, like in regression, in the independent samples t test, and in the paired samples t test. Mathematical Formula \\[ f(x|p) = \\frac{\\Gamma\\left(\\frac{p+1}{2}\\right)}{\\Gamma\\left(\\frac{p}{2}\\right)}\\frac{1}{\\sqrt{p\\pi}}\\frac{1}{\\left(1 + \\left(\\frac{x^2}{p}\\right)\\right)^{(p+1)/2}} \\] Notice that, similar to the chi squared distribution, the t distribution has only one parameter, the degrees of freedom \\(p\\). As the single parameter \\(p\\) is varied from \\(p=1\\), to \\(p=2\\), …, \\(p=5\\), and larger and larger numbers, the resulting distribution becomes more and more normal in shape. Note that the expressions \\(\\Gamma\\left(\\frac{p+1}{2}\\right)\\) and \\(\\Gamma(p/2)\\), refer to the Gamma function. (See Gamma Function in Wikipedia for details.) Graphical Form Comments When the degrees of freedom \\(p=30\\), the resulting t distribution is almost indistinguishable visually from the normal distribution. This is one of the reasons that a sample size of 30 is often used as a rule of thumb for the sample size being “large enough” to assume the sampling distribution of the sample mean is approximately normal.",
    "sentences": ["A close friend of the normal distribution is the t distribution.", "Although the t distribution is seldom used to model real life data, the distribution is used extensively in hypothesis testing.", "For example, it is the sampling distribution of the one sample t statistic.", "It also shows up in many other places, like in regression, in the independent samples t test, and in the paired samples t test.", "Mathematical Formula \\[ f(x|p) = \\frac{\\Gamma\\left(\\frac{p+1}{2}\\right)}{\\Gamma\\left(\\frac{p}{2}\\right)}\\frac{1}{\\sqrt{p\\pi}}\\frac{1}{\\left(1 + \\left(\\frac{x^2}{p}\\right)\\right)^{(p+1)/2}} \\] Notice that, similar to the chi squared distribution, the t distribution has only one parameter, the degrees of freedom \\(p\\).", "As the single parameter \\(p\\) is varied from \\(p=1\\), to \\(p=2\\), …, \\(p=5\\), and larger and larger numbers, the resulting distribution becomes more and more normal in shape.", "Note that the expressions \\(\\Gamma\\left(\\frac{p+1}{2}\\right)\\) and \\(\\Gamma(p/2)\\), refer to the Gamma function.", "(See Gamma Function in Wikipedia for details.) Graphical Form Comments When the degrees of freedom \\(p=30\\), the resulting t distribution is almost indistinguishable visually from the normal distribution.", "This is one of the reasons that a sample size of 30 is often used as a rule of thumb for the sample size being “large enough” to assume the sampling distribution of the sample mean is approximately normal."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "The t Distribution"
  },
  {
    "id": 180,
    "title": "📍 The F Distribution",
    "url": "MakingInference.html#thefdistribution",
    "content": "The F Distribution Another commonly used distribution for test statistics, like in ANOVA and regression, is the F distribution. Technically speaking, the F distribution is the ratio of two chi squared random variables that are each divided by their respective degrees of freedom. Mathematical Formula \\[ f(x|p_1,p_2) = \\frac{\\Gamma\\left(\\frac{p_1+p_2}{2}\\right)}{\\Gamma\\left(\\frac{p_1}{2}\\right)\\Gamma\\left(\\frac{p_2}{2}\\right)}\\frac{\\left(\\frac{p_1}{p_2}\\right)^{p_1/2}x^{(p_1-2)/2}}{\\left(1+\\left(\\frac{p_1}{p_2}\\right)x\\right)^{(p_1+p_2)/2}} \\] where \\(x\\geq 0\\) and the parameters \\(p_1\\) and \\(p_2\\) are the “numerator” and “denominator” degrees of freedom, respectively. Graphical Form Comments The effects of the parameters \\(p_1\\) and \\(p_2\\) on the F distribution are complicated, but generally speaking, as they both increase the distribution becomes more and more normal in shape.",
    "sentences": ["Another commonly used distribution for test statistics, like in ANOVA and regression, is the F distribution.", "Technically speaking, the F distribution is the ratio of two chi squared random variables that are each divided by their respective degrees of freedom.", "Mathematical Formula \\[ f(x|p_1,p_2) = \\frac{\\Gamma\\left(\\frac{p_1+p_2}{2}\\right)}{\\Gamma\\left(\\frac{p_1}{2}\\right)\\Gamma\\left(\\frac{p_2}{2}\\right)}\\frac{\\left(\\frac{p_1}{p_2}\\right)^{p_1/2}x^{(p_1-2)/2}}{\\left(1+\\left(\\frac{p_1}{p_2}\\right)x\\right)^{(p_1+p_2)/2}} \\] where \\(x\\geq 0\\) and the parameters \\(p_1\\) and \\(p_2\\) are the “numerator” and “denominator” degrees of freedom, respectively.", "Graphical Form Comments The effects of the parameters \\(p_1\\) and \\(p_2\\) on the F distribution are complicated, but generally speaking, as they both increase the distribution becomes more and more normal in shape."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "The F Distribution"
  },
  {
    "id": 181,
    "title": "📍 Nonparametric Methods",
    "url": "MakingInference.html#nonparametricmethods",
    "content": "Nonparametric Methods Nonparametric methods place minimal assumptions on the distribution of data. They allow the data to “speak for itself.” They are typically less powerful than the parametric alternatives, but are more broadly applicable because fewer assumptions need to be satisfied. Nonparametric methods include Rank Sum Tests and Permutation Tests.",
    "sentences": ["Nonparametric methods place minimal assumptions on the distribution of data.", "They allow the data to “speak for itself.” They are typically less powerful than the parametric alternatives, but are more broadly applicable because fewer assumptions need to be satisfied.", "Nonparametric methods include Rank Sum Tests and Permutation Tests."],
    "type": "section",
    "page_title": "Making Inference",
    "section_title": "Nonparametric Methods"
  },
  {
    "id": 182,
    "title": "Numerical Summaries",
    "url": "NumericalSummaries.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Numerical Summaries There are many ways to numerically summarize data. The fundamental idea is to describe the center, or most probable values of the data, as well as the spread, or the possible values of the data. Mean \\[ \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} \\] Measure of Center | 1 Quantitative Variable Overview The “balance point” or “center of mass” of quantitative data. It is calculated by taking the numerical sum of the values divided by the number of values. Typically used in tandem with the standard deviation. Most appropriate for describing the most typical values for relatively normally distributed data. Influenced by outliers, so it is not appropriate for describing strongly skewed data. R Instructions To calculate a mean in R use the code: mean(object) object must be a quantitative variable, what R calls a “numeric vector.” Usually this is a column from a data set. Use na.rm=TRUE if there are missing values in object so that the code reads mean(object, na.rm=TRUE). Example Code Hover your mouse over the example codes to learn more. mean “mean” is an R function used to calculate the mean of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the mean function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 77.88235 Note that the single number showing above is the average Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveTemp =  “AveTemp” is just a name we made up. It will contain the results of the mean(…) function. mean( “mean” is an R function used to calculate the mean. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month aveTemp 5 65.55 6 79.1 7 83.9 8 83.97 9 76.9 Note that R calculated the mean Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, note that to get the “nicely formatted” table, you would have to use library(pander) airquality %>% group_by(Month) %>% summarise(aveTemp = mean(Temp)) %>% pander() A note about missing values in data… mean “mean” is an R function used to calculate the mean of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Ozone “Ozone” is a quantitative variable (numeric vector) from the “airquality” dataset. ,  The comma allows us to specify optional commands. na.rm=TRUE Missing values are called “NA” in R. If data contains missing values, mean(...) will give “NA” as the result unless we “remove” (rm) the “NA” (na) values. )Closing parenthsis for the mean function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 42.12931 Note that the single number showing above is the average Ozone from the airquality dataset. Because the Ozone column had missing values, we had to use the option na.rm=TRUE to get the mean to calculate. If we had left it off, we would have gotten an “NA” result: mean(airquality$Ozone) ## [1] NA Explanation The mathematical formula used to compute the mean of data is given by the formula to the left. Although the formula looks complicated, all it states is “add all the data values up and divide by the total number of values.” Read on to learn what all the symbols in the formula represent. Symbols in the Formula \\(\\bar{x}\\) is read “x-bar” and is the symbol typically used for the sample mean, the mean computed on a sample of data from a population. \\(\\Sigma\\), the capital Greek letter “sigma,” is the symbol used to imply “add all of the data values up.” The \\(x_i\\)’s are the data values. The \\(i\\) in the \\(x_i\\) is stated to go from \\(i=1\\) all the way up to \\(n\\). In other words, data value 1 is represented by \\(x_1\\), data value 2: \\(x_2\\), \\(\\ldots\\), up through the last data value \\(x_n\\). In general, we just write \\(x_i\\). \\(n\\) represents the sample size, or number of data values. Population Mean When all of the data from a population is available, the population mean is calculated instead of the sample mean. The mathematical formula for the population mean is the same as the formula for the sample mean, but is written with slightly different notation. \\[ \\mu = \\frac{\\sum_{i=1}^N x_i}{N} \\] Notice that the symbol for the population mean is \\(\\mu\\), pronounced “mew,” another Greek letter. (Review your Greek alphabet.) The only other difference between the two formulas is that the sample mean uses a sample of data, denoted by \\(n\\), while the population mean uses all the population data, denoted by \\(N\\). Physical Interpretation The mean is sometimes described as the “balance point” of the data. The following example will demonstrate. Say there are \\(n=5\\) data points with the following values. \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) The sample mean is calculated as follows. \\[ \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} = \\frac{2 + 5 + 6 + 7 + 10}{5} = 6 \\] If these values were plotted, and an “infinitely thin bar” connected the points, then the bar would “balance” at the mean (the triangle) as shown below. Middle of the Deviations The above plot demonstrates that there are equal, but opposite, “sums of deviations” to either side of the mean. Note that a deviation is defined as the distance from the mean to a given point. Thus, \\(x_1\\) has a deviation of -4 from the mean, \\(x_2\\) a deviation of -1, \\(x_3\\) a deviation of 0, \\(x_4\\) a deviation of 1, and \\(x_5\\) a deviation of 4. To the left there is a sum of deviations equal to -5 and on the right, a sum of deviations equal to 5. This can be verified to hold for any scenario. Effect of Outliers The mean can be strongly influenced by outliers, points that deviate abnormally from the mean. This is shown below by changing \\(x_5\\) to be 20. Note that the deviation of \\(x_5\\) is 12, and the sum of deviations to the left of the mean (\\(\\bar{x}=8\\)) is \\(-1 + -2 + -3 + -6 = -12\\). The mean of the altered data \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 20\\) is now \\(\\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} = \\frac{2 + 5 + 6 + 7 + 20}{5} = 8\\). Median \\[ \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} \\] \\(\\uparrow\\) even \\(n\\) odd \\(\\downarrow\\) \\[ x_{((n+1)/2)} \\] Measure of Center | 1 Quantitative Variable Overview The “middle data point,” i.e., the 50\\(^{th}\\) percentile. Half of the data is below the median and half is above the median. Typically used in tandem with the five-number summary to describe skewed data because it is not heavily influenced by outliers, i.e., it is robust. Can also be used with normally distributed data, but the mean and standard deviation are more useful measures in such cases. R Instructions To calculate a median in R use the code: median(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code median “median” is an R function used to calculate the median of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the median function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 79 Note that the single number showing above is the median Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. medTemp =  “medTemp” is just a name we made up. It will contain the results of the median(…) function. median( “median” is an R function used to calculate the median. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month medTemp 5 66 6 78 7 84 8 82 9 76 Note that R calculated the median Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(medTemp = median(Temp)) %>% pander() Explanation The mathematical formula used to compute the median of data depends on whether \\(n\\), the number of data points in the sample, is even or odd. If \\(n\\) is even, then there is no “middle” data point, so the middle two values are averaged. \\[ \\text{Median} = \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} \\] If \\(n\\) is odd, then the middle data point is the median. \\[ \\text{Median} = x_{((n+1)/2)} \\] Symbols in the Formula There is no generally accepted symbol for the median. Sometimes a capital \\(M\\) or even lower-case \\(m\\) is used, but generally the word median is just written out. \\(x_{(n/2)}\\) represents the data value that is in the \\((n/2)^{th}\\) position in the ordered list of values. It only exists when \\(n\\) is even. \\(x_{(n/2+1)}\\) represents the data value that immediately follows the \\((n/2)^{th}\\) value in the ordered list of values. It only exists when \\(n\\) is even. \\(x_{((n+1)/2)}\\) represents the data value that is in the \\(((n+1)/2)^{th}\\) position in the ordered list of values. It only exists when \\(n\\) is odd. \\(n\\) represents the sample size, or number of data values in the sample. Population Median When all of the data from a population is available, the population median is calculated by the above formulas with the slight change that \\(N\\), the total number of data values in the population, instead of \\(n\\), the number of values in the sample, is used. If \\(N\\) is even, then there is no “middle” data point, so the middle two values are averaged. \\[ \\text{Median} = \\frac{x_{(N/2)}+x_{(N/2+1)}}{2} \\] If \\(N\\) is odd, then the middle data point is the median. \\[ \\text{Median} = x_{((N+1)/2)} \\] Physical Interpretation The median is the \\(50^{th}\\) percentile of the data. Say there are \\(n=5\\) data points in the sample with the following values. \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) The sample median is calculated as follows. Note that \\(n=5\\) is odd. \\[ \\text{Median} = x_{((n+1)/2)} = x_{((5+1)/2)} = x_{(3)} = 6 \\] When these values are plotted it is clear that exactly 50% of the data (excluding the median) is to either side of the median. Second Example Say there was a sixth value in the data set equal to 10, so that \\(n=6\\) is even. \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) \\(x_6 = 10\\) \\[ \\text{Median} = \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} = \\frac{x_{(6/2)}+x_{(6/2+1)}}{2} = \\frac{x_{(3)}+x_{(4)}}{2} = \\frac{6+7}{2} = 6.5 \\] Effect of Outliers The median is not greatly influenced by outliers. It is said to be robust. This is shown below by changing \\(x_6\\) to be 20, which does not change the value of the median. Mode Most Frequent Value Measure of Center | 1 Quantitative or Qualitative Variable Overview The most commonly occurring value. There may be more than one mode. Seldom used, but sometimes useful. R Instructions R will not calculate a mode directly. However, to tabulate the number of times each value occurs in a dataset, use the code: table(object) object can be quantitative or qualitative, but should contain at least one repeated value or table() is not useful. Example Code Hover your mouse over the example codes to learn more. table “table” is an R function used to count how many times each observation occurs in a list of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Month “Month” is a qualitative vari",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Numerical Summaries There are many ways to numerically summarize data.", "The fundamental idea is to describe the center, or most probable values of the data, as well as the spread, or the possible values of the data.", "Mean \\[ \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} \\] Measure of Center | 1 Quantitative Variable Overview The “balance point” or “center of mass” of quantitative data.", "It is calculated by taking the numerical sum of the values divided by the number of values.", "Typically used in tandem with the standard deviation.", "Most appropriate for describing the most typical values for relatively normally distributed data.", "Influenced by outliers, so it is not appropriate for describing strongly skewed data.", "R Instructions To calculate a mean in R use the code: mean(object) object must be a quantitative variable, what R calls a “numeric vector.” Usually this is a column from a data set.", "Use na.rm=TRUE if there are missing values in object so that the code reads mean(object, na.rm=TRUE).", "Example Code Hover your mouse over the example codes to learn more.", "mean “mean” is an R function used to calculate the mean of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset.", "Type “View(airquality)” in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.", ")Closing parenthsis for the mean function.", "    Press Enter to run the code if you have typed it in yourself.", "You can also click here to view the output.", " …  Click to View Output.", "## [1] 77.88235 Note that the single number showing above is the average Temp from the airquality dataset.", "library(tidyverse) tidyverse is an R Package that is very useful for working with data.", "airquality airquality is a dataset in R.", " %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.", "   group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column.", "Month “Month” is a column from the airquality dataset that can be treated as qualitative.", ") Functions must always end with a closing parenthesis.", " %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.", "   summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data.", "aveTemp =  “AveTemp” is just a name we made up.", "It will contain the results of the mean(…) function.", "mean( “mean” is an R function used to calculate the mean.", "Temp Temp is a quantitative variable (numeric vector) from the airquality dataset.", ") Functions must always end with a closing parenthesis.", ") Functions must always end with a closing parenthesis.", "    Press Enter to run the code.", " …  Click to View Output.", "Month aveTemp 5 65.55 6 79.1 7 83.9 8 83.97 9 76.9 Note that R calculated the mean Temp for each month in Month from the airquality dataset.", "May (5), June (6), July (7), August (8), and September (9), respectively.", "Further, note that to get the “nicely formatted” table, you would have to use library(pander) airquality %>% group_by(Month) %>% summarise(aveTemp = mean(Temp)) %>% pander() A note about missing values in data… mean “mean” is an R function used to calculate the mean of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset.", "Type “View(airquality)” in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Ozone “Ozone” is a quantitative variable (numeric vector) from the “airquality” dataset.", ",  The comma allows us to specify optional commands.", "na.rm=TRUE Missing values are called “NA” in R.", "If data contains missing values, mean(...) will give “NA” as the result unless we “remove” (rm) the “NA” (na) values."],
    "type": "page",
    "page_title": "Numerical Summaries"
  },
  {
    "id": 183,
    "title": "📍 Mean",
    "url": "NumericalSummaries.html#mean",
    "content": "Mean \\[ \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} \\] Measure of Center | 1 Quantitative Variable Overview The “balance point” or “center of mass” of quantitative data. It is calculated by taking the numerical sum of the values divided by the number of values. Typically used in tandem with the standard deviation. Most appropriate for describing the most typical values for relatively normally distributed data. Influenced by outliers, so it is not appropriate for describing strongly skewed data. R Instructions To calculate a mean in R use the code: mean(object) object must be a quantitative variable, what R calls a “numeric vector.” Usually this is a column from a data set. Use na.rm=TRUE if there are missing values in object so that the code reads mean(object, na.rm=TRUE). Example Code Hover your mouse over the example codes to learn more. mean “mean” is an R function used to calculate the mean of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the mean function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 77.88235 Note that the single number showing above is the average Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveTemp =  “AveTemp” is just a name we made up. It will contain the results of the mean(…) function. mean( “mean” is an R function used to calculate the mean. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month aveTemp 5 65.55 6 79.1 7 83.9 8 83.97 9 76.9 Note that R calculated the mean Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, note that to get the “nicely formatted” table, you would have to use library(pander) airquality %>% group_by(Month) %>% summarise(aveTemp = mean(Temp)) %>% pander() A note about missing values in data… mean “mean” is an R function used to calculate the mean of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Ozone “Ozone” is a quantitative variable (numeric vector) from the “airquality” dataset. ,  The comma allows us to specify optional commands. na.rm=TRUE Missing values are called “NA” in R. If data contains missing values, mean(...) will give “NA” as the result unless we “remove” (rm) the “NA” (na) values. )Closing parenthsis for the mean function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 42.12931 Note that the single number showing above is the average Ozone from the airquality dataset. Because the Ozone column had missing values, we had to use the option na.rm=TRUE to get the mean to calculate. If we had left it off, we would have gotten an “NA” result: mean(airquality$Ozone) ## [1] NA",
    "sentences": ["\\[ \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} \\]", "Measure of Center | 1 Quantitative Variable", "Overview The “balance point” or “center of mass” of quantitative data.", "It is calculated by taking the numerical sum of the values divided by the number of values.", "Typically used in tandem with the standard deviation.", "Most appropriate for describing the most typical values for relatively normally distributed data.", "Influenced by outliers, so it is not appropriate for describing strongly skewed data.", "R Instructions To calculate a mean in R use the code: mean(object) object must be a quantitative variable, what R calls a “numeric vector.” Usually this is a column from a data set.", "Use na.rm=TRUE if there are missing values in object so that the code reads mean(object, na.rm=TRUE).", "Example Code Hover your mouse over the example codes to learn more."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Mean"
  },
  {
    "id": 184,
    "title": "📍 Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview The “balance point” or “center of mass” of quantitative data. It is calculated by taking the numerical sum of the values divided by the number of values. Typically used in tandem with the standard deviation. Most appropriate for describing the most typical values for relatively normally distributed data. Influenced by outliers, so it is not appropriate for describing strongly skewed data.",
    "sentences": ["The “balance point” or “center of mass” of quantitative data.", "It is calculated by taking the numerical sum of the values divided by the number of values.", "Typically used in tandem with the standard deviation.", "Most appropriate for describing the most typical values for relatively normally distributed data.", "Influenced by outliers, so it is not appropriate for describing strongly skewed data."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 185,
    "title": "📍 R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To calculate a mean in R use the code: mean(object) object must be a quantitative variable, what R calls a “numeric vector.” Usually this is a column from a data set. Use na.rm=TRUE if there are missing values in object so that the code reads mean(object, na.rm=TRUE). Example Code Hover your mouse over the example codes to learn more. mean “mean” is an R function used to calculate the mean of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the mean function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 77.88235 Note that the single number showing above is the average Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveTemp =  “AveTemp” is just a name we made up. It will contain the results of the mean(…) function. mean( “mean” is an R function used to calculate the mean. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month aveTemp 5 65.55 6 79.1 7 83.9 8 83.97 9 76.9 Note that R calculated the mean Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, note that to get the “nicely formatted” table, you would have to use library(pander) airquality %>% group_by(Month) %>% summarise(aveTemp = mean(Temp)) %>% pander() A note about missing values in data… mean “mean” is an R function used to calculate the mean of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Ozone “Ozone” is a quantitative variable (numeric vector) from the “airquality” dataset. ,  The comma allows us to specify optional commands. na.rm=TRUE Missing values are called “NA” in R. If data contains missing values, mean(...) will give “NA” as the result unless we “remove” (rm) the “NA” (na) values. )Closing parenthsis for the mean function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 42.12931 Note that the single number showing above is the average Ozone from the airquality dataset. Because the Ozone column had missing values, we had to use the option na.rm=TRUE to get the mean to calculate. If we had left it off, we would have gotten an “NA” result: mean(airquality$Ozone) ## [1] NA",
    "sentences": ["To calculate a mean in R use the code: mean(object) object must be a quantitative variable, what R calls a “numeric vector.” Usually this is a column from a data set.", "Use na.rm=TRUE if there are missing values in object so that the code reads mean(object, na.rm=TRUE).", "Example Code Hover your mouse over the example codes to learn more.", "mean “mean” is an R function used to calculate the mean of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset.", "Type “View(airquality)” in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 186,
    "title": "📍 Explanation",
    "url": "NumericalSummaries.html#explanation",
    "content": "Explanation The mathematical formula used to compute the mean of data is given by the formula to the left. Although the formula looks complicated, all it states is “add all the data values up and divide by the total number of values.” Read on to learn what all the symbols in the formula represent. Symbols in the Formula \\(\\bar{x}\\) is read “x-bar” and is the symbol typically used for the sample mean, the mean computed on a sample of data from a population. \\(\\Sigma\\), the capital Greek letter “sigma,” is the symbol used to imply “add all of the data values up.” The \\(x_i\\)’s are the data values. The \\(i\\) in the \\(x_i\\) is stated to go from \\(i=1\\) all the way up to \\(n\\). In other words, data value 1 is represented by \\(x_1\\), data value 2: \\(x_2\\), \\(\\ldots\\), up through the last data value \\(x_n\\). In general, we just write \\(x_i\\). \\(n\\) represents the sample size, or number of data values. Population Mean When all of the data from a population is available, the population mean is calculated instead of the sample mean. The mathematical formula for the population mean is the same as the formula for the sample mean, but is written with slightly different notation. \\[ \\mu = \\frac{\\sum_{i=1}^N x_i}{N} \\] Notice that the symbol for the population mean is \\(\\mu\\), pronounced “mew,” another Greek letter. (Review your Greek alphabet.) The only other difference between the two formulas is that the sample mean uses a sample of data, denoted by \\(n\\), while the population mean uses all the population data, denoted by \\(N\\). Physical Interpretation The mean is sometimes described as the “balance point” of the data. The following example will demonstrate. Say there are \\(n=5\\) data points with the following values. \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) The sample mean is calculated as follows. \\[ \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} = \\frac{2 + 5 + 6 + 7 + 10}{5} = 6 \\] If these values were plotted, and an “infinitely thin bar” connected the points, then the bar would “balance” at the mean (the triangle) as shown below. Middle of the Deviations The above plot demonstrates that there are equal, but opposite, “sums of deviations” to either side of the mean. Note that a deviation is defined as the distance from the mean to a given point. Thus, \\(x_1\\) has a deviation of -4 from the mean, \\(x_2\\) a deviation of -1, \\(x_3\\) a deviation of 0, \\(x_4\\) a deviation of 1, and \\(x_5\\) a deviation of 4. To the left there is a sum of deviations equal to -5 and on the right, a sum of deviations equal to 5. This can be verified to hold for any scenario. Effect of Outliers The mean can be strongly influenced by outliers, points that deviate abnormally from the mean. This is shown below by changing \\(x_5\\) to be 20. Note that the deviation of \\(x_5\\) is 12, and the sum of deviations to the left of the mean (\\(\\bar{x}=8\\)) is \\(-1 + -2 + -3 + -6 = -12\\). The mean of the altered data \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 20\\) is now \\(\\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} = \\frac{2 + 5 + 6 + 7 + 20}{5} = 8\\).",
    "sentences": ["The mathematical formula used to compute the mean of data is given by the formula to the left.", "Although the formula looks complicated, all it states is “add all the data values up and divide by the total number of values.” Read on to learn what all the symbols in the formula represent.", "Symbols in the Formula \\(\\bar{x}\\) is read “x-bar” and is the symbol typically used for the sample mean, the mean computed on a sample of data from a population.", "\\(\\Sigma\\), the capital Greek letter “sigma,” is the symbol used to imply “add all of the data values up.” The \\(x_i\\)’s are the data values.", "The \\(i\\) in the \\(x_i\\) is stated to go from \\(i=1\\) all the way up to \\(n\\).", "In other words, data value 1 is represented by \\(x_1\\), data value 2: \\(x_2\\), \\(\\ldots\\), up through the last data value \\(x_n\\).", "In general, we just write \\(x_i\\).", "\\(n\\) represents the sample size, or number of data values.", "Population Mean When all of the data from a population is available, the population mean is calculated instead of the sample mean.", "The mathematical formula for the population mean is the same as the formula for the sample mean, but is written with slightly different notation."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Explanation"
  },
  {
    "id": 187,
    "title": "📍 Median",
    "url": "NumericalSummaries.html#median",
    "content": "Median \\[ \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} \\] \\(\\uparrow\\) even \\(n\\) odd \\(\\downarrow\\) \\[ x_{((n+1)/2)} \\] Measure of Center | 1 Quantitative Variable Overview The “middle data point,” i.e., the 50\\(^{th}\\) percentile. Half of the data is below the median and half is above the median. Typically used in tandem with the five-number summary to describe skewed data because it is not heavily influenced by outliers, i.e., it is robust. Can also be used with normally distributed data, but the mean and standard deviation are more useful measures in such cases. R Instructions To calculate a median in R use the code: median(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code median “median” is an R function used to calculate the median of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the median function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 79 Note that the single number showing above is the median Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. medTemp =  “medTemp” is just a name we made up. It will contain the results of the median(…) function. median( “median” is an R function used to calculate the median. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month medTemp 5 66 6 78 7 84 8 82 9 76 Note that R calculated the median Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(medTemp = median(Temp)) %>% pander()",
    "sentences": ["\\[ \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} \\] \\(\\uparrow\\) even \\(n\\) odd \\(\\downarrow\\) \\[ x_{((n+1)/2)} \\]", "Measure of Center | 1 Quantitative Variable", "Overview The “middle data point,” i.e., the 50\\(^{th}\\) percentile.", "Half of the data is below the median and half is above the median.", "Typically used in tandem with the five-number summary to describe skewed data because it is not heavily influenced by outliers, i.e., it is robust.", "Can also be used with normally distributed data, but the mean and standard deviation are more useful measures in such cases.", "R Instructions To calculate a median in R use the code: median(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code median “median” is an R function used to calculate the median of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Median"
  },
  {
    "id": 188,
    "title": "📍 Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview The “middle data point,” i.e., the 50\\(^{th}\\) percentile. Half of the data is below the median and half is above the median. Typically used in tandem with the five-number summary to describe skewed data because it is not heavily influenced by outliers, i.e., it is robust. Can also be used with normally distributed data, but the mean and standard deviation are more useful measures in such cases.",
    "sentences": ["The “middle data point,” i.e., the 50\\(^{th}\\) percentile.", "Half of the data is below the median and half is above the median.", "Typically used in tandem with the five-number summary to describe skewed data because it is not heavily influenced by outliers, i.e., it is robust.", "Can also be used with normally distributed data, but the mean and standard deviation are more useful measures in such cases."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 189,
    "title": "📍 R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To calculate a median in R use the code: median(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code median “median” is an R function used to calculate the median of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the median function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 79 Note that the single number showing above is the median Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. medTemp =  “medTemp” is just a name we made up. It will contain the results of the median(…) function. median( “median” is an R function used to calculate the median. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month medTemp 5 66 6 78 7 84 8 82 9 76 Note that R calculated the median Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(medTemp = median(Temp)) %>% pander()",
    "sentences": ["To calculate a median in R use the code: median(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code median “median” is an R function used to calculate the median of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset.", "Type “View(airquality)” in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.", ")Closing parenthsis for the median function.", "    Press Enter to run the code if you have typed it in yourself.", "You can also click here to view the output."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 190,
    "title": "📍 Explanation",
    "url": "NumericalSummaries.html#explanation",
    "content": "Explanation The mathematical formula used to compute the median of data depends on whether \\(n\\), the number of data points in the sample, is even or odd. If \\(n\\) is even, then there is no “middle” data point, so the middle two values are averaged. \\[ \\text{Median} = \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} \\] If \\(n\\) is odd, then the middle data point is the median. \\[ \\text{Median} = x_{((n+1)/2)} \\] Symbols in the Formula There is no generally accepted symbol for the median. Sometimes a capital \\(M\\) or even lower-case \\(m\\) is used, but generally the word median is just written out. \\(x_{(n/2)}\\) represents the data value that is in the \\((n/2)^{th}\\) position in the ordered list of values. It only exists when \\(n\\) is even. \\(x_{(n/2+1)}\\) represents the data value that immediately follows the \\((n/2)^{th}\\) value in the ordered list of values. It only exists when \\(n\\) is even. \\(x_{((n+1)/2)}\\) represents the data value that is in the \\(((n+1)/2)^{th}\\) position in the ordered list of values. It only exists when \\(n\\) is odd. \\(n\\) represents the sample size, or number of data values in the sample. Population Median When all of the data from a population is available, the population median is calculated by the above formulas with the slight change that \\(N\\), the total number of data values in the population, instead of \\(n\\), the number of values in the sample, is used. If \\(N\\) is even, then there is no “middle” data point, so the middle two values are averaged. \\[ \\text{Median} = \\frac{x_{(N/2)}+x_{(N/2+1)}}{2} \\] If \\(N\\) is odd, then the middle data point is the median. \\[ \\text{Median} = x_{((N+1)/2)} \\] Physical Interpretation The median is the \\(50^{th}\\) percentile of the data. Say there are \\(n=5\\) data points in the sample with the following values. \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) The sample median is calculated as follows. Note that \\(n=5\\) is odd. \\[ \\text{Median} = x_{((n+1)/2)} = x_{((5+1)/2)} = x_{(3)} = 6 \\] When these values are plotted it is clear that exactly 50% of the data (excluding the median) is to either side of the median. Second Example Say there was a sixth value in the data set equal to 10, so that \\(n=6\\) is even. \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) \\(x_6 = 10\\) \\[ \\text{Median} = \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} = \\frac{x_{(6/2)}+x_{(6/2+1)}}{2} = \\frac{x_{(3)}+x_{(4)}}{2} = \\frac{6+7}{2} = 6.5 \\] Effect of Outliers The median is not greatly influenced by outliers. It is said to be robust. This is shown below by changing \\(x_6\\) to be 20, which does not change the value of the median.",
    "sentences": ["The mathematical formula used to compute the median of data depends on whether \\(n\\), the number of data points in the sample, is even or odd.", "If \\(n\\) is even, then there is no “middle” data point, so the middle two values are averaged.", "\\[ \\text{Median} = \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} \\] If \\(n\\) is odd, then the middle data point is the median.", "\\[ \\text{Median} = x_{((n+1)/2)} \\] Symbols in the Formula There is no generally accepted symbol for the median.", "Sometimes a capital \\(M\\) or even lower-case \\(m\\) is used, but generally the word median is just written out.", "\\(x_{(n/2)}\\) represents the data value that is in the \\((n/2)^{th}\\) position in the ordered list of values.", "It only exists when \\(n\\) is even.", "\\(x_{(n/2+1)}\\) represents the data value that immediately follows the \\((n/2)^{th}\\) value in the ordered list of values.", "It only exists when \\(n\\) is even.", "\\(x_{((n+1)/2)}\\) represents the data value that is in the \\(((n+1)/2)^{th}\\) position in the ordered list of values."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Explanation"
  },
  {
    "id": 191,
    "title": "📍 Mode",
    "url": "NumericalSummaries.html#mode",
    "content": "Mode Most Frequent Value Measure of Center | 1 Quantitative or Qualitative Variable Overview The most commonly occurring value. There may be more than one mode. Seldom used, but sometimes useful. R Instructions R will not calculate a mode directly. However, to tabulate the number of times each value occurs in a dataset, use the code: table(object) object can be quantitative or qualitative, but should contain at least one repeated value or table() is not useful. Example Code Hover your mouse over the example codes to learn more. table “table” is an R function used to count how many times each observation occurs in a list of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Month “Month” is a qualitative variable (technically a numeric vector) from the “airquality” dataset that contains repeated values. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## ## 5 6 7 8 9 ## 31 30 31 31 30 Note that the modes would be 5, 7, and 8 because these months all have the most (31) days in them. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveTemp = mean(Temp),  Computes the mean of the Temp column. medTemp = median(Temp),  Computes the median of the Temp column. sampleSize = n( ) Counts how many times each Month (the group_by statement) occurs in the dataset. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month aveTemp medTemp sampeSize 5 65.55 66 31 6 79.1 78 30 7 83.9 84 31 8 83.97 82 31 9 76.9 76 30 Note that R calculated the median Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(aveTemp = mean(Temp), medTemp = median(Temp), sampeSize = n()) %>% pander()",
    "sentences": ["Most Frequent Value", "Measure of Center | 1 Quantitative or Qualitative Variable", "Overview The most commonly occurring value.", "There may be more than one mode.", "Seldom used, but sometimes useful.", "R Instructions R will not calculate a mode directly.", "However, to tabulate the number of times each value occurs in a dataset, use the code: table(object) object can be quantitative or qualitative, but should contain at least one repeated value or table() is not useful.", "Example Code Hover your mouse over the example codes to learn more.", "table “table” is an R function used to count how many times each observation occurs in a list of data.", "( Parenthesis to begin the function."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Mode"
  },
  {
    "id": 192,
    "title": "📍 Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview The most commonly occurring value. There may be more than one mode. Seldom used, but sometimes useful.",
    "sentences": ["The most commonly occurring value.", "There may be more than one mode.", "Seldom used, but sometimes useful."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 193,
    "title": "📍 R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions R will not calculate a mode directly. However, to tabulate the number of times each value occurs in a dataset, use the code: table(object) object can be quantitative or qualitative, but should contain at least one repeated value or table() is not useful. Example Code Hover your mouse over the example codes to learn more. table “table” is an R function used to count how many times each observation occurs in a list of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Month “Month” is a qualitative variable (technically a numeric vector) from the “airquality” dataset that contains repeated values. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## ## 5 6 7 8 9 ## 31 30 31 31 30 Note that the modes would be 5, 7, and 8 because these months all have the most (31) days in them. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveTemp = mean(Temp),  Computes the mean of the Temp column. medTemp = median(Temp),  Computes the median of the Temp column. sampleSize = n( ) Counts how many times each Month (the group_by statement) occurs in the dataset. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month aveTemp medTemp sampeSize 5 65.55 66 31 6 79.1 78 30 7 83.9 84 31 8 83.97 82 31 9 76.9 76 30 Note that R calculated the median Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(aveTemp = mean(Temp), medTemp = median(Temp), sampeSize = n()) %>% pander()",
    "sentences": ["R will not calculate a mode directly.", "However, to tabulate the number of times each value occurs in a dataset, use the code: table(object) object can be quantitative or qualitative, but should contain at least one repeated value or table() is not useful.", "Example Code Hover your mouse over the example codes to learn more.", "table “table” is an R function used to count how many times each observation occurs in a list of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset.", "Type “View(airquality)” in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Month “Month” is a qualitative variable (technically a numeric vector) from the “airquality” dataset that contains repeated values."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 194,
    "title": "📍 Minimum",
    "url": "NumericalSummaries.html#minimum",
    "content": "Minimum \\[ x_{(1)} \\] Measure of Spread | 1 Quantitative Variable Overview The smallest occurring data value. One of the numerical summaries in the five-number summary. Typically not useful on its own. Gives a good feel for the spread in the left tail of the distribution when used with the five-number summary. R Instructions To calculate a minimum in R use the code: min(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more. min “min” is an R function used to calculate the minimum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 56 Note that the single number showing above is the minimum Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. minTemp =  “minTemp” is just a name we made up. It will contain the results of the median(…) function. min( “min” is an R function used to calculate the minimum. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month minTemp 5 56 6 65 7 73 8 72 9 63 Note that R calculated the minimum Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(minTemp = min(Temp)) %>% pander()",
    "sentences": ["\\[ x_{(1)} \\]", "Measure of Spread | 1 Quantitative Variable", "Overview The smallest occurring data value.", "One of the numerical summaries in the five-number summary.", "Typically not useful on its own.", "Gives a good feel for the spread in the left tail of the distribution when used with the five-number summary.", "R Instructions To calculate a minimum in R use the code: min(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more.", "min “min” is an R function used to calculate the minimum of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Minimum"
  },
  {
    "id": 195,
    "title": "📍 Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview The smallest occurring data value. One of the numerical summaries in the five-number summary. Typically not useful on its own. Gives a good feel for the spread in the left tail of the distribution when used with the five-number summary.",
    "sentences": ["The smallest occurring data value.", "One of the numerical summaries in the five-number summary.", "Typically not useful on its own.", "Gives a good feel for the spread in the left tail of the distribution when used with the five-number summary."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 196,
    "title": "📍 R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To calculate a minimum in R use the code: min(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more. min “min” is an R function used to calculate the minimum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 56 Note that the single number showing above is the minimum Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. minTemp =  “minTemp” is just a name we made up. It will contain the results of the median(…) function. min( “min” is an R function used to calculate the minimum. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month minTemp 5 56 6 65 7 73 8 72 9 63 Note that R calculated the minimum Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(minTemp = min(Temp)) %>% pander()",
    "sentences": ["To calculate a minimum in R use the code: min(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more.", "min “min” is an R function used to calculate the minimum of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset.", "Type “View(airquality)” in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.", ")Closing parenthsis for the function.", "    Press Enter to run the code if you have typed it in yourself."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 197,
    "title": "📍 Maximum",
    "url": "NumericalSummaries.html#maximum",
    "content": "Maximum \\[ x_{(n)} \\] Measure of Spread | 1 Quantitative Variable Overview The largest occurring data value. One of the numerical summaries in the five-number summary. Typically not useful on its own. Gives a good feel for the spread in the right tail of the distribution when used in the five-number summary. R Instructions To calculate a maximum in R use the code: max(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more. max “max” is an R function used to calculate the maximum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 97 Note that the single number showing above is the maximum Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. maxTemp =  “maxTemp” is just a name we made up. It will contain the results of the median(…) function. max( “max” is an R function used to calculate the maximum. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month maxTemp 5 81 6 93 7 92 8 97 9 93 Note that R calculated the maximum Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(maxTemp = max(Temp)) %>% pander()",
    "sentences": ["\\[ x_{(n)} \\]", "Measure of Spread | 1 Quantitative Variable", "Overview The largest occurring data value.", "One of the numerical summaries in the five-number summary.", "Typically not useful on its own.", "Gives a good feel for the spread in the right tail of the distribution when used in the five-number summary.", "R Instructions To calculate a maximum in R use the code: max(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more.", "max “max” is an R function used to calculate the maximum of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Maximum"
  },
  {
    "id": 198,
    "title": "📍 Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview The largest occurring data value. One of the numerical summaries in the five-number summary. Typically not useful on its own. Gives a good feel for the spread in the right tail of the distribution when used in the five-number summary.",
    "sentences": ["The largest occurring data value.", "One of the numerical summaries in the five-number summary.", "Typically not useful on its own.", "Gives a good feel for the spread in the right tail of the distribution when used in the five-number summary."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 199,
    "title": "📍 R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To calculate a maximum in R use the code: max(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more. max “max” is an R function used to calculate the maximum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 97 Note that the single number showing above is the maximum Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. maxTemp =  “maxTemp” is just a name we made up. It will contain the results of the median(…) function. max( “max” is an R function used to calculate the maximum. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month maxTemp 5 81 6 93 7 92 8 97 9 93 Note that R calculated the maximum Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(maxTemp = max(Temp)) %>% pander()",
    "sentences": ["To calculate a maximum in R use the code: max(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more.", "max “max” is an R function used to calculate the maximum of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset.", "Type “View(airquality)” in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.", ")Closing parenthsis for the function.", "    Press Enter to run the code if you have typed it in yourself."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 200,
    "title": "📍 Quartiles (five-number\r\nsummary)",
    "url": "NumericalSummaries.html#quartilesfivenumbersummary",
    "content": "Quartiles (five-number\r\nsummary) 25\\(^{th}\\), 50\\(^{th}\\), 75\\(^{th}\\) and 100\\(^{th}\\) Percentiles Measure of Center & Spread | 1 Quantitative Variable Overview Good for describing the spread of data, typically for skewed distributions. There are four quartiles. They make up the five-number summary when combined with the minimum. The second quartile is the median (50\\(^{th}\\) percentile) and the fourth quartile is the maximum (100\\(^{th}\\) percentile). The first quartile (\\(Q_1\\) or lower quartile) and third quartile (\\(Q_3\\) or upper quartile) show the spread of the “middle 50%” of the data, which is often called the interquartile range. Comparing the interquartile range to the minimum and maximum shows how the possible values spread out around the more probable values. R Instructions To calculate a five-number summary (and mean) in R use the code: quantile(object, percentile) object must be a quantitative variable, what R calls a “numeric vector.” percentile must be a value between 0 and 1. For the first quartile, it would be 0.25. For the third, it would be 0.75. Example Code Hover your mouse over the example codes to learn more. summary “summary” is an R function used to calculate the five-number summary (and mean) of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Median Mean 3rd Qu. 56 72 79 77.88 85 97 Showing above are the five-number summary and mean of Temp from the airquality dataset. Note, you must use pander(summary(airquality$Temp)) to get the nicely formatted output. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. min = min(Temp),  Computes the min of the Temp column. Q1 = quantile(Temp, 0.25),  Computes the first quartile of the Temp column. med = median(Temp),  Computes the second quartile of the Temp column, known as the median. Q3 = quantile(Temp, 0.75),  Computes the third quartile of the Temp column. max = max(Temp) Computes the max of the Temp column. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month min Q1 med Q3 max 5 56 60 66 69 81 6 65 76 78 82.75 93 7 73 81.5 84 86 92 8 72 79 82 88.5 97 9 63 71 76 81 93 Note that R calculated the five-number summary for Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(min = min(Temp), Q1 = quantile(Temp, c(.25)), med = median(Temp), Q3 = quantile(Temp, c(.75)), max = max(Temp)) %>% pander()",
    "sentences": ["25\\(^{th}\\), 50\\(^{th}\\), 75\\(^{th}\\) and 100\\(^{th}\\) Percentiles", "Measure of Center & Spread | 1 Quantitative Variable", "Overview Good for describing the spread of data, typically for skewed distributions.", "There are four quartiles.", "They make up the five-number summary when combined with the minimum.", "The second quartile is the median (50\\(^{th}\\) percentile) and the fourth quartile is the maximum (100\\(^{th}\\) percentile).", "The first quartile (\\(Q_1\\) or lower quartile) and third quartile (\\(Q_3\\) or upper quartile) show the spread of the “middle 50%” of the data, which is often called the interquartile range.", "Comparing the interquartile range to the minimum and maximum shows how the possible values spread out around the more probable values.", "R Instructions To calculate a five-number summary (and mean) in R use the code: quantile(object, percentile) object must be a quantitative variable, what R calls a “numeric vector.” percentile must be a value between 0 and 1.", "For the first quartile, it would be 0.25."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Quartiles (five-number\r\nsummary)"
  },
  {
    "id": 201,
    "title": "📍 Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview Good for describing the spread of data, typically for skewed distributions. There are four quartiles. They make up the five-number summary when combined with the minimum. The second quartile is the median (50\\(^{th}\\) percentile) and the fourth quartile is the maximum (100\\(^{th}\\) percentile). The first quartile (\\(Q_1\\) or lower quartile) and third quartile (\\(Q_3\\) or upper quartile) show the spread of the “middle 50%” of the data, which is often called the interquartile range. Comparing the interquartile range to the minimum and maximum shows how the possible values spread out around the more probable values.",
    "sentences": ["Good for describing the spread of data, typically for skewed distributions.", "There are four quartiles.", "They make up the five-number summary when combined with the minimum.", "The second quartile is the median (50\\(^{th}\\) percentile) and the fourth quartile is the maximum (100\\(^{th}\\) percentile).", "The first quartile (\\(Q_1\\) or lower quartile) and third quartile (\\(Q_3\\) or upper quartile) show the spread of the “middle 50%” of the data, which is often called the interquartile range.", "Comparing the interquartile range to the minimum and maximum shows how the possible values spread out around the more probable values."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 202,
    "title": "📍 R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To calculate a five-number summary (and mean) in R use the code: quantile(object, percentile) object must be a quantitative variable, what R calls a “numeric vector.” percentile must be a value between 0 and 1. For the first quartile, it would be 0.25. For the third, it would be 0.75. Example Code Hover your mouse over the example codes to learn more. summary “summary” is an R function used to calculate the five-number summary (and mean) of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Median Mean 3rd Qu. 56 72 79 77.88 85 97 Showing above are the five-number summary and mean of Temp from the airquality dataset. Note, you must use pander(summary(airquality$Temp)) to get the nicely formatted output. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. min = min(Temp),  Computes the min of the Temp column. Q1 = quantile(Temp, 0.25),  Computes the first quartile of the Temp column. med = median(Temp),  Computes the second quartile of the Temp column, known as the median. Q3 = quantile(Temp, 0.75),  Computes the third quartile of the Temp column. max = max(Temp) Computes the max of the Temp column. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month min Q1 med Q3 max 5 56 60 66 69 81 6 65 76 78 82.75 93 7 73 81.5 84 86 92 8 72 79 82 88.5 97 9 63 71 76 81 93 Note that R calculated the five-number summary for Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(min = min(Temp), Q1 = quantile(Temp, c(.25)), med = median(Temp), Q3 = quantile(Temp, c(.75)), max = max(Temp)) %>% pander()",
    "sentences": ["To calculate a five-number summary (and mean) in R use the code: quantile(object, percentile) object must be a quantitative variable, what R calls a “numeric vector.” percentile must be a value between 0 and 1.", "For the first quartile, it would be 0.25.", "For the third, it would be 0.75.", "Example Code Hover your mouse over the example codes to learn more.", "summary “summary” is an R function used to calculate the five-number summary (and mean) of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset.", "Type “View(airquality)” in R to see it.", "$ The $ allows us to access any variable from the airquality dataset."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 203,
    "title": "📍 Standard Deviation",
    "url": "NumericalSummaries.html#standarddeviation",
    "content": "Standard Deviation \\(s = \\sqrt{\\frac{\\sum_{i=1}^n(x_i-\\bar{x})^2}{n-1}}\\) Measure of Spread | 1 Quantitative Variable Overview Measures how spread out the data are from the mean. It is never negative and typically not zero. Larger values mean the data is highly variable. Smaller values mean the data is consistent and not as variable. It is typically used with the mean to describe the spread of relatively normally distributed data. The order of operations in the formula is important and for this reason it is sometimes called the “root mean squared error,” though the calculations are performed in reverse of that. (Study the formula on the left to understand.) The denominator \\(n-1\\) is called the degrees of freedom. R Instructions To calculate the standard deviation in R use the code: sd(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more. sd “sd” is an R function used to calculate the standard deviation of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 9.46527 Note that the single number showing above is the standard deviation of Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. sdTemp =  “sdTemp” is just a name we made up. It will contain the results of the sd(…) function. sd( “sd” is an R function used to calculate the standard deviation Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month sdTemp 5 6.855 6 6.599 7 4.316 8 6.585 9 8.356 Note that R calculated the standard deviation of Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively.",
    "sentences": ["\\(s = \\sqrt{\\frac{\\sum_{i=1}^n(x_i-\\bar{x})^2}{n-1}}\\)", "Measure of Spread | 1 Quantitative Variable", "Overview Measures how spread out the data are from the mean.", "It is never negative and typically not zero.", "Larger values mean the data is highly variable.", "Smaller values mean the data is consistent and not as variable.", "It is typically used with the mean to describe the spread of relatively normally distributed data.", "The order of operations in the formula is important and for this reason it is sometimes called the “root mean squared error,” though the calculations are performed in reverse of that.", "(Study the formula on the left to understand.) The denominator \\(n-1\\) is called the degrees of freedom.", "R Instructions To calculate the standard deviation in R use the code: sd(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Standard Deviation"
  },
  {
    "id": 204,
    "title": "📍 Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview Measures how spread out the data are from the mean. It is never negative and typically not zero. Larger values mean the data is highly variable. Smaller values mean the data is consistent and not as variable. It is typically used with the mean to describe the spread of relatively normally distributed data. The order of operations in the formula is important and for this reason it is sometimes called the “root mean squared error,” though the calculations are performed in reverse of that. (Study the formula on the left to understand.) The denominator \\(n-1\\) is called the degrees of freedom.",
    "sentences": ["Measures how spread out the data are from the mean.", "It is never negative and typically not zero.", "Larger values mean the data is highly variable.", "Smaller values mean the data is consistent and not as variable.", "It is typically used with the mean to describe the spread of relatively normally distributed data.", "The order of operations in the formula is important and for this reason it is sometimes called the “root mean squared error,” though the calculations are performed in reverse of that.", "(Study the formula on the left to understand.) The denominator \\(n-1\\) is called the degrees of freedom."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 205,
    "title": "📍 R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To calculate the standard deviation in R use the code: sd(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more. sd “sd” is an R function used to calculate the standard deviation of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 9.46527 Note that the single number showing above is the standard deviation of Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. sdTemp =  “sdTemp” is just a name we made up. It will contain the results of the sd(…) function. sd( “sd” is an R function used to calculate the standard deviation Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month sdTemp 5 6.855 6 6.599 7 4.316 8 6.585 9 8.356 Note that R calculated the standard deviation of Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively.",
    "sentences": ["To calculate the standard deviation in R use the code: sd(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more.", "sd “sd” is an R function used to calculate the standard deviation of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset.", "Type “View(airquality)” in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.", ")Closing parenthsis for the function.", "    Press Enter to run the code if you have typed it in yourself."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 206,
    "title": "📍 Explanation",
    "url": "NumericalSummaries.html#explanation",
    "content": "Explanation Data often varies. The values are not all the same. To capture, or measure how much data varies with a single number is difficult. There are a few different ideas on how to do it, but by far the most used measurement of the variability in data is the standard deviation. The first idea in measuring the variability in data is that there must be a reference point. Something from which everything varies. The most widely accepted reference point is the mean. A deviation is defined as the distance an observation lies from the reference point, the mean. This distance is obtained by subtraction in the order \\(x_i - \\bar{x}\\), where \\(x_i\\) is the data point value and \\(\\bar{x}\\) is the mean of the data. There are thus \\(n\\) deviations because there are \\(n\\) data points. Unfortunately, because of the order of subtraction in obtaining deviations, the average deviation will always work out to be zero. This is because the mean by nature splits the deviations evenly. Click here for details. One solution would be to take the absolute value of the deviations and obtain what is known as the “absolute mean deviation.” This is sometimes done, but a far more attractive choice (to mathematicians and statisticians) is to square each deviation. You’ll have to trust us that this is the better choice. Squaring a deviation results in the expression \\((x_i-\\bar{x})^2\\). SQUARE Summing up all of the squared deviations results in the expression \\(\\sum_{i=1}^n (x_i-\\bar{x})^2\\). Dividing the sum of the squared deviations by \\(n\\) would seem like an appropriate thing to do. Experience (and some fantastic statistical theory!) demonstrated that this is wrong. Dividing by \\(n-1\\), the degrees of freedom is right. MEAN To undo the squaring of the deviations, the final results are square rooted. ROOT The end result is the beautiful formula for \\(s\\), the standard deviation! (At least the symbol for standard deviation is a simple \\(s\\).) It is also know as the ROOT-MEAN-SQUARED ERROR. Error is another word for deviation. \\[ s = \\sqrt{\\frac{\\sum_{i=1}^n(x_i-\\bar{x})^2}{n-1}} \\] The standard deviation is thus the representative deviation of all deviations in a given data set. It is never negative and only zero if all values are the same in a data set. Larger values of \\(s\\) imply the data is highly variable, very spread out or very inconsistent. Smaller values mean the data is consistent and not as variable. Population Standard Deviation When all of the data from a population is available, the population standard deviation \\(\\sigma\\) (the lower-case Greek letter “sigma”) is calculated by the following formula. \\[ \\sigma = \\sqrt{\\frac{\\sum_{i=1}^N(x_i-\\mu)^2}{N}} \\] Note that \\(N\\) is the number of data points in the full population. In this formula the denominator is actually \\(N\\) and the deviations are calculated as the distance each data point is from the population mean \\(\\mu\\). An Example Say there are five data points given by \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) The mean of these values is \\(\\bar{x}=6\\) as shown here. The five deviations are \\((x_1 - \\bar{x}) = (2 - 6) = -4\\) \\((x_2 - \\bar{x}) = (5 - 6) = -1\\) \\((x_3 - \\bar{x}) = (6 - 6) = 0\\) \\((x_4 - \\bar{x}) = (7 - 6) = 1\\) \\((x_5 - \\bar{x}) = (10 - 6) = 4\\) The squared deviations are \\((x_1 - \\bar{x})^2 = (2 - 6)^2 = (-4)^2 = 16\\) \\((x_2 - \\bar{x})^2 = (5 - 6)^2 = (-1)^2 = 1\\) \\((x_3 - \\bar{x})^2 = (6 - 6)^2 = (0)^2 = 0\\) \\((x_4 - \\bar{x})^2 = (7 - 6)^2 = (1)^2 = 1\\) \\((x_5 - \\bar{x})^2 = (10 - 6)^2 = (4)^2 = 16\\) The sum of the squared deviations is \\[ \\sum_{i=1}^n (x_i-\\bar{x})^2 = 16 + 1 + 0 + 1 + 16 = 34 \\] Dividing this by the degrees of freedom, \\(n-1\\), gives \\[ \\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1} = \\frac{34}{5-1} = \\frac{34}{4} = 8.5 \\] Finally, \\(s\\) is obtained by taking the square root \\[ s = \\sqrt{\\frac{\\sum_{i=1}^n(x_i-\\bar{x})^2}{n-1}} = \\sqrt{8.5} \\approx 2.915 \\] The red lines below show how the standard deviation represents all deviations in this data set. Recall that the magnitudes of the individual deviations were \\(4, 1, 0, 1\\), and \\(4\\). The representative deviation is \\(2.915\\). Effect of Outliers Like the mean, the standard deviation is influenced by outliers. This is shown below by changing \\(x_5\\) to be 20. Note that the deviation of \\(x_5\\) is now 12 (instead of 4 like it was previously) and that the mean is now \\(8\\) (as shown here). The standard deviation of the altered data \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 20\\) is now \\(s \\approx 6.964\\). Not very “representative” of all the deviations. It is biased towards the largest deviation. It is important to be aware of outliers when reporting the standard deviation \\(s\\).",
    "sentences": ["Data often varies.", "The values are not all the same.", "To capture, or measure how much data varies with a single number is difficult.", "There are a few different ideas on how to do it, but by far the most used measurement of the variability in data is the standard deviation.", "The first idea in measuring the variability in data is that there must be a reference point.", "Something from which everything varies.", "The most widely accepted reference point is the mean.", "A deviation is defined as the distance an observation lies from the reference point, the mean.", "This distance is obtained by subtraction in the order \\(x_i - \\bar{x}\\), where \\(x_i\\) is the data point value and \\(\\bar{x}\\) is the mean of the data.", "There are thus \\(n\\) deviations because there are \\(n\\) data points."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Explanation"
  },
  {
    "id": 207,
    "title": "📍 Variance",
    "url": "NumericalSummaries.html#variance",
    "content": "Variance \\[s^2 = \\frac{\\sum_{i=1}^n(x_i-\\bar{x})^2}{n-1}\\] Measure of Spread | 1 Quantitative Variable Overview Great theoretical properties, but seldom used when describing data. Difficult to interpret in context of data because it is in squared units. The standard deviation is typically used instead because it is in the original units and is thus easier to interpret. R Instructions To calculate the variance in R use the code: var(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more. var “var” is an R function used to calculate the variance of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 89.59133 Note that the single number showing above is the variance of Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. varTemp =  “varTemp” is just a name we made up. It will contain the results of the var(…) function. var( “var” is an R function used to calculate the variance Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month varTemp 5 46.99 6 43.54 7 18.62 8 43.37 9 69.82 Note that R calculated the variance of Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively.",
    "sentences": ["\\[s^2 = \\frac{\\sum_{i=1}^n(x_i-\\bar{x})^2}{n-1}\\]", "Measure of Spread | 1 Quantitative Variable", "Overview Great theoretical properties, but seldom used when describing data.", "Difficult to interpret in context of data because it is in squared units.", "The standard deviation is typically used instead because it is in the original units and is thus easier to interpret.", "R Instructions To calculate the variance in R use the code: var(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more.", "var “var” is an R function used to calculate the variance of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Variance"
  },
  {
    "id": 208,
    "title": "📍 Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview Great theoretical properties, but seldom used when describing data. Difficult to interpret in context of data because it is in squared units. The standard deviation is typically used instead because it is in the original units and is thus easier to interpret.",
    "sentences": ["Great theoretical properties, but seldom used when describing data.", "Difficult to interpret in context of data because it is in squared units.", "The standard deviation is typically used instead because it is in the original units and is thus easier to interpret."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 209,
    "title": "📍 R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To calculate the variance in R use the code: var(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more. var “var” is an R function used to calculate the variance of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 89.59133 Note that the single number showing above is the variance of Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. varTemp =  “varTemp” is just a name we made up. It will contain the results of the var(…) function. var( “var” is an R function used to calculate the variance Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month varTemp 5 46.99 6 43.54 7 18.62 8 43.37 9 69.82 Note that R calculated the variance of Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively.",
    "sentences": ["To calculate the variance in R use the code: var(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more.", "var “var” is an R function used to calculate the variance of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset.", "Type “View(airquality)” in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.", ")Closing parenthsis for the function.", "    Press Enter to run the code if you have typed it in yourself."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 210,
    "title": "📍 Range",
    "url": "NumericalSummaries.html#range",
    "content": "Range \\[x_{(n)}-x_{(1)}\\] Measure of Spread | 1 Quantitative Variable Overview The difference between the maximum and minimum values. A general rule of thumb is that the range divided by four is roughly the standard deviation. Quick to obtain, but not as good as using the standard deviation. Was used more frequently before the advent of modern calculators. R Instructions R will not automatically compute the range. It is easiest to compute the max() and min() and perform the subtraction max - min yourself. To calculate the variance in R use the code: var(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more. max “max” is an R function used to calculate the maximum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.  -  Subtraction symbol. min “min” is an R function used to calculate the minimum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 41 Note that the single number showing above is the range of Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. rangeTemp =  “rangeTemp” is just a name we made up. It will contain the results of the range calculation. max( “max” is an R function used to calculate the maximum Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis.  -  Minus sign to perform subtraction. min( “min” is an R function used to calculate the minimum Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month rangeTemp 5 25 6 28 7 19 8 25 9 30 Note that R calculated the range of Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively.",
    "sentences": ["\\[x_{(n)}-x_{(1)}\\]", "Measure of Spread | 1 Quantitative Variable", "Overview The difference between the maximum and minimum values.", "A general rule of thumb is that the range divided by four is roughly the standard deviation.", "Quick to obtain, but not as good as using the standard deviation.", "Was used more frequently before the advent of modern calculators.", "R Instructions R will not automatically compute the range.", "It is easiest to compute the max() and min() and perform the subtraction max - min yourself.", "To calculate the variance in R use the code: var(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more.", "max “max” is an R function used to calculate the maximum of data."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Range"
  },
  {
    "id": 211,
    "title": "📍 Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview The difference between the maximum and minimum values. A general rule of thumb is that the range divided by four is roughly the standard deviation. Quick to obtain, but not as good as using the standard deviation. Was used more frequently before the advent of modern calculators.",
    "sentences": ["The difference between the maximum and minimum values.", "A general rule of thumb is that the range divided by four is roughly the standard deviation.", "Quick to obtain, but not as good as using the standard deviation.", "Was used more frequently before the advent of modern calculators."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 212,
    "title": "📍 R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions R will not automatically compute the range. It is easiest to compute the max() and min() and perform the subtraction max - min yourself. To calculate the variance in R use the code: var(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more. max “max” is an R function used to calculate the maximum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.  -  Subtraction symbol. min “min” is an R function used to calculate the minimum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 41 Note that the single number showing above is the range of Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. rangeTemp =  “rangeTemp” is just a name we made up. It will contain the results of the range calculation. max( “max” is an R function used to calculate the maximum Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis.  -  Minus sign to perform subtraction. min( “min” is an R function used to calculate the minimum Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month rangeTemp 5 25 6 28 7 19 8 25 9 30 Note that R calculated the range of Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively.",
    "sentences": ["R will not automatically compute the range.", "It is easiest to compute the max() and min() and perform the subtraction max - min yourself.", "To calculate the variance in R use the code: var(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more.", "max “max” is an R function used to calculate the maximum of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset.", "Type “View(airquality)” in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 213,
    "title": "📍 Percentile (Quantile)",
    "url": "NumericalSummaries.html#percentilequantile",
    "content": "Percentile (Quantile) \\(\\leftarrow\\)To the Left Measure of Location | 1 Quantitative Variable Overview The percent of data that is equal to or less than a given data point. Useful for describing the relative position of a data point within a data set. If the percentile is close to 100, then the observation is one of the largest. If it is close to zero, then the observation is one of the smallest. R Instructions To compute the quantile in R for a given percentile: quantile(object, percentile) object must be a quantitative variable, or as R terms it, a “numeric vector”. percentile must be a value between 0 and 1 or a “numeric vector” of such values. quantile( “quantile” is an R function used to calculate the data value corresponding to a given percentile. airquality$Temp Temp is a quantitative variable (numeric vector) being accessed from the airquality dataset with the $ sign. ,  Comma separating the two commands of the quantile function. 0.8 The value of 0.8 specifies the 80th percentile. Any other value from 0 to 1 inclusive could be used. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. ## 80% ## 86 The 80th percentile for Temp is shown in the output above.",
    "sentences": ["\\(\\leftarrow\\)To the Left", "Measure of Location | 1 Quantitative Variable", "Overview The percent of data that is equal to or less than a given data point.", "Useful for describing the relative position of a data point within a data set.", "If the percentile is close to 100, then the observation is one of the largest.", "If it is close to zero, then the observation is one of the smallest.", "R Instructions To compute the quantile in R for a given percentile: quantile(object, percentile) object must be a quantitative variable, or as R terms it, a “numeric vector”.", "percentile must be a value between 0 and 1 or a “numeric vector” of such values.", "quantile( “quantile” is an R function used to calculate the data value corresponding to a given percentile.", "airquality$Temp Temp is a quantitative variable (numeric vector) being accessed from the airquality dataset with the $ sign."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Percentile (Quantile)"
  },
  {
    "id": 214,
    "title": "📍 Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview The percent of data that is equal to or less than a given data point. Useful for describing the relative position of a data point within a data set. If the percentile is close to 100, then the observation is one of the largest. If it is close to zero, then the observation is one of the smallest.",
    "sentences": ["The percent of data that is equal to or less than a given data point.", "Useful for describing the relative position of a data point within a data set.", "If the percentile is close to 100, then the observation is one of the largest.", "If it is close to zero, then the observation is one of the smallest."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 215,
    "title": "📍 R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To compute the quantile in R for a given percentile: quantile(object, percentile) object must be a quantitative variable, or as R terms it, a “numeric vector”. percentile must be a value between 0 and 1 or a “numeric vector” of such values. quantile( “quantile” is an R function used to calculate the data value corresponding to a given percentile. airquality$Temp Temp is a quantitative variable (numeric vector) being accessed from the airquality dataset with the $ sign. ,  Comma separating the two commands of the quantile function. 0.8 The value of 0.8 specifies the 80th percentile. Any other value from 0 to 1 inclusive could be used. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. ## 80% ## 86 The 80th percentile for Temp is shown in the output above.",
    "sentences": ["To compute the quantile in R for a given percentile: quantile(object, percentile) object must be a quantitative variable, or as R terms it, a “numeric vector”.", "percentile must be a value between 0 and 1 or a “numeric vector” of such values.", "quantile( “quantile” is an R function used to calculate the data value corresponding to a given percentile.", "airquality$Temp Temp is a quantitative variable (numeric vector) being accessed from the airquality dataset with the $ sign.", ",  Comma separating the two commands of the quantile function.", "0.8 The value of 0.8 specifies the 80th percentile.", "Any other value from 0 to 1 inclusive could be used.", ") Functions must always end with a closing parenthesis.", "    Press Enter to run the code.", " …  Click to View Output."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 216,
    "title": "📍 Proportion",
    "url": "NumericalSummaries.html#proportion",
    "content": "Proportion \\[\\hat{p}=\\frac{x}{n}\\] Measure of Center | 1 Qualitative Variable Overview The percent of observations in the data that satisfy some requirement. Obtained by dividing the number of successes \\(x\\) by the number of total observations \\(n\\). Often referred to as a percentage.",
    "sentences": ["\\[\\hat{p}=\\frac{x}{n}\\]", "Measure of Center | 1 Qualitative Variable", "Overview The percent of observations in the data that satisfy some requirement.", "Obtained by dividing the number of successes \\(x\\) by the number of total observations \\(n\\).", "Often referred to as a percentage."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Proportion"
  },
  {
    "id": 217,
    "title": "📍 Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview The percent of observations in the data that satisfy some requirement. Obtained by dividing the number of successes \\(x\\) by the number of total observations \\(n\\). Often referred to as a percentage.",
    "sentences": ["The percent of observations in the data that satisfy some requirement.", "Obtained by dividing the number of successes \\(x\\) by the number of total observations \\(n\\).", "Often referred to as a percentage."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 218,
    "title": "📍 Correlation",
    "url": "NumericalSummaries.html#correlation",
    "content": "Correlation \\(r = \\frac{\\textstyle\\sum\\left(\\frac{x-\\bar{x}}{s_x}\\right)\\left(\\frac{y-\\bar{y}}{s_y}\\right)}{n-1}\\) Measure of Association | 2 Quantitative Variables Overview Describes the strength and direction of the association between two quantitative variables. Restricted to values between -1 and 1. A value of zero denotes no association between the two variables. A value of 1 or -1 implies a perfect positive or perfect negative association, respectively. R Instructions To calculate the correlation in R use the code: cor(object1,object2) object1 and object2 must both be a quantitative variables, what R calls “numeric vectors.” Example Code Hover your mouse over the example codes to learn more. cor “cor” is an R function used to calculate the standard deviation of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. ,  The comma is needed to separate the two quantitative variables of the cor() function. The space after the comma is not required. It just looks nice. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Wind “Wind” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] -0.4579879 Note that the single number showing next to the [1] above is the correlation of Temp and Wind from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. corTempWind =  “corTempWind” is just a name we made up. It will contain the results of the cor(…) function. cor( “cor” is an R function used to calculate the mean. Temp,  Temp is a quantitative variable (numeric vector) from the airquality dataset. Wind Wind is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month corTempWind 5 -0.3733 6 -0.121 7 -0.3052 8 -0.5076 9 -0.5705 Note that R calculated the correlation of Temp and Wind for each Month from the airquality dataset.",
    "sentences": ["\\(r = \\frac{\\textstyle\\sum\\left(\\frac{x-\\bar{x}}{s_x}\\right)\\left(\\frac{y-\\bar{y}}{s_y}\\right)}{n-1}\\)", "Measure of Association | 2 Quantitative Variables", "Overview Describes the strength and direction of the association between two quantitative variables.", "Restricted to values between -1 and 1.", "A value of zero denotes no association between the two variables.", "A value of 1 or -1 implies a perfect positive or perfect negative association, respectively.", "R Instructions To calculate the correlation in R use the code: cor(object1,object2) object1 and object2 must both be a quantitative variables, what R calls “numeric vectors.” Example Code Hover your mouse over the example codes to learn more.", "cor “cor” is an R function used to calculate the standard deviation of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Correlation"
  },
  {
    "id": 219,
    "title": "📍 Overview",
    "url": "NumericalSummaries.html#overview",
    "content": "Overview Describes the strength and direction of the association between two quantitative variables. Restricted to values between -1 and 1. A value of zero denotes no association between the two variables. A value of 1 or -1 implies a perfect positive or perfect negative association, respectively.",
    "sentences": ["Describes the strength and direction of the association between two quantitative variables.", "Restricted to values between -1 and 1.", "A value of zero denotes no association between the two variables.", "A value of 1 or -1 implies a perfect positive or perfect negative association, respectively."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "Overview"
  },
  {
    "id": 220,
    "title": "📍 R Instructions",
    "url": "NumericalSummaries.html#rinstructions",
    "content": "R Instructions To calculate the correlation in R use the code: cor(object1,object2) object1 and object2 must both be a quantitative variables, what R calls “numeric vectors.” Example Code Hover your mouse over the example codes to learn more. cor “cor” is an R function used to calculate the standard deviation of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. ,  The comma is needed to separate the two quantitative variables of the cor() function. The space after the comma is not required. It just looks nice. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Wind “Wind” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] -0.4579879 Note that the single number showing next to the [1] above is the correlation of Temp and Wind from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. corTempWind =  “corTempWind” is just a name we made up. It will contain the results of the cor(…) function. cor( “cor” is an R function used to calculate the mean. Temp,  Temp is a quantitative variable (numeric vector) from the airquality dataset. Wind Wind is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month corTempWind 5 -0.3733 6 -0.121 7 -0.3052 8 -0.5076 9 -0.5705 Note that R calculated the correlation of Temp and Wind for each Month from the airquality dataset.",
    "sentences": ["To calculate the correlation in R use the code: cor(object1,object2) object1 and object2 must both be a quantitative variables, what R calls “numeric vectors.” Example Code Hover your mouse over the example codes to learn more.", "cor “cor” is an R function used to calculate the standard deviation of data.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "airquality “airquality” is a dataset.", "Type “View(airquality)” in R to see it.", "$ The $ allows us to access any variable from the airquality dataset.", "Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.", ",  The comma is needed to separate the two quantitative variables of the cor() function.", "The space after the comma is not required."],
    "type": "section",
    "page_title": "Numerical Summaries",
    "section_title": "R Instructions"
  },
  {
    "id": 221,
    "title": "Intermediate Statistics Notes",
    "url": "Paige-sNotes.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Intermediate Statistics Notes (MATH 325) Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts Don’t forget to always load your libraries and to Knit often!! library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) This is the templet for the Hover Box Thingy Hover over words box! [insert description here]       …  Click to View Output. There are three parts to the help box: The starter code < a href=“javascript:showhide(‘[Insert classifying name]’)”> < div class=“hoverchunk” > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions! < span class=“tooltipr” > [What shows up in the box] < span class=“tooltiprtext” > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box! & nbsp;& nbsp;& nbsp;& nbsp; < span class=“tooltiprtext” >Press Enter to run the code. < /span> & nbsp;…& nbsp; < span class=“tooltiprtext” >Click to View Output.< /span > < /span > < /div > < /a > < div id=“[The classifying name that you put in the starter code!]” style=“display:none;” > *When using, make sure you fix all the spaces when using and use “< /div >” to show where the drop down is meant to end! Personal Notes OoOoooOOOoo let’s go assignments!! Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester. As well as some personal notes on those assignments! Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys! Skill Quiz - Introduction to R Look at data sets, computing means, and creating graphs. Mean of Distance from “cars” data set View(cars) mean(cars$dist) ## [1] 42.98 Scatter plot of “cars” data set plot(dist~speed, data=cars, col=\"skyblue\",pch=16) Creating new data set with “<- The Assignment Operator” create an airquality2 data set that is a copy of the airquality data set add a new column to the airquality2 data set called “Celcius” that converts Temp from degrees Fahrenheit (F) to Ceclcius (C) using the equation: Celcius = (Temp − 32) * 5/9. Don’t forget to use airquailty$ appropriately in that equation! Hint: Typing “?airquality” will allow a help file for the airquality data set to appear! write.csv(airquality,\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/airquality.cvs\") airquality2 <- airquality airquality2$Celcius <- round((airquality2$Temp-32)*5/9,1) View(airquality2) datatable(airquality2, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") PRACTICE : Scatter plot in Base R plot(Ozone~ Temp, data=airquality, xlab=\"Daily Maximum Temperature\", ylab=\"Mean Ozone in Parts per Billion (from 1300 to 1500 hours\", main=\"Exponential Growth in Ozone with Increasing Temp\", col=\"firebrick\",pch=16 ) pch Options! Week 2 | Describing Data in RStudio Talking about how to give good feedback and more coolio graphics in this week! Skill Quiz - Describing Data with R Numerical Summaries that MEASURE CENTER: Proportion Mean Mode Median Mean - Gives a good feel for the most typical values of quantitative data as long as the data is at least approximately normally distributed. Median - Gives a good feel for the most typical values of any style of distribution of quantitative data, but is especially useful for skewed data. Standard Deviation - Typically used to describe the spread of relatively normally distribued data. Five-Number Summaryt - Gives a good feel for how the possible values of quanititative data spread out around the more probable values. Practice Find the mean and standard deviation of the “Wind” speed variable in the airquality dataset. pander(summary(airquality$Wind)) Min. Median Mean 3rd Qu. 1.7 7.4 9.7 9.958 11.5 20.7 Create a Scatter plot of Temp VS. Wind plot(Wind~Temp, data=airquality, xlab= \"Daily Average Temperature\", ylab = \"Daily Average Wind Speed\", main= \"La Guardia Airport Warmer Weather Shows Less Wind\", col= \"gray\", pch= 19) Create a Histogram and Box plot of Solar.R hist(airquality$Solar.R, xlab= \"Daily Mean Radiation in Langleys (from 0800 to 1200 hours)\", main= \"Central Park, NYC Daily Average Radiation\", col= \"orange\") boxplot(Solar.R~Month, data=airquality, xlab= \"Month of the Year\", ylab= \"Radiation in Langleys (Averaged from 0800 to 1200 hours)\", main= \"Daily Mean Radiation High in July\",col=c(\"gray\",\"gray\",\"orangered\",\"gray\",\"gray\")) Graph Types Histogram - Show the distribution of heights for a sample of n=400 first grade boys. Box Plot - Compare the distribution of body weights of American adults for different ethnicities where there is a sample size of n=100 for each ethnicity. Dot Plots - Compare the distribution of bird beak lengths for different species of bird where there is a sample size of n=3 for each species. Class Activity - Principles of Good Graphics PRACTICE Histogram, Box plot, and Scatter plot with airquality data set hist(airquality$Wind, col=\"steelblue\", xlab=\"Daily Average Wind Speeds (mph)\", main=\"La Guardia Airport from May to September, 1973\") boxplot(Wind~Month, data=airquality, names=c(\"5\",\"6\",\"7\",\"8\",\"9\"), col=c(\"steelblue1\",\"steelblue2\",\"steelblue3\",\"steelblue3\",\"steelblue2\")) plot(Ozone~Temp, data=airquality, col= \"steelblue\", pch=19) Week 3 | Intro to Data Wrangling & Visulization Looking at the different categories of data that is found in the Index Page and how to discern quantitative from categorical data! Skills Quiz - Data Wrangling & Visualization Quick reminder! How do you view a dataset? View([insert the dataset here]) How do you open the help file ?([insert the dataset here]) Quanitative vs. Qualitative Quantitative Examples: length & width Qualitatve Examples: name, birthmonth, birthyear, sex, biggerfoot, & domhand PRACTICE : Use KidsFeet dataset Make a table displaying gender table(KidsFeet$sex)%>% pander() B G 20 19 Make a table displaying which foot is bigger Which foot is commonly bigger? : Left Foot table(KidsFeet$biggerfoot)%>% pander() L R 22 17 Make a table displaying which birthmonth is the most common Which birth month is most common among these sampled children? : March table(KidsFeet$birthmonth)%>% pander() 1 2 3 4 5 6 7 8 9 10 11 12 2 3 8 3 2 4 3 2 5 2 2 3 Make a table that shows “are girls or boys more likely to be left handed?” table(KidsFeet$domhand, KidsFeet$sex)%>% pander()   B G L 5 3 R 15 16 Make a table showing the summaries of children’s foot lengths according to their gender KidsFeet %>% group_by(sex) %>% summarise(Min=min(length), Q1 = quantile(length, 0.25), Median = (median(length)), Q3 = quantile(length,0.75), Max = max(length), Mean = mean(length), SD = sd(length))%>% pander() sex Min Q1 Median Q3 Max Mean SD B 22.9 24.35 24.95 25.8 27.5 25.11 1.217 G 21.6 23.65 24.2 25.1 26.7 24.32 1.33 For this particular sample of data, which gender has the longest feet on average? Boys Which gender shows the most consistency in length of feet among children in this sample? Boys PRACTICE : Use airquality dataset run an appropriate command to obtain the mean daily temperature at LaGuardia Airport for each month, separately. airquality %>% group_by(Month) %>% summarise(`Mean Temperature`= mean(Temp)) %>% pander() Month Mean Temperature 5 65.55 6 79.1 7 83.9 8 83.97 9 76.9 Observations: Which month experiences the coolest average temperature? May By how many degrees do the average temperatures of July and August differ? 83.97-83.9 = 0.07 Between which two consecutive months is there the largest difference in average temperature? May and June What are the BEST graphics for this data set? These 3 would be useful in depicting the above information, because they nicely display the month and their averages (1) Box plot boxplot(Temp ~ Month, data=airquality, xlab=\"Month\", ylab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\") (2) Scatter plot plot(Temp ~ Month, data=airquality, xlab=\"Month\", ylab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\") (3) Dot Plot stripchart(Temp ~ Month, data=airquality, ylab=\"Month\", xlab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\", method=\"stack\") What are the WORST graphics for depicting this information? these two are the worst because they don’t clearly show the months in order to allow us to compare the data of each month. (1) Scatter plot but the x axis is Day, not very clear variable plot(Temp ~ Day, data=airquality, xlab=\"Day of the Month\", ylab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\") (2) Histogram gives you the average of the whole time frame but not the specific times hist(airquality$Temp, xlab=\"Daily Temperature\", main=\"LaGuardia Airport (May to September 1973)\", col=\"slategray\") PRACTICE : Use Orange dataset View(Orange) Orange2 <- Orange %>% group_by(age)%>% summarise(MedianCir = median(circumference)) datatable(Orange2, options=list(lengthMenu =c(3,10,30)), extensions=“Responsive”) What are the BEST graphics for this data set? Scatter plot with median growth line visualizes the relationship/ captures the overall trends between two continuous variables plot(circumference ~ age, data=Orange, ylab=\"Trunk Circumference (mm)\", xlab=\"Age of Trees (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\", pch=15) Orange.m <- median(circumference ~ age, data=Orange) lines(names(Orange.m), Orange.m, col=\"ivory3\") legend(\"topleft\", legend=\"Median Growth\", lty=1, col='ivory3', bty='n') Box plot shows how the trunk circumference varies across different ages boxplot(circumference ~ age, data=Orange, ylab=\"Trunk Circumference (mm)\", xlab=\"Age of Trees (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\") Dot plot shows the distribution of data and helps visualize all the individual data points stripchart(circumference ~ age, data=Orange, ylab=\"Trunk Circumference (mm)\", xlab=\"Age of Trees (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\", pch=15, method=\"stack\", vertical=TRUE) Question: During which age interval did the most rapid overall median growth occur (in the circumference of the orange trees that were sampled)? 664 to 1004 days (third interval) What are the WORST graphics for this data set? Box plot x-axis is sectioned out by multiple variables instead of just one, thus, not really showing us anything of value boxplot(Orange, xlab=\"Age of Tree (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\") PRACTICE : Use the Riders data set Consider the Riders dataset in R. (You may need to load library(mosaicData).) How many total riders were observed on each day of the week? (Hint: the sum() function works the same way as mean()…) Riders2 <- Riders %>% group_by(day)%>% summarise(`Total Number of Riders Observed`= sum(riders)) datatable(Riders2) PRACTICE : Use the mtcars dataset How would you describe the dataset? First, type ?mtcars and look at the “Description” of the data set information Description: The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models). How many variables are in the mtcars data set? First, View()the data, then read how many columns are in the data set in the bottom left hand corner : 11 How many observations are in the mtcars data set? (Hint: try View(…)) First, View() the data, then read the amount of entries in the data set in the bottom left hand corner : 32 How many vehicles are represented in the dataset for 4, 6, and 8 cylinder vehicles? (Hint: use the table(…) function.) table(mtcars$cyl)%>% pander() 4 6 8 11 7 14 According to the mtcars data, on average, vehicles with 4 cylinders get the best (highest) gas mileage. What is the average mpg for automatic and manual transmission vehicles with 4 cylinders? (Round answers to the nearest tenth.) 0 = Automatic, 1 = Manual mtcars %>% filter(cyl == \"4\")%>% group_by(am)%>% summarise(`Mean Gas Mileage form 4 Cylinder mtcars Vehicles(mpg)`= round(mean(mpg),1))%>% mutate(Transmission = ifelse(am == 0, \"Automatic\",\"Manual\"))%>% pander() am Mean Gas Mileage form 4 Cylinder mtcars Vehicles(mpg) Transmission 0 22.9 Automatic 1 28.1 Manual According to the mtcars data, on average, vehicles with 8 cylinders have the best (fastest) quarter mile time. What is the mean quarter mile time (qsec) for automatic and manual transmission vehicles with 8 cylinders? (Round answers to nearest tenth.) mtcars %>% filter(cyl == \"8\")%>% group_by(am)%>% summarise(`Mean Quarter Mile Time for 8 Cylinder mtcars Vechiles (sec)`= round(mean(qsec),1))%>% mutate(Transmission = ifelse(am == 0, \"Automatic\",\"Manual\"))%>% pander() am Mean Quarter Mile Time for 8 Cylinder mtcars Vechiles (sec) Transmission 0 17.1 Automatic 1 14.6 Manual According to the mtcars data, how many thousands of pounds does the heaviest 6 cylinder car with an automatic transmission weigh? (Round to the nearest tenth.) mtcars %>% filter(cyl == \"6\", am == \"0\") %>% summarise(MaxWTAutomatic = round(max(wt),1))%>% pander() MaxWTAutomatic 3.5 How many more thousands of pounds does it weigh than the heaviest 6 cylinder car with a manual transmission? (Round to the nearest tenth.) 3.5 - 2.9 = 0.6 mtcars %>% filter(cyl == \"6\", am == \"1\") %>% summarise(MaxWTManual = round(max(wt),1))%>% pander() MaxWTManual 2.9 Assesment Quiz - Intro to Data Wrangling & Visualization Use the mtcars dataset in R to compute the mean “Gross horsepower” of both automatic and manual transmissio",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Intermediate Statistics Notes (MATH 325) Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts Don’t forget to always load your libraries and to Knit often!!", "library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) This is the templet for the Hover Box Thingy Hover over words box!", "[insert description here]       …  Click to View Output.", "There are three parts to the help box: The starter code < a href=“javascript:showhide(‘[Insert classifying name]’)”> < div class=“hoverchunk” > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions!", "< span class=“tooltipr” > [What shows up in the box] < span class=“tooltiprtext” > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box!", "& nbsp;& nbsp;& nbsp;& nbsp; < span class=“tooltiprtext” >Press Enter to run the code.", "< /span> & nbsp;…& nbsp; < span class=“tooltiprtext” >Click to View Output.< /span > < /span > < /div > < /a > < div id=“[The classifying name that you put in the starter code!]” style=“display:none;” > *When using, make sure you fix all the spaces when using and use “< /div >” to show where the drop down is meant to end!", "Personal Notes OoOoooOOOoo let’s go assignments!!", "Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester.", "As well as some personal notes on those assignments!", "Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys!", "Skill Quiz - Introduction to R Look at data sets, computing means, and creating graphs.", "Mean of Distance from “cars” data set View(cars) mean(cars$dist) ## [1] 42.98 Scatter plot of “cars” data set plot(dist~speed, data=cars, col=\"skyblue\",pch=16) Creating new data set with “<- The Assignment Operator” create an airquality2 data set that is a copy of the airquality data set add a new column to the airquality2 data set called “Celcius” that converts Temp from degrees Fahrenheit (F) to Ceclcius (C) using the equation: Celcius = (Temp − 32) * 5/9.", "Don’t forget to use airquailty$ appropriately in that equation!", "Hint: Typing “?airquality” will allow a help file for the airquality data set to appear!", "write.csv(airquality,\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/airquality.cvs\") airquality2 <- airquality airquality2$Celcius <- round((airquality2$Temp-32)*5/9,1) View(airquality2) datatable(airquality2, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") PRACTICE : Scatter plot in Base R plot(Ozone~ Temp, data=airquality, xlab=\"Daily Maximum Temperature\", ylab=\"Mean Ozone in Parts per Billion (from 1300 to 1500 hours\", main=\"Exponential Growth in Ozone with Increasing Temp\", col=\"firebrick\",pch=16 ) pch Options!", "Week 2 | Describing Data in RStudio Talking about how to give good feedback and more coolio graphics in this week!", "Skill Quiz - Describing Data with R Numerical Summaries that MEASURE CENTER: Proportion Mean Mode Median Mean - Gives a good feel for the most typical values of quantitative data as long as the data is at least approximately normally distributed.", "Median - Gives a good feel for the most typical values of any style of distribution of quantitative data, but is especially useful for skewed data.", "Standard Deviation - Typically used to describe the spread of relatively normally distribued data.", "Five-Number Summaryt - Gives a good feel for how the possible values of quanititative data spread out around the more probable values.", "Practice Find the mean and standard deviation of the “Wind” speed variable in the airquality dataset.", "pander(summary(airquality$Wind)) Min.", "Median Mean 3rd Qu.", "1.7 7.4 9.7 9.958 11.5 20.7 Create a Scatter plot of Temp VS.", "Wind plot(Wind~Temp, data=airquality, xlab= \"Daily Average Temperature\", ylab = \"Daily Average Wind Speed\", main= \"La Guardia Airport Warmer Weather Shows Less Wind\", col= \"gray\", pch= 19) Create a Histogram and Box plot of Solar.R hist(airquality$Solar.R, xlab= \"Daily Mean Radiation in Langleys (from 0800 to 1200 hours)\", main= \"Central Park, NYC Daily Average Radiation\", col= \"orange\") boxplot(Solar.R~Month, data=airquality, xlab= \"Month of the Year\", ylab= \"Radiation in Langleys (Averaged from 0800 to 1200 hours)\", main= \"Daily Mean Radiation High in July\",col=c(\"gray\",\"gray\",\"orangered\",\"gray\",\"gray\")) Graph Types Histogram - Show the distribution of heights for a sample of n=400 first grade boys.", "Box Plot - Compare the distribution of body weights of American adults for different ethnicities where there is a sample size of n=100 for each ethnicity.", "Dot Plots - Compare the distribution of bird beak lengths for different species of bird where there is a sample size of n=3 for each species.", "Class Activity - Principles of Good Graphics PRACTICE Histogram, Box plot, and Scatter plot with airquality data set hist(airquality$Wind, col=\"steelblue\", xlab=\"Daily Average Wind Speeds (mph)\", main=\"La Guardia Airport from May to September, 1973\") boxplot(Wind~Month, data=airquality, names=c(\"5\",\"6\",\"7\",\"8\",\"9\"), col=c(\"steelblue1\",\"steelblue2\",\"steelblue3\",\"steelblue3\",\"steelblue2\")) plot(Ozone~Temp, data=airquality, col= \"steelblue\", pch=19) Week 3 | Intro to Data Wrangling & Visulization Looking at the different categories of data that is found in the Index Page and how to discern quantitative from categorical data!", "Skills Quiz - Data Wrangling & Visualization Quick reminder!", "How do you view a dataset?", "View([insert the dataset here]) How do you open the help file ?([insert the dataset here]) Quanitative vs. Qualitative Quantitative Examples: length & width Qualitatve Examples: name, birthmonth, birthyear, sex, biggerfoot, & domhand PRACTICE : Use KidsFeet dataset Make a table displaying gender table(KidsFeet$sex)%>% pander() B G 20 19 Make a table displaying which foot is bigger Which foot is commonly bigger?", ": Left Foot table(KidsFeet$biggerfoot)%>% pander() L R 22 17 Make a table displaying which birthmonth is the most common Which birth month is most common among these sampled children?", ": March table(KidsFeet$birthmonth)%>% pander() 1 2 3 4 5 6 7 8 9 10 11 12 2 3 8 3 2 4 3 2 5 2 2 3 Make a table that shows “are girls or boys more likely to be left handed?” table(KidsFeet$domhand, KidsFeet$sex)%>% pander()   B G L 5 3 R 15 16 Make a table showing the summaries of children’s foot lengths according to their gender KidsFeet %>% group_by(sex) %>% summarise(Min=min(length), Q1 = quantile(length, 0.25), Median = (median(length)), Q3 = quantile(length,0.75), Max = max(length), Mean = mean(length), SD = sd(length))%>% pander() sex Min Q1 Median Q3 Max Mean SD B 22.9 24.35 24.95 25.8 27.5 25.11 1.217 G 21.6 23.65 24.2 25.1 26.7 24.32 1.33 For this particular sample of data, which gender has the longest feet on average?", "Boys Which gender shows the most consistency in length of feet among children in this sample?", "Boys PRACTICE : Use airquality dataset run an appropriate command to obtain the mean daily temperature at LaGuardia Airport for each month, separately.", "airquality %>% group_by(Month) %>% summarise(`Mean Temperature`= mean(Temp)) %>% pander() Month Mean Temperature 5 65.55 6 79.1 7 83.9 8 83.97 9 76.9 Observations: Which month experiences the coolest average temperature?", "May By how many degrees do the average temperatures of July and August differ?", "83.97-83.9 = 0.07 Between which two consecutive months is there the largest difference in average temperature?", "May and June What are the BEST graphics for this data set?", "These 3 would be useful in depicting the above information, because they nicely display the month and their averages (1) Box plot boxplot(Temp ~ Month, data=airquality, xlab=\"Month\", ylab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\") (2) Scatter plot plot(Temp ~ Month, data=airquality, xlab=\"Month\", ylab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\") (3) Dot Plot stripchart(Temp ~ Month, data=airquality, ylab=\"Month\", xlab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\", method=\"stack\") What are the WORST graphics for depicting this information?", "these two are the worst because they don’t clearly show the months in order to allow us to compare the data of each month.", "(1) Scatter plot but the x axis is Day, not very clear variable plot(Temp ~ Day, data=airquality, xlab=\"Day of the Month\", ylab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\") (2) Histogram gives you the average of the whole time frame but not the specific times hist(airquality$Temp, xlab=\"Daily Temperature\", main=\"LaGuardia Airport (May to September 1973)\", col=\"slategray\") PRACTICE : Use Orange dataset View(Orange) Orange2 <- Orange %>% group_by(age)%>% summarise(MedianCir = median(circumference)) datatable(Orange2, options=list(lengthMenu =c(3,10,30)), extensions=“Responsive”) What are the BEST graphics for this data set?", "Scatter plot with median growth line visualizes the relationship/ captures the overall trends between two continuous variables plot(circumference ~ age, data=Orange, ylab=\"Trunk Circumference (mm)\", xlab=\"Age of Trees (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\", pch=15) Orange.m <- median(circumference ~ age, data=Orange) lines(names(Orange.m), Orange.m, col=\"ivory3\") legend(\"topleft\", legend=\"Median Growth\", lty=1, col='ivory3', bty='n') Box plot shows how the trunk circumference varies across different ages boxplot(circumference ~ age, data=Orange, ylab=\"Trunk Circumference (mm)\", xlab=\"Age of Trees (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\") Dot plot shows the distribution of data and helps visualize all the individual data points stripchart(circumference ~ age, data=Orange, ylab=\"Trunk Circumference (mm)\", xlab=\"Age of Trees (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\", pch=15, method=\"stack\", vertical=TRUE) Question: During which age interval did the most rapid overall median growth occur (in the circumference of the orange trees that were sampled)?", "664 to 1004 days (third interval) What are the WORST graphics for this data set?", "Box plot x-axis is sectioned out by multiple variables instead of just one, thus, not really showing us anything of value boxplot(Orange, xlab=\"Age of Tree (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\") PRACTICE : Use the Riders data set Consider the Riders dataset in R.", "(You may need to load library(mosaicData).) How many total riders were observed on each day of the week?", "(Hint: the sum() function works the same way as mean()…) Riders2 <- Riders %>% group_by(day)%>% summarise(`Total Number of Riders Observed`= sum(riders)) datatable(Riders2) PRACTICE : Use the mtcars dataset How would you describe the dataset?", "First, type ?mtcars and look at the “Description” of the data set information Description: The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).", "How many variables are in the mtcars data set?"],
    "type": "page",
    "page_title": "Intermediate Statistics Notes"
  },
  {
    "id": 222,
    "title": "📍 Cheat Sheets",
    "url": "Paige-sNotes.html#cheatsheets",
    "content": "Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts",
    "sentences": "R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts",
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Cheat Sheets"
  },
  {
    "id": 223,
    "title": "📍 Don’t forget to always load your libraries and to Knit often!!",
    "url": "Paige-sNotes.html#dontforgettoalwaysloadyourlibrariesandtoknitoften",
    "content": "Don’t forget to always load your libraries and to Knit often!! library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) This is the templet for the Hover Box Thingy There are three parts to the help box: The starter code < a href=“javascript:showhide(‘[Insert classifying name]’)”> < div class=“hoverchunk” > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions! < span class=“tooltipr” > [What shows up in the box] < span class=“tooltiprtext” > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box! & nbsp;& nbsp;& nbsp;& nbsp; < span class=“tooltiprtext” >Press Enter to run the code. < /span> & nbsp;…& nbsp; < span class=“tooltiprtext” >Click to View Output.< /span > < /span > < /div > < /a > < div id=“[The classifying name that you put in the starter code!]” style=“display:none;” > *When using, make sure you fix all the spaces when using and use “< /div >” to show where the drop down is meant to end!",
    "sentences": ["library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven)", "This is the templet for the Hover Box Thingy", "There are three parts to the help box: The starter code < a href=“javascript:showhide(‘[Insert classifying name]’)”> < div class=“hoverchunk” > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions!", "< span class=“tooltipr” > [What shows up in the box] < span class=“tooltiprtext” > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box!", "& nbsp;& nbsp;& nbsp;& nbsp; < span class=“tooltiprtext” >Press Enter to run the code.", "< /span> & nbsp;…& nbsp; < span class=“tooltiprtext” >Click to View Output.< /span > < /span > < /div > < /a > < div id=“[The classifying name that you put in the starter code!]” style=“display:none;” > *When using, make sure you fix all the spaces when using and use “< /div >” to show where the drop down is meant to end!"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Don’t forget to always load your libraries and to Knit often!!"
  },
  {
    "id": 224,
    "title": "📍 Week 1 | Welcome to the Course & Introduction to RStudio",
    "url": "Paige-sNotes.html#week1welcometothecourseintroductiontorstudio",
    "content": "Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys! Skill Quiz - Introduction to R Look at data sets, computing means, and creating graphs. Mean of Distance from “cars” data set View(cars) mean(cars$dist) ## [1] 42.98 Scatter plot of “cars” data set plot(dist~speed, data=cars, col=\"skyblue\",pch=16) Creating new data set with “<- The Assignment Operator” create an airquality2 data set that is a copy of the airquality data set add a new column to the airquality2 data set called “Celcius” that converts Temp from degrees Fahrenheit (F) to Ceclcius (C) using the equation: Celcius = (Temp − 32) * 5/9. Don’t forget to use airquailty$ appropriately in that equation! Hint: Typing “?airquality” will allow a help file for the airquality data set to appear! write.csv(airquality,\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/airquality.cvs\") airquality2 <- airquality airquality2$Celcius <- round((airquality2$Temp-32)*5/9,1) View(airquality2) datatable(airquality2, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") PRACTICE : Scatter plot in Base R plot(Ozone~ Temp, data=airquality, xlab=\"Daily Maximum Temperature\", ylab=\"Mean Ozone in Parts per Billion (from 1300 to 1500 hours\", main=\"Exponential Growth in Ozone with Increasing Temp\", col=\"firebrick\",pch=16 ) pch Options!",
    "sentences": ["We were setting up RStudio and doing cool little graphic thingys!", "Skill Quiz - Introduction to R Look at data sets, computing means, and creating graphs.", "Mean of Distance from “cars” data set View(cars) mean(cars$dist) ## [1] 42.98 Scatter plot of “cars” data set plot(dist~speed, data=cars, col=\"skyblue\",pch=16) Creating new data set with “<- The Assignment Operator” create an airquality2 data set that is a copy of the airquality data set add a new column to the airquality2 data set called “Celcius” that converts Temp from degrees Fahrenheit (F) to Ceclcius (C) using the equation: Celcius = (Temp − 32) * 5/9.", "Don’t forget to use airquailty$ appropriately in that equation!", "Hint: Typing “?airquality” will allow a help file for the airquality data set to appear!", "write.csv(airquality,\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/airquality.cvs\") airquality2 <- airquality airquality2$Celcius <- round((airquality2$Temp-32)*5/9,1) View(airquality2) datatable(airquality2, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") PRACTICE : Scatter plot in Base R plot(Ozone~ Temp, data=airquality, xlab=\"Daily Maximum Temperature\", ylab=\"Mean Ozone in Parts per Billion (from 1300 to 1500 hours\", main=\"Exponential Growth in Ozone with Increasing Temp\", col=\"firebrick\",pch=16 ) pch Options!"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Week 1 | Welcome to the Course & Introduction to RStudio"
  },
  {
    "id": 225,
    "title": "📍 Skill Quiz - Introduction to R",
    "url": "Paige-sNotes.html#skillquizintroductiontor",
    "content": "Skill Quiz - Introduction to R Look at data sets, computing means, and creating graphs. Mean of Distance from “cars” data set View(cars) mean(cars$dist) Scatter plot of “cars” data set plot(dist~speed, data=cars, col=\"skyblue\",pch=16) Creating new data set with “<- The Assignment Operator” create an airquality2 data set that is a copy of the airquality data set add a new column to the airquality2 data set called “Celcius” that converts Temp from degrees Fahrenheit (F) to Ceclcius (C) using the equation: Celcius = (Temp − 32) * 5/9. Don’t forget to use airquailty$ appropriately in that equation! Hint: Typing “?airquality” will allow a help file for the airquality data set to appear! write.csv(airquality,\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/airquality.cvs\") airquality2 <- airquality airquality2$Celcius <- round((airquality2$Temp-32)*5/9,1) View(airquality2) datatable(airquality2, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\")",
    "sentences": ["Look at data sets, computing means, and creating graphs.", "Mean of Distance from “cars” data set", "View(cars) mean(cars$dist)", "Scatter plot of “cars” data set", "plot(dist~speed, data=cars, col=\"skyblue\",pch=16)", "Creating new data set with “<- The Assignment Operator”", "create an airquality2 data set that is a copy of the airquality data set add a new column to the airquality2 data set called “Celcius” that converts Temp from degrees Fahrenheit (F) to Ceclcius (C) using the equation: Celcius = (Temp − 32) * 5/9.", "Don’t forget to use airquailty$ appropriately in that equation!", "Hint: Typing “?airquality” will allow a help file for the airquality data set to appear!", "write.csv(airquality,\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/airquality.cvs\") airquality2 <- airquality airquality2$Celcius <- round((airquality2$Temp-32)*5/9,1) View(airquality2) datatable(airquality2, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\")"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skill Quiz - Introduction to R"
  },
  {
    "id": 226,
    "title": "📍 Skill Quiz - Describing Data with R",
    "url": "Paige-sNotes.html#skillquizdescribingdatawithr",
    "content": "Skill Quiz - Describing Data with R Numerical Summaries that MEASURE CENTER: Proportion Mean Mode Median Mean - Gives a good feel for the most typical values of quantitative data as long as the data is at least approximately normally distributed. Median - Gives a good feel for the most typical values of any style of distribution of quantitative data, but is especially useful for skewed data. Standard Deviation - Typically used to describe the spread of relatively normally distribued data. Five-Number Summaryt - Gives a good feel for how the possible values of quanititative data spread out around the more probable values. Find the mean and standard deviation of the “Wind” speed variable in the airquality dataset. pander(summary(airquality$Wind)) Create a Scatter plot of Temp VS. plot(Wind~Temp, data=airquality, xlab= \"Daily Average Temperature\", ylab = \"Daily Average Wind Speed\", main= \"La Guardia Airport Warmer Weather Shows Less Wind\", col= \"gray\", pch= 19)",
    "sentences": ["Numerical Summaries that MEASURE CENTER:", "Proportion Mean Mode Median", "Mean - Gives a good feel for the most typical values of quantitative data as long as the data is at least approximately normally distributed.", "Median - Gives a good feel for the most typical values of any style of distribution of quantitative data, but is especially useful for skewed data.", "Standard Deviation - Typically used to describe the spread of relatively normally distribued data.", "Five-Number Summaryt - Gives a good feel for how the possible values of quanititative data spread out around the more probable values.", "Find the mean and standard deviation of the “Wind” speed variable in the airquality dataset.", "pander(summary(airquality$Wind))", "Create a Scatter plot of Temp VS.", "plot(Wind~Temp, data=airquality, xlab= \"Daily Average Temperature\", ylab = \"Daily Average Wind Speed\", main= \"La Guardia Airport Warmer Weather Shows Less Wind\", col= \"gray\", pch= 19)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skill Quiz - Describing Data with R"
  },
  {
    "id": 227,
    "title": "📍 Class Activity - Principles of Good Graphics",
    "url": "Paige-sNotes.html#classactivityprinciplesofgoodgraphics",
    "content": "Class Activity - Principles of Good Graphics Histogram, Box plot, and Scatter plot with airquality data set hist(airquality$Wind, col=\"steelblue\", xlab=\"Daily Average Wind Speeds (mph)\", main=\"La Guardia Airport from May to September, 1973\") boxplot(Wind~Month, data=airquality, names=c(\"5\",\"6\",\"7\",\"8\",\"9\"), col=c(\"steelblue1\",\"steelblue2\",\"steelblue3\",\"steelblue3\",\"steelblue2\")) plot(Ozone~Temp, data=airquality, col= \"steelblue\", pch=19)",
    "sentences": ["Histogram, Box plot, and Scatter plot with airquality data set", "hist(airquality$Wind, col=\"steelblue\", xlab=\"Daily Average Wind Speeds (mph)\", main=\"La Guardia Airport from May to September, 1973\")", "boxplot(Wind~Month, data=airquality, names=c(\"5\",\"6\",\"7\",\"8\",\"9\"), col=c(\"steelblue1\",\"steelblue2\",\"steelblue3\",\"steelblue3\",\"steelblue2\"))", "plot(Ozone~Temp, data=airquality, col= \"steelblue\", pch=19)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Principles of Good Graphics"
  },
  {
    "id": 228,
    "title": "📍 Skills Quiz - Data Wrangling & Visualization",
    "url": "Paige-sNotes.html#skillsquizdatawranglingvisualization",
    "content": "Skills Quiz - Data Wrangling & Visualization How do you view a dataset? View([insert the dataset here]) How do you open the help file ?([insert the dataset here]) Quanitative vs. Qualitative Quantitative Examples: length & width Qualitatve Examples: name, birthmonth, birthyear, sex, biggerfoot, & domhand PRACTICE : Use KidsFeet dataset Make a table displaying gender table(KidsFeet$sex)%>% pander() Make a table displaying which foot is bigger Which foot is commonly bigger? : Left Foot",
    "sentences": ["How do you view a dataset?", "View([insert the dataset here])", "How do you open the help file", "?([insert the dataset here])", "Quanitative vs. Qualitative", "Quantitative Examples: length & width Qualitatve Examples: name, birthmonth, birthyear, sex, biggerfoot, & domhand", "PRACTICE : Use KidsFeet dataset", "Make a table displaying gender", "table(KidsFeet$sex)%>% pander()", "Make a table displaying which foot is bigger Which foot is commonly bigger?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skills Quiz - Data Wrangling & Visualization"
  },
  {
    "id": 229,
    "title": "📍 Assesment Quiz - Intro to Data Wrangling & Visualization",
    "url": "Paige-sNotes.html#assesmentquizintrotodatawranglingvisualization",
    "content": "Assesment Quiz - Intro to Data Wrangling & Visualization Use the mtcars dataset in R to compute the mean “Gross horsepower” of both automatic and manual transmission 1974 Motor Trend vehicles. mtcars %>% group_by(am)%>% summarise(mean(hp))%>% pander() Use the mtcars dataset in R to make a graph that allows you to see how the quarter mile time (qsec) of 1974 Motor Trend vehicles is effected by the number of carburetors (carb) in the vehicle. Since both qsec and carb are quantitiative, a scatterplot is the best graphic! This helps show that the average qsec time (remember, average is the middle of the dots) drops (or gets faster) as the number of carburetors increases plot(qsec ~ carb, data=mtcars) On average, the more carburetors a vehicle has, the faster its quarter mile time. Run the following codes in R. Then select the statement that most appropriately interprets the resulting graph. The graph produced by the code given shows gas mileage on the y-axis and quarter mile times on the x-axis. Also, as indicated by the legend, the color of the points is determined by whether the vehicle is automatic or manual transmission. Since both transmission types show positive moderate correlations, we can conclude that higher quarter mile times (which means slower vehicles) correlate with higher gas mileages.",
    "sentences": ["Use the mtcars dataset in R to compute the mean “Gross horsepower” of both automatic and manual transmission 1974 Motor Trend vehicles.", "mtcars %>% group_by(am)%>% summarise(mean(hp))%>% pander()", "Use the mtcars dataset in R to make a graph that allows you to see how the quarter mile time (qsec) of 1974 Motor Trend vehicles is effected by the number of carburetors (carb) in the vehicle.", "Since both qsec and carb are quantitiative, a scatterplot is the best graphic!", "This helps show that the average qsec time (remember, average is the middle of the dots) drops (or gets faster) as the number of carburetors increases", "plot(qsec ~ carb, data=mtcars)", "On average, the more carburetors a vehicle has, the faster its quarter mile time.", "Run the following codes in R.", "Then select the statement that most appropriately interprets the resulting graph.", "The graph produced by the code given shows gas mileage on the y-axis and quarter mile times on the x-axis."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assesment Quiz - Intro to Data Wrangling & Visualization"
  },
  {
    "id": 230,
    "title": "📍 Class Activity - Types of Data, R Commands, and Graphics",
    "url": "Paige-sNotes.html#classactivitytypesofdatarcommandsandgraphics",
    "content": "Class Activity - Types of Data, R Commands, and Graphics Y Variable : the data we are after/ interested in Example with a Histogram hist(KidsFeet$birthmonth, breaks=seq(0.5,12.5,1)) Quantitative Data : data that is of units of measurement Examples: length, width KidsFeet %>% group_by(sex) %>% summarise(mean(length),max(length)) %>% pander() Categorical Data : a trait that cannot be measured Examples: sex, birth year, birth month table(KidsFeet$sex) %>% pander()",
    "sentences": ["Y Variable : the data we are after/ interested in Example with a Histogram", "hist(KidsFeet$birthmonth, breaks=seq(0.5,12.5,1))", "Quantitative Data : data that is of units of measurement", "Examples: length, width", "KidsFeet %>% group_by(sex) %>% summarise(mean(length),max(length)) %>% pander()", "Categorical Data : a trait that cannot be measured", "Examples: sex, birth year, birth month", "table(KidsFeet$sex) %>% pander()"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Types of Data, R Commands, and Graphics"
  },
  {
    "id": 231,
    "title": "📍 Class Activity - Reviewing It All",
    "url": "Paige-sNotes.html#classactivityreviewingitall",
    "content": "Class Activity - Reviewing It All Load and view the starwars dataset datatable(starwars, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") PRACTICE : Use starwars data set Use the starwars data set to create a meaningful histogram and supporting numerical summaries. hist(starwars$height) summary(starwars$height)%>% pander() you could also use favstats ( ), it gives you the standard deviation! Use the starwars data set to create a meaningful boxplot (preferably side-by-side boxplots) and supporting numerical summaries. boxplot(height~eye_color, data=starwars) this table was using favstats!!",
    "sentences": ["Load and view the starwars dataset", "datatable(starwars, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\")", "PRACTICE : Use starwars data set", "Use the starwars data set to create a meaningful histogram and supporting numerical summaries.", "hist(starwars$height)", "summary(starwars$height)%>% pander()", "you could also use favstats ( ), it gives you the standard deviation!", "Use the starwars data set to create a meaningful boxplot (preferably side-by-side boxplots) and supporting numerical summaries.", "boxplot(height~eye_color, data=starwars)", "this table was using favstats!!"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Reviewing It All"
  },
  {
    "id": 232,
    "title": "📍 Week 4 | Making Inference with t Tests",
    "url": "Paige-sNotes.html#week4makinginferencewithttests",
    "content": "Week 4 | Making Inference with t Tests Studying from the [Making Inference](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/MakingInference.html and t Test pages and work on learning the t Tests! There is are the: One Sample t Test Paired Samples t Test Independent Samples t Test",
    "sentences": ["Studying from the [Making Inference](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/MakingInference.html and t Test pages and work on learning the t Tests!", "There is are the:", "One Sample t Test Paired Samples t Test Independent Samples t Test"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Week 4 | Making Inference with t Tests"
  },
  {
    "id": 233,
    "title": "📍 Skills Quiz - t Tests",
    "url": "Paige-sNotes.html#skillsquizttests",
    "content": "Skills Quiz - t Tests Inference : the process of deciding whether patterns and trends in a sample of data from a population can be assumed to be true for the full population. does the group define the crowd? A pattern can appear one way in a sample of data, but be completely different for the full population. That is why we use statistical inference to decide when it is safe (and when it is not) to conclude that a pattern in a sample holds in the full population. P-value : the most commonly used method of deciding when to reject a null hypothesis. The things needed to compute a p-value are: A test statistic A probabiltiy distribution of the test statistics that would be possible if the null hypothesis is true If a researcher was using a signficance level of 0.05 and obtained a p-value of 0.419, they can safely conclude that the null hypothesis is true and that the alternative hypothesis is the false. If P is high, keep the guy! Decision Errors : they are two types of errors, Type I or II, that indicate to us whether or not we made a mistake in: Rejecting the null when it was true (Type I) Accepting the null when it was false (Type II) For example: In a typical U.S. Court of Law, the null hypothesis is that the person on trial is innocent. - Thus, convicting an innocent man of a crime would be an example of a Type I Error - And letting a guilty man go free would be an example of a Type II Error",
    "sentences": ["Inference : the process of deciding whether patterns and trends in a sample of data from a population can be assumed to be true for the full population.", "does the group define the crowd?", "A pattern can appear one way in a sample of data, but be completely different for the full population.", "That is why we use statistical inference to decide when it is safe (and when it is not) to conclude that a pattern in a sample holds in the full population.", "P-value : the most commonly used method of deciding when to reject a null hypothesis.", "The things needed to compute a p-value are: A test statistic A probabiltiy distribution of the test statistics that would be possible if the null hypothesis is true If a researcher was using a signficance level of 0.05 and obtained a p-value of 0.419, they can safely conclude that the null hypothesis is true and that the alternative hypothesis is the false.", "If P is high, keep the guy!", "Decision Errors : they are two types of errors, Type I or II, that indicate to us whether or not we made a mistake in: Rejecting the null when it was true (Type I) Accepting the null when it was false (Type II)", "For example: In a typical U.S.", "Court of Law, the null hypothesis is that the person on trial is innocent."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skills Quiz - t Tests"
  },
  {
    "id": 234,
    "title": "📍 Assessment Quiz - t Tests",
    "url": "Paige-sNotes.html#assessmentquizttests",
    "content": "Assessment Quiz - t Tests In a typical year, there are 52 weeks. However, 52 x 7 = 364, and as most of us know, there are 365 days in a year. This means that every year, at least one day gets to happen more than 52 times. Use appropriate R commands and the Births78 dataset to determine which day of the week in 1978 occurred 53 times. Births78 %>% group_by(wday) %>% summarise(n()) %>% pander() Use the Births78 dataset in RStudio to test the following hypotheses. H0:μWednesday−μThursday=0 Ha:μWednesday−μThursday≠0 Find the p-value of the test. Birf <-filter(Births78,wday %in% c(\"Wed\",\"Thu\")) t.test(births ~ wday, data=Birf, mu=0, alternative=\"two.sided\", conf.level=0.95) Answer : P-value = 0.8855",
    "sentences": ["In a typical year, there are 52 weeks.", "However, 52 x 7 = 364, and as most of us know, there are 365 days in a year.", "This means that every year, at least one day gets to happen more than 52 times.", "Use appropriate R commands and the Births78 dataset to determine which day of the week in 1978 occurred 53 times.", "Births78 %>% group_by(wday) %>% summarise(n()) %>% pander()", "Use the Births78 dataset in RStudio to test the following hypotheses.", "H0:μWednesday−μThursday=0 Ha:μWednesday−μThursday≠0", "Find the p-value of the test.", "Birf <-filter(Births78,wday %in% c(\"Wed\",\"Thu\")) t.test(births ~ wday, data=Birf, mu=0, alternative=\"two.sided\", conf.level=0.95)", "Answer : P-value = 0.8855"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assessment Quiz - t Tests"
  },
  {
    "id": 235,
    "title": "📍 Class Activity - Making Inference",
    "url": "Paige-sNotes.html#classactivitymakinginference",
    "content": "Class Activity - Making Inference Studying from the Making Inference page! Managing Decision Errors hypothesis -> beliefs null (initial) BELIEF, alternative BELIEF Goal: to discover truth Alternatively, you can think about it like: Type I Error -> Action (Throwing away truth) effected by alpha; small a, small error vice versa -Type II Error -> (Failing to move to truth) effected by beta; small b, small error vice versa Alpha and Beta work inversely of eachother! - high a -> low b - low a -> high b The P-value measures how much the evidence differs from what we expected under the null hypothesis(belief) P = “probability” P is low, Ho must go! P is high, keep the guy! null gives us a frame of reference from where the data COULD fall",
    "sentences": ["Studying from the Making Inference page!", "Managing Decision Errors", "hypothesis -> beliefs null (initial) BELIEF, alternative BELIEF Goal: to discover truth", "Alternatively, you can think about it like:", "Type I Error -> Action (Throwing away truth) effected by alpha; small a, small error vice versa -Type II Error -> (Failing to move to truth) effected by beta; small b, small error vice versa", "Alpha and Beta work inversely of eachother!", "- high a -> low b - low a -> high b", "The P-value measures how much the evidence differs from what we expected under the null hypothesis(belief) P = “probability” P is low, Ho must go!", "P is high, keep the guy!", "null gives us a frame of reference from where the data COULD fall"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Making Inference"
  },
  {
    "id": 236,
    "title": "📍 Class Activity - t Tests",
    "url": "Paige-sNotes.html#classactivityttests",
    "content": "Class Activity - t Tests P-value measures the probability of the test statistic (what is happening in front of you) if something is impossible, then we should reject the belief(the null) since the possibility of that belief happening is close to 0 Hypothesis = “WE BELIEVE” Finding P-values with t Tests! One-Sample t Test What is the average foot size of 4th graders? t.test(KidsFeet$length, mu=28, alternative=\"two.sided\", conf.level=0.95) %>% pander() We have sufficient evidence to believe that kids feet are not averaged aroun 28 cm. Instead, they are averaged around 24 to 25 cm. Paired Samples t Test Is the length measurement of someone’s foot the same measurement of 3 width’s of their foot? t.test(KidsFeet$length, KidsFeet$width*3, paired=TRUE, mu=0, alternative=\"two.sided\",conf.level=0.95) %>% pander()",
    "sentences": ["P-value measures the probability of the test statistic (what is happening in front of you) if something is impossible, then we should reject the belief(the null) since the possibility of that belief happening is close to 0 Hypothesis = “WE BELIEVE”", "Finding P-values with t Tests!", "One-Sample t Test", "What is the average foot size of 4th graders?", "t.test(KidsFeet$length, mu=28, alternative=\"two.sided\", conf.level=0.95) %>% pander()", "We have sufficient evidence to believe that kids feet are not averaged aroun 28 cm.", "Instead, they are averaged around 24 to 25 cm.", "Paired Samples t Test", "Is the length measurement of someone’s foot the same measurement of 3 width’s of their foot?", "t.test(KidsFeet$length, KidsFeet$width*3, paired=TRUE, mu=0, alternative=\"two.sided\",conf.level=0.95) %>% pander()"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - t Tests"
  },
  {
    "id": 237,
    "title": "📍 Week 5 | Wilcoxon Tests",
    "url": "Paige-sNotes.html#week5wilcoxontests",
    "content": "Week 5 | Wilcoxon Tests Studying from the Wilcoxon Tests page! (the first nonparameteric test we encounter!) Wilcoxon Signed-Rank Test Wilcoxon Rank Sum (Mann-Whiteny) Test",
    "sentences": ["Studying from the Wilcoxon Tests page!", "(the first nonparameteric test we encounter!)", "Wilcoxon Signed-Rank Test Wilcoxon Rank Sum (Mann-Whiteny) Test"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Week 5 | Wilcoxon Tests"
  },
  {
    "id": 238,
    "title": "📍 Skills Quiz - Wilcoxon Tests",
    "url": "Paige-sNotes.html#skillsquizwilcoxontests",
    "content": "Skills Quiz - Wilcoxon Tests are NON-PARAMETRIC tests uses sums of ranks and mathematical counting techniques (a NON-PARAMETRIC distribution) to calculate the p-value t test (that are PARAMETRIC) rely on t distributions (which are PARAMETRIC distributions) to calculate the p-value while Wilcoxon tests the distribution of the Wilcoxon Test Statistic can be usefully approximated by a normal distribution when the sample size of the data being used in the test is large However, this distribution of the test statistic can never be exactly normal because the test statistic can only ever be a whole number ignores the specific values of the data and only utilize the relative positions of the data, i.e., the ranks (hence the reason they are often called Rank-Based Tests.) 1) Wilcoxon Signed-Rank Test was originally created to test hypotheses about the value of the median, but can be used to test hypotheses about the mean when data is SYMMETRIC A useful custom graphic that shows all of the data as well as a five-number summary of the data is a box plot overlaid with a dot plot The style of tests that can be performed with this test are: One Sample: testing hypotheses about the center of a distribution (where the center is subtracted from each value). Paired Samples: testing hypotheses about the center of the distribution of differences. The CornHeights Analysis Example is shown below: corn <- c(14, 56, 60, 16, 6, 8, -48, 49, 24, 28, 29, 41, -67, 23, 75) boxplot(corn, horizontal=TRUE, col=\"cornsilk\", main=\"Differences in Corn Plant Heights\", xlab=\"Cross-Fertilized Height minus Self-Fertilized Height\") stripchart(corn, pch=16, method=\"stack\", col=\"darkgray\", add=TRUE) Here is the number summary and Wilcoxon test that goes along with it!",
    "sentences": ["are NON-PARAMETRIC tests uses sums of ranks and mathematical counting techniques (a NON-PARAMETRIC distribution) to calculate the p-value t test (that are PARAMETRIC) rely on t distributions (which are PARAMETRIC distributions) to calculate the p-value while Wilcoxon tests", "the distribution of the Wilcoxon Test Statistic can be usefully approximated by a normal distribution when the sample size of the data being used in the test is large However, this distribution of the test statistic can never be exactly normal because the test statistic can only ever be a whole number", "ignores the specific values of the data and only utilize the relative positions of the data, i.e., the ranks (hence the reason they are often called Rank-Based Tests.)", "1) Wilcoxon Signed-Rank Test", "was originally created to test hypotheses about the value of the median, but can be used to test hypotheses about the mean when data is SYMMETRIC A useful custom graphic that shows all of the data as well as a five-number summary of the data is a box plot overlaid with a dot plot The style of tests that can be performed with this test are:", "One Sample: testing hypotheses about the center of a distribution (where the center is subtracted from each value).", "Paired Samples: testing hypotheses about the center of the distribution of differences.", "The CornHeights Analysis Example is shown below:", "corn <- c(14, 56, 60, 16, 6, 8, -48, 49, 24, 28, 29, 41, -67, 23, 75) boxplot(corn, horizontal=TRUE, col=\"cornsilk\", main=\"Differences in Corn Plant Heights\", xlab=\"Cross-Fertilized Height minus Self-Fertilized Height\") stripchart(corn, pch=16, method=\"stack\", col=\"darkgray\", add=TRUE)", "Here is the number summary and Wilcoxon test that goes along with it!"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skills Quiz - Wilcoxon Tests"
  },
  {
    "id": 239,
    "title": "📍 Assessment Quiz - Wilcoxon Tests",
    "url": "Paige-sNotes.html#assessmentquizwilcoxontests",
    "content": "Assessment Quiz - Wilcoxon Tests Use the Salaries dataset in R to find the number of male and female assistant professors in the dataset. This can be done in two different ways! Use the favstats() command SalariesAss <- filter(Salaries, rank == \"AsstProf\") pander(favstats(salary ~ sex, data=SalariesAss)) Use the group_by() and summarise() commands Salaries %>% group_by(rank, sex) %>% summarise(n()) %>% pander() Create an appropriate graphic using the Salaries dataset in R that would allow you to compare the distribution of salaries for faculty in discipline A (“theoretical”) and discipline B (“applied”) departments. boxplot(salary ~ discipline, data=SalariesAss, col=\"white\", main=\"Faculty From U.S. Colleges\", xlab=\"Discipline\", ylab=\"Salaries\") The graphics that can me used for Wilcoxon Rank Sum Test are BOXPLOTS & DOT PLOTS, not SCATTER PLOTS",
    "sentences": ["Use the Salaries dataset in R to find the number of male and female assistant professors in the dataset.", "This can be done in two different ways!", "Use the favstats() command", "SalariesAss <- filter(Salaries, rank == \"AsstProf\") pander(favstats(salary ~ sex, data=SalariesAss))", "Use the group_by() and summarise() commands", "Salaries %>% group_by(rank, sex) %>% summarise(n()) %>% pander()", "Create an appropriate graphic using the Salaries dataset in R that would allow you to compare the distribution of salaries for faculty in discipline A (“theoretical”) and discipline B (“applied”) departments.", "boxplot(salary ~ discipline, data=SalariesAss, col=\"white\", main=\"Faculty From U.S.", "Colleges\", xlab=\"Discipline\", ylab=\"Salaries\")", "The graphics that can me used for Wilcoxon Rank Sum Test are BOXPLOTS & DOT PLOTS, not SCATTER PLOTS"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assessment Quiz - Wilcoxon Tests"
  },
  {
    "id": 240,
    "title": "📍 Class Activity - Wilcoxon Tests",
    "url": "Paige-sNotes.html#classactivitywilcoxontests",
    "content": "Class Activity - Wilcoxon Tests What is the Wilcoxon Test? not concerned about the distance between values we care about the location shift/ difference in ranks - sample size, standard deviation does NOT matter! Steps to Wilcoxon Rank Sum (Mann-Whitney Test): Order Data Rank it Pull out ranks from each group Add the ranks What is the possibility of each sum being possible? Use Frank Wilcoxon spikey distribution to determine the extremities of the data Finite and discrete (normal distribution is infinite and continuous) When they detect any ties in ranks, they shift from a finite realm to a more continuous realm in order to compute it properly A <- c(68, 68, 59, 72, 64, 67, 70, 74) B <- c(60, 67, 61, 62, 67, 63, 56, 58) wilcox.test(A,B) What is the question(s) this test would answer? Do the two genders tend to pick their favorite numbers differently? Do the test scores of students from school X tend to be higher than the test score of students from school Y?",
    "sentences": ["What is the Wilcoxon Test?", "not concerned about the distance between values we care about the location shift/ difference in ranks - sample size, standard deviation does NOT matter!", "Steps to Wilcoxon Rank Sum (Mann-Whitney Test):", "Order Data Rank it Pull out ranks from each group Add the ranks What is the possibility of each sum being possible?", "Use Frank Wilcoxon spikey distribution to determine the extremities of the data Finite and discrete (normal distribution is infinite and continuous)", "When they detect any ties in ranks, they shift from a finite realm to a more continuous realm in order to compute it properly", "A <- c(68, 68, 59, 72, 64, 67, 70, 74) B <- c(60, 67, 61, 62, 67, 63, 56, 58) wilcox.test(A,B)", "What is the question(s) this test would answer?", "Do the two genders tend to pick their favorite numbers differently?", "Do the test scores of students from school X tend to be higher than the test score of students from school Y?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Wilcoxon Tests"
  },
  {
    "id": 241,
    "title": "📍 Class Activity - Wilcoxon Tests, Boxplots, and P-Values",
    "url": "Paige-sNotes.html#classactivitywilcoxontestsboxplotsandpvalues",
    "content": "Class Activity - Wilcoxon Tests, Boxplots, and P-Values Which function would you use to add a new column to your dataset? What is the goal of statistics? to quantify things (the chance of this happening if this is the truth!) test statistic shows us the measurement of how far we are from the null Which plots work best to display one quantitative variable that is separated into 2 or more groups? box plot & dot plot Box plots + Dot plots are great to show off this kind of data (because it shows the median!) Median line closer to the bottom of the box plot = Right Skewed Median line is closer to the top of the box plot = Left Skewed What two things are needed to calculate a p-value? a test statistic and a sampling distribution of that test statistics When would an independent samples t test be inappropriate to use but a Wilcoxon Rank Sum Test be appropriate? Independent Samples t Test : looks at average needs raw data (times, measurements, etc.) Wilcoxon Rank Sum Test : looks at median needs ranks (first place, second place, etc.) What is an appropriate set of hypotheses to use for an independent samples t test? For a Wilcoxon Rank Sum Test? Independent Samples t Test : mu Wilcoxon Rank Sum Test : Median or “stochastically”",
    "sentences": ["Which function would you use to add a new column to your dataset?", "What is the goal of statistics?", "to quantify things (the chance of this happening if this is the truth!) test statistic shows us the measurement of how far we are from the null", "Which plots work best to display one quantitative variable that is separated into 2 or more groups?", "box plot & dot plot Box plots + Dot plots are great to show off this kind of data (because it shows the median!) Median line closer to the bottom of the box plot = Right Skewed Median line is closer to the top of the box plot = Left Skewed", "What two things are needed to calculate a p-value?", "a test statistic and a sampling distribution of that test statistics", "When would an independent samples t test be inappropriate to use but a Wilcoxon Rank Sum Test be appropriate?", "Independent Samples t Test : looks at average needs raw data (times, measurements, etc.) Wilcoxon Rank Sum Test : looks at median needs ranks (first place, second place, etc.)", "What is an appropriate set of hypotheses to use for an independent samples t test?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Wilcoxon Tests, Boxplots, and P-Values"
  },
  {
    "id": 242,
    "title": "📍 Week 6 | Two-way ANOVA",
    "url": "Paige-sNotes.html#week6twowayanova",
    "content": "Week 6 | Two-way ANOVA Studying from the ANOVA page and its one quantitative column and the two categorical columns! One-Way ANOVA Test Two-Way ANOVA Test",
    "sentences": ["Studying from the ANOVA page and its one quantitative column and the two categorical columns!", "One-Way ANOVA Test Two-Way ANOVA Test"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Week 6 | Two-way ANOVA"
  },
  {
    "id": 243,
    "title": "📍 Skills Quiz - ANOVA",
    "url": "Paige-sNotes.html#skillsquizanova",
    "content": "Skills Quiz - ANOVA What is an example of a “factor”? Hair color! Run the code “View(warpbreaks)” in R. Use that data set to identify each of the following as either a “factor” or a “level” of a factor. datatable(warpbreaks,options=list(c(3,10,30))) What are the factors for this data set? Tension Wool What are the levels of factors? Medium (from the Tension factor) High (from the Tension factor) A (from the Wool factor) Low (from the Tension Factor) B (from the Wool factor) Why is the “Breaks” column not a factor? The column contains quanitative values than the qualitative values",
    "sentences": ["What is an example of a “factor”?", "Hair color!", "Run the code “View(warpbreaks)” in R.", "Use that data set to identify each of the following as either a “factor” or a “level” of a factor.", "datatable(warpbreaks,options=list(c(3,10,30)))", "What are the factors for this data set?", "Tension Wool", "What are the levels of factors?", "Medium (from the Tension factor) High (from the Tension factor) A (from the Wool factor) Low (from the Tension Factor) B (from the Wool factor) Why is the “Breaks” column not a factor?", "The column contains quanitative values than the qualitative values"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skills Quiz - ANOVA"
  },
  {
    "id": 244,
    "title": "📍 Assessment Quiz - ANOVA",
    "url": "Paige-sNotes.html#assessmentquizanova",
    "content": "Assessment Quiz - ANOVA In a certain student’s ANOVA analysis the “Between groups variance” was 18.52 while the “Within groups variance” was 4.9. What was the value of the test statistic of their ANOVA test? ANSWER : F = 3.78 As shown in the Explanation tab of the ANOVA page of the Math 325 Notebook, the ANOVA test statistic is an F statistic. It is calculated by taking the “Between groups variance” and dividing this by the “Within groups variance”. Thus 18.52/4.9 = 3.779592 which rounds to 3.78. Make three Two-Way ANOVA graphs looking at the length, domhand, and sex columns of the KidsFeet dataset. Based on your p-values and these graphs, which of the following is a correct conclusion to reach? xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1)))",
    "sentences": ["In a certain student’s ANOVA analysis the “Between groups variance” was 18.52 while the “Within groups variance” was 4.9.", "What was the value of the test statistic of their ANOVA test?", "ANSWER : F = 3.78", "As shown in the Explanation tab of the ANOVA page of the Math 325 Notebook, the ANOVA test statistic is an F statistic.", "It is calculated by taking the “Between groups variance” and dividing this by the “Within groups variance”.", "Thus 18.52/4.9 = 3.779592 which rounds to 3.78.", "Make three Two-Way ANOVA graphs looking at the length, domhand, and sex columns of the KidsFeet dataset.", "Based on your p-values and these graphs, which of the following is a correct conclusion to reach?", "xyplot(length ~ domhand, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Dominant Hand\", ylab=\"Length of Child's Longest Foot\") xyplot(length ~ sex, data=KidsFeet, type=c(\"p\",\"a\"), main=\"4th Grade Students\", col='dodgerblue', xlab=\"Child's Gender\", ylab=\"Length of Child's Longest Foot\")", "xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c(\"p\",\"a\"), main=\"4th Grade Students\", auto.key=list(corner=c(1,1)))"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assessment Quiz - ANOVA"
  },
  {
    "id": 245,
    "title": "📍 Class Activity - One- Way ANOVA",
    "url": "Paige-sNotes.html#classactivityonewayanova",
    "content": "Class Activity - One- Way ANOVA What does ANOVA stand for? Analysis Of Variance! tests several means simultaneously Rejecting a null -> I know that there is a different Fail to reject the null -> I don’t know if they differ! If you conduct a ton of tests on one data set, the probability of a Type 1 Error increases! we would expect that one of the tests ex. conduct two tests, nearly doubles the Type 1 Error Thankfully, ANOVA protects against those potential errors Hypothesis of ANOVA Null hypothesis = all means are equal Alternative hypothesis = at least one mean is different problem(s): doesn’t tell us which one is different or how many are different! Explain the words “factor” and “level”",
    "sentences": ["What does ANOVA stand for?", "Analysis Of Variance!", "tests several means simultaneously", "Rejecting a null -> I know that there is a different Fail to reject the null -> I don’t know if they differ!", "If you conduct a ton of tests on one data set, the probability of a Type 1 Error increases!", "we would expect that one of the tests ex.", "conduct two tests, nearly doubles the Type 1 Error Thankfully, ANOVA protects against those potential errors", "Hypothesis of ANOVA", "Null hypothesis = all means are equal Alternative hypothesis = at least one mean is different problem(s): doesn’t tell us which one is different or how many are different!", "Explain the words “factor” and “level”"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - One- Way ANOVA"
  },
  {
    "id": 246,
    "title": "📍 Class Activity - Two- Way ANOVA",
    "url": "Paige-sNotes.html#classactivitytwowayanova",
    "content": "Class Activity - Two- Way ANOVA Looking at the DayCare.RmD file! How many Day Care Centers were included in the study? 10 How many Centers were included in the treatment group? The Control Group? 6 and 4 During which weeks did the researchers simply observe the Day Care Centers? first four During which weeks was the fine applied to the treatment group? w 5-16 During which weeks was the fine removed from the treatment group? w 17 What was the main response variable of interest that the researchers recorded each week on the Day Care Centers? The number of late children (does the fine change behavior?) How to change from wide to long data?",
    "sentences": ["Looking at the DayCare.RmD file!", "How many Day Care Centers were included in the study?", "10 How many Centers were included in the treatment group?", "The Control Group?", "6 and 4 During which weeks did the researchers simply observe the Day Care Centers?", "first four During which weeks was the fine applied to the treatment group?", "w 5-16 During which weeks was the fine removed from the treatment group?", "w 17 What was the main response variable of interest that the researchers recorded each week on the Day Care Centers?", "The number of late children (does the fine change behavior?)", "How to change from wide to long data?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Two- Way ANOVA"
  },
  {
    "id": 247,
    "title": "📍 Week 7 | The Kruskall-Wallis Test",
    "url": "Paige-sNotes.html#week7thekruskallwallistest",
    "content": "Week 7 | The Kruskall-Wallis Test Studying from the Kruskal-Wallis Test page and its the second nonparametric test that we see thats quantitative to multiple categorical!",
    "sentences": "Studying from the Kruskal-Wallis Test page and its the second nonparametric test that we see thats quantitative to multiple categorical!",
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Week 7 | The Kruskall-Wallis Test"
  },
  {
    "id": 248,
    "title": "📍 Skills Quiz - Kruskal-Wallis Test",
    "url": "Paige-sNotes.html#skillsquizkruskalwallistest",
    "content": "Skills Quiz - Kruskal-Wallis Test Use the SaratogaHouses data set for the following Kruskal-Wallis Test! table(SaratogaHouses$fuel) Say a homeowner in Saratoga County, New York is curious about whether upgrading their home from an oil heating fuel system to either a gas or electric fuel system would increase the resale value of their home. Use the SaratogaHouses dataset in R to answer the question, “which heating system results in the highest distribution of home resale values (price)? Null Hypothesis : The price of homes with either gas, oil, or electric heating fuel systems all come from the same distribution Alternative Hypothesis : At least one type of fuel systems results in a different distribution of prices of homes kruskal.test(price ~ fuel, data=SaratogaHouses) The Kruskal-Wallis Rank Sum Test results in a p-value of 2.2e-16, meaning there is sufficent evidence to conclude the hypothesis the distribution of prices of homes is different for at least one of the fuel system types. The graphic that best depicts the results of the Kruskall-Wallis Test… A Boxplot!! boxplot(price ~ fuel, data=SaratogaHouses) Questions regarding the previous graph:",
    "sentences": ["Use the SaratogaHouses data set for the following Kruskal-Wallis Test!", "table(SaratogaHouses$fuel)", "Say a homeowner in Saratoga County, New York is curious about whether upgrading their home from an oil heating fuel system to either a gas or electric fuel system would increase the resale value of their home.", "Use the SaratogaHouses dataset in R to answer the question, “which heating system results in the highest distribution of home resale values (price)?", "Null Hypothesis : The price of homes with either gas, oil, or electric heating fuel systems all come from the same distribution Alternative Hypothesis : At least one type of fuel systems results in a different distribution of prices of homes", "kruskal.test(price ~ fuel, data=SaratogaHouses)", "The Kruskal-Wallis Rank Sum Test results in a p-value of 2.2e-16, meaning there is sufficent evidence to conclude the hypothesis the distribution of prices of homes is different for at least one of the fuel system types.", "The graphic that best depicts the results of the Kruskall-Wallis Test… A Boxplot!!", "boxplot(price ~ fuel, data=SaratogaHouses)", "Questions regarding the previous graph:"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skills Quiz - Kruskal-Wallis Test"
  },
  {
    "id": 249,
    "title": "📍 Assessement Quiz - Kruskal-Wallis",
    "url": "Paige-sNotes.html#assessementquizkruskalwallis",
    "content": "Assessement Quiz - Kruskal-Wallis Use the Salaries dataset in R, library(car), to test the hypotheses and report the test statisic and the conlusion of your test \\[ H_0 : \\text{The distribution of salary is the same for Associate, Assistant, and Full Professors.} \\] \\[ H_a : \\text{The distribution of salary is different for at least one type of Professor.} \\] kruskal.test(salary ~ rank, data=Salaries) boxplot(salary ~ rank, data=Salaries) Salaries %>% group_by(rank) %>% summarise(`Median Salary`=median(salary), .groups=\"drop\") %>% pander(caption=\"Median Salary by Rank\") A waiter at a local restaurant collects data on how much they earn in tips (per person) when groups of people each pay separately for their meal versus when one member of the group pays for everyone. Their very small sample of data is as follows. Group ID Number of People in Group Type of Payment Total Tip Amount Per Person Amount 1 5 Each Paid $16 $3.20 2 3 One Paid for All $10 $3.33 3 4 One Paid for All $9 $2.25 4 4 Each Paid $15 $3.75 5 6 Each Paid $21 $3.50 Which hypothesis test would be most appropriate for deciding if the average per person tip amount is higher when people that eat in groups each pay individually for their meal? What is the correct way to describe the test statistic, H, of the Kruskal-Wallis Test? It is a combined measurement of how much the average ranks differ between each groupp-0. ANOVA uses the F = (Between groups variance)/(Within groups variance) test statistic. The One Sample t Test measures how far the sample mean is from mu for its test statistic. The Wilcoxon Rank Sum test sums the ranks from one of the groups for its test statistic.",
    "sentences": ["Use the Salaries dataset in R, library(car), to test the hypotheses and report the test statisic and the conlusion of your test", "\\[ H_0 : \\text{The distribution of salary is the same for Associate, Assistant, and Full Professors.} \\] \\[ H_a : \\text{The distribution of salary is different for at least one type of Professor.} \\]", "kruskal.test(salary ~ rank, data=Salaries)", "boxplot(salary ~ rank, data=Salaries)", "Salaries %>% group_by(rank) %>% summarise(`Median Salary`=median(salary), .groups=\"drop\") %>% pander(caption=\"Median Salary by Rank\")", "A waiter at a local restaurant collects data on how much they earn in tips (per person) when groups of people each pay separately for their meal versus when one member of the group pays for everyone.", "Their very small sample of data is as follows.", "Group ID Number of People in Group Type of Payment Total Tip Amount Per Person Amount 1 5 Each Paid $16 $3.20 2 3 One Paid for All $10 $3.33 3 4 One Paid for All $9 $2.25 4 4 Each Paid $15 $3.75 5 6 Each Paid $21 $3.50", "Which hypothesis test would be most appropriate for deciding if the average per person tip amount is higher when people that eat in groups each pay individually for their meal?", "What is the correct way to describe the test statistic, H, of the Kruskal-Wallis Test?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assessement Quiz - Kruskal-Wallis"
  },
  {
    "id": 250,
    "title": "📍 Class Activity - Idea Approval for the Week 11 “Consulting\r\nOpportunity or Research Project”",
    "url": "Paige-sNotes.html#classactivityideaapprovalfortheweek11consultingopp",
    "content": "Class Activity - Idea Approval for the Week 11 “Consulting\r\nOpportunity or Research Project” Find a data analysis opportunity for a client* where you use data the client provides to answer research questions they have. *A client could be a small business owner (maybe your parents, an aunt, uncle, or someone else you personally know), a teacher you TA for, your boss you currently work for or that you worked for in the past, an old high school teacher, or even a roommate. The client could potentially even be yourself if say you were trying to buy a new car or some other important item. However, this analysis has the greatest chance of contributing to your resume if it is performed for someone else rather than just yourself. But if you did something for yourself that was say “blog worthy” and that other people would also find interesting, then that could be meaningful as well. In the space below, state which option you are choosing, and provide details on how you are going to make this work. Over the next couple of weeks your teacher will follow up with you to see how you are doing with implementing your idea. If you have any concerns about whether your idea is a good idea or not, discuss that in person with your teacher right away. Make sure you only submit an idea that you are fairly confident will work well in the space below. Make sure it powerful and correct!! START WITH THE QUESTION OR GRAPH hide technical details use two way thinkin",
    "sentences": ["Find a data analysis opportunity for a client* where you use data the client provides to answer research questions they have.", "*A client could be a small business owner (maybe your parents, an aunt, uncle, or someone else you personally know), a teacher you TA for, your boss you currently work for or that you worked for in the past, an old high school teacher, or even a roommate.", "The client could potentially even be yourself if say you were trying to buy a new car or some other important item.", "However, this analysis has the greatest chance of contributing to your resume if it is performed for someone else rather than just yourself.", "But if you did something for yourself that was say “blog worthy” and that other people would also find interesting, then that could be meaningful as well.", "In the space below, state which option you are choosing, and provide details on how you are going to make this work.", "Over the next couple of weeks your teacher will follow up with you to see how you are doing with implementing your idea.", "If you have any concerns about whether your idea is a good idea or not, discuss that in person with your teacher right away.", "Make sure you only submit an idea that you are fairly confident will work well in the space below.", "Make sure it powerful and correct!!"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Idea Approval for the Week 11 “Consulting\r\nOpportunity or Research Project”"
  },
  {
    "id": 251,
    "title": "📍 Class Activity - Kruskal-Wallis Test",
    "url": "Paige-sNotes.html#classactivitykruskalwallistest",
    "content": "Class Activity - Kruskal-Wallis Test nonparametric equivalent of a One-Way ANOVA -> rank based test (not caring about data values! ) will have to use as.numeric() function to make columns into mumeric values for groups (C) use mutate(new column name = interaction(group 1, group 2)) Any similarities this test has to the Wilcoxon Rank Sum test? Discuss any differences? Similarities both are nonparametric tests! don’t get a lot of powerful insights Differences Testing Groups Wilcoxon = 2 groups Kruskal-Wallis = 3 groups! Can dump all the groups at once Test Statisitic Wilcoxon (F) Kruskal-Wallis (H) C -> number of samples/groups (need 3 or more!) N -> Total of all sample sizes n_i -> each individual sample (n_i = n_1 + n_2 …) R bar _i -> the mean rank for each sample What can you conclude when the p-value of the Kruskal-Wallis Test is significant? Significant : bet your money on that choice! Not Significant: using p-value to say “maybe” / suggesting that this is more likely to be the Tell them where to go from what the data does or doesn’t give (what’s the next step?)",
    "sentences": ["nonparametric equivalent of a One-Way ANOVA -> rank based test (not caring about data values!", ") will have to use as.numeric() function to make columns into mumeric values for groups (C) use mutate(new column name = interaction(group 1, group 2))", "Any similarities this test has to the Wilcoxon Rank Sum test?", "Discuss any differences?", "Similarities both are nonparametric tests!", "don’t get a lot of powerful insights Differences Testing Groups Wilcoxon = 2 groups Kruskal-Wallis = 3 groups!", "Can dump all the groups at once Test Statisitic Wilcoxon (F) Kruskal-Wallis (H) C -> number of samples/groups (need 3 or more!) N -> Total of all sample sizes n_i -> each individual sample (n_i = n_1 + n_2 …) R bar _i -> the mean rank for each sample", "What can you conclude when the p-value of the Kruskal-Wallis Test is significant?", "Significant : bet your money on that choice!", "Not Significant: using p-value to say “maybe” / suggesting that this is more likely to be the Tell them where to go from what the data does or doesn’t give (what’s the next step?)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Kruskal-Wallis Test"
  },
  {
    "id": 252,
    "title": "📍 Skill Quiz - Simple Linear Regression",
    "url": "Paige-sNotes.html#skillquizsimplelinearregression",
    "content": "Skill Quiz - Simple Linear Regression There are five assupmtions of the simple linear regression model: Constant Variance Independent Errors Normal Errors Fixed X Values Linear Relation Questions about the assumptions: Which regression assumption(s) does the residuals versus fitted values plot diagnose? Linear Relation Constance Variance Which regression assumption(s) does the Q-Q Plot of the residuals diagnose? Which regression assumption(s) does the Residuals versus Order plot diagnose? (Remember, this plot can only be created when the data was collected in a specific order.) Independent Errors Perform the following simple linear regression in R: plot(Height ~ Volume, data = trees) trees.lm <- lm(Height ~ Volume, data = trees) abline(trees.lm)",
    "sentences": ["There are five assupmtions of the simple linear regression model:", "Constant Variance Independent Errors Normal Errors Fixed X Values Linear Relation", "Questions about the assumptions:", "Which regression assumption(s) does the residuals versus fitted values plot diagnose?", "Linear Relation Constance Variance", "Which regression assumption(s) does the Q-Q Plot of the residuals diagnose?", "Which regression assumption(s) does the Residuals versus Order plot diagnose?", "(Remember, this plot can only be created when the data was collected in a specific order.)", "Independent Errors Perform the following simple linear regression in R:", "plot(Height ~ Volume, data = trees) trees.lm <- lm(Height ~ Volume, data = trees) abline(trees.lm)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skill Quiz - Simple Linear Regression"
  },
  {
    "id": 253,
    "title": "📍 Assessment Quiz - Simple Linear Regression",
    "url": "Paige-sNotes.html#assessmentquizsimplelinearregression",
    "content": "Assessment Quiz - Simple Linear Regression Use the KidsFeet dataset from library(mosaic) to perform a regression of the length of a child’s foot (Y) on the width of a child’s foot (X). What is the estimated slope and intercept of the least squares regression line for this data? chillin <- lm(length ~ width, data = KidsFeet) pander(summary(chillin)) Intercept: 9.82 Slope: 1.66 Continue using the KidsFeet regression that you performed in the last problem. In other words, the regression of the length of a child’s foot (Y) on the width of the child’s foot (X). Plot the regression in R. plot(length ~ width, data= KidsFeet) abline(chillin) The residual plot from the regression of the length of a child’s foot on the width of the child’s foot (using the KidsFeet dataset from library(mosaic)) shows the following residual plots. Write a statement that most correctly interprets these plots.",
    "sentences": ["Use the KidsFeet dataset from library(mosaic) to perform a regression of the length of a child’s foot (Y) on the width of a child’s foot (X).", "What is the estimated slope and intercept of the least squares regression line for this data?", "chillin <- lm(length ~ width, data = KidsFeet) pander(summary(chillin))", "Intercept: 9.82 Slope: 1.66", "Continue using the KidsFeet regression that you performed in the last problem.", "In other words, the regression of the length of a child’s foot (Y) on the width of the child’s foot (X).", "Plot the regression in R.", "plot(length ~ width, data= KidsFeet) abline(chillin)", "The residual plot from the regression of the length of a child’s foot on the width of the child’s foot (using the KidsFeet dataset from library(mosaic)) shows the following residual plots.", "Write a statement that most correctly interprets these plots."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assessment Quiz - Simple Linear Regression"
  },
  {
    "id": 254,
    "title": "📍 Class Activity - Simple Linear Regression",
    "url": "Paige-sNotes.html#classactivitysimplelinearregression",
    "content": "Class Activity - Simple Linear Regression Here is the Regression Applet! https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_en.html Dots are unique and beautiful! They represent individuals and help to show us a trend The Mathematical Model of Regression (including the equation of the best-fit line). \\[ \\underbrace{Y_i}_\\text{Some Label about the Y-axis} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label about the X-axis} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] - True Regression Relation = the law (the trend) Error Term = the individual in accordance to the law Error Term Normally Distributed = the difference between the predicted values from the actual values (the errors) follow a normal distribution THE LINE IS THE AVERAGE!!!!!!! (best fit line) Y hat is the line (the prediction) ex. what the weather might be tommorrow Y is the individiual point ex. what the weather is today",
    "sentences": ["Here is the Regression Applet!", "https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_en.html Dots are unique and beautiful!", "They represent individuals and help to show us a trend", "The Mathematical Model of Regression (including the equation of the best-fit line).", "\\[ \\underbrace{Y_i}_\\text{Some Label about the Y-axis} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label about the X-axis} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] - True Regression Relation = the law (the trend)", "Error Term = the individual in accordance to the law Error Term Normally Distributed = the difference between the predicted values from the actual values (the errors) follow a normal distribution", "THE LINE IS THE AVERAGE!!!!!!!", "(best fit line) Y hat is the line (the prediction) ex.", "what the weather might be tommorrow Y is the individiual point ex.", "what the weather is today"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Simple Linear Regression"
  },
  {
    "id": 255,
    "title": "📍 Class Activity - Simple Linear Regression (Part 2)",
    "url": "Paige-sNotes.html#classactivitysimplelinearregressionpart2",
    "content": "Class Activity - Simple Linear Regression (Part 2) Review the following: Slope of a line, including the statistics symbol for it. Y-intercept of a line, including the statistics symbol for it. \\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i \\text{ where } \\epsilon_i ~ N(0,\\sigma^2)\\] \\(\\beta_0\\) : True y-intercept \\(\\beta_1\\) : True Slope \\(H_0 : \\beta_1 = 0\\) \\(H_a : \\beta_1 \\neq 0\\) \\[\\hat{Y}_i = b_0 + b_1X_i\\] \\(\\hat{Y}_i\\) : Estimated line \\(b_0\\) : Sample y-intercept \\(b_1\\) : Sample Slope \\[E\\{Y_i\\} = \\beta_0 + \\beta_1X_i\\] \\(E\\{Y_i\\}\\) : True Line The “error” term.",
    "sentences": ["Review the following:", "Slope of a line, including the statistics symbol for it.", "Y-intercept of a line, including the statistics symbol for it.", "\\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i \\text{ where } \\epsilon_i ~ N(0,\\sigma^2)\\]", "\\(\\beta_0\\) : True y-intercept \\(\\beta_1\\) : True Slope \\(H_0 : \\beta_1 = 0\\) \\(H_a : \\beta_1 \\neq 0\\)", "\\[\\hat{Y}_i = b_0 + b_1X_i\\]", "\\(\\hat{Y}_i\\) : Estimated line \\(b_0\\) : Sample y-intercept \\(b_1\\) : Sample Slope", "\\[E\\{Y_i\\} = \\beta_0 + \\beta_1X_i\\]", "\\(E\\{Y_i\\}\\) : True Line", "The “error” term."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Simple Linear Regression (Part 2)"
  },
  {
    "id": 256,
    "title": "📍 Skills Quiz - Mulitple Linear Regression",
    "url": "Paige-sNotes.html#skillsquizmulitplelinearregression",
    "content": "Skills Quiz - Mulitple Linear Regression As explained in the help file (?SaratogaHouses), the SaratogaHouses data set contains data about many houses from Saratoga County, New York in the year 2006. Suppose a family is in search of a home that was: newly constructed has three bedrooms trying to decide how big of a livingArea they can afford whether or not the price of the home is significantly impacted by adding a fireplace Use the SH2 data set (the filtered down version of the SaratogaHouses data set) and a “two-lines” multiple regression model to describe the price of a house according to the size of the livingArea of the house and whether or not the house has a fireplace (fireplaces is only 0 or 1 for the SH2 data). SH2 <- filter(SaratogaHouses, bedrooms == 3, newConstruction ==\"Yes\") The two-lines regression model for this situation would be most appropriately labeled as: \\[\\underbrace{Y_i}_\\text{Price} = \\beta_0 + \\beta_1 \\underbrace{X_{2i}}_\\text{livingArea} + \\beta_2 \\underbrace{X_{2i}}_\\text{fireplaces} + \\beta_3X_{1i}X_{2i}+\\epsilon_i\\] \\[X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Fireplace} \\\\ 0, & \\text{No Fireplace} \\end{array}\\right.\\] \\(\\beta_0\\) = The average price of a home with no fireplace and a living area of zero square feet. Since this is unrealistic, this parameter doesn’t actually carry any meaning for this particular regression model. \\(\\beta_1\\) = The change in the average price of a home without a fireplace as the living area increases by 1 additional square foot. \\(\\beta_2\\) = The difference in the average price of a home with a fireplace as compared to a home without a fireplace for homes with zero square feet of living area. \\(\\beta_3\\) = The change in the effect of 1 additional square foot in the living area on the average price of homes with a fireplace as compared to homes without a fireplace.",
    "sentences": ["As explained in the help file (?SaratogaHouses), the SaratogaHouses data set contains data about many houses from Saratoga County, New York in the year 2006.", "Suppose a family is in search of a home that was:", "newly constructed has three bedrooms trying to decide how big of a livingArea they can afford whether or not the price of the home is significantly impacted by adding a fireplace", "Use the SH2 data set (the filtered down version of the SaratogaHouses data set) and a “two-lines” multiple regression model to describe the price of a house according to the size of the livingArea of the house and whether or not the house has a fireplace (fireplaces is only 0 or 1 for the SH2 data).", "SH2 <- filter(SaratogaHouses, bedrooms == 3, newConstruction ==\"Yes\")", "The two-lines regression model for this situation would be most appropriately labeled as:", "\\[\\underbrace{Y_i}_\\text{Price} = \\beta_0 + \\beta_1 \\underbrace{X_{2i}}_\\text{livingArea} + \\beta_2 \\underbrace{X_{2i}}_\\text{fireplaces} + \\beta_3X_{1i}X_{2i}+\\epsilon_i\\]", "\\[X_{2i} = \\left\\{\\begin{array}{ll} 1, & \\text{Fireplace} \\\\ 0, & \\text{No Fireplace} \\end{array}\\right.\\]", "\\(\\beta_0\\) = The average price of a home with no fireplace and a living area of zero square feet.", "Since this is unrealistic, this parameter doesn’t actually carry any meaning for this particular regression model."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skills Quiz - Mulitple Linear Regression"
  },
  {
    "id": 257,
    "title": "📍 Assessment Quiz - Multiple Linear Regression",
    "url": "Paige-sNotes.html#assessmentquizmultiplelinearregression",
    "content": "Assessment Quiz - Multiple Linear Regression What is the estimate of \\(\\beta_1\\) in the regression model below? \\[\\underbrace{Y_i}_\\text{length of foot} = \\beta_0 + \\beta_1 \\underbrace{X_{1i}}_\\text{width fo foot} + \\beta_2 \\underbrace{X_{2i}}_\\text{0 if boy, 1 if girl} + \\beta_3X_{1i}X_{2i} + \\epsilon_i\\] KidsFeet.lm <- lm(length ~ width + sex + width:sex, data=KidsFeet) pander(summary(KidsFeet.lm)) Answer: 1.5423, the model above is based of the two lines model. Then, lookign at the width row, that represents \\(\\beta_1\\)/ slope, it will give us 1.5423. State the p-value of the hypothesis test in the regression model (both shown below). \\[\\underbrace{Y_i}_\\text{stopping distance} = \\beta_0 + \\beta_1 \\underbrace{X_{1i}}_\\text{speed of car} + \\beta_2 \\underbrace{X_{1i}^2}_\\text{(speed)^2} + \\epsilon_i \\text{ where } \\epsilon_i ~ N(0,\\sigma^2)\\] \\[H_0: \\beta_2 = 0\\] \\[H_a: \\beta_2 \\neq 0\\] lm.car <- lm(dist ~ speed + I(speed^2), data= cars) pander(summary(lm.car))",
    "sentences": ["What is the estimate of \\(\\beta_1\\) in the regression model below?", "\\[\\underbrace{Y_i}_\\text{length of foot} = \\beta_0 + \\beta_1 \\underbrace{X_{1i}}_\\text{width fo foot} + \\beta_2 \\underbrace{X_{2i}}_\\text{0 if boy, 1 if girl} + \\beta_3X_{1i}X_{2i} + \\epsilon_i\\]", "KidsFeet.lm <- lm(length ~ width + sex + width:sex, data=KidsFeet) pander(summary(KidsFeet.lm))", "Answer: 1.5423, the model above is based of the two lines model.", "Then, lookign at the width row, that represents \\(\\beta_1\\)/ slope, it will give us 1.5423.", "State the p-value of the hypothesis test in the regression model (both shown below).", "\\[\\underbrace{Y_i}_\\text{stopping distance} = \\beta_0 + \\beta_1 \\underbrace{X_{1i}}_\\text{speed of car} + \\beta_2 \\underbrace{X_{1i}^2}_\\text{(speed)^2} + \\epsilon_i \\text{ where } \\epsilon_i ~ N(0,\\sigma^2)\\]", "\\[H_0: \\beta_2 = 0\\]", "\\[H_a: \\beta_2 \\neq 0\\]", "lm.car <- lm(dist ~ speed + I(speed^2), data= cars) pander(summary(lm.car))"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assessment Quiz - Multiple Linear Regression"
  },
  {
    "id": 258,
    "title": "📍 Class Activity - Mulitple Linear Regression (Part 1)",
    "url": "Paige-sNotes.html#classactivitymulitplelinearregressionpart1",
    "content": "Class Activity - Mulitple Linear Regression (Part 1) Very wild west type of test! A space we can’t even depict on a graph Two lines model -> only Brother Saunders class will know whatcha mean \\[\\overbrace{Y_i}^\\text{Dependent Variable} = \\underbrace{\\beta_0}_\\text{Intercept} + \\underbrace{\\beta_1}_\\text{Slope} \\overbrace{X_i}^\\text{Independent Variable}+\\underbrace{\\beta_2}_\\text{Change in Intercept}X_{2i} + \\underbrace{\\beta_3}_\\text{Change in Slope}X_iX_{2i}+ \\underbrace{\\epsilon_i}_\\text{Errors Quantified (variation left over)}\\] intercept + slope -> visible (estimates) \\(\\beta_2\\) and beta 3 -> not visible, depicts change beta 3 = 0 -> parallel beta 2 = 0 -> same y intercept Important Elements of a Multiple Linear Regression Analysis: Use the mtcars dataset in R to reproduce the graphic shown below, or at least a graphic that shows the same information. Then perform a multiple linear regression that would give the equations for the two lines shown on the graphic. Check your work with your peers, and check off the items below that you are able to complete. Recreate the scatter plot of the mtcars dataset mylmcarz <-lm(mpg ~ qsec + am + qsec:am, data=mtcars) palette(c(\"dodgerblue\",\"red\")) plot(mpg ~ qsec, data=mtcars, col=as.factor(am), pch = 16) c <- coef(mylmcarz) curve(c[1] + c[2]*x, add=TRUE, col= \"dodgerblue\") curve(c[1]+c[3] + (c[2]+c[4])*x, add=TRUE, col=\"red\")",
    "sentences": ["Very wild west type of test!", "A space we can’t even depict on a graph Two lines model -> only Brother Saunders class will know whatcha mean", "\\[\\overbrace{Y_i}^\\text{Dependent Variable} = \\underbrace{\\beta_0}_\\text{Intercept} + \\underbrace{\\beta_1}_\\text{Slope} \\overbrace{X_i}^\\text{Independent Variable}+\\underbrace{\\beta_2}_\\text{Change in Intercept}X_{2i} + \\underbrace{\\beta_3}_\\text{Change in Slope}X_iX_{2i}+ \\underbrace{\\epsilon_i}_\\text{Errors Quantified (variation left over)}\\]", "intercept + slope -> visible (estimates) \\(\\beta_2\\) and beta 3 -> not visible, depicts change beta 3 = 0 -> parallel beta 2 = 0 -> same y intercept", "Important Elements of a Multiple Linear Regression Analysis:", "Use the mtcars dataset in R to reproduce the graphic shown below, or at least a graphic that shows the same information.", "Then perform a multiple linear regression that would give the equations for the two lines shown on the graphic.", "Check your work with your peers, and check off the items below that you are able to complete.", "Recreate the scatter plot of the mtcars dataset", "mylmcarz <-lm(mpg ~ qsec + am + qsec:am, data=mtcars) palette(c(\"dodgerblue\",\"red\")) plot(mpg ~ qsec, data=mtcars, col=as.factor(am), pch = 16) c <- coef(mylmcarz) curve(c[1] + c[2]*x, add=TRUE, col= \"dodgerblue\") curve(c[1]+c[3] + (c[2]+c[4])*x, add=TRUE, col=\"red\")"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Mulitple Linear Regression (Part 1)"
  },
  {
    "id": 259,
    "title": "📍 Class Activity - Mulitple Linear Regression (Part 2)",
    "url": "Paige-sNotes.html#classactivitymulitplelinearregressionpart2",
    "content": "Class Activity - Mulitple Linear Regression (Part 2) Consider the multiple linear regression model given by: \\[Y_i = \\beta_0X_{1i} + \\beta_2X_{2i} + \\beta_3X_{1i}X_{2i}+ \\epsilon_i \\] - \\(X_{2i}\\) = 1 : Group B \\(X_{2i}\\) = 0 : Group A The “two-lines” model is a simple way to compare two groups in statistics. It uses two main parts: A regular number (\\(X_{1i}\\)) that can be any value. A special number (\\(X_{2i}\\)) that’s either 0 or 1. This special number (also called a “dummy variable” or “indicator variable”) helps turn information about groups (like “Group A” or “Group B”) into numbers we can use in math. By using these two parts, we can create two separate lines on a graph: One line for Group A One line for Group B",
    "sentences": ["Consider the multiple linear regression model given by:", "\\[Y_i = \\beta_0X_{1i} + \\beta_2X_{2i} + \\beta_3X_{1i}X_{2i}+ \\epsilon_i \\] - \\(X_{2i}\\) = 1 : Group B", "\\(X_{2i}\\) = 0 : Group A", "The “two-lines” model is a simple way to compare two groups in statistics.", "It uses two main parts:", "A regular number (\\(X_{1i}\\)) that can be any value.", "A special number (\\(X_{2i}\\)) that’s either 0 or 1.", "This special number (also called a “dummy variable” or “indicator variable”) helps turn information about groups (like “Group A” or “Group B”) into numbers we can use in math.", "By using these two parts, we can create two separate lines on a graph:", "One line for Group A One line for Group B"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Mulitple Linear Regression (Part 2)"
  },
  {
    "id": 260,
    "title": "📍 Skill Quiz - Logisitic Regression",
    "url": "Paige-sNotes.html#skillquizlogisiticregression",
    "content": "Skill Quiz - Logisitic Regression What does \\(P(Y_i = 1|X_i)\\) represent in logistic regression? The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s). The symbol used to represent the notation in logistic regression is \\(\\pi_i\\) The mathematical model for simple logistic regression is given by: \\[P(Y_i = 1) = \\frac{e^{stuff}}{1+e^{stuff}}= \\pi_i\\] The content for “stuff” being a simple linear regression of the form \\(\\beta_0 + \\beta_1x_i\\) With some algebra, the simple logistic regression model can be reorganized as: \\[\\frac{\\pi_i}{1-\\pi_i}= e^{\\beta_0}e^{\\beta_1x_i}\\] We can assume that: \\(\\frac{\\pi_i}{1-\\pi_i}\\) is the odds of success, i.e. the odds that \\(Y_i = 1\\) The odds of success equal \\(e^{\\beta_0}\\) when \\(x_i = 0\\) The odds of success increase by the factor \\(e^{\\beta_1}\\) for every one unit increase in \\(x_i\\)",
    "sentences": ["What does \\(P(Y_i = 1|X_i)\\) represent in logistic regression?", "The probability of a success \\(Y_i = 1\\) given the information contained in the explanatory variable(s).", "The symbol used to represent the notation in logistic regression is \\(\\pi_i\\)", "The mathematical model for simple logistic regression is given by:", "\\[P(Y_i = 1) = \\frac{e^{stuff}}{1+e^{stuff}}= \\pi_i\\]", "The content for “stuff” being a simple linear regression of the form \\(\\beta_0 + \\beta_1x_i\\)", "With some algebra, the simple logistic regression model can be reorganized as:", "\\[\\frac{\\pi_i}{1-\\pi_i}= e^{\\beta_0}e^{\\beta_1x_i}\\]", "We can assume that:", "\\(\\frac{\\pi_i}{1-\\pi_i}\\) is the odds of success, i.e. the odds that \\(Y_i = 1\\) The odds of success equal \\(e^{\\beta_0}\\) when \\(x_i = 0\\) The odds of success increase by the factor \\(e^{\\beta_1}\\) for every one unit increase in \\(x_i\\)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skill Quiz - Logisitic Regression"
  },
  {
    "id": 261,
    "title": "📍 Assessment Quiz - Logistic Regression",
    "url": "Paige-sNotes.html#assessmentquizlogisticregression",
    "content": "Assessment Quiz - Logistic Regression Use an appropriate test to determine if the birth weight in ounces of a baby (wt) can be used to predict the probability that the mother smoked at all (smoke>0, or, if your Gestation data set has words in the “smoke” column, use smoke!=“never”) during or prior to the pregnancy. The graphic of the correct analysis is shown below. What are the values of b0 and b1 for the curve shown in the graphic below? Gestation.glm <- glm((smoke == \"never\")~wt, data=Gestation, family=binomial) pander(summary(Gestation.glm)) (Dispersion parameter for binomial family taken to be 1 ) Here is the output of a logistic regression performed in R. What is the predicted value for \\(P(Y_i = 1 | x_i = 25)\\)? There are TWO different ways you can get this answer: it would look like this: exp(-6.6035 + 0.307025) / (1 + exp(-6.6035 + 0.307025)) = 0.7448821",
    "sentences": ["Use an appropriate test to determine if the birth weight in ounces of a baby (wt) can be used to predict the probability that the mother smoked at all (smoke>0, or, if your Gestation data set has words in the “smoke” column, use smoke!=“never”) during or prior to the pregnancy.", "The graphic of the correct analysis is shown below.", "What are the values of b0 and b1 for the curve shown in the graphic below?", "Gestation.glm <- glm((smoke == \"never\")~wt, data=Gestation, family=binomial) pander(summary(Gestation.glm))", "(Dispersion parameter for binomial family taken to be 1 )", "Here is the output of a logistic regression performed in R.", "What is the predicted value for \\(P(Y_i = 1 | x_i = 25)\\)?", "There are TWO different ways you can get this answer:", "it would look like this:", "exp(-6.6035 + 0.307025) / (1 + exp(-6.6035 + 0.307025)) = 0.7448821"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assessment Quiz - Logistic Regression"
  },
  {
    "id": 262,
    "title": "📍 Class Activity -Simple Logistic Regression (Part 1)",
    "url": "Paige-sNotes.html#classactivitysimplelogisticregressionpart1",
    "content": "Class Activity -Simple Logistic Regression (Part 1) Main question template: Whats the probability that this thing happens, given **the things that are happening*?** What’s the probability that the length of the foot we have belongs to a girl or a boy? WILL THIS HAPPEN OR NOT BASED ON THIS KNOWN INFORMATION?? a yes or no question! Explain each symbol of the simple logistic regression model, and what it represents \\[P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i\\] \\(\\pi_i\\) = P (pi is a letter ) e = a number! \\(e^{\\beta_0 +\\beta_1X_i}\\) = serves as a bounds to go from 0 (impossibility) to 1 (certainty) e to stuff / 1 + e to stuff Explain how the simple logistic regression model is being used in the Challenger analysis. (What is the explanatory variable of the analysis, and what is it allowing us to predict?)",
    "sentences": ["Main question template:", "Whats the probability that this thing happens, given **the things that are happening*?**", "What’s the probability that the length of the foot we have belongs to a girl or a boy?", "WILL THIS HAPPEN OR NOT BASED ON THIS KNOWN INFORMATION??", "a yes or no question!", "Explain each symbol of the simple logistic regression model, and what it represents", "\\[P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i\\]", "\\(\\pi_i\\) = P (pi is a letter ) e = a number!", "\\(e^{\\beta_0 +\\beta_1X_i}\\) = serves as a bounds to go from 0 (impossibility) to 1 (certainty) e to stuff / 1 + e to stuff", "Explain how the simple logistic regression model is being used in the Challenger analysis."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity -Simple Logistic Regression (Part 1)"
  },
  {
    "id": 263,
    "title": "📍 Class Activity - Simple Logistic Regression (Part 2)",
    "url": "Paige-sNotes.html#classactivitysimplelogisticregressionpart2",
    "content": "Class Activity - Simple Logistic Regression (Part 2) \\[$log(odds) = \\beta_0 + \\beta_iX_i\\] the log of the odds are linear Probability = \\(\\frac{Successes}{Total}\\) doesn’t change in a consistent way What we graph! depicts to a “moment/time of decision” Odds = $ {Failure}$ “the times as likely to succeed” What we interpret! nothing to infinite! (never really graph the odds) multiplication likeliness, slope effects the odds! \\[\\frac{\\pi_i}{1 - \\pi_i} = e^{\\beta_0 + \\beta_1X_i}= \\underbrace{e^{\\beta_0}}_\\text{baseline odds}\\underbrace{(e^{\\beta_1})^{X_i}}}_\\text{multiplicative change to the odds}\\] When graphing, the BIG CHANGE IN THE CURVE LINE shows that you found something crazy and pretty cool! Recreate the graphic shown below in R using the KidsFeet dataset. Then answer the following questions!",
    "sentences": ["\\[$log(odds) = \\beta_0 + \\beta_iX_i\\]", "the log of the odds are linear", "Probability = \\(\\frac{Successes}{Total}\\) doesn’t change in a consistent way What we graph!", "depicts to a “moment/time of decision” Odds = $ {Failure}$ “the times as likely to succeed” What we interpret!", "nothing to infinite!", "(never really graph the odds) multiplication likeliness, slope effects the odds!", "\\[\\frac{\\pi_i}{1 - \\pi_i} = e^{\\beta_0 + \\beta_1X_i}= \\underbrace{e^{\\beta_0}}_\\text{baseline odds}\\underbrace{(e^{\\beta_1})^{X_i}}}_\\text{multiplicative change to the odds}\\]", "When graphing, the BIG CHANGE IN THE CURVE LINE shows that you found something crazy and pretty cool!", "Recreate the graphic shown below in R using the KidsFeet dataset.", "Then answer the following questions!"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Simple Logistic Regression (Part 2)"
  },
  {
    "id": 264,
    "title": "📍 Week 11 | Consulting Opportunity or Research Project",
    "url": "Paige-sNotes.html#week11consultingopportunityorresearchproject",
    "content": "Week 11 | Consulting Opportunity or Research Project Skill Quiz - Practice Final Exam Use an appropriate test and the starwars dataset in R to determine if, on average, the species of Wookiees, Gungans, or Kaminoans are taller. Class Activity - Presenting your Findings What are the five rubric categories of an analysis in this class? Hypothesis & Questions Analysis Graphical Summary Interpretation Presentation Why is each of these elements important in the analysis? How does a p-value help inform your conclusions for a particular analysis? Significant: Non significant: We can gamble on this, but there’s a sense that something is hiding here. There is a suspicious that that there is more here, but to find whats missing we can look deeper into this! What do I do when my data doesn’t pass the normality tests??? Transform it! There are several ways to transforming your y value by doing: log(yvalue) (y-value)^2 The way to check transformation corrected your distribution would be to do the code: hist(datasetname$yvalue) From there, you have to work with the transformed data in the testing space and then write your interpretation based off the non-transformed data. Suck it up! If you transformed it and it doesn’t create a normal distribution, no matter how much you change and check it, you just have to explain the following We have to interpret the results with CAUTION We CANNOT promise that our results are definitive",
    "sentences": ["Skill Quiz - Practice Final Exam Use an appropriate test and the starwars dataset in R to determine if, on average, the species of Wookiees, Gungans, or Kaminoans are taller.", "Class Activity - Presenting your Findings What are the five rubric categories of an analysis in this class?", "Hypothesis & Questions Analysis Graphical Summary Interpretation Presentation Why is each of these elements important in the analysis?", "How does a p-value help inform your conclusions for a particular analysis?", "Significant: Non significant: We can gamble on this, but there’s a sense that something is hiding here.", "There is a suspicious that that there is more here, but to find whats missing we can look deeper into this!", "What do I do when my data doesn’t pass the normality tests???", "Transform it!", "There are several ways to transforming your y value by doing: log(yvalue) (y-value)^2 The way to check transformation corrected your distribution would be to do the code: hist(datasetname$yvalue) From there, you have to work with the transformed data in the testing space and then write your interpretation based off the non-transformed data.", "Suck it up!"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Week 11 | Consulting Opportunity or Research Project"
  },
  {
    "id": 265,
    "title": "📍 Skill Quiz - Practice Final Exam",
    "url": "Paige-sNotes.html#skillquizpracticefinalexam",
    "content": "Skill Quiz - Practice Final Exam Use an appropriate test and the starwars dataset in R to determine if, on average, the species of Wookiees, Gungans, or Kaminoans are taller.",
    "sentences": "Use an appropriate test and the starwars dataset in R to determine if, on average, the species of Wookiees, Gungans, or Kaminoans are taller.",
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skill Quiz - Practice Final Exam"
  },
  {
    "id": 266,
    "title": "📍 Class Activity - Presenting your Findings",
    "url": "Paige-sNotes.html#classactivitypresentingyourfindings",
    "content": "Class Activity - Presenting your Findings What are the five rubric categories of an analysis in this class? Hypothesis & Questions Analysis Graphical Summary Interpretation Presentation Why is each of these elements important in the analysis? How does a p-value help inform your conclusions for a particular analysis? Significant: Non significant: We can gamble on this, but there’s a sense that something is hiding here. There is a suspicious that that there is more here, but to find whats missing we can look deeper into this! What do I do when my data doesn’t pass the normality tests??? There are several ways to transforming your y value by doing: log(yvalue) (y-value)^2 The way to check transformation corrected your distribution would be to do the code: hist(datasetname$yvalue) From there, you have to work with the transformed data in the testing space and then write your interpretation based off the non-transformed data. If you transformed it and it doesn’t create a normal distribution, no matter how much you change and check it, you just have to explain the following We have to interpret the results with CAUTION We CANNOT promise that our results are definitive",
    "sentences": ["What are the five rubric categories of an analysis in this class?", "Hypothesis & Questions Analysis Graphical Summary Interpretation Presentation", "Why is each of these elements important in the analysis?", "How does a p-value help inform your conclusions for a particular analysis?", "Significant: Non significant: We can gamble on this, but there’s a sense that something is hiding here.", "There is a suspicious that that there is more here, but to find whats missing we can look deeper into this!", "What do I do when my data doesn’t pass the normality tests???", "There are several ways to transforming your y value by doing: log(yvalue) (y-value)^2 The way to check transformation corrected your distribution would be to do the code: hist(datasetname$yvalue) From there, you have to work with the transformed data in the testing space and then write your interpretation based off the non-transformed data.", "If you transformed it and it doesn’t create a normal distribution, no matter how much you change and check it, you just have to explain the following We have to interpret the results with CAUTION We CANNOT promise that our results are definitive"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Presenting your Findings"
  },
  {
    "id": 267,
    "title": "📍 Week 12 | Chi Squared Test",
    "url": "Paige-sNotes.html#week12chisquaredtest",
    "content": "Week 12 | Chi Squared Test Skills Quiz - Chi Squared Test Using the Titanic data set, answer the following: Refers to the (Explanation file of the Chi Squared Tests)[file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/ChiSquaredTests.html] in the Statistics Notebook! What do the rows and columns factors called in the following table? Rows : Class Columns : Survival How would you state the null and alternative hypotheses for a chi squared test of independence? \\[H_0 : \\text{The row variable and column variable are independent.}\\] \\[H_a : \\text{The row variable and column variable are associated.}\\] The two things need to obtain a p-value for the chi-squared test of independence are: The Test Statistic \\[\\chi^2 = \\sum_{i=1}^m \\frac{(O_i - E_i)^2}{E_i}\\] The distribution of the test statistic that is caluclated under the assumption that the null hypothesis is true The \\(E_i\\) in the \\(X^2\\) test statistic formula are called the expected counts. They are obtained by: \\[E_i = \\frac{\\text{(row total)*(column total)}}{\\text{(total total)}}\\] These values show us what values we would expect to observe if the null hypothesis was true. In other words, they provide the counts we would expect if the row variable column variable were independent. The \\(X^2\\) test statistic can be assumed to follow a: chi-squared distribution with degrees of freedom : \\(p = (r-1)*(c-1)\\) The chi-squared distribution is a parametric distribution because it has a single parameter known as the degrees of freedom, p. Person Residuals are used for interpreting the results of a chi-squared test when the alternative hypothesis can be concluded to be the truth! They show a relative measurement of how much the observed counts differ from the expected counts. Use the HairEyeColor data set to answer the following questions: glasses <- cbind( Males = c(Glasses = 5, Contacts = 12, None = 18), Females = c(Glasses = 4, Contacts = 14, None = 22)) pander(glasses)   Males Females Glasses 5 4 Contacts 12 14 None 18 22 What can we interpret from these bar plots from the data? The gender of an individual does not seem to be associated with whether a person wears glasses, contacts, or no eye correction because the pattern of the bars is essentially the same for male and females. The data shows that the most common results for either males or females is to not wear glasses or contacts. It also shows that the least common results for both genders is to wear glasses. In other words, the pattern for both genders is the same. barplot(glasses, beside=TRUE, legend.text=TRUE, args.legend=list(x = \"topright\", cex=0.6, bty=\"n\"), xlim= c(0, 10)) What are the null and alternative hypotheses for this data? \\[H_0 : \\text{Corrective eye wearing and gender are independent.}\\] \\[H_a : \\text{Corrective eye wearing and gender are associated.}\\] Determine if a chi-squared test of independence is appropriate for the glasses data. Yes, the requirements are met because the average of the expected counts is greater than 5 and all expected counts are greater than 1, even though some expected counts are less than 5. chis.glasses <- chisq.test(glasses) ## Warning in chisq.test(glasses): Chi-squared ## approximation may be incorrect pander(chis.glasses$expected)   Males Females Glasses 4.2 4.8 Contacts 12.13 13.87 None 18.67 21.33 View the results of our chi-squared test of the data. pander(chis.glasses) Pearson’s Chi-squared test: glasses Test statistic df P value 0.3331 2 0.8466 What are the interpretations we can get from these results? This confirms our original suspicion that we saw in the graphic! There is insufficient evidence to conclude the corrective eye wearing gender are associated. We will continue to assume the null hypothesis that whether someone wears glasses, contacts, or no corrective eye wear is independent of their gender. What about the Pearson Residuals? Since we failed to reject the null there is no interpretation to make for these data so we are not interested in the Pearson Residuals. However, so that you get the opportunity to see what the Pearson Residuals look like when we fail to reject, run the following code in R and notice that none of the residuals stand out as being exceptionally large in magnitude. pander(chis.glasses$residuals)   Males Females Glasses 0.3904 -0.3651 Contacts -0.03828 0.03581 None -0.1543 0.1443 This is always the case when we fail to reject the null hypothesis in a chi-squared test of independence! Asia has become a major competitor with the U.S and Western Europe in education. The following table presents the counts of university degrees awarded to students in engineering and science (natural and social sciences) for the three regions. The data (observed values) are depicted below: education <- cbind( `United States` = c(Engineering = 61941, `Natural Science` = 111158, `Social Science` = 182166), `Western Europe` = c(Engineering = 158931, `Natural Science` = 140126, `Social Science` = 116353), Asia = c(280772, 242879, 236018)) pander(education)   United States Western Europe Asia Engineering 61941 158931 280772 Natural Science 111158 140126 242879 Social Science 182166 116353 236018 Questions to ask: Are there any differences in the numbers of degrees awarded to each field for the different regions? In other words, are field and region associated? Or can we assume that all three countries are similar in their patterns of degrees being awarded? These questions can be stated in a hypothesis: \\[H_0 : \\text{Field and Region are independent/ not associated.}\\] \\[H_a : \\text{Field and Region are not independent/ associated}\\] Conducting the Chi-squared test chi.ed <- chisq.test(education) chi.ed ## ## Pearson's Chi-squared test ## ## data: education ## X-squared = 69890, df = 4, p-value < 2.2e-16 This shows sufficient evidence to conclude that field and region are associated! Checking requirements: edexpect <- chi.ed$expected pander(edexpect)   United States Western Europe Asia Engineering 116455 136171 249018 Natural Science 114719 134140 245305 Social Science 124091 145099 265346 Create an appropriate graphic in R for this analysis: use ylim= and xlim to change the stretching use cex= in args.legend= function to shrink the size of the legend barplot(education, beside=TRUE, legend.text=TRUE, args.legend= list(x=\"topleft\", bty=\"n\",cex=0.59), main=\"College Degrees Awarded by Region\", ylim=c(0, 300000)) Obtain the residuals edresid <- chi.ed$residuals pander(edresid)   United States Western Europe Asia Engineering -159.7 61.68 63.63 Natural Science -10.51 16.34 -4.897 Social Science 164.9 -75.47 -56.94 Questions that are answered by residuals: Which region differs the most from expected when is comes to the number of Engineering degrees awarded? United States Which region differs the most from expected when it comes to the number of Social Science degrees awarded? United States Review time!! Consider the InsectSprays dataset in R. Test the hypothesis that the mean number of bugs killed by insecticides A, B, C, D, E, and F are all the same against the alternative that at least one mean is different. datatable(InsectSprays, options(list=c(3,10,30))) What hypothesis test should we use? bug.ova <- aov(count ~ spray, data = InsectSprays) summary(bug.ova) ## Df Sum Sq Mean Sq F value Pr(>F) ## spray 5 2669 533.8 34.7 <2e-16 *** ## Residuals 66 1015 15.4 ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 The p-value of the test is very small because the test statistic is 34.7 showing that there is sufficient evidence to conclude that at least one bug spray results in a different average number of bugs per agricultural experimental unit than the other sprays. What is the appropriateness of this test? par(mfrow=c(1,2)) plot(bug.ova, which=1:2, pch=16) - The appropriateness of the test is questionable because the residual plot shows that the constant (equal) variance is questionable (the first graph) and the Q-Q Plot suggests the errors are likely not normally distributed. Thus, we should not make any conclusion about these insect sprays until next week when we can use the non-parametric version of ANOVA called the “Kruskal-Wallis Test.” In other words, this test is inconclusive due to difficulties with the requirements not being satisfied.",
    "sentences": ["Skills Quiz - Chi Squared Test Using the Titanic data set, answer the following: Refers to the (Explanation file of the Chi Squared Tests)[file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/ChiSquaredTests.html] in the Statistics Notebook!", "What do the rows and columns factors called in the following table?", "Rows : Class Columns : Survival How would you state the null and alternative hypotheses for a chi squared test of independence?", "\\[H_0 : \\text{The row variable and column variable are independent.}\\] \\[H_a : \\text{The row variable and column variable are associated.}\\] The two things need to obtain a p-value for the chi-squared test of independence are: The Test Statistic \\[\\chi^2 = \\sum_{i=1}^m \\frac{(O_i - E_i)^2}{E_i}\\] The distribution of the test statistic that is caluclated under the assumption that the null hypothesis is true The \\(E_i\\) in the \\(X^2\\) test statistic formula are called the expected counts.", "They are obtained by: \\[E_i = \\frac{\\text{(row total)*(column total)}}{\\text{(total total)}}\\] These values show us what values we would expect to observe if the null hypothesis was true.", "In other words, they provide the counts we would expect if the row variable column variable were independent.", "The \\(X^2\\) test statistic can be assumed to follow a: chi-squared distribution with degrees of freedom : \\(p = (r-1)*(c-1)\\) The chi-squared distribution is a parametric distribution because it has a single parameter known as the degrees of freedom, p.", "Person Residuals are used for interpreting the results of a chi-squared test when the alternative hypothesis can be concluded to be the truth!", "They show a relative measurement of how much the observed counts differ from the expected counts.", "Use the HairEyeColor data set to answer the following questions: glasses <- cbind( Males = c(Glasses = 5, Contacts = 12, None = 18), Females = c(Glasses = 4, Contacts = 14, None = 22)) pander(glasses)   Males Females Glasses 5 4 Contacts 12 14 None 18 22 What can we interpret from these bar plots from the data?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Week 12 | Chi Squared Test"
  },
  {
    "id": 268,
    "title": "📍 Skills Quiz - Chi Squared Test",
    "url": "Paige-sNotes.html#skillsquizchisquaredtest",
    "content": "Skills Quiz - Chi Squared Test Using the Titanic data set, answer the following: Refers to the (Explanation file of the Chi Squared Tests)[file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/ChiSquaredTests.html] in the Statistics Notebook! What do the rows and columns factors called in the following table? Rows : Class Columns : Survival How would you state the null and alternative hypotheses for a chi squared test of independence? \\[H_0 : \\text{The row variable and column variable are independent.}\\] \\[H_a : \\text{The row variable and column variable are associated.}\\] The two things need to obtain a p-value for the chi-squared test of independence are: The Test Statistic \\[\\chi^2 = \\sum_{i=1}^m \\frac{(O_i - E_i)^2}{E_i}\\] The distribution of the test statistic that is caluclated under the assumption that the null hypothesis is true",
    "sentences": ["Using the Titanic data set, answer the following:", "Refers to the (Explanation file of the Chi Squared Tests)[file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/ChiSquaredTests.html] in the Statistics Notebook!", "What do the rows and columns factors called in the following table?", "Rows : Class Columns : Survival", "How would you state the null and alternative hypotheses for a chi squared test of independence?", "\\[H_0 : \\text{The row variable and column variable are independent.}\\]", "\\[H_a : \\text{The row variable and column variable are associated.}\\]", "The two things need to obtain a p-value for the chi-squared test of independence are: The Test Statistic", "\\[\\chi^2 = \\sum_{i=1}^m \\frac{(O_i - E_i)^2}{E_i}\\]", "The distribution of the test statistic that is caluclated under the assumption that the null hypothesis is true"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skills Quiz - Chi Squared Test"
  },
  {
    "id": 269,
    "title": "📍 Assessement Quiz - Chi Squared",
    "url": "Paige-sNotes.html#assessementquizchisquared",
    "content": "Assessement Quiz - Chi Squared Perform an appropriate test in R to determine if the dominant hand of children is related to the season of the year they are born in. Select the answer showing the P-value of your test. (Note that this test is not quite appropriate for these data. Due to the small sample sizes, the average expected count is only 4.875, but all expected counts are above 1.) First part: Turn the data into a table! Kids2 <- KidsFeet %>% mutate( season = case_when( birthmonth %in% c(12,1,2) ~ \"Winter\", birthmonth %in% c(3,4,5) ~ \"Spring\", birthmonth %in% c(6,7,8) ~ \"Summer\", birthmonth %in% c(9,10,11) ~ \"Fall\" ) ) kids.tab <- table(Kids2$domhand, Kids2$season) pander(kids.tab) Part 2: Use the chi squared test! chisq.test(kids.tab) Answer: P-value = 0.6943 When performing a chi-squared test, there are always two ways to display the barplot of the table of observed counts. Which of the plots listed below depicts the same set of observed counts that are depicted in this plot?",
    "sentences": ["Perform an appropriate test in R to determine if the dominant hand of children is related to the season of the year they are born in.", "Select the answer showing the P-value of your test.", "(Note that this test is not quite appropriate for these data.", "Due to the small sample sizes, the average expected count is only 4.875, but all expected counts are above 1.)", "First part: Turn the data into a table!", "Kids2 <- KidsFeet %>% mutate( season = case_when( birthmonth %in% c(12,1,2) ~ \"Winter\", birthmonth %in% c(3,4,5) ~ \"Spring\", birthmonth %in% c(6,7,8) ~ \"Summer\", birthmonth %in% c(9,10,11) ~ \"Fall\" ) ) kids.tab <- table(Kids2$domhand, Kids2$season) pander(kids.tab)", "Part 2: Use the chi squared test!", "chisq.test(kids.tab)", "Answer: P-value = 0.6943", "When performing a chi-squared test, there are always two ways to display the barplot of the table of observed counts."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Assessement Quiz - Chi Squared"
  },
  {
    "id": 270,
    "title": "📍 Class Activity - Chi Squared Test",
    "url": "Paige-sNotes.html#classactivitychisquaredtest",
    "content": "Class Activity - Chi Squared Test Study the example codes provided for the combine function c( … ) Answer the following: Explain to your neighbor what the following code does using the “row-bind” function: rbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) ) rbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) ) creates whats called a “vector” or a list/ set of values (has to be the same length) names the numbers as Good, Bruised, or Rotten, while assigning those assignments as vectors Apples and Oranges How do the results change if you use the “column-bind” function instead? cbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) ) cbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) ) Presents it in a different way! you can also put t([data set name]) to transpose the data set",
    "sentences": ["Study the example codes provided for the combine function c( … )", "Answer the following:", "Explain to your neighbor what the following code does using the “row-bind” function:", "rbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) )", "rbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) )", "creates whats called a “vector” or a list/ set of values (has to be the same length) names the numbers as Good, Bruised, or Rotten, while assigning those assignments as vectors Apples and Oranges", "How do the results change if you use the “column-bind” function instead?", "cbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) )", "cbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) )", "Presents it in a different way!"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Chi Squared Test"
  },
  {
    "id": 271,
    "title": "📍 Class Activity - Chi Squared Survey Results",
    "url": "Paige-sNotes.html#classactivitychisquaredsurveyresults",
    "content": "Class Activity - Chi Squared Survey Results Step 1: Fetching the data To use SAS data, use this code and change the pathing: gss2021 <- read_sas(“Data/gss2021.sas7bdat”, NULL) Pick the columns that you are interested in! Use GSS column title meaning Columns I thought were interesting: SPANKING : Favor of spanking to discipline child 1 - 4 1 : Strongly Agree 4: Strongly Disagree POLMURDR : Citizen questioned as a murder suspect 1 (Yes) or 2 (No) Step 2: Create a table Run the following codes: gss2021 <- read_sas(\"Data/gss2021.sas7bdat\", NULL) table(gss2021$SPANKING) table(gss2021$POLMURDR) table(gss2021$SPANKING,gss2021$POLMURDR)",
    "sentences": ["Step 1: Fetching the data", "To use SAS data, use this code and change the pathing:", "gss2021 <- read_sas(“Data/gss2021.sas7bdat”, NULL)", "Pick the columns that you are interested in!", "Use GSS column title meaning Columns I thought were interesting: SPANKING : Favor of spanking to discipline child 1 - 4 1 : Strongly Agree 4: Strongly Disagree POLMURDR : Citizen questioned as a murder suspect 1 (Yes) or 2 (No)", "Step 2: Create a table", "Run the following codes:", "gss2021 <- read_sas(\"Data/gss2021.sas7bdat\", NULL) table(gss2021$SPANKING)", "table(gss2021$POLMURDR)", "table(gss2021$SPANKING,gss2021$POLMURDR)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Chi Squared Survey Results"
  },
  {
    "id": 272,
    "title": "📍 Week 13 | Randomizing Testing",
    "url": "Paige-sNotes.html#week13randomizingtesting",
    "content": "Week 13 | Randomizing Testing Skills Quiz - Randomization Testing The idea behind permutation testing is that the null hypothesis can be reworded to state that “any pattern that has been witnessed in the sampled data is simple due to random chance .” Permutation tests can be applied to any hypothesis testing scenario in order to compute the p-value of the test in away that does not require any assumption s of the data! The most difficult part of any permutation test : figuring out how to permute the data! - which is performed differently for each hypothesis test Thus, being able to identify the hypothesis test from the stated hypotheses is an important skill you have hopefully started to develop. To see how you are doing with this skill, match the following null hypotheses with their appropriate test. Chi-Squared Test \\(H_0 : \\text{the two variables are not associated}\\) Wilcoxon Signed-Rank Test \\(H_0 : \\text{median of differences = 0}\\) Paired Samples t Test \\(H_0 : \\mu_d = 0\\) Wilcoxon Rank Sum Test \\(H_0 : difference in medians = 0\\) Independent Samples t Test \\(H_0 : \\mu_1 = \\mu_2\\) ANOVA \\(H_0 : \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4\\) set.seed(1140411) #allows us each to get the same set of random values sample1 <- rnorm(30, 69, 2.5) #get a random sample of n=30 normally distributed values with mu=69 and sigma=2.5 sample2 <- rnorm(30, 69, 2.5) #get another random sample of n=30 normally distributed values with mu=69 and sigma=2.5 theData <- data.frame(values = c(sample1,sample2), group = rep(c(1,2), each=30)) #load the random samples into a data set datatable(theData) boxplot(values ~ group, data = theData) Both sample 1 and sample 2 in the above code are samples of size n = 30 from a normal distribution with mean 69 and standard deviation 2.5. Thus they are each sample from the same distribution! The hypothesis would be : \\(H_0 : \\mu_1 = \\mu_2\\) for both of these data Suppose we just had theData dataset without knowledge of the population this data came from. The permutation test of the stated null hypothesis would then be coded in R as: myTest <- t.test(values ~ group, data = theData, mu =0) observedTestStat <- myTest$statistic observedTestStat N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N ) { permutedTest <- t.test(sample(values) ~ group, data = theData, mu = 0 permutedTestStats[i] <- permutedTest$statisitc } hist(permutedTestStats) abline(v=observedTestStat) sum(permutedTestStats >= observedTestStat)/N sum(permutedTestStats <= observedTestStat)/N myTest4 <- t.test(values ~ group, data = theData, mu =0) observedTestStat4 <- myTest4$statistic print(observedTestStat4) ## t ## -0.7304735 N <- 2000 permutedTestStats4 <- rep(NA, N) for(i in 1:N ){ permutedTest4 <- t.test(sample(values) ~ group, data = theData, mu = 0) permutedTestStats4[i] <- permutedTest4$statistic } hist(permutedTestStats4) abline(v=observedTestStat4) greatvalue <- sum(permutedTestStats4 >= observedTestStat4)/N print(greatvalue) ## [1] 0.765 lessvalue <- sum(permutedTestStats4 <= observedTestStat4)/N print(lessvalue) ## [1] 0.235 twovalue <- 2*sum(permutedTestStats4 <= observedTestStat4)/N print(twovalue) ## [1] 0.47 Assessment Quiz - Randomization Testing Class Activity - Permutation Testing (Part 1) How does the picture of Legos in your Permutation Tesing Overview page demonstrate the idea of permutation testing? This image represents: - clear structure/pattern - blocks were certainly organized and not sorted by random chance - suggests real pattern that is not random - if there is structure in the data, then “mixing up the data and dumping it out again” will show very different patterns from the original This image represents: - a pile of toy blocks in a random pattern - as if the toy blocks were put in a bag, shaken up, and dumped out - This is the idea of the permutation test - if the data was just random to begin with, then we should see a very similar pattern by “mixing up the data and dumping it out again.” How is the distribution of the test statistic created using permutation testing? The distribution is created by: Randomly shuffling (permuting) the data labels many times Calculating the test statistic for each permutation Building a distribution from all these permuted test statistics How does this distribution differ from a parametric distribution? Unlike parametric distributions (like normal or t-distributions) which follow theoretical mathematical formulas, permutation distributions: Are created directly from the actual data Don’t assume any particular shape or underlying distribution Better reflect the true sampling distribution for the specific dataset How is the p-value calculated from a permutation test? The p-value is calculated by: Finding how many permuted test statistics are as extreme or more extreme than the observed test statistic Dividing this count by the total number of permutations What is a for loop? What does it allow you to do? A for loop is a programming construct that: Allows you to repeat a set of instructions multiple times In permutation testing, it’s used to automate the process of repeatedly shuffling data and calculating test statistics Here are some R codes to implement in the premutation testing concepts described: Function to perform permutation test permutation_test <- function(data, labels, n_permutations = 1000) { observed_stat <- calculate_test_statistic(data, labels) Store permuted statistics perm_stats <- numeric(n_permutations) For loop to perform permutations for(i in 1:n_permutations) { ***Randomly shuffle labels*** shuffled_labels <- sample(labels) ***Calculate and store test statistic*** perm_stats[i] <- calculate_test_statistic(data, shuffled_labels) } Calculate p-value p_value <- mean(perm_stats >= observed_stat) Create distribution plot hist(perm_stats, main = “Permutation Distribution”, xlab = “Test Statistic”) abline(v = observed_stat, col = “red”) return(p_value) } This code implements the key concepts we see in the selection: Uses a for loop to repeat the permutation process Randomly shuffles the data labels Calculates test statistics for each permutation Creates a distribution from the permuted statistics Calculates the p-value by comparing observed vs permuted statistics",
    "sentences": ["Skills Quiz - Randomization Testing The idea behind permutation testing is that the null hypothesis can be reworded to state that “any pattern that has been witnessed in the sampled data is simple due to random chance .” Permutation tests can be applied to any hypothesis testing scenario in order to compute the p-value of the test in away that does not require any assumption s of the data!", "The most difficult part of any permutation test : figuring out how to permute the data!", "- which is performed differently for each hypothesis test Thus, being able to identify the hypothesis test from the stated hypotheses is an important skill you have hopefully started to develop.", "To see how you are doing with this skill, match the following null hypotheses with their appropriate test.", "Chi-Squared Test \\(H_0 : \\text{the two variables are not associated}\\) Wilcoxon Signed-Rank Test \\(H_0 : \\text{median of differences = 0}\\) Paired Samples t Test \\(H_0 : \\mu_d = 0\\) Wilcoxon Rank Sum Test \\(H_0 : difference in medians = 0\\) Independent Samples t Test \\(H_0 : \\mu_1 = \\mu_2\\) ANOVA \\(H_0 : \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4\\) set.seed(1140411) #allows us each to get the same set of random values sample1 <- rnorm(30, 69, 2.5) #get a random sample of n=30 normally distributed values with mu=69 and sigma=2.5 sample2 <- rnorm(30, 69, 2.5) #get another random sample of n=30 normally distributed values with mu=69 and sigma=2.5 theData <- data.frame(values = c(sample1,sample2), group = rep(c(1,2), each=30)) #load the random samples into a data set datatable(theData) boxplot(values ~ group, data = theData) Both sample 1 and sample 2 in the above code are samples of size n = 30 from a normal distribution with mean 69 and standard deviation 2.5.", "Thus they are each sample from the same distribution!", "The hypothesis would be : \\(H_0 : \\mu_1 = \\mu_2\\) for both of these data Suppose we just had theData dataset without knowledge of the population this data came from.", "The permutation test of the stated null hypothesis would then be coded in R as: myTest <- t.test(values ~ group, data = theData, mu =0) observedTestStat <- myTest$statistic observedTestStat N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N ) { permutedTest <- t.test(sample(values) ~ group, data = theData, mu = 0 permutedTestStats[i] <- permutedTest$statisitc } hist(permutedTestStats) abline(v=observedTestStat) sum(permutedTestStats >= observedTestStat)/N sum(permutedTestStats <= observedTestStat)/N myTest4 <- t.test(values ~ group, data = theData, mu =0) observedTestStat4 <- myTest4$statistic print(observedTestStat4) ## t ## -0.7304735 N <- 2000 permutedTestStats4 <- rep(NA, N) for(i in 1:N ){ permutedTest4 <- t.test(sample(values) ~ group, data = theData, mu = 0) permutedTestStats4[i] <- permutedTest4$statistic } hist(permutedTestStats4) abline(v=observedTestStat4) greatvalue <- sum(permutedTestStats4 >= observedTestStat4)/N print(greatvalue) ## [1] 0.765 lessvalue <- sum(permutedTestStats4 <= observedTestStat4)/N print(lessvalue) ## [1] 0.235 twovalue <- 2*sum(permutedTestStats4 <= observedTestStat4)/N print(twovalue) ## [1] 0.47", "Assessment Quiz - Randomization Testing", "Class Activity - Permutation Testing (Part 1) How does the picture of Legos in your Permutation Tesing Overview page demonstrate the idea of permutation testing?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Week 13 | Randomizing Testing"
  },
  {
    "id": 273,
    "title": "📍 Skills Quiz - Randomization Testing",
    "url": "Paige-sNotes.html#skillsquizrandomizationtesting",
    "content": "Skills Quiz - Randomization Testing The idea behind permutation testing is that the null hypothesis can be reworded to state that “any pattern that has been witnessed in the sampled data is simple due to random chance .” Permutation tests can be applied to any hypothesis testing scenario in order to compute the p-value of the test in away that does not require any assumption s of the data! The most difficult part of any permutation test : figuring out how to permute the data! - which is performed differently for each hypothesis test Thus, being able to identify the hypothesis test from the stated hypotheses is an important skill you have hopefully started to develop. To see how you are doing with this skill, match the following null hypotheses with their appropriate test. Chi-Squared Test \\(H_0 : \\text{the two variables are not associated}\\) Wilcoxon Signed-Rank Test \\(H_0 : \\text{median of differences = 0}\\)",
    "sentences": ["The idea behind permutation testing is that the null hypothesis can be reworded to state that “any pattern that has been witnessed in the sampled data is simple due to random chance .”", "Permutation tests can be applied to any hypothesis testing scenario in order to compute the p-value of the test in away that does not require any assumption s of the data!", "The most difficult part of any permutation test : figuring out how to permute the data!", "- which is performed differently for each hypothesis test", "Thus, being able to identify the hypothesis test from the stated hypotheses is an important skill you have hopefully started to develop.", "To see how you are doing with this skill, match the following null hypotheses with their appropriate test.", "Chi-Squared Test", "\\(H_0 : \\text{the two variables are not associated}\\)", "Wilcoxon Signed-Rank Test", "\\(H_0 : \\text{median of differences = 0}\\)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Skills Quiz - Randomization Testing"
  },
  {
    "id": 274,
    "title": "📍 Class Activity - Permutation Testing (Part 1)",
    "url": "Paige-sNotes.html#classactivitypermutationtestingpart1",
    "content": "Class Activity - Permutation Testing (Part 1) How does the picture of Legos in your Permutation Tesing Overview page demonstrate the idea of permutation testing? This image represents: - clear structure/pattern - blocks were certainly organized and not sorted by random chance - suggests real pattern that is not random - if there is structure in the data, then “mixing up the data and dumping it out again” will show very different patterns from the original This image represents: - a pile of toy blocks in a random pattern - as if the toy blocks were put in a bag, shaken up, and dumped out - This is the idea of the permutation test - if the data was just random to begin with, then we should see a very similar pattern by “mixing up the data and dumping it out again.” How is the distribution of the test statistic created using permutation testing? The distribution is created by: Randomly shuffling (permuting) the data labels many times Calculating the test statistic for each permutation Building a distribution from all these permuted test statistics How does this distribution differ from a parametric distribution? Unlike parametric distributions (like normal or t-distributions) which follow theoretical mathematical formulas, permutation distributions: Are created directly from the actual data Don’t assume any particular shape or underlying distribution Better reflect the true sampling distribution for the specific dataset How is the p-value calculated from a permutation test? The p-value is calculated by:",
    "sentences": ["How does the picture of Legos in your Permutation Tesing Overview page demonstrate the idea of permutation testing?", "This image represents: - clear structure/pattern - blocks were certainly organized and not sorted by random chance - suggests real pattern that is not random - if there is structure in the data, then “mixing up the data and dumping it out again” will show very different patterns from the original This image represents: - a pile of toy blocks in a random pattern - as if the toy blocks were put in a bag, shaken up, and dumped out - This is the idea of the permutation test - if the data was just random to begin with, then we should see a very similar pattern by “mixing up the data and dumping it out again.”", "How is the distribution of the test statistic created using permutation testing?", "The distribution is created by:", "Randomly shuffling (permuting) the data labels many times Calculating the test statistic for each permutation Building a distribution from all these permuted test statistics", "How does this distribution differ from a parametric distribution?", "Unlike parametric distributions (like normal or t-distributions) which follow theoretical mathematical formulas, permutation distributions:", "Are created directly from the actual data Don’t assume any particular shape or underlying distribution Better reflect the true sampling distribution for the specific dataset", "How is the p-value calculated from a permutation test?", "The p-value is calculated by:"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Permutation Testing (Part 1)"
  },
  {
    "id": 275,
    "title": "📍 Class Activity - Premutation Testing (Part 2)",
    "url": "Paige-sNotes.html#classactivitypremutationtestingpart2",
    "content": "Class Activity - Premutation Testing (Part 2) Perform an independent two-sample t test using a permutation test. Use the mtcars dataset and test whether the average weight of the four cylinder cars differs from the average weight of eight cylinder cars. Use the following psuedo code as a starting spot. Step 1 myTest <- …perform the initial test… observedTestStat <- …get the test statistic… Step 2 N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- …perform test with permutedData… permutedTestStats[i] <- …get test statistic… } hist(permutedTestStats) abline(v=observedTestStat) Step 3 sum(permutedTestStats >= observedTestStat)/N sum(permutedTestStats <= observedTestStat)/N vroom <- mtcars %>% dplyr::select(cyl, wt) %>% dplyr::filter(cyl %in% c(4, 8)) #Step 1 myTest <- t.test(wt ~ cyl, data = vroom, mu = 0) observedTestStat <- myTest$statistic #Step 2 N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- t.test(sample(wt) ~ cyl, data = vroom, mu=0) permutedTestStats[i] <- permutedTest$statistic } hist(permutedTestStats, main= \"Permutation Test Distribution\", xlab = \"Test Statistic\") abline(v=observedTestStat, col= \"red\", lwd =2) #Step 3 grea <- sum(permutedTestStats >= observedTestStat)/N sum(permutedTestStats <= observedTestStat)/N print(observedTestStat) The observed test statistic (observedTestStat) for the previous test is [answer]. (round to two decimal places) : -6.44",
    "sentences": ["Perform an independent two-sample t test using a permutation test.", "Use the mtcars dataset and test whether the average weight of the four cylinder cars differs from the average weight of eight cylinder cars.", "Use the following psuedo code as a starting spot.", "Step 1 myTest <- …perform the initial test… observedTestStat <- …get the test statistic…", "Step 2 N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- …perform test with permutedData… permutedTestStats[i] <- …get test statistic… } hist(permutedTestStats) abline(v=observedTestStat)", "Step 3 sum(permutedTestStats >= observedTestStat)/N sum(permutedTestStats <= observedTestStat)/N", "vroom <- mtcars %>% dplyr::select(cyl, wt) %>% dplyr::filter(cyl %in% c(4, 8)) #Step 1 myTest <- t.test(wt ~ cyl, data = vroom, mu = 0) observedTestStat <- myTest$statistic #Step 2 N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- t.test(sample(wt) ~ cyl, data = vroom, mu=0) permutedTestStats[i] <- permutedTest$statistic } hist(permutedTestStats, main= \"Permutation Test Distribution\", xlab = \"Test Statistic\") abline(v=observedTestStat, col= \"red\", lwd =2)", "#Step 3 grea <- sum(permutedTestStats >= observedTestStat)/N sum(permutedTestStats <= observedTestStat)/N", "print(observedTestStat)", "The observed test statistic (observedTestStat) for the previous test is [answer]."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Class Activity - Premutation Testing (Part 2)"
  },
  {
    "id": 276,
    "title": "📍 Favorite Visuals/ Tips",
    "url": "Paige-sNotes.html#favoritevisualstips",
    "content": "Favorite Visuals/ Tips Analysis Tips Visual making questions Ask yourself the following questions when making a visual: Does the title present new information or does it restate what the axis state? Is there any information that should or could be explained? Do the colors/ style add to the presentation of the information? Is it accessible? (do you need to clarify how to use the graphics?) Most importantly… did you spell everything right?[^1] # Using ```{r, eval=FALSE} turns off the chunk, but still shows it. # Useful when you want to remember code, but not run it in this file. Background Tips Don’t try to write up a ton of stuff, try to get to the point! More words \\(\\neq\\) “the better” Less is more, have them explore! use “tabset” commands Cool Graphics Histograms Great for showing the distribution of data for a single quantitative variable when the sample size is large. Dotplots are a good alternative for smaller sample sizes. Gives a good feel for the mean and standard deviation of the data. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. cars “cars” is a dataset. Type “View(cars)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=dist “x=” declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_histogram( The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used. binwidth=5,  The “binwidth” command controls the width of the bars in the histogram. fill=“firebrick1”, The “fill” command controls the color of the insides of each bar. color=“yellow” The “color” command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function.  +  The addition symbol + is used to add further elements to the ggplot.    ggtitle( The “ggtitle(” function is used to add a title to the plot. “Races of Lightning McQueen”,  This is the cool sick title of the graph, always make it interesting and not re-explaining the axis labels.  +  The addition symbol + is used to add further elements to the ggplot. xlab(“Speed (mph)”)  The “xlab(” command allows you name the x axis.  +  The addition symbol + is used to add further elements to the ggplot. ylab(“Distance”) The “ylab(” command allows you to label the y axis.  +  The addition symbol + is used to add further elements to the ggplot. theme(   The “theme(” function allows us to manipulate the title, axis labels, and tick size. plot.title=   The “plot.title=” allows us to manipulate the title in a various amount of ways. size=20,color=“dodgerblue”,face=“bold”,family=“serif”),  These commands allow us to manipulate size, color, bold/italic, or font of the title. axis.title.x =   The “axis.title.x =” allows us to modify the font size of the axis title (change the x to a y and it changes it for the y axis). element_text(size=16),  The “element_text(size=” allows us to modify the font size. axis.text.x =  The “axis.text.x =” allows us to modify the size of the values/labels along the x axis. element_text(size=12),  The “element_text(size=” allows us to modify the font size. axis.title.y =   The “axis.title.y =” allows us to modify the font size of the axis title. element_text(size=16),  The “element_text(size=” allows us to modify the font size. axis.text.y =  The “axis.text.y =” allows us to modify the size of the values/labels along the y axis. element_text(size=12)  The “element_text(size=” allows us to modify the font size. )Closing parenthsis for the labs function.     Press Enter to run the code.  …  Click to View Output. Scatter Plots Depicts the actual values of the data points, which are \\((x,y)\\) pairs. Works well for small or large sample sizes. Visualizes well the correlation between the two variables. Should be used in linear regression contexts whenever possible. Rent <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/Rent.csv\") Rent_filtered <- Rent %>% mutate(`Walking Minutes to the MC`= round((CrowFlyMetersToMC/1609)*16.67,0))%>% dplyr::select(Gender,Name,Address,`Walking Minutes to the MC`,Residents,AvgFloorPlanCost, Latitude, Longitude)%>% mutate(`Monthly Floor Plan Cost`=round(AvgFloorPlanCost/3.5,2))%>% filter(Gender == \"F\")%>% filter(`Monthly Floor Plan Cost` < 300)%>% rename(`Semester Floor Plan Cost` = AvgFloorPlanCost)%>% rename(`Apartment Complex`= Name)%>% arrange(`Monthly Floor Plan Cost`) plot_ly( the “plot_ly” function creates an interactive scatter plot Rent_filtered, the filtered “Rent” data set x=~Monthly Floor Plan Cost, the x variable y= ~Residents, the y variable type = ‘scatter’, specifies the type of plot mode = ‘markers’, defines the drawing mode for the scatter plot markers = list(size = 15), customizes the appearance of the markers color = ~ Monthly Floor Plan Cost, Determines the color of each marker based on a variable colors = ‘Blues’, specifies the color palette to use for the markers text = ~ paste ( defines the test that appears when you hover over the marker Apartment Complex, shows the name of the apartment complex ”  n”, adds a line break/indent (there is a backward slash in front of the n) “$”, displays the dollar sign Monthly Floor Plan Cost)) shows the monthly floor plan cost of each apartment complex when you hover over the marker %>% the pipe operator layout( modifies the layout and appearance of the plot adding titles, annotaitons, and other layout options! title = “Popular BYU-Idaho Female Student Housing”, sets the main title of the plot annotations = list( adds annotations to the plot list(x=0.7,y=1.03, plots where the annotation shows up text = “(Light Blue - Cheap, Dark Blue - Expensive)”, The annotation itself! showarrow = FALSE, xref= ‘paper’, xanchor = ‘center’, yanchor = ‘auto’, I don’t know tbh font = list( allows the customization of the x and y axis titles size = 10, color = “gray”))), changes the size and color of the x and y axis titles xaxis = list( allows you to state the desired title of the x axis title = “Rent Cost (USD)”), the title of the x axis yaxis = list( allows you to state the desired title of the y axis title = “Residents”)) the title of the y axis      Press Enter to run the code.  …  Click to View Output. plot_ly(Rent_filtered, x= ~`Monthly Floor Plan Cost`, y= ~Residents, type = 'scatter', mode = 'markers', markers = list(size = 15), color = ~`Monthly Floor Plan Cost`, colors = 'Blues', text = ~paste(`Apartment Complex`,\"\\n\",\"$\",`Monthly Floor Plan Cost`))%>% layout (title = \"Popular BYU-Idaho Female Student Housing\",annotations =list(list(x=0.7, y= 1.03, text = \"(Light Blue - Cheap, Dark Blue - Expensive)\", showarrow = FALSE, xref= 'paper', yref = 'paper', xanchor = 'center', yanchor = 'auto', font = list(size = 10, color = \"gray\"))), xaxis = list(title = \"Rent Cost (USD)\"), yaxis = list(title = \"Residents\")) palette(c(\"skyblue\",\"firebrick\")) plot(mpg ~ qsec, data=mtcars, col=as.factor(am), pch=16, xlab=\"Quarter Mile Time (seconds)\", ylab=\"Miles per Gallon\", main=\"1974 Motor Trend Vehicles\") legend(\"topright\", pch=16, legend=c(\"automatic\",\"manual\"), title=\"Transmission\", bty='n', col=palette()) ggplot to plot_ly example[^2] ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams) Bar Charts Depicts the number of occurrances for each category, or level, of the qualitative variable. Similar to a histogram, but there is no natural way to order the bars. Thus the white-space between each bar. It is called a Pareto chart if the bars are ordered from tallest to shortest. Clustered and stacked bar charts are often used to display information for two qualitative variables simultaneously. myplot <- ggplot(starwars, aes(x=hair_color, fill=gender)) + geom_bar() ggplotly(myplot) Boxplots Graphical depiction of the five-number summary. Great for comparing the distributions of data across several groups or categories. Provides a quick visual understanding of the location of the median as well as the range of the data. Can be useful in showing outliers. Sample size should be larger than at least five, or computing the five-number summary is not very meaningful. Side-by-side dotplots are a good alternative for smaller sample sizes. plot_ly(KidsFeet, y=~length, x=~as.factor(sex),type=\"box\",color=~sex, colors=c(\"lightblue\",\"pink\")) %>% layout( title=\"4th Grader's Average Foot Size\", xaxis=list(title=\"Gender\"),yaxis=list(title=\"Length (cm)\")) food <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv\") ## New names: ## Rows: 125 Columns: 59 ## ── Column specification ## ────────────────────────── Delimiter: \",\" chr ## (19): GPA, weight, gender, breakfast, comf... dbl ## (40): calories_chicken, calories_day, comf... ## ℹ Use `spec()` to retrieve the full column ## specification for this data. ℹ Specify the column ## types or set `show_col_types = FALSE` to quiet ## this message. ## • `comfort_food_reasons_coded` -> ## `comfort_food_reasons_coded...9` ## • `comfort_food_reasons_coded` -> ## `comfort_food_reasons_coded...11` Foood <- food %>% dplyr::select(weight, breakfast, fries, drink, soup) %>% dplyr::filter(!is.na(suppressWarnings(as.numeric(weight))) + is.na(drink)) %>% mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) %>% mutate(weight = as.numeric(weight)) boxplot(weight ~ `Food Perception Score`, data=Foood, col=c(\"darkseagreen1\",\"palegreen\",\"palegreen3\",\"palegreen4\",\"mediumseagreen\"), xlab=\"Food Perception Score\", main=\"Weight of College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=Foood, pch=16,vertical=TRUE, add=TRUE, col=\"gray28\") Dot Plots Depicts the actual values of each data point. Best for small sample sizes or for datasets where there are lots of repeated values. Histograms or boxplots are better alternatives for large sample sizes when there are few repeated values. Great for comparing the distribution of data across several groups or categories. ggplot(KidsFeet, aes(x=factor(sex),y=length, fill=sex)) + coord_flip() + geom_dotplot(binaxis=\"y\", stackdir=\"up\", position=\"dodge\", dotsize=0.45, binwidth=0.5) + scale_fill_manual(values=c(\"B\"=\"lightblue\",\"G\"=\"pink\")) + labs(title= \"Kennedy 4th Grader's Foot Size\", x=\"Length (cm)\", y=\"Gender\") Tables Always remember to pipe in a pander when you can! datatable(Rent_filtered, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\")   \\(H_0\\) True \\(H_0\\) False Reject \\(H_0\\) Type I Error Correct Decision Accept \\(H_0\\) Correct Decision Type II Error favstats(height~eye_color, data=starwars)%>% pander() eye_color min Q1 median Q3 max mean sd n missing black 122 173 188 206 229 185 31.78 9 1 blue 150 170 180 186 234 182.2 20.32 19 0 blue-gray 182 182 182 182 182 182 NA 1 0 brown 66 164 183 185 193 167.3 34.11 19 2 dark NA NA NA NA NA NA NA 0 1 gold 191 191 191 191 191 191 NA 1 0 green, yellow 216 216 216 216 216 216 NA 1 0 hazel 170 172 174 176 178 174 5.657 2 1 orange 112 172 184 198.5 224 180.5 33.53 8 0 pink 180 180 180 180 180 180 NA 1 0 red 96 97 190 191 200 154.8 53.36 5 0 red, blue 96 96 96 96 96 96 NA 1 0 unknown 79 107.5 136 164.5 193 136 80.61 2 1 white 178 178 178 178 178 178 NA 1 0 yellow 94 167.5 175 198 264 177.8 42.22 11 0 HSS <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/HighSchoolSeniors.csv\") HSS.GJ <- dplyr::select(HSS, c(Reaction_time,Outdoor_Activities_Hours,Video_Games_Hours)) %>% filter_all(all_vars(!is.na(.))) %>% filter(Outdoor_Activities_Hours != Video_Games_Hours) %>% filter(abs(Outdoor_Activities_Hours - Video_Games_Hours) >= 10) %>% filter(Reaction_time > 0.1 & Reaction_time < 0.4) %>% mutate(More_Activity = if_else(Video_Games_Hours > Outdoor_Activities_Hours, \"Gamer\", \"Jock\")) %>% rename(`Reaction Time (Seconds)` = Reaction_time) %>% rename(`Time Spent Outside (Hours)` = Outdoor_Activities_Hours) %>% rename(`Time Spent Playing Video Games (Hours)` = Video_Games_Hours) %>% rename(`Gamer or Jock?` = More_Activity) HSS.G <- dplyr::select(HSS.GJ, c(`Reaction Time (Seconds)`,`Gamer or Jock?`)) %>% filter(`Gamer or Jock?` == \"Gamer\") HSS.J <- dplyr::select(HSS.GJ, c(`Reaction Time (Seconds)`,`Gamer or Jock?`)) %>% filter(`Gamer or Jock?` == \"Jock\") datatable(HSS.GJ, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") We added a classification column in this one! It’s pretty cool!! datatable(Foood, options=list(pageLength = 10)) %>% formatStyle('Food Perception Score', backgroundColor = 'lightgreen') kable function is pretty cool tooooooo!! questions <- c(\"Which one of these pictures do you associate the word 'breakfast'?\", \"Which picture do you associate with the word 'drink'?\", \"Which of these pictures you associate with word 'fries'?\", \"Which of the two pictures you associate with the word 'soup'?\") foodoptions <- c(\"Cereal / Donut\", \"1- Orange Juice / 2- Soda\", \"1- McDonald's Fries / 2- Home Fries\", \"1- Veggie Soup / 2- Creamy Soup\") collegestusurvey <- data.frame( Question = questions, Options = foodoptions, stringsAsFactors = FALSE) kable(collegestusurvey, col.names = c(\"Questions\", \"Food Options\"), caption =\"Food Survey\") Food Survey Questions Food Options Which one of these pictures do you associate the word ‘breakfast’? Cereal / Donut Which picture do you associate with the word ‘drink’? 1- Orange Juice / 2- Soda Which of these pictures you associate with word ‘fries’? 1- McDonald’s Fries / 2- Home Fries Which of the two pictures you associate with the word ‘soup’? 1- Veggie Soup / 2- Creamy Soup Foodie <- favstats(weight ~ `Food Perception Score`, data = Foood)[,-10] Foodie2 <- as.data.frame(Foodie) kable(Foodie2) Food Perception Score min Q1 median Q3 max mean sd n 0 129 137.50 140.0 175.0 187 154.4286 24.31637 7 1 110 125.00 145.0 169.0 210 146.5946 25.90952 37 2 100 138.00 160.0 175.0 265 160.1887 28.97881 53 3 105 137.50 175.0 198.5 264 176.3333 48.17330 15 4 130 151.25 167.5 187.5 200 167.5000 26.78619 6 Maps Helps to show where things are in reference to something! mc_icon <- makeAwesomeIcon(icon= \"university\", iconColor = \"white\", markerColor = \"lightblue\", library = \"fa\") leaflet(data = Rent_filtered)%>% addTiles() %>% setView(lng=-111.7833595717528, lat=43.82217013118815,zoom =14.511)%>% addAwesomeMarkers(lng=-111.7827756599601, lat=43.81821783971177, icon= mc_icon, popup= \"Manwarding Center(MC)\")%>% # addMarkers(lng=-111.7826513, lat= 43.82271564, popup= \"HILL'S COLLEGE AVE APTS<br>5 Min. Walk\")%>% # addMarkers(lng=-111.7876455, lat= 43.82246794, popup= \"PINES, WOMEN<br>6 Min. Walk\")%>% # addMarkers(lng=-111.7882195, lat=43.81930189, popup= \"DAVENPORT APARTMENTS<br>5 Min. Walk\")%>% # addMarkers(lng= -111.7754656, lat=43.8174285, popup= \"BUENA VISTA<br>6 Min. Walk\")%>% # addMarkers(lng=-111.7805055, lat=43.82015341, popup= \"RIVIERA APARTMENTS<br>3 Min. Walk\")%>% # addMarkers(lng=-111.7798457, lat=43.82065271, popup= \"BAYSIDE MANOR<br>3 Min. Walk\")%>% # addMarkers(lng=-111.7890135, lat=43.81866711, popup= \"BROOKLYN APARTMENTS<br>5 Min. Walk\")%>% # addMarkers(lng=-111.7871091, lat=43.82469723, popup= \"COTTONWOOD - WOMEN<br>8 Min. Walk\")%>% # addMarkers(lng=-111.7806664, lat=43.82124102, popup= \"CRESTWOOD COTTAGE<br>4 Min. Walk\")%>% # addMarkers(lng=-111.7877153, lat=43.81913933, popup= \"ROYAL CREST<br>4 Min. Walk\")%>% # addMarkers(lng=-111.7792019, lat=43.82347422, popup= \"BLUE DOOR, WOMEN<br>6 Min. Walk\")%>% # addMarkers(lng=-111.7883, lat=43.8200102, popup= \"SUNSET HALL<br>5 Min. Walk\")%>% addMarkers(lng=~Longitude, lat=~Latitude, popup=~paste(`Apartment Complex`,\"<br>\",\"Walk Mins.\",`Walking Minutes to the MC`)) %>% addLegend(position=\"topright\", colors= c(\"lightblue\",\"cornflowerblue\"), labels = c(\"Manwarding Center (MC)\",\"Apartment Complexes\"), title= \"Building Locations\")%>% addControl(\"<strong>Click the marker to see additional information<\/strong>\",position = \"topright\", className = \"map-caption\")",
    "sentences": ["Analysis Tips Visual making questions Ask yourself the following questions when making a visual: Does the title present new information or does it restate what the axis state?", "Is there any information that should or could be explained?", "Do the colors/ style add to the presentation of the information?", "Is it accessible?", "(do you need to clarify how to use the graphics?) Most importantly… did you spell everything right?[^1] # Using ```{r, eval=FALSE} turns off the chunk, but still shows it.", "# Useful when you want to remember code, but not run it in this file.", "Background Tips Don’t try to write up a ton of stuff, try to get to the point!", "More words \\(\\neq\\) “the better” Less is more, have them explore!", "use “tabset” commands", "Cool Graphics Histograms Great for showing the distribution of data for a single quantitative variable when the sample size is large."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Favorite Visuals/ Tips"
  },
  {
    "id": 277,
    "title": "📍 Analysis Tips",
    "url": "Paige-sNotes.html#analysistips",
    "content": "Analysis Tips Visual making questions Ask yourself the following questions when making a visual: Does the title present new information or does it restate what the axis state? Is there any information that should or could be explained? Do the colors/ style add to the presentation of the information? Is it accessible? (do you need to clarify how to use the graphics?) Most importantly… did you spell everything right?[^1] # Using ```{r, eval=FALSE} turns off the chunk, but still shows it. # Useful when you want to remember code, but not run it in this file. Background Tips Don’t try to write up a ton of stuff, try to get to the point! More words \\(\\neq\\) “the better” Less is more, have them explore! use “tabset” commands",
    "sentences": ["Visual making questions Ask yourself the following questions when making a visual: Does the title present new information or does it restate what the axis state?", "Is there any information that should or could be explained?", "Do the colors/ style add to the presentation of the information?", "Is it accessible?", "(do you need to clarify how to use the graphics?) Most importantly… did you spell everything right?[^1] # Using ```{r, eval=FALSE} turns off the chunk, but still shows it.", "# Useful when you want to remember code, but not run it in this file.", "Background Tips Don’t try to write up a ton of stuff, try to get to the point!", "More words \\(\\neq\\) “the better” Less is more, have them explore!", "use “tabset” commands"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Analysis Tips"
  },
  {
    "id": 278,
    "title": "📍 Visual making questions",
    "url": "Paige-sNotes.html#visualmakingquestions",
    "content": "Visual making questions Ask yourself the following questions when making a visual: Does the title present new information or does it restate what the axis state? Is there any information that should or could be explained? Do the colors/ style add to the presentation of the information? Is it accessible? (do you need to clarify how to use the graphics?) Most importantly… did you spell everything right?[^1] # Using ```{r, eval=FALSE} turns off the chunk, but still shows it. # Useful when you want to remember code, but not run it in this file.",
    "sentences": ["Ask yourself the following questions when making a visual:", "Does the title present new information or does it restate what the axis state?", "Is there any information that should or could be explained?", "Do the colors/ style add to the presentation of the information?", "Is it accessible?", "(do you need to clarify how to use the graphics?) Most importantly… did you spell everything right?[^1]", "# Using ```{r, eval=FALSE} turns off the chunk, but still shows it.", "# Useful when you want to remember code, but not run it in this file."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Visual making questions"
  },
  {
    "id": 279,
    "title": "📍 Background Tips",
    "url": "Paige-sNotes.html#backgroundtips",
    "content": "Background Tips Don’t try to write up a ton of stuff, try to get to the point! More words \\(\\neq\\) “the better” Less is more, have them explore! use “tabset” commands",
    "sentences": ["Don’t try to write up a ton of stuff, try to get to the point!", "More words \\(\\neq\\) “the better” Less is more, have them explore!", "use “tabset” commands"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Background Tips"
  },
  {
    "id": 280,
    "title": "📍 Cool Graphics",
    "url": "Paige-sNotes.html#coolgraphics",
    "content": "Cool Graphics Histograms Great for showing the distribution of data for a single quantitative variable when the sample size is large. Dotplots are a good alternative for smaller sample sizes. Gives a good feel for the mean and standard deviation of the data. ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. cars “cars” is a dataset. Type “View(cars)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=dist “x=” declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_histogram( The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used. binwidth=5,  The “binwidth” command controls the width of the bars in the histogram. fill=“firebrick1”, The “fill” command controls the color of the insides of each bar. color=“yellow” The “color” command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function.  +  The addition symbol + is used to add further elements to the ggplot.    ggtitle( The “ggtitle(” function is used to add a title to the plot. “Races of Lightning McQueen”,  This is the cool sick title of the graph, always make it interesting and not re-explaining the axis labels.  +  The addition symbol + is used to add further elements to the ggplot. xlab(“Speed (mph)”)  The “xlab(” command allows you name the x axis.  +  The addition symbol + is used to add further elements to the ggplot. ylab(“Distance”) The “ylab(” command allows you to label the y axis.  +  The addition symbol + is used to add further elements to the ggplot. theme(   The “theme(” function allows us to manipulate the title, axis labels, and tick size. plot.title=   The “plot.title=” allows us to manipulate the title in a various amount of ways. size=20,color=“dodgerblue”,face=“bold”,family=“serif”),  These commands allow us to manipulate size, color, bold/italic, or font of the title. axis.title.x =   The “axis.title.x =” allows us to modify the font size of the axis title (change the x to a y and it changes it for the y axis). element_text(size=16),  The “element_text(size=” allows us to modify the font size. axis.text.x =  The “axis.text.x =” allows us to modify the size of the values/labels along the x axis. element_text(size=12),  The “element_text(size=” allows us to modify the font size. axis.title.y =   The “axis.title.y =” allows us to modify the font size of the axis title. element_text(size=16),  The “element_text(size=” allows us to modify the font size. axis.text.y =  The “axis.text.y =” allows us to modify the size of the values/labels along the y axis. element_text(size=12)  The “element_text(size=” allows us to modify the font size. )Closing parenthsis for the labs function.     Press Enter to run the code.  …  Click to View Output.",
    "sentences": ["Histograms Great for showing the distribution of data for a single quantitative variable when the sample size is large.", "Dotplots are a good alternative for smaller sample sizes.", "Gives a good feel for the mean and standard deviation of the data.", "ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign.", "( Parenthesis to begin the function.", "Must touch the last letter of the function.", "cars “cars” is a dataset.", "Type “View(cars)” in R to see it.", ",  The comma allows us to specify optional commands to the function.", "The space after the comma is not required."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Cool Graphics"
  },
  {
    "id": 281,
    "title": "📍 Histograms",
    "url": "Paige-sNotes.html#histograms",
    "content": "Histograms Great for showing the distribution of data for a single quantitative variable when the sample size is large. Dotplots are a good alternative for smaller sample sizes. Gives a good feel for the mean and standard deviation of the data.",
    "sentences": ["Great for showing the distribution of data for a single quantitative variable when the sample size is large.", "Dotplots are a good alternative for smaller sample sizes.", "Gives a good feel for the mean and standard deviation of the data."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Histograms"
  },
  {
    "id": 282,
    "title": "📍 Scatter Plots",
    "url": "Paige-sNotes.html#scatterplots",
    "content": "Scatter Plots Depicts the actual values of the data points, which are \\((x,y)\\) pairs. Works well for small or large sample sizes. Visualizes well the correlation between the two variables. Should be used in linear regression contexts whenever possible. Rent <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/Rent.csv\") Rent_filtered <- Rent %>% mutate(`Walking Minutes to the MC`= round((CrowFlyMetersToMC/1609)*16.67,0))%>% dplyr::select(Gender,Name,Address,`Walking Minutes to the MC`,Residents,AvgFloorPlanCost, Latitude, Longitude)%>% mutate(`Monthly Floor Plan Cost`=round(AvgFloorPlanCost/3.5,2))%>% filter(Gender == \"F\")%>% filter(`Monthly Floor Plan Cost` < 300)%>% rename(`Semester Floor Plan Cost` = AvgFloorPlanCost)%>% rename(`Apartment Complex`= Name)%>% arrange(`Monthly Floor Plan Cost`) plot_ly(Rent_filtered, x= ~`Monthly Floor Plan Cost`, y= ~Residents, type = 'scatter', mode = 'markers', markers = list(size = 15), color = ~`Monthly Floor Plan Cost`, colors = 'Blues', text = ~paste(`Apartment Complex`,\"\\n\",\"$\",`Monthly Floor Plan Cost`))%>% layout (title = \"Popular BYU-Idaho Female Student Housing\",annotations =list(list(x=0.7, y= 1.03, text = \"(Light Blue - Cheap, Dark Blue - Expensive)\", showarrow = FALSE, xref= 'paper', yref = 'paper', xanchor = 'center', yanchor = 'auto', font = list(size = 10, color = \"gray\"))), xaxis = list(title = \"Rent Cost (USD)\"), yaxis = list(title = \"Residents\")) palette(c(\"skyblue\",\"firebrick\")) plot(mpg ~ qsec, data=mtcars, col=as.factor(am), pch=16, xlab=\"Quarter Mile Time (seconds)\", ylab=\"Miles per Gallon\", main=\"1974 Motor Trend Vehicles\") legend(\"topright\", pch=16, legend=c(\"automatic\",\"manual\"), title=\"Transmission\", bty='n', col=palette()) ggplot to plot_ly example[^2] ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams)",
    "sentences": ["Depicts the actual values of the data points, which are \\((x,y)\\) pairs.", "Works well for small or large sample sizes.", "Visualizes well the correlation between the two variables.", "Should be used in linear regression contexts whenever possible.", "Rent <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/Rent.csv\") Rent_filtered <- Rent %>% mutate(`Walking Minutes to the MC`= round((CrowFlyMetersToMC/1609)*16.67,0))%>% dplyr::select(Gender,Name,Address,`Walking Minutes to the MC`,Residents,AvgFloorPlanCost, Latitude, Longitude)%>% mutate(`Monthly Floor Plan Cost`=round(AvgFloorPlanCost/3.5,2))%>% filter(Gender == \"F\")%>% filter(`Monthly Floor Plan Cost` < 300)%>% rename(`Semester Floor Plan Cost` = AvgFloorPlanCost)%>% rename(`Apartment Complex`= Name)%>% arrange(`Monthly Floor Plan Cost`)", "plot_ly(Rent_filtered, x= ~`Monthly Floor Plan Cost`, y= ~Residents, type = 'scatter', mode = 'markers', markers = list(size = 15), color = ~`Monthly Floor Plan Cost`, colors = 'Blues', text = ~paste(`Apartment Complex`,\"\\n\",\"$\",`Monthly Floor Plan Cost`))%>% layout (title = \"Popular BYU-Idaho Female Student Housing\",annotations =list(list(x=0.7, y= 1.03, text = \"(Light Blue - Cheap, Dark Blue - Expensive)\", showarrow = FALSE, xref= 'paper', yref = 'paper', xanchor = 'center', yanchor = 'auto', font = list(size = 10, color = \"gray\"))), xaxis = list(title = \"Rent Cost (USD)\"), yaxis = list(title = \"Residents\"))", "palette(c(\"skyblue\",\"firebrick\")) plot(mpg ~ qsec, data=mtcars, col=as.factor(am), pch=16, xlab=\"Quarter Mile Time (seconds)\", ylab=\"Miles per Gallon\", main=\"1974 Motor Trend Vehicles\") legend(\"topright\", pch=16, legend=c(\"automatic\",\"manual\"), title=\"Transmission\", bty='n', col=palette())", "ggplot to plot_ly example[^2]", "ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Scatter Plots"
  },
  {
    "id": 283,
    "title": "📍 Bar Charts",
    "url": "Paige-sNotes.html#barcharts",
    "content": "Bar Charts Depicts the number of occurrances for each category, or level, of the qualitative variable. Similar to a histogram, but there is no natural way to order the bars. Thus the white-space between each bar. It is called a Pareto chart if the bars are ordered from tallest to shortest. Clustered and stacked bar charts are often used to display information for two qualitative variables simultaneously. myplot <- ggplot(starwars, aes(x=hair_color, fill=gender)) + geom_bar() ggplotly(myplot)",
    "sentences": ["Depicts the number of occurrances for each category, or level, of the qualitative variable.", "Similar to a histogram, but there is no natural way to order the bars.", "Thus the white-space between each bar.", "It is called a Pareto chart if the bars are ordered from tallest to shortest.", "Clustered and stacked bar charts are often used to display information for two qualitative variables simultaneously.", "myplot <- ggplot(starwars, aes(x=hair_color, fill=gender)) + geom_bar() ggplotly(myplot)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Bar Charts"
  },
  {
    "id": 284,
    "title": "📍 Boxplots",
    "url": "Paige-sNotes.html#boxplots",
    "content": "Boxplots Graphical depiction of the five-number summary. Great for comparing the distributions of data across several groups or categories. Provides a quick visual understanding of the location of the median as well as the range of the data. Can be useful in showing outliers. Sample size should be larger than at least five, or computing the five-number summary is not very meaningful. Side-by-side dotplots are a good alternative for smaller sample sizes. plot_ly(KidsFeet, y=~length, x=~as.factor(sex),type=\"box\",color=~sex, colors=c(\"lightblue\",\"pink\")) %>% layout( title=\"4th Grader's Average Foot Size\", xaxis=list(title=\"Gender\"),yaxis=list(title=\"Length (cm)\")) food <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv\") Foood <- food %>% dplyr::select(weight, breakfast, fries, drink, soup) %>% dplyr::filter(!is.na(suppressWarnings(as.numeric(weight))) + is.na(drink)) %>% mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) %>% mutate(weight = as.numeric(weight)) boxplot(weight ~ `Food Perception Score`, data=Foood, col=c(\"darkseagreen1\",\"palegreen\",\"palegreen3\",\"palegreen4\",\"mediumseagreen\"), xlab=\"Food Perception Score\", main=\"Weight of College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=Foood, pch=16,vertical=TRUE, add=TRUE, col=\"gray28\")",
    "sentences": ["Graphical depiction of the five-number summary.", "Great for comparing the distributions of data across several groups or categories.", "Provides a quick visual understanding of the location of the median as well as the range of the data.", "Can be useful in showing outliers.", "Sample size should be larger than at least five, or computing the five-number summary is not very meaningful.", "Side-by-side dotplots are a good alternative for smaller sample sizes.", "plot_ly(KidsFeet, y=~length, x=~as.factor(sex),type=\"box\",color=~sex, colors=c(\"lightblue\",\"pink\")) %>% layout( title=\"4th Grader's Average Foot Size\", xaxis=list(title=\"Gender\"),yaxis=list(title=\"Length (cm)\"))", "food <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv\")", "Foood <- food %>% dplyr::select(weight, breakfast, fries, drink, soup) %>% dplyr::filter(!is.na(suppressWarnings(as.numeric(weight))) + is.na(drink)) %>% mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) %>% mutate(weight = as.numeric(weight)) boxplot(weight ~ `Food Perception Score`, data=Foood, col=c(\"darkseagreen1\",\"palegreen\",\"palegreen3\",\"palegreen4\",\"mediumseagreen\"), xlab=\"Food Perception Score\", main=\"Weight of College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=Foood, pch=16,vertical=TRUE, add=TRUE, col=\"gray28\")"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Boxplots"
  },
  {
    "id": 285,
    "title": "📍 Dot Plots",
    "url": "Paige-sNotes.html#dotplots",
    "content": "Dot Plots Depicts the actual values of each data point. Best for small sample sizes or for datasets where there are lots of repeated values. Histograms or boxplots are better alternatives for large sample sizes when there are few repeated values. Great for comparing the distribution of data across several groups or categories. ggplot(KidsFeet, aes(x=factor(sex),y=length, fill=sex)) + coord_flip() + geom_dotplot(binaxis=\"y\", stackdir=\"up\", position=\"dodge\", dotsize=0.45, binwidth=0.5) + scale_fill_manual(values=c(\"B\"=\"lightblue\",\"G\"=\"pink\")) + labs(title= \"Kennedy 4th Grader's Foot Size\", x=\"Length (cm)\", y=\"Gender\")",
    "sentences": ["Depicts the actual values of each data point.", "Best for small sample sizes or for datasets where there are lots of repeated values.", "Histograms or boxplots are better alternatives for large sample sizes when there are few repeated values.", "Great for comparing the distribution of data across several groups or categories.", "ggplot(KidsFeet, aes(x=factor(sex),y=length, fill=sex)) + coord_flip() + geom_dotplot(binaxis=\"y\", stackdir=\"up\", position=\"dodge\", dotsize=0.45, binwidth=0.5) + scale_fill_manual(values=c(\"B\"=\"lightblue\",\"G\"=\"pink\")) + labs(title= \"Kennedy 4th Grader's Foot Size\", x=\"Length (cm)\", y=\"Gender\")"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Dot Plots"
  },
  {
    "id": 286,
    "title": "📍 Tables",
    "url": "Paige-sNotes.html#tables",
    "content": "Tables Always remember to pipe in a pander when you can! datatable(Rent_filtered, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") favstats(height~eye_color, data=starwars)%>% pander() HSS <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/HighSchoolSeniors.csv\") HSS.GJ <- dplyr::select(HSS, c(Reaction_time,Outdoor_Activities_Hours,Video_Games_Hours)) %>% filter_all(all_vars(!is.na(.))) %>% filter(Outdoor_Activities_Hours != Video_Games_Hours) %>% filter(abs(Outdoor_Activities_Hours - Video_Games_Hours) >= 10) %>% filter(Reaction_time > 0.1 & Reaction_time < 0.4) %>% mutate(More_Activity = if_else(Video_Games_Hours > Outdoor_Activities_Hours, \"Gamer\", \"Jock\")) %>% rename(`Reaction Time (Seconds)` = Reaction_time) %>% rename(`Time Spent Outside (Hours)` = Outdoor_Activities_Hours) %>% rename(`Time Spent Playing Video Games (Hours)` = Video_Games_Hours) %>% rename(`Gamer or Jock?` = More_Activity) HSS.G <- dplyr::select(HSS.GJ, c(`Reaction Time (Seconds)`,`Gamer or Jock?`)) %>% filter(`Gamer or Jock?` == \"Gamer\") HSS.J <- dplyr::select(HSS.GJ, c(`Reaction Time (Seconds)`,`Gamer or Jock?`)) %>% filter(`Gamer or Jock?` == \"Jock\") datatable(HSS.GJ, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") We added a classification column in this one! It’s pretty cool!! datatable(Foood, options=list(pageLength = 10)) %>% formatStyle('Food Perception Score', backgroundColor = 'lightgreen') kable function is pretty cool tooooooo!! questions <- c(\"Which one of these pictures do you associate the word 'breakfast'?\", \"Which picture do you associate with the word 'drink'?\", \"Which of these pictures you associate with word 'fries'?\", \"Which of the two pictures you associate with the word 'soup'?\") foodoptions <- c(\"Cereal / Donut\", \"1- Orange Juice / 2- Soda\", \"1- McDonald's Fries / 2- Home Fries\", \"1- Veggie Soup / 2- Creamy Soup\") collegestusurvey <- data.frame( Question = questions, Options = foodoptions, stringsAsFactors = FALSE) kable(collegestusurvey, col.names = c(\"Questions\", \"Food Options\"), caption =\"Food Survey\") Foodie <- favstats(weight ~ `Food Perception Score`, data = Foood)[,-10] Foodie2 <- as.data.frame(Foodie) kable(Foodie2)",
    "sentences": ["Always remember to pipe in a pander when you can!", "datatable(Rent_filtered, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\")", "favstats(height~eye_color, data=starwars)%>% pander()", "HSS <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/HighSchoolSeniors.csv\") HSS.GJ <- dplyr::select(HSS, c(Reaction_time,Outdoor_Activities_Hours,Video_Games_Hours)) %>% filter_all(all_vars(!is.na(.))) %>% filter(Outdoor_Activities_Hours != Video_Games_Hours) %>% filter(abs(Outdoor_Activities_Hours - Video_Games_Hours) >= 10) %>% filter(Reaction_time > 0.1 & Reaction_time < 0.4) %>% mutate(More_Activity = if_else(Video_Games_Hours > Outdoor_Activities_Hours, \"Gamer\", \"Jock\")) %>% rename(`Reaction Time (Seconds)` = Reaction_time) %>% rename(`Time Spent Outside (Hours)` = Outdoor_Activities_Hours) %>% rename(`Time Spent Playing Video Games (Hours)` = Video_Games_Hours) %>% rename(`Gamer or Jock?` = More_Activity) HSS.G <- dplyr::select(HSS.GJ, c(`Reaction Time (Seconds)`,`Gamer or Jock?`)) %>% filter(`Gamer or Jock?` == \"Gamer\") HSS.J <- dplyr::select(HSS.GJ, c(`Reaction Time (Seconds)`,`Gamer or Jock?`)) %>% filter(`Gamer or Jock?` == \"Jock\") datatable(HSS.GJ, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\")", "We added a classification column in this one!", "It’s pretty cool!!", "datatable(Foood, options=list(pageLength = 10)) %>% formatStyle('Food Perception Score', backgroundColor = 'lightgreen')", "kable function is pretty cool tooooooo!!", "questions <- c(\"Which one of these pictures do you associate the word 'breakfast'?\", \"Which picture do you associate with the word 'drink'?\", \"Which of these pictures you associate with word 'fries'?\", \"Which of the two pictures you associate with the word 'soup'?\") foodoptions <- c(\"Cereal / Donut\", \"1- Orange Juice / 2- Soda\", \"1- McDonald's Fries / 2- Home Fries\", \"1- Veggie Soup / 2- Creamy Soup\") collegestusurvey <- data.frame( Question = questions, Options = foodoptions, stringsAsFactors = FALSE) kable(collegestusurvey, col.names = c(\"Questions\", \"Food Options\"), caption =\"Food Survey\")", "Foodie <- favstats(weight ~ `Food Perception Score`, data = Foood)[,-10] Foodie2 <- as.data.frame(Foodie) kable(Foodie2)"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Tables"
  },
  {
    "id": 287,
    "title": "📍 Maps",
    "url": "Paige-sNotes.html#maps",
    "content": "Maps Helps to show where things are in reference to something! mc_icon <- makeAwesomeIcon(icon= \"university\", iconColor = \"white\", markerColor = \"lightblue\", library = \"fa\") leaflet(data = Rent_filtered)%>% addTiles() %>% setView(lng=-111.7833595717528, lat=43.82217013118815,zoom =14.511)%>% addAwesomeMarkers(lng=-111.7827756599601, lat=43.81821783971177, icon= mc_icon, popup= \"Manwarding Center(MC)\")%>% # addMarkers(lng=-111.7826513, lat= 43.82271564, popup= \"HILL'S COLLEGE AVE APTS<br>5 Min. Walk\")%>% # addMarkers(lng=-111.7876455, lat= 43.82246794, popup= \"PINES, WOMEN<br>6 Min. Walk\")%>% # addMarkers(lng=-111.7882195, lat=43.81930189, popup= \"DAVENPORT APARTMENTS<br>5 Min. Walk\")%>% # addMarkers(lng= -111.7754656, lat=43.8174285, popup= \"BUENA VISTA<br>6 Min. Walk\")%>% # addMarkers(lng=-111.7805055, lat=43.82015341, popup= \"RIVIERA APARTMENTS<br>3 Min. Walk\")%>% # addMarkers(lng=-111.7798457, lat=43.82065271, popup= \"BAYSIDE MANOR<br>3 Min. Walk\")%>% # addMarkers(lng=-111.7890135, lat=43.81866711, popup= \"BROOKLYN APARTMENTS<br>5 Min. Walk\")%>% # addMarkers(lng=-111.7871091, lat=43.82469723, popup= \"COTTONWOOD - WOMEN<br>8 Min. Walk\")%>% # addMarkers(lng=-111.7806664, lat=43.82124102, popup= \"CRESTWOOD COTTAGE<br>4 Min. Walk\")%>% # addMarkers(lng=-111.7877153, lat=43.81913933, popup= \"ROYAL CREST<br>4 Min. Walk\")%>% # addMarkers(lng=-111.7792019, lat=43.82347422, popup= \"BLUE DOOR, WOMEN<br>6 Min. Walk\")%>% # addMarkers(lng=-111.7883, lat=43.8200102, popup= \"SUNSET HALL<br>5 Min. Walk\")%>% addMarkers(lng=~Longitude, lat=~Latitude, popup=~paste(`Apartment Complex`,\"<br>\",\"Walk Mins.\",`Walking Minutes to the MC`)) %>% addLegend(position=\"topright\", colors= c(\"lightblue\",\"cornflowerblue\"), labels = c(\"Manwarding Center (MC)\",\"Apartment Complexes\"), title= \"Building Locations\")%>% addControl(\"<strong>Click the marker to see additional information<\/strong>\",position = \"topright\", className = \"map-caption\")",
    "sentences": ["Helps to show where things are in reference to something!", "mc_icon <- makeAwesomeIcon(icon= \"university\", iconColor = \"white\", markerColor = \"lightblue\", library = \"fa\") leaflet(data = Rent_filtered)%>% addTiles() %>% setView(lng=-111.7833595717528, lat=43.82217013118815,zoom =14.511)%>% addAwesomeMarkers(lng=-111.7827756599601, lat=43.81821783971177, icon= mc_icon, popup= \"Manwarding Center(MC)\")%>% # addMarkers(lng=-111.7826513, lat= 43.82271564, popup= \"HILL'S COLLEGE AVE APTS<br>5 Min.", "Walk\")%>% # addMarkers(lng=-111.7876455, lat= 43.82246794, popup= \"PINES, WOMEN<br>6 Min.", "Walk\")%>% # addMarkers(lng=-111.7882195, lat=43.81930189, popup= \"DAVENPORT APARTMENTS<br>5 Min.", "Walk\")%>% # addMarkers(lng= -111.7754656, lat=43.8174285, popup= \"BUENA VISTA<br>6 Min.", "Walk\")%>% # addMarkers(lng=-111.7805055, lat=43.82015341, popup= \"RIVIERA APARTMENTS<br>3 Min.", "Walk\")%>% # addMarkers(lng=-111.7798457, lat=43.82065271, popup= \"BAYSIDE MANOR<br>3 Min.", "Walk\")%>% # addMarkers(lng=-111.7890135, lat=43.81866711, popup= \"BROOKLYN APARTMENTS<br>5 Min.", "Walk\")%>% # addMarkers(lng=-111.7871091, lat=43.82469723, popup= \"COTTONWOOD - WOMEN<br>8 Min.", "Walk\")%>% # addMarkers(lng=-111.7806664, lat=43.82124102, popup= \"CRESTWOOD COTTAGE<br>4 Min."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Maps"
  },
  {
    "id": 288,
    "title": "📍 Styling and Formats",
    "url": "Paige-sNotes.html#stylingandformats",
    "content": "Styling and Formats I typed some words. Some blue words, some highlighted words, some big words. Make a link to a meaningful page of the Statistics-Notebook: Customization Help <- Read through this page! Or to a page on the internet: R Colors Word Stylez Colors < span style=“color:[INSERT COLOR];”>[INSERT WORDS] < /span > Highlighting < span style=“background-color:[INSERT COLOR];”>[INSERT WORDS] < /span > Font Size < span style=“font-size:[INSERT SIZE]em;”>[INSERT WORDS] < /span > Underlining < u>[INSERT WORDS] < /u > Bolding, Italicize, or both! One * -> Italicize Two ** -> Bold Three *** -> Combo Shrinking font-size < div style = “font-size:.8em;” > Changing Words to Gray Text < div style = “color:gray;” > Formatting Section making { .tabset .tabset-pills OR fade } Making a Caption < div style =“font-size:.8em;color:#888888;” > [INSERT THE WORDS YOU WANT TO SAY HERE!] < / div > Inserting a Picture ! [ _ ] ( [COPY AND PASTE PICTURE INTO THE PARANTHESES] ) Practice header is 3 stars and in all caps like this: PRACTICE 3 dashes makes line in between texts (make sure they aren’t touching any text and that they are spaced away from things) < br > < br > (makes bigger gap inbetween text, the more you add the wider the gap!) one dash makes a bullet point pressing space 4 times make a sub bullet point! will this make another one? Pathing Data Set Code Need to path to data set? Use this code: [Data set name] <- read_csv(“C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/[Data set name]”) or do read_csv(../../Data/[Data Set Name].csv) Also, to create a csv. file in R, use this code! write.csv([data set name],“C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/[data set name].csv”, row.names = FALSE) Themes Want a different theme? Here are some options! “default” “bootstrap” -> dark blue buttons “cerulean” -> light blue buttons “cosmo” -> light blue square button “darkly” -> dark blue button + black background “flatly” -> DARK blue button with green text! “journal” -> pink buttons “lumen” -> greyish blue buttons “paper” -> blue buttons, lame text “readable” -> blue buttons, text is big and bubbly “sandstone” -> tan buttons “simplex” -> red buttons “spacelab” -> darker blue gray buttons, softer shape “united” -> redish orange buttons “yeti” -> greyish more squared buttons “architect” (from prettydoc package) “cayman” “hpstr” “leonids” “tactile” “html_clean” (from the rmdformats package) “html_docco” “material” “readthedown” Text Side by Side Want to make something side by side < div style=“display: flex; justify-content: space-around;” > < div > [Insert words] < /div > < div > [more words right here] < /div > < / div > Hyper linking Want to make hyper link your stuff from one part to go back and forth between where you are and the source? (did that even make sense??) Put a [ ^ 1 ] next to the source you are wanting to link the sentence to. There is an example to show you in the visual making questions In the sources, put [ ^ 1 ] : [ The name for your hyper link in brackets ] ( the link of the video in parentheses ) [^1] : It would look like this! ggplot into plot_ly Assignment Operator (<-) your ggplot a name use ggplotly([Insert name you <- to]) to change the ggplot to a plot_ly! example is in the [^2]:scatter plot section!",
    "sentences": ["I typed some words.", "Some blue words, some highlighted words, some big words.", "Make a link to a meaningful page of the Statistics-Notebook: Customization Help <- Read through this page!", "Or to a page on the internet: R Colors", "Word Stylez Colors < span style=“color:[INSERT COLOR];”>[INSERT WORDS] < /span > Highlighting < span style=“background-color:[INSERT COLOR];”>[INSERT WORDS] < /span > Font Size < span style=“font-size:[INSERT SIZE]em;”>[INSERT WORDS] < /span > Underlining < u>[INSERT WORDS] < /u > Bolding, Italicize, or both!", "One * -> Italicize Two ** -> Bold Three *** -> Combo Shrinking font-size < div style = “font-size:.8em;” > Changing Words to Gray Text < div style = “color:gray;” >", "Formatting Section making { .tabset .tabset-pills OR fade } Making a Caption < div style =“font-size:.8em;color:#888888;” > [INSERT THE WORDS YOU WANT TO SAY HERE!] < / div > Inserting a Picture !", "[ _ ] ( [COPY AND PASTE PICTURE INTO THE PARANTHESES] ) Practice header is 3 stars and in all caps like this: PRACTICE 3 dashes makes line in between texts (make sure they aren’t touching any text and that they are spaced away from things) < br > < br > (makes bigger gap inbetween text, the more you add the wider the gap!) one dash makes a bullet point pressing space 4 times make a sub bullet point!", "will this make another one?", "Pathing Data Set Code Need to path to data set?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Styling and Formats"
  },
  {
    "id": 289,
    "title": "📍 Word Stylez",
    "url": "Paige-sNotes.html#wordstylez",
    "content": "Word Stylez Colors < span style=“color:[INSERT COLOR];”>[INSERT WORDS] < /span > Highlighting < span style=“background-color:[INSERT COLOR];”>[INSERT WORDS] < /span > Font Size < span style=“font-size:[INSERT SIZE]em;”>[INSERT WORDS] < /span > Underlining < u>[INSERT WORDS] < /u > Bolding, Italicize, or both! One * -> Italicize Two ** -> Bold Three *** -> Combo Shrinking font-size < div style = “font-size:.8em;” > Changing Words to Gray Text < div style = “color:gray;” >",
    "sentences": ["Colors < span style=“color:[INSERT COLOR];”>[INSERT WORDS] < /span >", "Highlighting < span style=“background-color:[INSERT COLOR];”>[INSERT WORDS] < /span >", "Font Size < span style=“font-size:[INSERT SIZE]em;”>[INSERT WORDS] < /span >", "Underlining < u>[INSERT WORDS] < /u >", "Bolding, Italicize, or both!", "One * -> Italicize Two ** -> Bold Three *** -> Combo", "Shrinking font-size < div style = “font-size:.8em;” >", "Changing Words to Gray Text < div style = “color:gray;” >"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Word Stylez"
  },
  {
    "id": 290,
    "title": "📍 Formatting",
    "url": "Paige-sNotes.html#formatting",
    "content": "Formatting Section making { .tabset .tabset-pills OR fade } Making a Caption < div style =“font-size:.8em;color:#888888;” > [INSERT THE WORDS YOU WANT TO SAY HERE!] < / div > Inserting a Picture ! [ _ ] ( [COPY AND PASTE PICTURE INTO THE PARANTHESES] ) Practice header is 3 stars and in all caps like this: PRACTICE 3 dashes makes line in between texts (make sure they aren’t touching any text and that they are spaced away from things) < br > < br > (makes bigger gap inbetween text, the more you add the wider the gap!) one dash makes a bullet point pressing space 4 times make a sub bullet point! will this make another one? Pathing Data Set Code Need to path to data set? Use this code: [Data set name] <- read_csv(“C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/[Data set name]”) or do read_csv(../../Data/[Data Set Name].csv) Also, to create a csv. file in R, use this code! write.csv([data set name],“C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/[data set name].csv”, row.names = FALSE) Themes Want a different theme? Here are some options! “default” “bootstrap” -> dark blue buttons “cerulean” -> light blue buttons “cosmo” -> light blue square button “darkly” -> dark blue button + black background “flatly” -> DARK blue button with green text! “journal” -> pink buttons “lumen” -> greyish blue buttons “paper” -> blue buttons, lame text “readable” -> blue buttons, text is big and bubbly “sandstone” -> tan buttons “simplex” -> red buttons “spacelab” -> darker blue gray buttons, softer shape “united” -> redish orange buttons “yeti” -> greyish more squared buttons “architect” (from prettydoc package) “cayman” “hpstr” “leonids” “tactile” “html_clean” (from the rmdformats package) “html_docco” “material” “readthedown”",
    "sentences": ["Section making { .tabset .tabset-pills OR fade }", "Making a Caption < div style =“font-size:.8em;color:#888888;” > [INSERT THE WORDS YOU WANT TO SAY HERE!] < / div >", "Inserting a Picture !", "[ _ ] ( [COPY AND PASTE PICTURE INTO THE PARANTHESES] ) Practice header is 3 stars and in all caps like this: PRACTICE 3 dashes makes line in between texts (make sure they aren’t touching any text and that they are spaced away from things) < br > < br > (makes bigger gap inbetween text, the more you add the wider the gap!) one dash makes a bullet point pressing space 4 times make a sub bullet point!", "will this make another one?", "Pathing Data Set Code Need to path to data set?", "Use this code: [Data set name] <- read_csv(“C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/[Data set name]”) or do read_csv(../../Data/[Data Set Name].csv) Also, to create a csv.", "file in R, use this code!", "write.csv([data set name],“C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/[data set name].csv”, row.names = FALSE)", "Themes Want a different theme?"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Formatting"
  },
  {
    "id": 291,
    "title": "📍 Final Exam Practice",
    "url": "Paige-sNotes.html#finalexampractice",
    "content": "Final Exam Practice Perform an appropriate hypothesis test in R to decide if the medians displayed in the graph are significantly different or not. (Uses KidsFeet dataset) t.test(width ~ sex, data= KidsFeet, mu=0, alternative= \"two.sided\", conf.level = 0.95) What is the correct conclusion to make in a logistic regression when the goodness of fit test (either one) gives a p-value of 0.352? This shows the logistic regression provides significantly useful results. Continue to believe the logistic regression was appropriate to perform on the given data. Realize that there was insufficient evidence to conclude that the logistic regression was a good fit for the data. The logistic regression simply isn’t a good fit for the data in that case. A permutation test for a Kruskal-Wallis Test Statistic is performed with the following result: Note that the numbers above the bars state the frequency of observations contained in that bar.",
    "sentences": ["Perform an appropriate hypothesis test in R to decide if the medians displayed in the graph are significantly different or not.", "(Uses KidsFeet dataset)", "t.test(width ~ sex, data= KidsFeet, mu=0, alternative= \"two.sided\", conf.level = 0.95)", "What is the correct conclusion to make in a logistic regression when the goodness of fit test (either one) gives a p-value of 0.352?", "This shows the logistic regression provides significantly useful results.", "Continue to believe the logistic regression was appropriate to perform on the given data.", "Realize that there was insufficient evidence to conclude that the logistic regression was a good fit for the data.", "The logistic regression simply isn’t a good fit for the data in that case.", "A permutation test for a Kruskal-Wallis Test Statistic is performed with the following result:", "Note that the numbers above the bars state the frequency of observations contained in that bar."],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Final Exam Practice"
  },
  {
    "id": 292,
    "title": "📍 Final Exam",
    "url": "Paige-sNotes.html#finalexam",
    "content": "Final Exam YoungAdults <- read_csv(\"https://raw.githubusercontent.com/saundersg/Statistics-Notebook/master/Data/YoungAdults.csv\") oolm <- lm(Weight ~ Height, data=YoungAdults) predict(oolm, data.frame(Height = 183)) ayogm <- glm(as.factor(YoungAdults$Lying) ~ Religiosity_score, data=YoungAdults, family=binomial) summary(ayogm) wilcox.test(Religiosity_score ~ Dominant_hand, data=YoungAdults) ggplot(data=YoungAdults, aes(x=as.factor(Dominant_hand), y=Religiosity_score)) + geom_boxplot() prunedudes <- YoungAdults %>% filter(Number_of_siblings == 0) %>% filter(Smoking == \"never smoked\")%>% filter(Lying == \"never\") t.test(Fear_score ~ Only_child, data = YoungAdults, mu = 0, alternative= \"two.sided\", conf.level = 0.95) YoungAdults.hwg <- YoungAdults %>% dplyr::select(Height, Weight, Gender) %>% na.omit() ggplot(YoungAdults.hwg, aes(x=Height, y=Weight, color=Gender))+ geom_point() + geom_smooth(method = \"lm\", se=F) YoungAdults_biggerfam <- YoungAdults %>% filter(Number_of_siblings > 3) plot(Religiosity_score ~ Fear_score, data= YoungAdults_biggerfam) relig_vs_fear.lm <- lm(Religiosity_score ~ Fear_score, data=YoungAdults_biggerfam) abline(relig_vs_fear.lm) observedTestStat <- summary(relig_vs_fear.lm)[[4]][2,3]",
    "sentences": ["YoungAdults <- read_csv(\"https://raw.githubusercontent.com/saundersg/Statistics-Notebook/master/Data/YoungAdults.csv\")", "oolm <- lm(Weight ~ Height, data=YoungAdults) predict(oolm, data.frame(Height = 183))", "ayogm <- glm(as.factor(YoungAdults$Lying) ~ Religiosity_score, data=YoungAdults, family=binomial) summary(ayogm)", "wilcox.test(Religiosity_score ~ Dominant_hand, data=YoungAdults)", "ggplot(data=YoungAdults, aes(x=as.factor(Dominant_hand), y=Religiosity_score)) + geom_boxplot()", "prunedudes <- YoungAdults %>% filter(Number_of_siblings == 0) %>% filter(Smoking == \"never smoked\")%>% filter(Lying == \"never\")", "t.test(Fear_score ~ Only_child, data = YoungAdults, mu = 0, alternative= \"two.sided\", conf.level = 0.95)", "YoungAdults.hwg <- YoungAdults %>% dplyr::select(Height, Weight, Gender) %>% na.omit() ggplot(YoungAdults.hwg, aes(x=Height, y=Weight, color=Gender))+ geom_point() + geom_smooth(method = \"lm\", se=F)", "YoungAdults_biggerfam <- YoungAdults %>% filter(Number_of_siblings > 3) plot(Religiosity_score ~ Fear_score, data= YoungAdults_biggerfam) relig_vs_fear.lm <- lm(Religiosity_score ~ Fear_score, data=YoungAdults_biggerfam) abline(relig_vs_fear.lm)", "observedTestStat <- summary(relig_vs_fear.lm)[[4]][2,3]"],
    "type": "section",
    "page_title": "Intermediate Statistics Notes",
    "section_title": "Final Exam"
  },
  {
    "id": 293,
    "title": "Permutation Tests",
    "url": "PermutationTests.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Permutation Tests A nonparametric approach to computing the p-value for any test statistic in just about any scenario. Overview In almost all hypothesis testing scenarios, the null hypothesis can be interpreted as follows. \\(H_0\\): Any pattern that has been witnessed in the sampled data is simply due to random chance. Permutation Tests depend completely on this single idea. If all patterns in the data really are simply due to random chance, then the null hypothesis is true. Further, random re-samples of the data should show similar lack of patterns. However, if the pattern in the data is real, then random re-samples of the data will show very different patterns from the original. Consider the following image. In that image, the toy blocks on the left show a clear pattern or structure. They are nicely organized into colored piles. This suggests a real pattern that is not random. Someone certainly organized those blocks into that pattern. The blocks didn’t land that way by random chance. On the other hand, the pile of toy blocks shown on the right is certainly a random pattern. This is a pattern that would result if the toy blocks were put into a bag, shaken up, and dumped out. This is the idea of the permutation test. If there is structure in the data, then “mixing up the data and dumping it out again” will show very different patterns from the original. However, if the data was just random to begin with, then we would see a similar pattern by “mixing up the data and dumping it out again.” The process of a permutation test is: Compute a test statistic for the original data. Re-sample the data (“shake it up and dump it out”) thousands of times, computing a new test statistic each time, to create a sampling distribution of the test statistic. Compute the p-value of the permutation test as the percentage of test statistics that are as extreme or more extreme than the one originally observed. In review, the sampling distribution is created by permuting (randomly rearranging) the data thousands of times and calculating a test statistic on each permuted version of the data. A histogram of the test statistics then provides the sampling distribution of the test statistic needed to compute the p-value of the original test statistic. R Instructions Any permutation test can be performed in R with a for loop. #Step 1 Compute a test statistic for the original data. myTest <- …perform the initial test… This could be a t.test, wilcox.test, aov, kruskal.test, lm, glm, chisq.test, or any other R code that results in a test statistic. It could even simply be the mean or standard deviation of the data. observedTestStat <- …get the test statistic… Save the test statistic of your test into the object called observedTestStat. For tests that always result in a single test statistic like a t.test, wilcox.test, kruskal.test, and chisq.test it is myTest$statistic. For an aov, lm, or glm try printing summary(myTest)[] to see what values you are interested in using. observedTestStat Print the value of the test statistic of your test to the screen. This is the value that we now need to use to compute the P-value from by finding the probability that a randomly computed test statistic would be “more extreme” than this originally observed value. #Step 2 Re-sample the data (“shake it up and dump it out”) thousands of times, computing a new test statistic each time, to create a sampling distribution of the test statistic. N <- 2000       N is the number of times you will reuse the data to create the sampling distribution of the test statistic. A typical choice is 2000, but sometimes 10000, or 100000 reuses are needed before useful answers can be obtained. permutedTestStats <-  This is a storage container that will be used to store the test statistics from each of the thousands of reuses of the data. rep(NA, N) The rep() function repeats a given value N times. This particular statement repeats NA’s or “missing values” N times. This gives us N “empty” storage spots inside of permutedTestStats that we can use to store the N test statistics from the N reuses of the data we will make in our for loop. for The for loop is a programming tool that lets us tell R to run a certain code over and over again for a certain number of times.  (i in   In R, the for loop must be followed by a space, then an opening parenthesis, then a variable name (in this case the variable is called “i”), then the word “in” then a list of values. 1:N The 1:N gives R the list of values 1, 2, 3, … and so on all the way up to N. These values are passed into i one at a time and the code inside the for loop is performed first for i=1, then again for i=2, then again for i=3 and so on until finally i=N. At that point, the for loop ends. ) Required closing parenthesis on the for (i in 1:N) statement. { This bracket opens the code section of the for loop. Any code placed between the opening { and closing } brackets will be performed over and over again for each value of i=1, i=2, … up through i=N.    Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized. permutedTest <- …perform test with permutedData… The same test that was performed on the original data, should be performed again but with randomly permuted data. The easiest way to permute data is with sample(y-variable-name) inside your test. See the Explanation tab for details.    Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized. permutedTestStats This is the storage container that was built prior to the for (i in 1:N) code. Inside the for loop, this container is filled value by value using the square brackets [i]. [i] The square brackets [i] allows us to access the “i”th position of permutedTestStats. Remember, since this code is inside of the for loop, i=1 the first time through the code, then i=2 the second time through the code, i=3 the third time through, and so on up until i=N the Nth time through the code.  <- …get test statistic… The test statistic from permutedTest is accessed here and stored into permutedTestStats[i]. } The closing } bracket ends the code that is repeated over and over again inside the for loop. hist(permutedTestStats) Creating a histogram of the sampling distribution of the test statistics obtained from the reused and permuted data allows us to visually compare the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. abline(v=observedTestStat) This adds the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. #Step 3 Compute the p-value of the permutation test as the percentage of test statistics that are as extreme or more extreme than the one originally observed. sum(permutedTestStats >= observedTestStat)/N This computes a “greater than” p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the right hand side of the histogram. sum(permutedTestStats <= observedTestStat)/N This computes a “less than” p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the left hand side of the histogram. myTest <- ...perform the initial test... This could be a t.test, wilcox.test, aov, kruskal.test, lm, glm, chisq.test, or any other R code that results in a test statistic. It could even simply be the mean or standard deviation of the data. observedTestStat <- ...get the test statistic... Save the test statistic of your test into the object called observedTestStat. For tests that always result in a single test statistic like a t.test, wilcox.test, kruskal.test, and chisq.test it is myTest$statistic. For an aov, lm, or glm try printing summary(myTest)[] to see what values you are interested in using. N <- 2000 N is the number of times you will reuse the data to create the sampling distribution of the test statistic. A typical choice is 2000, but sometimes 10000, or 100000 reuses are needed before useful answers can be obtained. permutedTestStats <- rep(NA, N) This is a storage container that will be used to store the test statistics from each of the thousands of reuses of the data. The rep() function repeats a given value N times. This particular statement repeats NA’s or “missing values” N times. This gives us N “empty” storage spots inside of permutedTestStats that we can use to store the N test statistics from the N reuses of the data we will make in our for loop. for (i in 1:N)\\{ The for loop is a programming tool that lets us tell R to run a certain code over and over again for a certain number of times. In R, the for loop must be followed by a space, then an opening parenthesis, then a variable name (in this case the variable is called “i”), then the word “in” then a list of values. The 1:N gives R the list of values 1, 2, 3, … and so on all the way up to N. These values are passed into i one at a time and the code inside the for loop is performed first for i=1, then again for i=2, then again for i=3 and so on until finally i=N. At that point, the for loop ends. There is a required closing parenthesis on the for (i in 1:N) statement. Any code placed between the opening { and closing } brackets will be performed over and over again for each value of i=1, i=2, … up through i=N. Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized. permutedData <- ...randomly permute the data... This is the most important part of the permutation test and takes some thinking. The data must be randomly reorganized in a way consistent with the null hypothesis. What that means exactly is specific to each scenario. Read the Explanation tab for further details on the logic you should use here. permutedTest <- ...perform test with permutedData... The same test that was performed on the original data, should be performed again on the randomly permuted data. permutedTestStats[i] <- ...get test statistic... This is the storage container that was built prior to the for (i in 1:N) code. Inside the for loop, this container is filled value by value using the square brackets [i]. The square brackets [i] allows us to access the “i”th position of permutedTestStats. Remember, since this code is inside of the for loop, i=1 the first time through the code, then i=2 the second time through the code, i=3 the third time through, and so on up until i=N the Nth time through the code. The test statistic from permutedTest is accessed here and stored into permutedTestStats[i]. } The closing } bracket ends the code that is repeated over and over again inside the for loop. hist(permutedTestStats) Creating a histogram of the sampling distribution of the test statistics obtained from the reused and permuted data allows us to visually compare the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. abline(v=observedTestStat) This adds the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. sum(permutedTestStats >= observedTestStat)/N This computes a “greater than” p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the right hand side of the histogram. sum(permutedTestStats <= observedTestStat)/N This computes a “less than” p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the left hand side of the histogram. Explanation The most difficult part of a permutation test is in the random permuting of the data. How the permuting is performed depends on the type of hypothesis test being performed. It is important to remember that the permutation test only changes the way the p-value is calculated. Everything else about the original test is unchanged when switching to a permutation test. Independent Samples t Test For the independent sample t Test, we will use the data from the independent sleep analysis. In that analysis, we were using the sleep data to test the hypotheses: \\[ H_0: \\mu_\\text{Extra Hours of Sleep with Drug 1} - \\mu_\\text{Extra Hours of Sleep with Drug 2} = 0 \\] \\[ H_a: \\mu_\\text{Extra Hours of Sleep with Drug 1} - \\mu_\\text{Extra Hours of Sleep with Drug 2} \\neq 0 \\] We used a significance level of \\(\\alpha = 0.05\\) and obtained a P-value of \\(0.07939\\). Let’s demonstrate how a permutation test could be used to obtain this same p-value. (Technically you only need to use a permutation test when the requirements of the original test were not satisfied. However, it is also reasonable to perform a permutation test anytime you want. No requirements need to be checked when performing a permutation test.) # First run the initial test and gain the test statistic: myTest <- t.test(extra ~ group, data = sleep, mu = 0) observedTestStat <- myTest$statistic # Now we run the permutations to create a distribution of test statistics N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- t.test(sample(extra) ~ group, data = sleep, mu = 0) permutedTestStats[i] <- permutedTest$statistic } # Now we show a histogram of that distribution hist(permutedTestStats, col = \"skyblue\") abline(v = observedTestStat, col = \"red\", lwd = 3) #Greater-Than p-value: Not the correct one in this case sum(permutedTestStats >= observedTestStat)/N # Less-Than p-value: Not the correct one for this data sum(permutedTestStats <= observedTestStat)/N # Two-Sided p-value: This is the one we want based on our alternative hypothesis. 2*sum(permutedT",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Permutation Tests A nonparametric approach to computing the p-value for any test statistic in just about any scenario.", "Overview In almost all hypothesis testing scenarios, the null hypothesis can be interpreted as follows.", "\\(H_0\\): Any pattern that has been witnessed in the sampled data is simply due to random chance.", "Permutation Tests depend completely on this single idea.", "If all patterns in the data really are simply due to random chance, then the null hypothesis is true.", "Further, random re-samples of the data should show similar lack of patterns.", "However, if the pattern in the data is real, then random re-samples of the data will show very different patterns from the original.", "Consider the following image.", "In that image, the toy blocks on the left show a clear pattern or structure.", "They are nicely organized into colored piles.", "This suggests a real pattern that is not random.", "Someone certainly organized those blocks into that pattern.", "The blocks didn’t land that way by random chance.", "On the other hand, the pile of toy blocks shown on the right is certainly a random pattern.", "This is a pattern that would result if the toy blocks were put into a bag, shaken up, and dumped out.", "This is the idea of the permutation test.", "If there is structure in the data, then “mixing up the data and dumping it out again” will show very different patterns from the original.", "However, if the data was just random to begin with, then we would see a similar pattern by “mixing up the data and dumping it out again.” The process of a permutation test is: Compute a test statistic for the original data.", "Re-sample the data (“shake it up and dump it out”) thousands of times, computing a new test statistic each time, to create a sampling distribution of the test statistic.", "Compute the p-value of the permutation test as the percentage of test statistics that are as extreme or more extreme than the one originally observed.", "In review, the sampling distribution is created by permuting (randomly rearranging) the data thousands of times and calculating a test statistic on each permuted version of the data.", "A histogram of the test statistics then provides the sampling distribution of the test statistic needed to compute the p-value of the original test statistic.", "R Instructions Any permutation test can be performed in R with a for loop.", "#Step 1 Compute a test statistic for the original data.", "myTest <- …perform the initial test… This could be a t.test, wilcox.test, aov, kruskal.test, lm, glm, chisq.test, or any other R code that results in a test statistic.", "It could even simply be the mean or standard deviation of the data.", "observedTestStat <- …get the test statistic… Save the test statistic of your test into the object called observedTestStat.", "For tests that always result in a single test statistic like a t.test, wilcox.test, kruskal.test, and chisq.test it is myTest$statistic.", "For an aov, lm, or glm try printing summary(myTest)[] to see what values you are interested in using.", "observedTestStat Print the value of the test statistic of your test to the screen.", "This is the value that we now need to use to compute the P-value from by finding the probability that a randomly computed test statistic would be “more extreme” than this originally observed value.", "#Step 2 Re-sample the data (“shake it up and dump it out”) thousands of times, computing a new test statistic each time, to create a sampling distribution of the test statistic.", "N <- 2000       N is the number of times you will reuse the data to create the sampling distribution of the test statistic.", "A typical choice is 2000, but sometimes 10000, or 100000 reuses are needed before useful answers can be obtained.", "permutedTestStats <-  This is a storage container that will be used to store the test statistics from each of the thousands of reuses of the data.", "rep(NA, N) The rep() function repeats a given value N times.", "This particular statement repeats NA’s or “missing values” N times.", "This gives us N “empty” storage spots inside of permutedTestStats that we can use to store the N test statistics from the N reuses of the data we will make in our for loop.", "for The for loop is a programming tool that lets us tell R to run a certain code over and over again for a certain number of times.", " (i in   In R, the for loop must be followed by a space, then an opening parenthesis, then a variable name (in this case the variable is called “i”), then the word “in” then a list of values.", "1:N The 1:N gives R the list of values 1, 2, 3, … and so on all the way up to N.", "These values are passed into i one at a time and the code inside the for loop is performed first for i=1, then again for i=2, then again for i=3 and so on until finally i=N.", "At that point, the for loop ends.", ") Required closing parenthesis on the for (i in 1:N) statement.", "{ This bracket opens the code section of the for loop.", "Any code placed between the opening { and closing } brackets will be performed over and over again for each value of i=1, i=2, … up through i=N.", "   Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized.", "permutedTest <- …perform test with permutedData… The same test that was performed on the original data, should be performed again but with randomly permuted data.", "The easiest way to permute data is with sample(y-variable-name) inside your test.", "See the Explanation tab for details."],
    "type": "page",
    "page_title": "Permutation Tests"
  },
  {
    "id": 294,
    "title": "📍 Overview",
    "url": "PermutationTests.html#overview",
    "content": "Overview In almost all hypothesis testing scenarios, the null hypothesis can be interpreted as follows. \\(H_0\\): Any pattern that has been witnessed in the sampled data is simply due to random chance. Permutation Tests depend completely on this single idea. If all patterns in the data really are simply due to random chance, then the null hypothesis is true. Further, random re-samples of the data should show similar lack of patterns. However, if the pattern in the data is real, then random re-samples of the data will show very different patterns from the original. Consider the following image. In that image, the toy blocks on the left show a clear pattern or structure. They are nicely organized into colored piles. This suggests a real pattern that is not random. Someone certainly organized those blocks into that pattern. The blocks didn’t land that way by random chance. On the other hand, the pile of toy blocks shown on the right is certainly a random pattern. This is a pattern that would result if the toy blocks were put into a bag, shaken up, and dumped out. This is the idea of the permutation test. If there is structure in the data, then “mixing up the data and dumping it out again” will show very different patterns from the original. However, if the data was just random to begin with, then we would see a similar pattern by “mixing up the data and dumping it out again.” The process of a permutation test is: Compute a test statistic for the original data. Re-sample the data (“shake it up and dump it out”) thousands of times, computing a new test statistic each time, to create a sampling distribution of the test statistic. Compute the p-value of the permutation test as the percentage of test statistics that are as extreme or more extreme than the one originally observed. In review, the sampling distribution is created by permuting (randomly rearranging) the data thousands of times and calculating a test statistic on each permuted version of the data. A histogram of the test statistics then provides the sampling distribution of the test statistic needed to compute the p-value of the original test statistic.",
    "sentences": ["In almost all hypothesis testing scenarios, the null hypothesis can be interpreted as follows.", "\\(H_0\\): Any pattern that has been witnessed in the sampled data is simply due to random chance.", "Permutation Tests depend completely on this single idea.", "If all patterns in the data really are simply due to random chance, then the null hypothesis is true.", "Further, random re-samples of the data should show similar lack of patterns.", "However, if the pattern in the data is real, then random re-samples of the data will show very different patterns from the original.", "Consider the following image.", "In that image, the toy blocks on the left show a clear pattern or structure.", "They are nicely organized into colored piles.", "This suggests a real pattern that is not random."],
    "type": "section",
    "page_title": "Permutation Tests",
    "section_title": "Overview"
  },
  {
    "id": 295,
    "title": "📍 R Instructions",
    "url": "PermutationTests.html#rinstructions",
    "content": "R Instructions Any permutation test can be performed in R with a for loop. #Step 1 Compute a test statistic for the original data. myTest <- …perform the initial test… This could be a t.test, wilcox.test, aov, kruskal.test, lm, glm, chisq.test, or any other R code that results in a test statistic. It could even simply be the mean or standard deviation of the data. observedTestStat <- …get the test statistic… Save the test statistic of your test into the object called observedTestStat. For tests that always result in a single test statistic like a t.test, wilcox.test, kruskal.test, and chisq.test it is myTest$statistic. For an aov, lm, or glm try printing summary(myTest)[] to see what values you are interested in using. observedTestStat Print the value of the test statistic of your test to the screen. This is the value that we now need to use to compute the P-value from by finding the probability that a randomly computed test statistic would be “more extreme” than this originally observed value. #Step 2 Re-sample the data (“shake it up and dump it out”) thousands of times, computing a new test statistic each time, to create a sampling distribution of the test statistic. N <- 2000       N is the number of times you will reuse the data to create the sampling distribution of the test statistic. A typical choice is 2000, but sometimes 10000, or 100000 reuses are needed before useful answers can be obtained. permutedTestStats <-  This is a storage container that will be used to store the test statistics from each of the thousands of reuses of the data. rep(NA, N) The rep() function repeats a given value N times. This particular statement repeats NA’s or “missing values” N times. This gives us N “empty” storage spots inside of permutedTestStats that we can use to store the N test statistics from the N reuses of the data we will make in our for loop. for The for loop is a programming tool that lets us tell R to run a certain code over and over again for a certain number of times.  (i in   In R, the for loop must be followed by a space, then an opening parenthesis, then a variable name (in this case the variable is called “i”), then the word “in” then a list of values. 1:N The 1:N gives R the list of values 1, 2, 3, … and so on all the way up to N. These values are passed into i one at a time and the code inside the for loop is performed first for i=1, then again for i=2, then again for i=3 and so on until finally i=N. At that point, the for loop ends. ) Required closing parenthesis on the for (i in 1:N) statement. { This bracket opens the code section of the for loop. Any code placed between the opening { and closing } brackets will be performed over and over again for each value of i=1, i=2, … up through i=N.    Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized. permutedTest <- …perform test with permutedData… The same test that was performed on the original data, should be performed again but with randomly permuted data. The easiest way to permute data is with sample(y-variable-name) inside your test. See the Explanation tab for details.    Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized. permutedTestStats This is the storage container that was built prior to the for (i in 1:N) code. Inside the for loop, this container is filled value by value using the square brackets [i]. [i] The square brackets [i] allows us to access the “i”th position of permutedTestStats. Remember, since this code is inside of the for loop, i=1 the first time through the code, then i=2 the second time through the code, i=3 the third time through, and so on up until i=N the Nth time through the code.  <- …get test statistic… The test statistic from permutedTest is accessed here and stored into permutedTestStats[i]. } The closing } bracket ends the code that is repeated over and over again inside the for loop. hist(permutedTestStats) Creating a histogram of the sampling distribution of the test statistics obtained from the reused and permuted data allows us to visually compare the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. abline(v=observedTestStat) This adds the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. #Step 3 Compute the p-value of the permutation test as the percentage of test statistics that are as extreme or more extreme than the one originally observed. sum(permutedTestStats >= observedTestStat)/N This computes a “greater than” p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the right hand side of the histogram. sum(permutedTestStats <= observedTestStat)/N This computes a “less than” p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the left hand side of the histogram. myTest <- ...perform the initial test... This could be a t.test, wilcox.test, aov, kruskal.test, lm, glm, chisq.test, or any other R code that results in a test statistic. It could even simply be the mean or standard deviation of the data. observedTestStat <- ...get the test statistic... Save the test statistic of your test into the object called observedTestStat. For tests that always result in a single test statistic like a t.test, wilcox.test, kruskal.test, and chisq.test it is myTest$statistic. For an aov, lm, or glm try printing summary(myTest)[] to see what values you are interested in using. N <- 2000 N is the number of times you will reuse the data to create the sampling distribution of the test statistic. A typical choice is 2000, but sometimes 10000, or 100000 reuses are needed before useful answers can be obtained. permutedTestStats <- rep(NA, N) This is a storage container that will be used to store the test statistics from each of the thousands of reuses of the data. The rep() function repeats a given value N times. This particular statement repeats NA’s or “missing values” N times. This gives us N “empty” storage spots inside of permutedTestStats that we can use to store the N test statistics from the N reuses of the data we will make in our for loop. for (i in 1:N)\\{ The for loop is a programming tool that lets us tell R to run a certain code over and over again for a certain number of times. In R, the for loop must be followed by a space, then an opening parenthesis, then a variable name (in this case the variable is called “i”), then the word “in” then a list of values. The 1:N gives R the list of values 1, 2, 3, … and so on all the way up to N. These values are passed into i one at a time and the code inside the for loop is performed first for i=1, then again for i=2, then again for i=3 and so on until finally i=N. At that point, the for loop ends. There is a required closing parenthesis on the for (i in 1:N) statement. Any code placed between the opening { and closing } brackets will be performed over and over again for each value of i=1, i=2, … up through i=N. Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized. permutedData <- ...randomly permute the data... This is the most important part of the permutation test and takes some thinking. The data must be randomly reorganized in a way consistent with the null hypothesis. What that means exactly is specific to each scenario. Read the Explanation tab for further details on the logic you should use here. permutedTest <- ...perform test with permutedData... The same test that was performed on the original data, should be performed again on the randomly permuted data. permutedTestStats[i] <- ...get test statistic... This is the storage container that was built prior to the for (i in 1:N) code. Inside the for loop, this container is filled value by value using the square brackets [i]. The square brackets [i] allows us to access the “i”th position of permutedTestStats. Remember, since this code is inside of the for loop, i=1 the first time through the code, then i=2 the second time through the code, i=3 the third time through, and so on up until i=N the Nth time through the code. The test statistic from permutedTest is accessed here and stored into permutedTestStats[i]. } The closing } bracket ends the code that is repeated over and over again inside the for loop. hist(permutedTestStats) Creating a histogram of the sampling distribution of the test statistics obtained from the reused and permuted data allows us to visually compare the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. abline(v=observedTestStat) This adds the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. sum(permutedTestStats >= observedTestStat)/N This computes a “greater than” p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the right hand side of the histogram. sum(permutedTestStats <= observedTestStat)/N This computes a “less than” p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the left hand side of the histogram.",
    "sentences": ["Any permutation test can be performed in R with a for loop.", "#Step 1 Compute a test statistic for the original data.", "myTest <- …perform the initial test… This could be a t.test, wilcox.test, aov, kruskal.test, lm, glm, chisq.test, or any other R code that results in a test statistic.", "It could even simply be the mean or standard deviation of the data.", "observedTestStat <- …get the test statistic… Save the test statistic of your test into the object called observedTestStat.", "For tests that always result in a single test statistic like a t.test, wilcox.test, kruskal.test, and chisq.test it is myTest$statistic.", "For an aov, lm, or glm try printing summary(myTest)[] to see what values you are interested in using.", "observedTestStat Print the value of the test statistic of your test to the screen.", "This is the value that we now need to use to compute the P-value from by finding the probability that a randomly computed test statistic would be “more extreme” than this originally observed value.", "#Step 2 Re-sample the data (“shake it up and dump it out”) thousands of times, computing a new test statistic each time, to create a sampling distribution of the test statistic."],
    "type": "section",
    "page_title": "Permutation Tests",
    "section_title": "R Instructions"
  },
  {
    "id": 296,
    "title": "📍 Explanation",
    "url": "PermutationTests.html#explanation",
    "content": "Explanation The most difficult part of a permutation test is in the random permuting of the data. How the permuting is performed depends on the type of hypothesis test being performed. It is important to remember that the permutation test only changes the way the p-value is calculated. Everything else about the original test is unchanged when switching to a permutation test. Independent Samples t Test For the independent sample t Test, we will use the data from the independent sleep analysis. In that analysis, we were using the sleep data to test the hypotheses: \\[ H_0: \\mu_\\text{Extra Hours of Sleep with Drug 1} - \\mu_\\text{Extra Hours of Sleep with Drug 2} = 0 \\] \\[ H_a: \\mu_\\text{Extra Hours of Sleep with Drug 1} - \\mu_\\text{Extra Hours of Sleep with Drug 2} \\neq 0 \\] We used a significance level of \\(\\alpha = 0.05\\) and obtained a P-value of \\(0.07939\\). Let’s demonstrate how a permutation test could be used to obtain this same p-value. (Technically you only need to use a permutation test when the requirements of the original test were not satisfied. However, it is also reasonable to perform a permutation test anytime you want. No requirements need to be checked when performing a permutation test.) # First run the initial test and gain the test statistic: myTest <- t.test(extra ~ group, data = sleep, mu = 0) observedTestStat <- myTest$statistic # Now we run the permutations to create a distribution of test statistics N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- t.test(sample(extra) ~ group, data = sleep, mu = 0) permutedTestStats[i] <- permutedTest$statistic } # Now we show a histogram of that distribution hist(permutedTestStats, col = \"skyblue\") abline(v = observedTestStat, col = \"red\", lwd = 3) #Greater-Than p-value: Not the correct one in this case sum(permutedTestStats >= observedTestStat)/N # Less-Than p-value: Not the correct one for this data sum(permutedTestStats <= observedTestStat)/N # Two-Sided p-value: This is the one we want based on our alternative hypothesis. 2*sum(permutedTestStats <= observedTestStat)/N Note The Wilcoxon Rank Sum test is run using the same code except with myTest <- wilcox.test(y ~ x, data=...) instead of t.test(...) in both Step’s 1 and 2. Other Examples Paired Samples t Test (and Wilcoxon Signed-Rank) (click to show/hide) Paired Data Example See the Sleep Paired t Test example for the background and context of the study. Here is how to perform the test as a permutation test instead of a t test. The question that this sleep data can answer concerns which drug is more effective at increasing the amount of extra sleep an individual receives. The associated hypotheses would be \\[ H_0: \\mu_d = 0 \\] \\[ H_a: \\mu_d \\neq 0 \\] where \\(\\mu_d\\) denotes the true mean of the differences between the observations for each drug obtained from each individual. Differences would be obtained by \\(d_i = \\text{extra}_{1i} - \\text{extra}_{2i}\\). To perform a permutation test of the hypothesis that the drugs are equally effective, we use the following code. # Perform the initial test: myTest <- with(sleep, t.test(extra[group==1], extra[group==2], paired = TRUE, mu = 0)) # Get the test statistic from the test: observedTestStat <- myTest$statistic # Obtain the permutation sampling distribution N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permuteValues <- sample(c(-1,1), size=10, replace=TRUE) permutedTest <- with(sleep, t.test(permuteValues*(extra[group==1] - extra[group==2]), mu = 0)) #Note, t.test(group1 - group2) is the same as t.test(group1, group2, paired=TRUE). permutedTestStats[i] <- permutedTest$statistic } hist(permutedTestStats) abline(v=observedTestStat, col='skyblue', lwd=3) # Greater than p-value: (not what we want here) sum(permutedTestStats >= observedTestStat)/N ## [1] 1 # Less than p-value: sum(permutedTestStats <= observedTestStat)/N ## [1] 0.002 # Correct two sided p-value for this study: 2*sum(permutedTestStats <= observedTestStat)/N ## [1] 0.004 Note: ANOVA (click to show/hide) One-Way ANOVA For this example, we will use the data from the chick weights analysis. # Again, we run the initial test and find the test statistic myTest <- aov(weight ~ feed, data = chickwts) observedTestStat <- summary(myTest)[[1]]$`F value`[1] # For this permutation, we need to shake up the groups similar to the Independent Sample example N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- aov(sample(weight) ~ feed, data = chickwts) permutedTestStats[i] <- summary(permutedTest)[[1]]$`F value`[1] } # The histogram of this distribution gives an interesting insight into the results hist(permutedTestStats, col = \"skyblue\", xlim = c(0,16)) abline(v = observedTestStat, col = \"red\", lwd = 3) # Here is the greater-than p-value (since the F-distribution is right skewed # this is the only p-value of interest.) sum(permutedTestStats >= observedTestStat)/N Two-Way ANOVA For the two-way ANOVA, I will use the data from the warpbreaks analysis. # The initial test is done in the same way as one-way ANOVA but there is a little more to find the test statistic myTest <- aov(breaks ~ wool + tension + wool:tension, data=warpbreaks) # This first test statistic is the comparison between the two types of wool observedTestStatW <- summary(myTest)[[1]]$`F value`[1] # This second test statistic is the comparison between the three types of tension observedTestStatT <- summary(myTest)[[1]]$`F value`[2] # The third test statistic is the comparison of the interaction of wool types and tension observedTestStatWT <- summary(myTest)[[1]]$`F value`[3] # Now comes three different permutations for the test. First is for wool, second is for tension, and third is the interaction N <- 2000 permutedTestStatsW <- rep(NA, N) permutedTestStatsT <- rep(NA, N) permutedTestStatsWT <- rep(NA, N) for (i in 1:N){ permutedTest <- aov(sample(breaks) ~ wool + tension + wool:tension, data=warpbreaks) permutedTestStatsW[i] <- summary(permutedTest)[[1]]$`F value`[1] permutedTestStatsT[i] <- summary(permutedTest)[[1]]$`F value`[2] permutedTestStatsWT[i] <- summary(permutedTest)[[1]]$`F value`[3] } # We likewise need three differenct plots to show the distribution. First is wool, second is tension, and third is the interaction hist(permutedTestStatsW, col = \"skyblue\", xlim = c(3,14)) abline(v = observedTestStatW, col = \"red\", lwd = 3) hist(permutedTestStatsT, col = \"skyblue\") abline(v = observedTestStatT, col = \"red\", lwd = 3) hist(permutedTestStatsWT, col = \"skyblue\") abline(v = observedTestStatWT, col = \"red\", lwd = 3) # Greater-than p-value: the three situations are in order sum(permutedTestStatsW >= observedTestStatW)/N sum(permutedTestStatsT >= observedTestStatT)/N sum(permutedTestStatsWT >= observedTestStatWT)/N # Less-than p-value: again, they are in order sum(permutedTestStatsW <= observedTestStatW)/N sum(permutedTestStatsT <= observedTestStatT)/N sum(permutedTestStatsWT <= observedTestStatWT)/N # Two-sided p-values: 2*sum(permutedTestStatsW >= observedTestStatW)/N 2*sum(permutedTestStatsT >= observedTestStatT)/N 2*sum(permutedTestStatsWT >= observedTestStatWT)/N Simple Linear Regression (click to show/hide) For this example, I will use the trees dataset to compare the Girth and Height of black cherry trees. # The test and then the test statistic is found in a similar way to that of an ANOVA (this is the t statistic) myTest <- lm(Height ~ Girth, data = trees) observedTestStat <- summary(myTest)[[4]][2,3] # The permutation part is set up in this way N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- lm(sample(Height) ~ Girth, data = trees) permutedTestStats[i] <- summary(permutedTest)[[4]][2,3] } # Here, as before, is the histogram of the distribution of the test statistics hist(permutedTestStats, col = \"skyblue\") abline(v = observedTestStat, col = \"red\", lwd = 3) # Less-than p-value: sum(permutedTestStats <= observedTestStat)/N # Greater-than p-value: sum(permutedTestStats >= observedTestStat)/N # Two-Sided p-value: 2*sum(permutedTestStats >= observedTestStat)/N",
    "sentences": ["The most difficult part of a permutation test is in the random permuting of the data.", "How the permuting is performed depends on the type of hypothesis test being performed.", "It is important to remember that the permutation test only changes the way the p-value is calculated.", "Everything else about the original test is unchanged when switching to a permutation test.", "Independent Samples t Test For the independent sample t Test, we will use the data from the independent sleep analysis.", "In that analysis, we were using the sleep data to test the hypotheses: \\[ H_0: \\mu_\\text{Extra Hours of Sleep with Drug 1} - \\mu_\\text{Extra Hours of Sleep with Drug 2} = 0 \\] \\[ H_a: \\mu_\\text{Extra Hours of Sleep with Drug 1} - \\mu_\\text{Extra Hours of Sleep with Drug 2} \\neq 0 \\] We used a significance level of \\(\\alpha = 0.05\\) and obtained a P-value of \\(0.07939\\).", "Let’s demonstrate how a permutation test could be used to obtain this same p-value.", "(Technically you only need to use a permutation test when the requirements of the original test were not satisfied.", "However, it is also reasonable to perform a permutation test anytime you want.", "No requirements need to be checked when performing a permutation test.) # First run the initial test and gain the test statistic: myTest <- t.test(extra ~ group, data = sleep, mu = 0) observedTestStat <- myTest$statistic # Now we run the permutations to create a distribution of test statistics N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- t.test(sample(extra) ~ group, data = sleep, mu = 0) permutedTestStats[i] <- permutedTest$statistic } # Now we show a histogram of that distribution hist(permutedTestStats, col = \"skyblue\") abline(v = observedTestStat, col = \"red\", lwd = 3) #Greater-Than p-value: Not the correct one in this case sum(permutedTestStats >= observedTestStat)/N # Less-Than p-value: Not the correct one for this data sum(permutedTestStats <= observedTestStat)/N # Two-Sided p-value: This is the one we want based on our alternative hypothesis."],
    "type": "section",
    "page_title": "Permutation Tests",
    "section_title": "Explanation"
  },
  {
    "id": 297,
    "title": "R Commands",
    "url": "RCommands.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus R Commands “For the things we have to learn before we can do them, we learn by doing them.” ― Aristotle, The Nicomachean Ethics Getting Started Hover your mouse here to begin. This book requires that you interact with it to learn. Hovering is the first step. Now click right here on these words to get started. The Help Command Getting help in R is easy. Usage ?something This command pulls up the help file for whatever you write in the place of something. Examples Click to view. Hover to learn. The quick way to access the help function in R. cars The name of a dataset can be typed to open the help file for that dataset.     Press Enter to run the code.  Click to Show Output  Click to View Output. The quick way to access the help function in R. data The name of an R function, like data can also be used to open the help file for that function.     Press Enter to run the code.  Click to Show Output  Click to View Output. The quick way to access the help function in R. mean The mean function computes the mean of a column of quantitative data. Typing the name of an R function, like mean can also be used to open the help file for that function.     Press Enter to run the code.  Click to Show Output  Click to View Output. $ The Selection Operator Once you have a dataset, you need to be able to access columns from it. Usage DataSetName$ColumnName The $ operator allows you to access the individual columns of a dataset. Tip: think of the data set as a “store” from which you “purchase” a column using “money”: $. Example Code airquality The airqaulity dataset. This could be the name of any dataset instead of airquality. $ Grabs the column, or variable, from the dataset to be used. This is typically used when computing say the mean (or other statistic) of a single column of the data. Wind The name of any column of the dataset can be entered after the dollar sign. In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality).     Press Enter to run the code.  Click to Show Output  Click to View Output. This allows you to compute things about that column, like the mean or standard deviation. mean( The mean function computes the mean of a column of quantitative data. airquality The airquality dataset. This could be the name of any dataset instead of airquality. $ Grabs the column, or variable, from the dataset to be used. This is typically used when computing say the mean (or other statistic) of a single column of the data. Wind The name of any column of the dataset can be entered after the dollar sign. In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality). ) Closing parenthesis to the mean() function.     Press Enter to run the code.  Click to Show Output  Click to View Output. 9.958 sd( The sd function computes the standard deviation of a column of quantitative data. airquality The airqaulity dataset. This could be the name of any dataset instead of airquality. $ Grabs the column, or variable, from the dataset to be used. This is typically used when computing say the mean (or other statistic) of a single column of the data. Wind The name of any column of the dataset can be entered after the dollar sign. In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality). ) Closing parenthesis to the sd() function.     Press Enter to run the code.  Click to Show Output  Click to View Output. 3.523 See Numerical Summaries for more stats functions like mean() and sd(). <- The Assignment Operator Being able to save your work is important! Usage    Keyboard Shortcut: Alt - NameYouCreate <- some R commands <- (Less than symbol < with a hyphen -) is called the assignment operator and lets you store the results of the some R commands into an object called NameYouCreate. NameYouCreate is any name that begins with a letter, but can use numbers, periods, and underscores thereafter. To use spaces in the name, you must use `your Name` encased in back-ticks, but this is not recommended. Example Code cars2 First we name the object we are creating. In this case, we are making a copy of the cars dataset, so it is logical to call it cars2, but it could be bob, c2 or any name you wanted to use. Just be careful to not use names that are already in use!   <-   The <- assignment operator will take whatever is on the right hand side and save it into the name written on the left hand side. cars In this case the cars dataset is being copied to cars2 so that we can change cars2 without changing the original cars dataset.     Press Enter to run the code. cars2 The new copy of the cars dataset that we just created $ftpersec The $ selection operator can be used to create a new column in a dataset when used with the <- assignment operator.  <-  The <- assignment operator will take the results of the right-hand-side and save them into the name on the left-hand-side. cars2$speed * 5280 / 3600 This calculation converts the miles per hour of the cars2 speed column into feet per seconds because there are 5280 feet in a mile and 60 minutes in an hour and 60 seconds in a minute. View(cars2) The cars2 dataset now contains a 3rd column called feetpersec. Compare this to the original cars dataset to see how it changed.  Click to Show Output  Click to View Output. c( ) The Combine Function Think of this function as the “back-pack” function, just like putting different books into one back-pack. Usage c(value 1, value 2, value 3, ... ) The c( ) function combines values into a single object called a “vector”. values 1, 2, 3, ... can be numbers or characters, i.e., words, but must be all of one type or the other. Example Code Classlist <- Classlist is a new object being created using the assignment operator <- that will contain the four names listed above.  c( The combine function c( ) is being used in this case to group character values representing names of students into a single object named “Classlist”. “Jackson”, “Jared”, “Jill”, “Jane”) These are the values we are grouping into the object named Classlist.     Press Enter to run the code. Ages <-   The assignment operator <- is being used to create the object called Ages that will contain the ages of each student on the Classlist. c( The R function “c()” allows us to group together values in order to save them into an object. 8, 9, 7, 8 The values, separated by comma’s, that are being grouped together. In this case, numbers are being grouped together. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code. Colors <-   The assignment operator <- is being used to create the object called Colors that will have one color for each student on the Classlist. c( The R function “c()” allows us to group together values in order to save them into an object. “red”, “blue”, “green”, “yellow” The values, separated by comma’s, that are being grouped together. In this case, characters are being grouped together. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. table( ) This is a way to quickly count how many times each value occurs in a column or columns. Usage table(NameOfDataset$columnName) table(NameOfDataset$columnName1, NameOfDataset$columnName2) The table( ) function counts how many times each value in a column of data occurs. NameOfDataset is the ane of a data set, like cars or airquality or KidsFeet. columnName is the name of a column from the data set. columnName1 and columnName2 are two different names of columns from the data set. Example Code speedCounts <-speedCounts is a new object being created using the assignment operator <- that will contain the counts of how many times each “speed” occurs in the cars data set speed column.  table( The table function table( ) is being used in this case to count how many times each speed occurs in the cars data set speed column. cars This is the name of the data set. $ The $ is used to access a given column from the data set. speed This is the name of the column we are interested in from the cars data set. ) Always close off your functions in R with a closing parathesis. speedCounts Typing the name of an object will print the results to the screen.     Press Enter to run the code.  Click to Show Output  Click to View Output. ## ## 4 7 8 9 10 11 12 13 14 15 16 17 18 19 20 22 ## 2 2 1 1 3 2 4 4 4 3 2 3 4 3 5 1 ## 23 24 25 ## 1 4 1 Notice how the speed of “4” occurs 2 times, same for the speed of 7, but the speed of 8 only occurs 1 time and so on with the other speeds. The first row of the output is the value from the speed column. The number on the second line shows how many times that value occurred in the speed column. library(mosaic) library(mosaic) is needed to access the KidsFeet data set that is used in this example. If you don’t have the mosaic library, you will need to run install.packages(\"mosaic\") to install it first. From then on, you can open mosaic to use it with the command library(mosaic). You need only install packages once. You must library them each time you wish to use them. birthdays <-birthdays is a new object being created using the assignment operator <- that will contain the counts of how many birthdays occur in each month for each gender in the KidsFeet dataset.  table( The table function table( ) is being used in this case to count how many birthdays occur in each month for children of each gender. KidsFeet This is the name of the data set. $ The $ is used to access a given column from the data set. sex This is the name of the column we are interested in becoming the rows of our final table. ,  Comma separating the two columns of the data set you want to table. KidsFeet This is the name of the data set. $ The $ is used to access a given column from the data set. birthmonth This is the name of the column we are interested in becoming the columns of our final table. ) Always close off your functions in R with a closing parathesis. birthdays Typing the name of an object will print the results to the screen.     Press Enter to run the code.  Click to Show Output  Click to View Output. ## ## 1 2 3 4 5 6 7 8 9 10 11 12 ## B 1 2 3 2 1 1 2 2 2 1 1 2 ## G 1 1 5 1 1 3 1 0 3 1 1 1 The left column contains the “sex” values of “B” and “G” (Boy and Girl). The top row contains the birthmonths (1 through 12). The numbers within the row of the table next to the “B” show how many Boys had birthdays in each month of the year. The numbers within the row of the table next to the “G” show how many Girls had birthdays in each month of the year. filter( ) Used to reduce a dataset to a smaller set of rows than the original dataset contained. Usage filter(NameOfDataset, columnName filteringRules) filter() is the function that filters out certain rows of the dataset. NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet. columnName is the name of one of the columns from the dataset. You can use colnames(NameOfDataset) or View(NameOfDataset) to see the names. filteringRules consists of some Logical Expression (see table below) that selects only the rows from the original dataset that meet the criterion. Filtering Rule Logical Expression Equals one “thing” columnName == something Equals Any Of Several “things” columnName %in% c(something1,something2,...) Not Equal (one thing) columnName != something Not Equals Any of (several things) !columnName %in% c(something1,something2,...) Less Than columnName < value Less Then or Equal to columnName <= value Greater Than columnName > value Greater Than or Equal to columnName >= value AND expression1 & expression2 OR expression1 | expression2 Equals NA is.na(columnName) Not NA !is.na(columnName) Example Code library(tidyverse) The tidyverse library is needed to access the filter function used in the following example codes. library(mosaic) The mosaic library is needed to access the KidsFeet data set used in the following example codes. Equals one “thing”… Kids87 <-  Kids87 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. birthyear A quantitative column of the KidsFeet dataset that we want to use to reduce the dataset.  == 87 This “filtering rule” filters the data down to just those children who had a birthyear equal to 87. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. KidsBoys <-  KidsBoys is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. sex A categorical column of the KidsFeet dataset that we want to use to reduce the dataset.  == “B” This “filtering rule” filters the data down to just those children who are boys. Words must be quoted “B” but values are just typed directly. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. Equals Any of Several “things”… KidsSummer <-  KidsSummer is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. birthmonth ",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus R Commands “For the things we have to learn before we can do them, we learn by doing them.” ― Aristotle, The Nicomachean Ethics Getting Started Hover your mouse here to begin.", "This book requires that you interact with it to learn.", "Hovering is the first step.", "Now click right here on these words to get started.", "The Help Command Getting help in R is easy.", "Usage ?something This command pulls up the help file for whatever you write in the place of something.", "Examples Click to view.", "Hover to learn.", "The quick way to access the help function in R.", "cars The name of a dataset can be typed to open the help file for that dataset.", "    Press Enter to run the code.", " Click to Show Output  Click to View Output.", "The quick way to access the help function in R.", "data The name of an R function, like data can also be used to open the help file for that function.", "    Press Enter to run the code.", " Click to Show Output  Click to View Output.", "The quick way to access the help function in R.", "mean The mean function computes the mean of a column of quantitative data.", "Typing the name of an R function, like mean can also be used to open the help file for that function.", "    Press Enter to run the code.", " Click to Show Output  Click to View Output.", "$ The Selection Operator Once you have a dataset, you need to be able to access columns from it.", "Usage DataSetName$ColumnName The $ operator allows you to access the individual columns of a dataset.", "Tip: think of the data set as a “store” from which you “purchase” a column using “money”: $.", "Example Code airquality The airqaulity dataset.", "This could be the name of any dataset instead of airquality.", "$ Grabs the column, or variable, from the dataset to be used.", "This is typically used when computing say the mean (or other statistic) of a single column of the data.", "Wind The name of any column of the dataset can be entered after the dollar sign.", "In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality).", "    Press Enter to run the code.", " Click to Show Output  Click to View Output.", "This allows you to compute things about that column, like the mean or standard deviation.", "mean( The mean function computes the mean of a column of quantitative data.", "airquality The airquality dataset.", "This could be the name of any dataset instead of airquality.", "$ Grabs the column, or variable, from the dataset to be used.", "This is typically used when computing say the mean (or other statistic) of a single column of the data.", "Wind The name of any column of the dataset can be entered after the dollar sign.", "In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality).", ") Closing parenthesis to the mean() function.", "    Press Enter to run the code.", " Click to Show Output  Click to View Output.", "9.958 sd( The sd function computes the standard deviation of a column of quantitative data.", "airquality The airqaulity dataset.", "This could be the name of any dataset instead of airquality.", "$ Grabs the column, or variable, from the dataset to be used.", "This is typically used when computing say the mean (or other statistic) of a single column of the data.", "Wind The name of any column of the dataset can be entered after the dollar sign.", "In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality)."],
    "type": "page",
    "page_title": "R Commands"
  },
  {
    "id": 298,
    "title": "📍 Getting Started",
    "url": "RCommands.html#gettingstarted",
    "content": "Getting Started Hover your mouse here to begin. This book requires that you interact with it to learn. Hovering is the first step. Now click right here on these words to get started.",
    "sentences": ["Hover your mouse here to begin.", "This book requires that you interact with it to learn.", "Hovering is the first step.", "Now click right here on these words to get started."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "Getting Started"
  },
  {
    "id": 299,
    "title": "📍 ? The Help Command",
    "url": "RCommands.html#thehelpcommand",
    "content": "? The Help Command Getting help in R is easy. Usage ?something This command pulls up the help file for whatever you write in the place of something. Examples Click to view. Hover to learn. The quick way to access the help function in R. cars The name of a dataset can be typed to open the help file for that dataset.     Press Enter to run the code.  Click to Show Output  Click to View Output. The quick way to access the help function in R. data The name of an R function, like data can also be used to open the help file for that function.     Press Enter to run the code.  Click to Show Output  Click to View Output. The quick way to access the help function in R. mean The mean function computes the mean of a column of quantitative data. Typing the name of an R function, like mean can also be used to open the help file for that function.     Press Enter to run the code.  Click to Show Output  Click to View Output.",
    "sentences": ["Getting help in R is easy.", "Usage ?something This command pulls up the help file for whatever you write in the place of something.", "Examples Click to view.", "Hover to learn.", "The quick way to access the help function in R.", "cars The name of a dataset can be typed to open the help file for that dataset.", "    Press Enter to run the code.", " Click to Show Output  Click to View Output.", "The quick way to access the help function in R.", "data The name of an R function, like data can also be used to open the help file for that function."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "? The Help Command"
  },
  {
    "id": 300,
    "title": "📍 $ The Selection Operator",
    "url": "RCommands.html#theselectionoperator",
    "content": "$ The Selection Operator Once you have a dataset, you need to be able to access columns from it. Usage DataSetName$ColumnName The $ operator allows you to access the individual columns of a dataset. Tip: think of the data set as a “store” from which you “purchase” a column using “money”: $. Example Code airquality The airqaulity dataset. This could be the name of any dataset instead of airquality. $ Grabs the column, or variable, from the dataset to be used. This is typically used when computing say the mean (or other statistic) of a single column of the data. Wind The name of any column of the dataset can be entered after the dollar sign. In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality).     Press Enter to run the code.  Click to Show Output  Click to View Output. This allows you to compute things about that column, like the mean or standard deviation. mean( The mean function computes the mean of a column of quantitative data. airquality The airquality dataset. This could be the name of any dataset instead of airquality. $ Grabs the column, or variable, from the dataset to be used. This is typically used when computing say the mean (or other statistic) of a single column of the data. Wind The name of any column of the dataset can be entered after the dollar sign. In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality). ) Closing parenthesis to the mean() function.     Press Enter to run the code.  Click to Show Output  Click to View Output. 9.958 sd( The sd function computes the standard deviation of a column of quantitative data. airquality The airqaulity dataset. This could be the name of any dataset instead of airquality. $ Grabs the column, or variable, from the dataset to be used. This is typically used when computing say the mean (or other statistic) of a single column of the data. Wind The name of any column of the dataset can be entered after the dollar sign. In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality). ) Closing parenthesis to the sd() function.     Press Enter to run the code.  Click to Show Output  Click to View Output. 3.523 See Numerical Summaries for more stats functions like mean() and sd().",
    "sentences": ["Once you have a dataset, you need to be able to access columns from it.", "Usage DataSetName$ColumnName The $ operator allows you to access the individual columns of a dataset.", "Tip: think of the data set as a “store” from which you “purchase” a column using “money”: $.", "Example Code airquality The airqaulity dataset.", "This could be the name of any dataset instead of airquality.", "$ Grabs the column, or variable, from the dataset to be used.", "This is typically used when computing say the mean (or other statistic) of a single column of the data.", "Wind The name of any column of the dataset can be entered after the dollar sign.", "In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality).", "    Press Enter to run the code."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "$ The Selection Operator"
  },
  {
    "id": 301,
    "title": "📍 <- The Assignment Operator",
    "url": "RCommands.html#theassignmentoperator",
    "content": "<- The Assignment Operator Being able to save your work is important! Usage    Keyboard Shortcut: Alt - NameYouCreate <- some R commands <- (Less than symbol < with a hyphen -) is called the assignment operator and lets you store the results of the some R commands into an object called NameYouCreate. NameYouCreate is any name that begins with a letter, but can use numbers, periods, and underscores thereafter. To use spaces in the name, you must use `your Name` encased in back-ticks, but this is not recommended. Example Code cars2 First we name the object we are creating. In this case, we are making a copy of the cars dataset, so it is logical to call it cars2, but it could be bob, c2 or any name you wanted to use. Just be careful to not use names that are already in use!   <-   The <- assignment operator will take whatever is on the right hand side and save it into the name written on the left hand side. cars In this case the cars dataset is being copied to cars2 so that we can change cars2 without changing the original cars dataset.     Press Enter to run the code. cars2 The new copy of the cars dataset that we just created $ftpersec The $ selection operator can be used to create a new column in a dataset when used with the <- assignment operator.  <-  The <- assignment operator will take the results of the right-hand-side and save them into the name on the left-hand-side. cars2$speed * 5280 / 3600 This calculation converts the miles per hour of the cars2 speed column into feet per seconds because there are 5280 feet in a mile and 60 minutes in an hour and 60 seconds in a minute. View(cars2) The cars2 dataset now contains a 3rd column called feetpersec. Compare this to the original cars dataset to see how it changed.  Click to Show Output  Click to View Output.",
    "sentences": ["Being able to save your work is important!", "Usage    Keyboard Shortcut: Alt - NameYouCreate <- some R commands <- (Less than symbol < with a hyphen -) is called the assignment operator and lets you store the results of the some R commands into an object called NameYouCreate.", "NameYouCreate is any name that begins with a letter, but can use numbers, periods, and underscores thereafter.", "To use spaces in the name, you must use `your Name` encased in back-ticks, but this is not recommended.", "Example Code cars2 First we name the object we are creating.", "In this case, we are making a copy of the cars dataset, so it is logical to call it cars2, but it could be bob, c2 or any name you wanted to use.", "Just be careful to not use names that are already in use!", "  <-   The <- assignment operator will take whatever is on the right hand side and save it into the name written on the left hand side.", "cars In this case the cars dataset is being copied to cars2 so that we can change cars2 without changing the original cars dataset.", "    Press Enter to run the code."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "<- The Assignment Operator"
  },
  {
    "id": 302,
    "title": "📍 c( ) The Combine Function",
    "url": "RCommands.html#cthecombinefunction",
    "content": "c( ) The Combine Function Think of this function as the “back-pack” function, just like putting different books into one back-pack. Usage c(value 1, value 2, value 3, ... ) The c( ) function combines values into a single object called a “vector”. values 1, 2, 3, ... can be numbers or characters, i.e., words, but must be all of one type or the other. Example Code Classlist <- Classlist is a new object being created using the assignment operator <- that will contain the four names listed above.  c( The combine function c( ) is being used in this case to group character values representing names of students into a single object named “Classlist”. “Jackson”, “Jared”, “Jill”, “Jane”) These are the values we are grouping into the object named Classlist.     Press Enter to run the code. Ages <-   The assignment operator <- is being used to create the object called Ages that will contain the ages of each student on the Classlist. c( The R function “c()” allows us to group together values in order to save them into an object. 8, 9, 7, 8 The values, separated by comma’s, that are being grouped together. In this case, numbers are being grouped together. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code. Colors <-   The assignment operator <- is being used to create the object called Colors that will have one color for each student on the Classlist. c( The R function “c()” allows us to group together values in order to save them into an object. “red”, “blue”, “green”, “yellow” The values, separated by comma’s, that are being grouped together. In this case, characters are being grouped together. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output.",
    "sentences": ["Think of this function as the “back-pack” function, just like putting different books into one back-pack.", "Usage c(value 1, value 2, value 3, ...", ") The c( ) function combines values into a single object called a “vector”.", "values 1, 2, 3, ...", "can be numbers or characters, i.e., words, but must be all of one type or the other.", "Example Code Classlist <- Classlist is a new object being created using the assignment operator <- that will contain the four names listed above.", " c( The combine function c( ) is being used in this case to group character values representing names of students into a single object named “Classlist”.", "“Jackson”, “Jared”, “Jill”, “Jane”) These are the values we are grouping into the object named Classlist.", "    Press Enter to run the code.", "Ages <-   The assignment operator <- is being used to create the object called Ages that will contain the ages of each student on the Classlist."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "c( ) The Combine Function"
  },
  {
    "id": 303,
    "title": "📍 table( )",
    "url": "RCommands.html#table",
    "content": "table( ) This is a way to quickly count how many times each value occurs in a column or columns. Usage table(NameOfDataset$columnName) table(NameOfDataset$columnName1, NameOfDataset$columnName2) The table( ) function counts how many times each value in a column of data occurs. NameOfDataset is the ane of a data set, like cars or airquality or KidsFeet. columnName is the name of a column from the data set. columnName1 and columnName2 are two different names of columns from the data set. Example Code speedCounts <-speedCounts is a new object being created using the assignment operator <- that will contain the counts of how many times each “speed” occurs in the cars data set speed column.  table( The table function table( ) is being used in this case to count how many times each speed occurs in the cars data set speed column. cars This is the name of the data set. $ The $ is used to access a given column from the data set. speed This is the name of the column we are interested in from the cars data set. ) Always close off your functions in R with a closing parathesis. speedCounts Typing the name of an object will print the results to the screen.     Press Enter to run the code.  Click to Show Output  Click to View Output. ## ## 4 7 8 9 10 11 12 13 14 15 16 17 18 19 20 22 ## 2 2 1 1 3 2 4 4 4 3 2 3 4 3 5 1 ## 23 24 25 ## 1 4 1 Notice how the speed of “4” occurs 2 times, same for the speed of 7, but the speed of 8 only occurs 1 time and so on with the other speeds. The first row of the output is the value from the speed column. The number on the second line shows how many times that value occurred in the speed column. library(mosaic) library(mosaic) is needed to access the KidsFeet data set that is used in this example. If you don’t have the mosaic library, you will need to run install.packages(\"mosaic\") to install it first. From then on, you can open mosaic to use it with the command library(mosaic). You need only install packages once. You must library them each time you wish to use them. birthdays <-birthdays is a new object being created using the assignment operator <- that will contain the counts of how many birthdays occur in each month for each gender in the KidsFeet dataset.  table( The table function table( ) is being used in this case to count how many birthdays occur in each month for children of each gender. KidsFeet This is the name of the data set. $ The $ is used to access a given column from the data set. sex This is the name of the column we are interested in becoming the rows of our final table. ,  Comma separating the two columns of the data set you want to table. KidsFeet This is the name of the data set. $ The $ is used to access a given column from the data set. birthmonth This is the name of the column we are interested in becoming the columns of our final table. ) Always close off your functions in R with a closing parathesis. birthdays Typing the name of an object will print the results to the screen.     Press Enter to run the code.  Click to Show Output  Click to View Output. ## ## 1 2 3 4 5 6 7 8 9 10 11 12 ## B 1 2 3 2 1 1 2 2 2 1 1 2 ## G 1 1 5 1 1 3 1 0 3 1 1 1 The left column contains the “sex” values of “B” and “G” (Boy and Girl). The top row contains the birthmonths (1 through 12). The numbers within the row of the table next to the “B” show how many Boys had birthdays in each month of the year. The numbers within the row of the table next to the “G” show how many Girls had birthdays in each month of the year.",
    "sentences": ["This is a way to quickly count how many times each value occurs in a column or columns.", "Usage table(NameOfDataset$columnName) table(NameOfDataset$columnName1, NameOfDataset$columnName2) The table( ) function counts how many times each value in a column of data occurs.", "NameOfDataset is the ane of a data set, like cars or airquality or KidsFeet.", "columnName is the name of a column from the data set.", "columnName1 and columnName2 are two different names of columns from the data set.", "Example Code speedCounts <-speedCounts is a new object being created using the assignment operator <- that will contain the counts of how many times each “speed” occurs in the cars data set speed column.", " table( The table function table( ) is being used in this case to count how many times each speed occurs in the cars data set speed column.", "cars This is the name of the data set.", "$ The $ is used to access a given column from the data set.", "speed This is the name of the column we are interested in from the cars data set."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "table( )"
  },
  {
    "id": 304,
    "title": "📍 filter( )",
    "url": "RCommands.html#filter",
    "content": "filter( ) Used to reduce a dataset to a smaller set of rows than the original dataset contained. Usage filter(NameOfDataset, columnName filteringRules) filter() is the function that filters out certain rows of the dataset. NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet. columnName is the name of one of the columns from the dataset. You can use colnames(NameOfDataset) or View(NameOfDataset) to see the names. filteringRules consists of some Logical Expression (see table below) that selects only the rows from the original dataset that meet the criterion. Filtering Rule Logical Expression Equals one “thing” columnName == something Equals Any Of Several “things” columnName %in% c(something1,something2,...) Not Equal (one thing) columnName != something Not Equals Any of (several things) !columnName %in% c(something1,something2,...) Less Than columnName < value Less Then or Equal to columnName <= value Greater Than columnName > value Greater Than or Equal to columnName >= value AND expression1 & expression2 OR expression1 | expression2 Equals NA is.na(columnName) Not NA !is.na(columnName) Example Code library(tidyverse) The tidyverse library is needed to access the filter function used in the following example codes. library(mosaic) The mosaic library is needed to access the KidsFeet data set used in the following example codes. Equals one “thing”… Kids87 <-  Kids87 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. birthyear A quantitative column of the KidsFeet dataset that we want to use to reduce the dataset.  == 87 This “filtering rule” filters the data down to just those children who had a birthyear equal to 87. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. KidsBoys <-  KidsBoys is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. sex A categorical column of the KidsFeet dataset that we want to use to reduce the dataset.  == “B” This “filtering rule” filters the data down to just those children who are boys. Words must be quoted “B” but values are just typed directly. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. Equals Any of Several “things”… KidsSummer <-  KidsSummer is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. birthmonth The column of the KidsFeet dataset that we want to use to reduce the dataset.  %in% c(6,7,8) This is the “filtering rule”. It will filter the data down to just those children who were born during the summer, i.e., birthmonth equal to either 6, 7, or 8. Notice how the c( ) function is being used to combine the values of 6, 7, and 8 together into a single list of numbers. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. Does not equal one thing… KidsNotJosh <-  KidsNotJosh is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. name The column of the KidsFeet dataset that we want to use to reduce the dataset.  != “Josh” This is the “filtering rule”. It will filter the data down to just those children who are NOT named “Josh”. In this case, it removed just two students who were named “Josh”. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. Less than… KidsLength24 <-  KidsLength24 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. length The column of the KidsFeet dataset that we want to use to reduce the dataset.  < 24 This is the “filtering rule”. It will filter the data down to just those children who have a foot length less than 24 cm. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. Less than or equal to… KidsLessEq24 <-  KidsLessEq24 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. length The column of the KidsFeet dataset that we want to use to reduce the dataset.  <= 24 This is the “filtering rule”. It will filter the data down to just those children who have a foot length less than or equal to 24 cm. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. Greater than… KidsWider9 <-  KidsNotJosh is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. width The column of the KidsFeet dataset that we want to use to reduce the dataset.  > 9 This is the “filtering rule”. It will filter the data down to just those children who have a foot width greater than 9 cm. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. Greater than or equal to… KidsWiderEq9 <-  KidsWiderEq9 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. width The column of the KidsFeet dataset that we want to use to reduce the dataset.  >= 9 This is the “filtering rule”. It will filter the data down to just those children who have a foot width greater than or equal to 9 cm. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. The “and” statement… GirlsWide9 <-  GirlsWide9 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. sex The first column of the KidsFeet dataset that we want to use to reduce the dataset.  == “G” This is the first “filtering rule”. It will filter the data down to just those children who are girls.  &  The & is the AND statement. It joins to filtering criteria together into a single criteria where both conditions must be met. In this case, it ensures we get only girls with foot widths greater than 9 cm. width The second column of the KidsFeet dataset that we want to use to reduce the dataset.  > 9 This is the second “filtering rule”. It will filter the data down to just those children who have a foot width greater than 9 cm. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. The “or” statement… KidsWinter <-  KidsWinter is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. birthmonth The first column of the KidsFeet dataset that we want to use to reduce the dataset.  <= 2 This is the first “filtering rule”. It will filter the data down to just those children who are born in January or February.  |  The | is the OR statement. It joins to filtering criteria together into a single criteria where either condition gives us what we want. In this case, it keeps any child born in January, February, November, or December. birthmonth The second column of the KidsFeet dataset that we want to use to reduce the dataset. In this case, it is the same as the first column.  >= 11 This is the second “filtering rule”. It will filter the data down to just those children who are born in November or December. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output.",
    "sentences": ["Used to reduce a dataset to a smaller set of rows than the original dataset contained.", "Usage filter(NameOfDataset, columnName filteringRules) filter() is the function that filters out certain rows of the dataset.", "NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet.", "columnName is the name of one of the columns from the dataset.", "You can use colnames(NameOfDataset) or View(NameOfDataset) to see the names.", "filteringRules consists of some Logical Expression (see table below) that selects only the rows from the original dataset that meet the criterion.", "Filtering Rule Logical Expression Equals one “thing” columnName == something Equals Any Of Several “things” columnName %in% c(something1,something2,...) Not Equal (one thing) columnName != something Not Equals Any of (several things) !columnName %in% c(something1,something2,...) Less Than columnName < value Less Then or Equal to columnName <= value Greater Than columnName > value Greater Than or Equal to columnName >= value AND expression1 & expression2 OR expression1 | expression2 Equals NA is.na(columnName) Not NA !is.na(columnName) Example Code library(tidyverse) The tidyverse library is needed to access the filter function used in the following example codes.", "library(mosaic) The mosaic library is needed to access the KidsFeet data set used in the following example codes.", "Equals one “thing”… Kids87 <-  Kids87 is a name we made up.", "The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "filter( )"
  },
  {
    "id": 305,
    "title": "📍 select( )",
    "url": "RCommands.html#select",
    "content": "select( ) Used to select out certain columns from a dataset. Usage select(NameOfDataset, listOfColumnNames) select( ) is the function that selects out certain columns of the dataset. NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet. listOfColumnNames is a vector of names of columns from the dataset, usually supplied inside a combine c(...) statement. Example Code KidsNameBirth <-  KidsNameBirth is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the select(...) function into this name. select(KidsFeet,   “select” is a function from library(tidyverse) that selects out specified columns from the original dataset in the order specified. c(name, birthyear, birthmonth) The columns of the KidsFeet dataset that we want to select out of the original dataset. Notice how the concatenation function c(...) is used to list out the columns we want. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. KidsBigLength <-  KidsBigLength is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the select(...) function into this name. select(KidsFeet,   “select” is a function from library(tidyverse) that selects out specified columns from the original dataset in the order specified. c(biggerfoot, length) The columns of the KidsFeet dataset that we want to select out of the original dataset. The order in which columns are selected is the order in which they are placed in the new data set. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output.",
    "sentences": ["Used to select out certain columns from a dataset.", "Usage select(NameOfDataset, listOfColumnNames) select( ) is the function that selects out certain columns of the dataset.", "NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet.", "listOfColumnNames is a vector of names of columns from the dataset, usually supplied inside a combine c(...) statement.", "Example Code KidsNameBirth <-  KidsNameBirth is a name we made up.", "The assignment operator <- will save the reduced version of the KidsFeet dataset created by the select(...) function into this name.", "select(KidsFeet,   “select” is a function from library(tidyverse) that selects out specified columns from the original dataset in the order specified.", "c(name, birthyear, birthmonth) The columns of the KidsFeet dataset that we want to select out of the original dataset.", "Notice how the concatenation function c(...) is used to list out the columns we want.", ") Always close off your functions in R with a closing parathesis."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "select( )"
  },
  {
    "id": 306,
    "title": "📍 %>% The Pipe Operator",
    "url": "RCommands.html#thepipeoperator",
    "content": "%>% The Pipe Operator Just like the pipes in your kitchen sink, the pipe operator takes “water from the sink” and “sends it down to somewhere else.” Usage    Keyboard Shortcut: Ctrl Shift M NameOfDataset %>%   some R commands that follow on the next line %>%, the pipe operator, is created by typing percent symbols % on both sides of a greater than symbol >. It lets you take whatever is on the left of the symbol and “pipe it down into” some R commands that follow on the next line. NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet. Note: you should load library(tidyverse) before using the %>% operator. Example Code Kids2 <-  This provides a name for the new reduced version of the KidsFeet dataset that is going to be created by the combined use of filter(...) and select(...). KidsFeet KidsFeet is a dataset found in library(mosaic). Click on this code to View the dataset and the resulting Kids2 dataset.  %>%  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line.    filter( “filter” is a function from library(tidyverse) that allows us to reduce the number of rows in the KidsFeet dataset by filtering according to certain criteria. birthyear Represents the column of data that we want to use to reduce the rows of the dataset.  == 87 This is the “filtering rule”. It will filter the data down to just those children who had a birthyear equal to 87. ) Always close off your functions in R with a closing parathesis.  %>%  The pipe operator that will send the filtered version of the KidsFeet dataset down inside of the code on the following line.    select( “select” is a function from library(tidyverse) that selects out specified columns from the current dataset in the order specified. c(name, birthyear, length) The columns of the filtered KidsFeet dataset that we want to select. Notice how the concatenation function c(...) is used to list out the columns we want. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output.",
    "sentences": ["Just like the pipes in your kitchen sink, the pipe operator takes “water from the sink” and “sends it down to somewhere else.” Usage    Keyboard Shortcut: Ctrl Shift M NameOfDataset %>%   some R commands that follow on the next line %>%, the pipe operator, is created by typing percent symbols % on both sides of a greater than symbol >.", "It lets you take whatever is on the left of the symbol and “pipe it down into” some R commands that follow on the next line.", "NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet.", "Note: you should load library(tidyverse) before using the %>% operator.", "Example Code Kids2 <-  This provides a name for the new reduced version of the KidsFeet dataset that is going to be created by the combined use of filter(...) and select(...).", "KidsFeet KidsFeet is a dataset found in library(mosaic).", "Click on this code to View the dataset and the resulting Kids2 dataset.", " %>%  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line.", "   filter( “filter” is a function from library(tidyverse) that allows us to reduce the number of rows in the KidsFeet dataset by filtering according to certain criteria.", "birthyear Represents the column of data that we want to use to reduce the rows of the dataset."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "%>% The Pipe Operator"
  },
  {
    "id": 307,
    "title": "📍 summarise( ) and group_by( )",
    "url": "RCommands.html#summariseandgroupby",
    "content": "summarise( ) and group_by( ) Compute numerical summaries on data or on groupings within the data. Usage NameofDataset %>%    summarise(nameYouLike = some_stats_function(columnName)) OR NameofDataset %>%    group_by(columnGroupsName) %>%    summarise(nameYouLike = some_stats_function(columnName)) NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet. %>% is the pipe operator that “pipes data” down into R commands on the next line. group_by(...) is an R function from library(tidyverse) that groups data according to a specified column (or columns). summarise(...) is an R function from library(tidyverse) that computes numerical summaries on data or groups of data. columnGroupsName is the name of a column that represents qualitative (categorical) data. This column is used to separate the dataset into little datasets, one “little dataset” for each group or category in the columnGroupsName column. nameYouLike is just that. Some name you come up with. some_stats_function(...) is a stats function like mean(...), sd(...), n(...) or so on. columnName is the name of a column from the dataset that you want to compute numerical summaries on. Example Code KidsFeet KidsFeet is a dataset found in library(mosaic).  %>%  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveLength A name we came up with that will store the results of the numerical summary.  = mean(length) This computes the mean(...) of the length column from the KidsFeet dataset. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. aveLength 24.72 KidsFeet KidsFeet is a dataset found in library(mosaic).  %>%  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveLength A name we came up with that will store the results of the numerical summary.  = mean(length),  This computes the mean(...) of the length column from the KidsFeet dataset.              sdLength A name we came up with that will store the results of the numerical summary.  = sd(length),  This computes the sd(...) of the length column from the KidsFeet dataset.              sampleSize A name we came up with that will store the results of the numerical summary.  = n( ) This computes the n(...), or sample size, of the length column from the KidsFeet dataset. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. aveLength sdLength sampleSize 24.72 1.318 39 KidsFeet KidsFeet is a dataset found in library(mosaic).  %>%  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the dataset up into “little groups” according to the column specified. sex “sex” is a column from the KidsFeet dataset that records the gender of each child. ) Always close off your functions in R with a closing parathesis.  %>%  The pipe operator that will send the grouped according to gender version of the KidsFeet dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveLength A name we came up with that will store the results of the numerical summary.  = mean(length),  This computes the mean(...) of the length column from the KidsFeet dataset.              sdLength A name we came up with that will store the results of the numerical summary.  = sd(length),  This computes the sd(...) of the length column from the KidsFeet dataset.              sampleSize A name we came up with that will store the results of the numerical summary.  = n( ) This computes the n(...), or sample size, of the length column from the KidsFeet dataset. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. sex aveLength sdLength sampleSize B 25.11 1.217 20 G 24.32 1.33 19 For more uses of summarise(...) and group_by(...) see the Example codes on the various “R Instructions” of the Numerical Summaries page.",
    "sentences": ["Compute numerical summaries on data or on groupings within the data.", "Usage NameofDataset %>%    summarise(nameYouLike = some_stats_function(columnName)) OR NameofDataset %>%    group_by(columnGroupsName) %>%    summarise(nameYouLike = some_stats_function(columnName)) NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet.", "%>% is the pipe operator that “pipes data” down into R commands on the next line.", "group_by(...) is an R function from library(tidyverse) that groups data according to a specified column (or columns).", "summarise(...) is an R function from library(tidyverse) that computes numerical summaries on data or groups of data.", "columnGroupsName is the name of a column that represents qualitative (categorical) data.", "This column is used to separate the dataset into little datasets, one “little dataset” for each group or category in the columnGroupsName column.", "nameYouLike is just that.", "Some name you come up with.", "some_stats_function(...) is a stats function like mean(...), sd(...), n(...) or so on."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "summarise( ) and group_by( )"
  },
  {
    "id": 308,
    "title": "📍 mutate( )",
    "url": "RCommands.html#mutate",
    "content": "mutate( ) Transform a column or add a new column of data to a data set. Usage NameofDataset %>%    mutate(nameYouLike = some_transformation) NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet. %>% is the pipe operator that “pipes data” down into R commands on the next line. nameYouLike is just that. Some name you come up with that will be the name of a new column in the dataset. some_transformation is just that. See the example codes for ideas. Example Code mtcars2 <-  mtcars2 is a new dataset we are creating that will contain all of mtcars data set along with a couple new columns we are creating. mtcars mtcars is a dataset found in base R. Typing View(mtcars) and ?mtcars in the console will help you learn more about the dataset.  %>%  The pipe operator that will send the mtcars dataset down inside of the code on the following line.    mutate( “mutate” is a function from library(tidyverse) that allows us to transform columns of data.          cyl_factor = as.factor(cyl), “cyl_factor” is a name we came up with that will store the results of the transformation of the “cyl” column. Here we are simply converting the “cyl” column from type numeric to a factor. Treating the “cyl” column as a factor could be useful in certain situations.          weight = wt * 1000 “weight” is a name we came up with that will store the results of the transformation of the “wt” column. Taking a closer look with ?mtcars shows us that wt is in 1000 lbs. Here we are just multiplying each row in the column by 1000.    ) Closing parenthesis for the mutate(…) function.     Press Enter to run the code.  Click to Show Output  Click to View Output. Notice the last two columns of cyl_factor and weight are what we added to the data set mtcars2. As seen by View(mtcars), the original mtcars data set did not have these two columns. We used the mutate function to create them and saved our work in a new data set called mtcars2.   mpg cyl disp hp drat wt qsec vs am gear carb disp2 cyl_factor weight Mazda RX4 21 6 160 110 3.9 2.62 16.46 0 1 4 4 25600 6 2620 Mazda RX4 Wag 21 6 160 110 3.9 2.875 17.02 0 1 4 4 25600 6 2875 Datsun 710 22.8 4 108 93 3.85 2.32 18.61 1 1 4 1 11664 4 2320 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 66564 6 3215 Hornet Sportabout 18.7 8 360 175 3.15 3.44 17.02 0 0 3 2 129600 8 3440 Valiant 18.1 6 225 105 2.76 3.46 20.22 1 0 3 1 50625 6 3460 Duster 360 14.3 8 360 245 3.21 3.57 15.84 0 0 3 4 129600 8 3570 Merc 240D 24.4 4 146.7 62 3.69 3.19 20 1 0 4 2 21521 4 3190 Merc 230 22.8 4 140.8 95 3.92 3.15 22.9 1 0 4 2 19825 4 3150 Merc 280 19.2 6 167.6 123 3.92 3.44 18.3 1 0 4 4 28090 6 3440 Merc 280C 17.8 6 167.6 123 3.92 3.44 18.9 1 0 4 4 28090 6 3440 Merc 450SE 16.4 8 275.8 180 3.07 4.07 17.4 0 0 3 3 76066 8 4070 Merc 450SL 17.3 8 275.8 180 3.07 3.73 17.6 0 0 3 3 76066 8 3730 Merc 450SLC 15.2 8 275.8 180 3.07 3.78 18 0 0 3 3 76066 8 3780 Cadillac Fleetwood 10.4 8 472 205 2.93 5.25 17.98 0 0 3 4 222784 8 5250 Lincoln Continental 10.4 8 460 215 3 5.424 17.82 0 0 3 4 211600 8 5424 Chrysler Imperial 14.7 8 440 230 3.23 5.345 17.42 0 0 3 4 193600 8 5345 Fiat 128 32.4 4 78.7 66 4.08 2.2 19.47 1 1 4 1 6194 4 2200 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 5730 4 1615 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.9 1 1 4 1 5055 4 1835 Toyota Corona 21.5 4 120.1 97 3.7 2.465 20.01 1 0 3 1 14424 4 2465 Dodge Challenger 15.5 8 318 150 2.76 3.52 16.87 0 0 3 2 101124 8 3520 AMC Javelin 15.2 8 304 150 3.15 3.435 17.3 0 0 3 2 92416 8 3435 Camaro Z28 13.3 8 350 245 3.73 3.84 15.41 0 0 3 4 122500 8 3840 Pontiac Firebird 19.2 8 400 175 3.08 3.845 17.05 0 0 3 2 160000 8 3845 Fiat X1-9 27.3 4 79 66 4.08 1.935 18.9 1 1 4 1 6241 4 1935 Porsche 914-2 26 4 120.3 91 4.43 2.14 16.7 0 1 5 2 14472 4 2140 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.9 1 1 5 2 9044 4 1513 Ford Pantera L 15.8 8 351 264 4.22 3.17 14.5 0 1 5 4 123201 8 3170 Ferrari Dino 19.7 6 145 175 3.62 2.77 15.5 0 1 5 6 21025 6 2770 Maserati Bora 15 8 301 335 3.54 3.57 14.6 0 1 5 8 90601 8 3570 Volvo 142E 21.4 4 121 109 4.11 2.78 18.6 1 1 4 2 14641 4 2780 Kids3 <-  Kids3 is a new dataset we are creating that will contain all of KidsFeet data set along with a couple new columns we are creating. KidsFeet KidsFeet is a dataset found in library(mosaic).  %>%  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line.    mutate( “mutate” is a function from library(tidyverse) that allows us to transform columns of data.          season = case_when( “season” is a name we came up with that will store the results of the transformation of the “birthmonth” column. The case_when(…) function from library(tidyverse) allows us to perform more complicated transformations with columns.            birthmonth %in% c(12,1,2) ~ “Winter”, The body of case_when(…) is of the form logical expression ~ \"newValueName\". This statement says that we want the values in the column “birthmonth” that are equal to 12, 1, and 2 to be assigned to the value “Winter” in the new “season” column.            birthmonth %in% c(3,4,5) ~ “Spring”, The body of case_when(…) is of the form logical expression ~ \"newValueName\". This statement says that we want the values in the column “birthmonth” that are equal to 3, 4, and 5 to be assigned to the value “Spring” in the new “season” column.            birthmonth %in% c(6,7,8) ~ “Summer”, The body of case_when(…) is of the form logical expression ~ \"newValueName\". This statement says that we want the values in the column “birthmonth” that are equal to 6, 7, and 8 to be assigned to the value “Summer” in the new “season” column.            birthmonth %in% c(9,10,11) ~ “Fall” The body of case_when(…) is of the form logical expression ~ \"newValueName\". This statement says that we want the values in the column “birthmonth” that are equal to 9, 10, and 11 to be assigned to the value “Fall” in the new “season” column.           ) Closing parenthesis of the case_when(…) function.         ) Closing parenthesis for the mutate(…) function.     Press Enter to run the code.  Click to Show Output  Click to View Output. Here is what the Kids3 data set looks like. It would be good if you compared this to the original KidsFeet data set by running View(KidsFeet) to see how things have changed. Notice that on the far right we have added the season column. name birthmonth birthyear length width sex biggerfoot domhand season David 5 88 24.4 8.4 B L R Spring Lars 10 87 25.4 8.8 B L L Fall Zach 12 87 24.5 9.7 B R R Winter Josh 1 88 25.2 9.8 B L R Winter Lang 2 88 25.1 8.9 B L R Winter Scotty 3 88 25.7 9.7 B R R Spring Edward 2 88 26.1 9.6 B L R Winter Caitlin 6 88 23 8.8 G L R Summer Eleanor 5 88 23.6 9.3 G R R Spring Damon 9 88 22.9 8.8 B R L Fall Mark 9 87 27.5 9.8 B R R Fall Ray 3 88 24.8 8.9 B L R Spring Cal 8 87 26.1 9.1 B L R Summer Cam 3 88 27 9.8 B L R Spring Julie 11 87 26 9.3 G L R Fall Kate 4 88 23.7 7.9 G R R Spring Caroline 12 87 24 8.7 G R L Winter Maggie 3 88 24.7 8.8 G R R Spring Lee 6 88 26.7 9 G L L Summer Heather 3 88 25.5 9.5 G R R Spring Andy 6 88 24 9.2 B R R Summer Josh 7 88 24.4 8.6 B L R Summer Laura 9 88 24 8.3 G R L Fall Erica 9 88 24.5 9 G L R Fall Peggy 10 88 24.2 8.1 G L R Fall Glen 7 88 27.1 9.4 B L R Summer Abby 2 88 26.1 9.5 G L R Winter David 12 87 25.5 9.5 B R R Winter Mike 11 88 24.2 8.9 B L R Fall Dwayne 8 88 23.9 9.3 B R L Summer Danielle 6 88 24 9.3 G L R Summer Caitlin 7 88 22.5 8.6 G R R Summer Leigh 3 88 24.5 8.6 G L R Spring Dylan 4 88 23.6 9 B R L Spring Peter 4 88 24.7 8.6 B R L Spring Hannah 3 88 22.9 8.5 G L R Spring Teshanna 3 88 26 9 G L R Spring Hayley 1 88 21.6 7.9 G R R Winter Alisha 9 88 24.6 8.8 G L R Fall Kids4 <-  Kids4 is a new dataset we are creating that will contain all of KidsFeet data set along with a couple new columns we are creating. KidsFeet KidsFeet is a dataset found in library(mosaic).  %>%  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line.    mutate( “mutate” is a function from library(tidyverse) that allows us to transform columns of data.          lengthIN = length / 2.54, “lengthIN” is a name we came up with that will store the results of the transformation of the “length” column. This is just converting the length data from cm to inches.          widthIN = width / 2.54, “widthIN” is a name we came up with that will store the results of the transformation of the “width” column. This is just converting the width data from cm to inches.          lengthSplit = ifelse(length < median(length),          “Under 50th Percentile”,          “50th Percentile or Greater”), “lengthSplit” is a name we came up with that will store the results of the ifelse(…) function. The ifelse(…) function in this case is being used to split the length column by the median of that column. The ifelse(…) function is of the form ifelse( Logical Condition , valueIfConditionTrue, valueIfConditionFalse).          gender = case_when( “gender” is a name we came up with that will store the results of the transformation of the “sex” column. The case_when(…) function from library(tidyverse) allows us to perform more complicated transformations with columns.            sex == “B” ~ “Boy”, The body of case_when(…) is of the form logical expression ~ \"newValueName\". This part of the case_when(…) function is being used to change the value of “B” to “Boy”.            sex == “G” ~ “Girl” The body of case_when(…) is of the form logical expression ~ \"newValueName\". This part of the case_when(…) function is being used to change the value of “G” to “Girl”.           ) Closing parenthesis for the case_when(…) function.         ) Closing parenthesis for the mutate(…) function.     Press Enter to run the code.  Click to Show Output  Click to View Output. Notice the addition (on the right) of three new columns: lengthIN, widthIN, and gender. name birthmonth birthyear length width sex biggerfoot domhand lengthIN widthIN lengthSplit gender David 5 88 24.4 8.4 B L R 9.606 3.307 Under 50th Percentile Boy Lars 10 87 25.4 8.8 B L L 10 3.465 50th Percentile or Greater Boy Zach 12 87 24.5 9.7 B R R 9.646 3.819 50th Percentile or Greater Boy Josh 1 88 25.2 9.8 B L R 9.921 3.858 50th Percentile or Greater Boy Lang 2 88 25.1 8.9 B L R 9.882 3.504 50th Percentile or Greater Boy Scotty 3 88 25.7 9.7 B R R 10.12 3.819 50th Percentile or Greater Boy Edward 2 88 26.1 9.6 B L R 10.28 3.78 50th Percentile or Greater Boy Caitlin 6 88 23 8.8 G L R 9.055 3.465 Under 50th Percentile Girl Eleanor 5 88 23.6 9.3 G R R 9.291 3.661 Under 50th Percentile Girl Damon 9 88 22.9 8.8 B R L 9.016 3.465 Under 50th Percentile Boy Mark 9 87 27.5 9.8 B R R 10.83 3.858 50th Percentile or Greater Boy Ray 3 88 24.8 8.9 B L R 9.764 3.504 50th Percentile or Greater Boy Cal 8 87 26.1 9.1 B L R 10.28 3.583 50th Percentile or Greater Boy Cam 3 88 27 9.8 B L R 10.63 3.858 50th Percentile or Greater Boy Julie 11 87 26 9.3 G L R 10.24 3.661 50th Percentile or Greater Girl Kate 4 88 23.7 7.9 G R R 9.331 3.11 Under 50th Percentile Girl Caroline 12 87 24 8.7 G R L 9.449 3.425 Under 50th Percentile Girl Maggie 3 88 24.7 8.8 G R R 9.724 3.465 50th Percentile or Greater Girl Lee 6 88 26.7 9 G L L 10.51 3.543 50th Percentile or Greater Girl Heather 3 88 25.5 9.5 G R R 10.04 3.74 50th Percentile or Greater Girl Andy 6 88 24 9.2 B R R 9.449 3.622 Under 50th Percentile Boy Josh 7 88 24.4 8.6 B L R 9.606 3.386 Under 50th Percentile Boy Laura 9 88 24 8.3 G R L 9.449 3.268 Under 50th Percentile Girl Erica 9 88 24.5 9 G L R 9.646 3.543 50th Percentile or Greater Girl Peggy 10 88 24.2 8.1 G L R 9.528 3.189 Under 50th Percentile Girl Glen 7 88 27.1 9.4 B L R 10.67 3.701 50th Percentile or Greater Boy Abby 2 88 26.1 9.5 G L R 10.28 3.74 50th Percentile or Greater Girl David 12 87 25.5 9.5 B R R 10.04 3.74 50th Percentile or Greater Boy Mike 11 88 24.2 8.9 B L R 9.528 3.504 Under 50th Percentile Boy Dwayne 8 88 23.9 9.3 B R L 9.409 3.661 Under 50th Percentile Boy Danielle 6 88 24 9.3 G L R 9.449 3.661 Under 50th Percentile Girl Caitlin 7 88 22.5 8.6 G R R 8.858 3.386 Under 50th Percentile Girl Leigh 3 88 24.5 8.6 G L R 9.646 3.386 50th Percentile or Greater Girl Dylan 4 88 23.6 9 B R L 9.291 3.543 Under 50th Percentile Boy Peter 4 88 24.7 8.6 B R L 9.724 3.386 50th Percentile or Greater Boy Hannah 3 88 22.9 8.5 G L R 9.016 3.346 Under 50th Percentile Girl Teshanna 3 88 26 9 G L R 10.24 3.543 50th Percentile or Greater Girl Hayley 1 88 21.6 7.9 G R R 8.504 3.11 Under 50th Percentile Girl Alisha 9 88 24.6 8.8 G L R 9.685 3.465 50th Percentile or Greater Girl airquality2 <-  airquality is a new dataset we are creating that will contain all of the airquality data set along with a new column we are creating. airquality airquality is a dataset found in base R. Typing View(airquality) and ?airquality in the console will help you learn more about the dataset.  %>%  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line.    mutate( “mutate” is a function from library(tidyverse) that allows us to transform columns of data. Month_Full = “Month_Full” is a name we came up with that will store the results of the transformation of the “Month” column.  month( month(…) is from library(lubridate) and changes the “Month” column from type integer to type datetime. Month, “Month” is the “Month” column from airquality.  label = TRUE, “label = TRUE” tells month(…) to change the month numbers to abbreviated month names.  abbr = FALSE “abbr = FALSE” changes the abbreviated month names to the full month names. ) Closing parenthesis for the month(…) function. ) Closing parenthesis for the mutate(…) function.     Press Enter to run the code.  Click to Show Output  Click to View Output. Insert Image Here Other case_when( ) Uses case_when(length > 25 & width > 9 ~ \"Long and Wide\", length < 25 & width > 9 ~ \"Short and Wide\", length > 25 & width < 9 ~ \"Long and Thin\", length < 25 & width < 9 ~ \"Short and Thin\") replace_na( ) Function newDataName <- dataName %>% mutate(newColumnName = replace_na(columnName, value)) as.numeric( ) Function newDataName <- dataName %>% mutate(newColumnName = as.numeric(columnName)) as.character( ) Function newDataName <- dataName %>% mutate(newColumnName = as.character(columnName)) as.factor( ) Function newDataName <- dataName %>% mutate(newColumnName = as.factor(columnName)) Pull Out Numbers Only with parse_number(…) newDataName <- dataName %>% mutate(newColumnName = parse_number(columnName)) # So stuff like c(\"500+\", \"20\", \"80 \", \"15a\") would become # just c(500, 20, 80, 15). That's nice!",
    "sentences": ["Transform a column or add a new column of data to a data set.", "Usage NameofDataset %>%    mutate(nameYouLike = some_transformation) NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet.", "%>% is the pipe operator that “pipes data” down into R commands on the next line.", "nameYouLike is just that.", "Some name you come up with that will be the name of a new column in the dataset.", "some_transformation is just that.", "See the example codes for ideas.", "Example Code mtcars2 <-  mtcars2 is a new dataset we are creating that will contain all of mtcars data set along with a couple new columns we are creating.", "mtcars mtcars is a dataset found in base R.", "Typing View(mtcars) and ?mtcars in the console will help you learn more about the dataset."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "mutate( )"
  },
  {
    "id": 309,
    "title": "📍 arrange( )",
    "url": "RCommands.html#arrange",
    "content": "arrange( ) Arrange data by a certain column, or columns, i.e. “sort” the data. Usage NameofDataset %>%    arrange(columnName1) Note: arrange(columnName1, columnName2, ...) is also possible. NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet. %>% is the pipe operator that “pipes data” down into R commands on the next line. arrange(...) is an R function from library(tidyverse) that arranges a data set by order for the column given. columnName1 is the name of a column from the dataset that you want to compute numerical summaries on. columnName2 is the name of a column from the dataset that you want to compute numerical summaries on. implies that you can arrange by as many columns as you want. Example Code KidsFeet KidsFeet is a dataset found in library(mosaic).  %>%  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line.    arrange( “arrange” is an R function from library(tidyverse) that arranges a data set by order for the column given. birthmonth birthmonth is the name of one of the columns of the KidsFeet data set. Specifying this name will cause the data to be sorted by birthmonth from 1 to 12. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. name birthmonth birthyear length width sex biggerfoot domhand Josh 1 88 25.2 9.8 B L R Hayley 1 88 21.6 7.9 G R R Lang 2 88 25.1 8.9 B L R Edward 2 88 26.1 9.6 B L R Abby 2 88 26.1 9.5 G L R Scotty 3 88 25.7 9.7 B R R Ray 3 88 24.8 8.9 B L R Cam 3 88 27 9.8 B L R Maggie 3 88 24.7 8.8 G R R Heather 3 88 25.5 9.5 G R R Leigh 3 88 24.5 8.6 G L R Hannah 3 88 22.9 8.5 G L R Teshanna 3 88 26 9 G L R Kate 4 88 23.7 7.9 G R R Dylan 4 88 23.6 9 B R L Peter 4 88 24.7 8.6 B R L David 5 88 24.4 8.4 B L R Eleanor 5 88 23.6 9.3 G R R Caitlin 6 88 23 8.8 G L R Lee 6 88 26.7 9 G L L Andy 6 88 24 9.2 B R R Danielle 6 88 24 9.3 G L R Josh 7 88 24.4 8.6 B L R Glen 7 88 27.1 9.4 B L R Caitlin 7 88 22.5 8.6 G R R Cal 8 87 26.1 9.1 B L R Dwayne 8 88 23.9 9.3 B R L Damon 9 88 22.9 8.8 B R L Mark 9 87 27.5 9.8 B R R Laura 9 88 24 8.3 G R L Erica 9 88 24.5 9 G L R Alisha 9 88 24.6 8.8 G L R Lars 10 87 25.4 8.8 B L L Peggy 10 88 24.2 8.1 G L R Julie 11 87 26 9.3 G L R Mike 11 88 24.2 8.9 B L R Zach 12 87 24.5 9.7 B R R Caroline 12 87 24 8.7 G R L David 12 87 25.5 9.5 B R R KidsFeet KidsFeet is a dataset found in library(mosaic).  %>%  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line.    arrange( “arrange” is an R function from library(tidyverse) that arranges a data set by order for the column given. desc( This causes the arranging to be done in descending order (highest to lowest). birthmonth birthmonth is the name of one of the columns of the KidsFeet data set. Specifying this name will cause the data to be sorted by birthmonth from 1 to 12. ) Always close off your functions in R with a closing parathesis. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. name birthmonth birthyear length width sex biggerfoot domhand Zach 12 87 24.5 9.7 B R R Caroline 12 87 24 8.7 G R L David 12 87 25.5 9.5 B R R Julie 11 87 26 9.3 G L R Mike 11 88 24.2 8.9 B L R Lars 10 87 25.4 8.8 B L L Peggy 10 88 24.2 8.1 G L R Damon 9 88 22.9 8.8 B R L Mark 9 87 27.5 9.8 B R R Laura 9 88 24 8.3 G R L Erica 9 88 24.5 9 G L R Alisha 9 88 24.6 8.8 G L R Cal 8 87 26.1 9.1 B L R Dwayne 8 88 23.9 9.3 B R L Josh 7 88 24.4 8.6 B L R Glen 7 88 27.1 9.4 B L R Caitlin 7 88 22.5 8.6 G R R Caitlin 6 88 23 8.8 G L R Lee 6 88 26.7 9 G L L Andy 6 88 24 9.2 B R R Danielle 6 88 24 9.3 G L R David 5 88 24.4 8.4 B L R Eleanor 5 88 23.6 9.3 G R R Kate 4 88 23.7 7.9 G R R Dylan 4 88 23.6 9 B R L Peter 4 88 24.7 8.6 B R L Scotty 3 88 25.7 9.7 B R R Ray 3 88 24.8 8.9 B L R Cam 3 88 27 9.8 B L R Maggie 3 88 24.7 8.8 G R R Heather 3 88 25.5 9.5 G R R Leigh 3 88 24.5 8.6 G L R Hannah 3 88 22.9 8.5 G L R Teshanna 3 88 26 9 G L R Lang 2 88 25.1 8.9 B L R Edward 2 88 26.1 9.6 B L R Abby 2 88 26.1 9.5 G L R Josh 1 88 25.2 9.8 B L R Hayley 1 88 21.6 7.9 G R R",
    "sentences": ["Arrange data by a certain column, or columns, i.e. “sort” the data.", "Usage NameofDataset %>%    arrange(columnName1) Note: arrange(columnName1, columnName2, ...) is also possible.", "NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet.", "%>% is the pipe operator that “pipes data” down into R commands on the next line.", "arrange(...) is an R function from library(tidyverse) that arranges a data set by order for the column given.", "columnName1 is the name of a column from the dataset that you want to compute numerical summaries on.", "columnName2 is the name of a column from the dataset that you want to compute numerical summaries on.", "implies that you can arrange by as many columns as you want.", "Example Code KidsFeet KidsFeet is a dataset found in library(mosaic).", " %>%  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "arrange( )"
  },
  {
    "id": 310,
    "title": "📍 pander( )",
    "url": "RCommands.html#pander",
    "content": "pander( ) Makes output of most commands “beautiful”. Usage library(pander) then… pander(someCode) OR someCode %>%    pander( ) Note: pander(stuff, caption=\"Some useful caption\", ...) is also possible. someCode is exactly that, some coding you have done that creates output that you want displayed nicely. %>% is the pipe operator that “pipes data” down into R commands on the next line. pander(...) is an R function from library(pander) that makes most R output look nice. other useful commands like split.table=Inf. Example Code pander( pander is an R function that makes output look nice. table(KidsFeet$sex, KidsFeet$birthmonth), Code that makes a table of how many boys and girls were born in each month of the year.   caption=“Counts of Birthdays by Month” The caption=” ” command is very useful for giving your output a small title. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. Counts of Birthdays by Month   1 2 3 4 5 6 7 8 9 10 11 12 B 1 2 3 2 1 1 2 2 2 1 1 2 G 1 1 5 1 1 3 1 0 3 1 1 1 KidsFeet KidsFeet is a dataset found in library(mosaic).  %>%  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the dataset up into “little groups” according to the column specified. sex “sex” is a column from the KidsFeet dataset that records the gender of each child. ) Always close off your functions in R with a closing parathesis.  %>%  The pipe operator that will send the grouped according to gender version of the KidsFeet dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveLength A name we came up with that will store the results of the numerical summary.  = mean(length),  This computes the mean(...) of the length column from the KidsFeet dataset.              sdLength A name we came up with that will store the results of the numerical summary.  = sd(length),  This computes the sd(...) of the length column from the KidsFeet dataset.              sampleSize A name we came up with that will store the results of the numerical summary.  = n( ) This computes the n(...), or sample size, of the length column from the KidsFeet dataset. ) Always close off your functions in R with a closing parathesis.  %>%  The pipe operator that will send the KidsFeet dataset down inside of the code on the following line.    pander( The pander function will make the output of the above code look nice. caption=“Doesn’t that look nice?”) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. Doesn’t that look nice? sex aveLength sdLength sampleSize B 25.11 1.217 20 G 24.32 1.33 19 What it looks like if you don't pander: # A tibble: 2 × 4 sex aveLength sdLength sampleSize <fct> <dbl> <dbl> <int> 1 B 25.105 1.21676 20 2 G 24.3211 1.33024 19",
    "sentences": ["Makes output of most commands “beautiful”.", "Usage library(pander) then… pander(someCode) OR someCode %>%    pander( ) Note: pander(stuff, caption=\"Some useful caption\", ...) is also possible.", "someCode is exactly that, some coding you have done that creates output that you want displayed nicely.", "%>% is the pipe operator that “pipes data” down into R commands on the next line.", "pander(...) is an R function from library(pander) that makes most R output look nice.", "other useful commands like split.table=Inf.", "Example Code pander( pander is an R function that makes output look nice.", "table(KidsFeet$sex, KidsFeet$birthmonth), Code that makes a table of how many boys and girls were born in each month of the year.", "  caption=“Counts of Birthdays by Month” The caption=” ” command is very useful for giving your output a small title.", ") Always close off your functions in R with a closing parathesis."],
    "type": "section",
    "page_title": "R Commands",
    "section_title": "pander( )"
  },
  {
    "id": 311,
    "title": "R Markdown Hints",
    "url": "RMarkdownHints.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus R Markdown Hints Think of an R Markdown File, or Rmd for short, as a command center. You write commands, then Knit the file, and an html output file is created according to your commands. Overview Click Here to Learn More Carefully read through all parts of this image to learn… Close The above tabs (blue bottons that read “Click Here to Learn More” and “Close”) were created with the code: ## {.tabset .tabset-pills .tabset-fade} ### Click Here to Learn More Carefully read through all parts of this image to learn... ![](./Images/Rmd2html.png) ### Close ## Creating Links To make a link use the code [Name of Link](addressForLink). Linking to parts of your textbook: [Numerical Summaries](NumericalSummaries.html) becomes Numerical Summaries [Boxplots](GraphicalSummaries.html#boxplots) becomes Boxplots [R Commands](RCommands.html) becomes R Commands Linking to outside resources: [R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) becomes R Colors Creating Headers There are six available sizes of headings you can use in an Rmd file (left in image) that show up as shown below (right in image). Emphasizing Words To italisize a word use the asterisk (Shift 8) *italisize*. To bold a word use the double asterisk **bold**. The back tic can be used tohighlightwords by placing back tics on each side of a word: highlight `. Bullet Points Simple Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: * This is the first item. * This is the second. * This is the third. Numbered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: 1. This is the first item. This is the second. This is the third. Lettered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: A) This is the first item. B) This is the second. C) This is the third. Nested Lists What is \\(2+2\\)? 4 8 What is \\(3\\times5\\)? 14 15 1. What is $2+2$? **4** b. What is $3\\times5$? 14 b. **15** Math Equations Use the dollar signs $x=5$ to write \\(x=5\\) or $z=\\frac{x-\\mu}{\\sigma}$ to write \\(z=\\frac{x-\\mu}{\\sigma}\\). For a nicely centered equation use the double dollar signs $$ $$ on separate lines $$ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} $$ to get \\[ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] Or $$ H_0: \\mu_\\text{Group 1} = \\mu_\\text{Group 2} $$ $$ H_a: \\mu_\\text{Group 1} \\neq \\mu_\\text{Group 2} $$ to get \\[ H_0: \\mu_{\\text{Group 1}} = \\mu_{\\text{Group 2}} \\] \\[ H_a: \\mu_{\\text{Group 1}} \\neq \\mu_{\\text{Group 2}} \\] Symbol list: Symbol LaTeX Math Code \\(\\alpha\\) $\\alpha$ \\(\\beta\\) $\\beta$ \\(\\sigma\\) $\\sigma$ \\(\\epsilon\\) $\\epsilon$ \\(\\bar{x}\\) $\\bar{x}$ \\(\\hat{Y}\\) $\\hat{Y}$ \\(=\\) $=$ \\(\\ne\\) $\\ne$ or $\\neq$ \\(>\\) $>$ \\(<\\) $<$ \\(\\ge\\) $\\ge$ \\(\\le\\) $\\le$ \\(\\{ \\}\\) $\\{ \\}$ \\(\\text{Type just text}\\) $\\text{Type just text}$ \\(\\overbrace{Y_i}^\\text{label}\\) $\\overbrace{Y_i}^\\text{label}$ \\(\\underbrace{Y_i}_\\text{label}\\) $\\underbrace{Y_i}_\\text{label}$ Here is a list of all supported LaTeX commands. Insert a Picture To add a picture to your document, say some notes you took down on paper from class, Use the code: ![](./Images/insertPictureNotes.jpg) to get… Tables There are many ways to make tables in R Markdown. Here is a simple way to make a “pipe” table. | Name | Age | Gender | |---------------|---------------|--------------| | Jill | 8 | Female | | Jack | 9 | Male | Name Age Gender Jill 8 Female Jack 9 Male Themes Notice in the YAML (at the top of the RMD file) there is a line that reads: “theme: cerulean” Other possible themes are “default”, “cerulean”, “journal”, “flatly”, “readable”, “spacelab”, “united”, and “cosmo”. You can also change the highlighting by adding the line “highlight: tango” to the YAML as follows. --- title: \"Markdown Hints\" output: html_document: theme: cerulean highlight: tango --- Other highlighting options are “default”, “tango”, “pygments”, “kate”, “monochrome”, “espresso”, “zenburn”, “haddock”, and “textmate”. More Information Go to the rmarkdown.rstudio.com website for more information on how to use R Markdown. Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus R Markdown Hints Think of an R Markdown File, or Rmd for short, as a command center. You write commands, then Knit the file, and an html output file is created according to your commands. Overview Click Here to Learn More Carefully read through all parts of this image to learn… Close The above tabs (blue bottons that read “Click Here to Learn More” and “Close”) were created with the code: ## {.tabset .tabset-pills .tabset-fade} ### Click Here to Learn More Carefully read through all parts of this image to learn... ![](./Images/Rmd2html.png) ### Close ## Creating Links To make a link use the code [Name of Link](addressForLink). Linking to parts of your textbook: [Numerical Summaries](NumericalSummaries.html) becomes Numerical Summaries [Boxplots](GraphicalSummaries.html#boxplots) becomes Boxplots [R Commands](RCommands.html) becomes R Commands Linking to outside resources: [R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) becomes R Colors Creating Headers There are six available sizes of headings you can use in an Rmd file (left in image) that show up as shown below (right in image). Emphasizing Words To italisize a word use the asterisk (Shift 8) *italisize*. To bold a word use the double asterisk **bold**. The back tic can be used tohighlightwords by placing back tics on each side of a word: highlight `. Bullet Points Simple Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: * This is the first item. * This is the second. * This is the third. Numbered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: 1. This is the first item. This is the second. This is the third. Lettered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: A) This is the first item. B) This is the second. C) This is the third. Nested Lists What is \\(2+2\\)? 4 8 What is \\(3\\times5\\)? 14 15 1. What is $2+2$? **4** b. What is $3\\times5$? 14 b. **15** Math Equations Use the dollar signs $x=5$ to write \\(x=5\\) or $z=\\frac{x-\\mu}{\\sigma}$ to write \\(z=\\frac{x-\\mu}{\\sigma}\\). For a nicely centered equation use the double dollar signs $$ $$ on separate lines $$ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} $$ to get \\[ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] Or $$ H_0: \\mu_\\text{Group 1} = \\mu_\\text{Group 2} $$ $$ H_a: \\mu_\\text{Group 1} \\neq \\mu_\\text{Group 2} $$ to get \\[ H_0: \\mu_{\\text{Group 1}} = \\mu_{\\text{Group 2}} \\] \\[ H_a: \\mu_{\\text{Group 1}} \\neq \\mu_{\\text{Group 2}} \\] Symbol list: Symbol LaTeX Math Code \\(\\alpha\\) $\\alpha$ \\(\\beta\\) $\\beta$ \\(\\sigma\\) $\\sigma$ \\(\\epsilon\\) $\\epsilon$ \\(\\bar{x}\\) $\\bar{x}$ \\(\\hat{Y}\\) $\\hat{Y}$ \\(=\\) $=$ \\(\\ne\\) $\\ne$ or $\\neq$ \\(>\\) $>$ \\(<\\) $<$ \\(\\ge\\) $\\ge$ \\(\\le\\) $\\le$ \\(\\{ \\}\\) $\\{ \\}$ \\(\\text{Type just text}\\) $\\text{Type just text}$ \\(\\overbrace{Y_i}^\\text{label}\\) $\\overbrace{Y_i}^\\text{label}$ \\(\\underbrace{Y_i}_\\text{label}\\) $\\underbrace{Y_i}_\\text{label}$ Here is a list of all supported LaTeX commands. Insert a Picture To add a picture to your document, say some notes you took down on paper from class, Use the code: ![](./Images/insertPictureNotes.jpg) to get… Tables There are many ways to make tables in R Markdown. Here is a simple way to make a “pipe” table. | Name | Age | Gender | |---------------|---------------|--------------| | Jill | 8 | Female | | Jack | 9 | Male | Name Age Gender Jill 8 Female Jack 9 Male Themes Notice in the YAML (at the top of the RMD file) there is a line that reads: “theme: cerulean” Other possible themes are “default”, “cerulean”, “journal”, “flatly”, “readable”, “spacelab”, “united”, and “cosmo”. You can also change the highlighting by adding the line “highlight: tango” to the YAML as follows. --- title: \"Markdown Hints\" output: html_document: theme: cerulean highlight: tango --- Other highlighting options are “default”, “tango”, “pygments”, “kate”, “monochrome”, “espresso”, “zenburn”, “haddock”, and “textmate”. More Information Go to the rmarkdown.rstudio.com website for more information on how to use R Markdown. Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus R Markdown Hints Think of an R Markdown File, or Rmd for short, as a command center. You write commands, then Knit the file, and an html output file is created according to your commands. Overview Click Here to Learn More Carefully read through all parts of this image to learn… Close The above tabs (blue bottons that read “Click Here to Learn More” and “Close”) were created with the code: ## {.tabset .tabset-pills .tabset-fade} ### Click Here to Learn More Carefully read through all parts of this image to learn... ![](./Images/Rmd2html.png) ### Close ## Creating Links To make a link use the code [Name of Link](addressForLink). Linking to parts of your textbook: [Numerical Summaries](NumericalSummaries.html) becomes Numerical Summaries [Boxplots](GraphicalSummaries.html#boxplots) becomes Boxplots [R Commands](RCommands.html) becomes R Commands Linking to outside resources: [R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) becomes R Colors Creating Headers There are six available sizes of headings you can use in an Rmd file (left in image) that show up as shown below (right in image). Emphasizing Words To italisize a word use the asterisk (Shift 8) *italisize*. To bold a word use the double asterisk **bold**. The back tic can be used tohighlightwords by placing back tics on each side of a word: highlight `. Bullet Points Simple Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: * This is the first item. * This is the second. * This is the third. Numbered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: 1. This is the first item. This is the second. This is the third. Lettered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: A) This is the first item. B) This is the second. C) This is the third. Nested Lists What is \\(2+2\\)? 4 8 What is \\(3\\times5\\)? 14 15 1. What is $2+2$? **4** b. What is $3\\times5$? 14 b. **15** Math Equations Use the dollar signs $x=5$ to write \\(x=5\\) or $z=\\frac{x-\\mu}{\\sigma}$ to write \\(z=\\frac{x-\\mu}{\\sigma}\\). For a nicely centered equation use the double dollar signs $$ $$ on separate lines $$ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} $$ to get \\[ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] Or $$ H_0: \\mu_\\text{Group 1} = \\mu_\\text{Group 2} $$ $$ H_a: \\mu_\\text{Group 1} \\neq \\mu_\\text{Group 2} $$ to get \\[ H_0: \\mu_{\\text{Group 1}} = \\mu_{\\text{Group 2}} \\] \\[ H_a: \\mu_{\\text{Group 1}} \\neq \\mu_{\\text{Group 2}} \\] Symbol list: Symbol LaTeX Math Code \\(\\alpha\\) $\\alpha$ \\(\\beta\\) $\\beta$ \\(\\sigma\\) $\\sigma$ \\(\\epsilon\\) $\\epsilon$ \\(\\bar{x}\\) $\\bar{x}$ \\(\\hat{Y}\\) $\\hat{Y}$ \\(=\\) $=$ \\(\\ne\\) $\\ne$ or $\\neq$ \\(>\\) $>$ \\(<\\) $<$ \\(\\ge\\) $\\ge$ \\(\\le\\) $\\le$ \\(\\{ \\}\\) $\\{ \\}$ \\(\\text{Type just text}\\) $\\text{Type just text}$ \\(\\overbrace{Y_i}^\\text{label}\\) $\\overbrace{Y_i}^\\text{label}$ \\(\\underbrace{Y_i}_\\text{label}\\) $\\underbrace{Y_i}_\\text{label}$ Here is a list of all supported LaTeX commands. Insert a Picture To add a picture to your document, say some notes you took down on paper from class, Use the code: ![](./Images/insertPictureNotes.jpg) to get… Tables There are many ways to make tables in R Markdown. Here is a simple way to make a “pipe” table. | Name | Age | Gender | |---------------|---------------|--------------| | Jill | 8 | Female | | Jack | 9 | Male | Name Age Gender Jill 8 Female Jack 9 Male Themes Notice in the YAML (at the top of the RMD file) there is a line that reads: “theme: cerulean” Other possible themes are “default”, “cerulean”, “journal”, “flatly”, “readable”, “spacelab”, “unit",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus R Markdown Hints Think of an R Markdown File, or Rmd for short, as a command center.", "You write commands, then Knit the file, and an html output file is created according to your commands.", "Overview Click Here to Learn More Carefully read through all parts of this image to learn… Close The above tabs (blue bottons that read “Click Here to Learn More” and “Close”) were created with the code: ## {.tabset .tabset-pills .tabset-fade} ### Click Here to Learn More Carefully read through all parts of this image to learn...", "![](./Images/Rmd2html.png) ### Close ## Creating Links To make a link use the code [Name of Link](addressForLink).", "Linking to parts of your textbook: [Numerical Summaries](NumericalSummaries.html) becomes Numerical Summaries [Boxplots](GraphicalSummaries.html#boxplots) becomes Boxplots [R Commands](RCommands.html) becomes R Commands Linking to outside resources: [R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) becomes R Colors Creating Headers There are six available sizes of headings you can use in an Rmd file (left in image) that show up as shown below (right in image).", "Emphasizing Words To italisize a word use the asterisk (Shift 8) *italisize*.", "To bold a word use the double asterisk **bold**.", "The back tic can be used tohighlightwords by placing back tics on each side of a word: highlight `.", "Bullet Points Simple Lists To achieve the result: This is the first item.", "This is the second.", "This is the third.", "Use the code: To achieve the result: * This is the first item.", "* This is the second.", "* This is the third.", "Numbered Lists To achieve the result: This is the first item.", "This is the second.", "This is the third.", "Use the code: To achieve the result: 1.", "This is the first item.", "This is the second.", "This is the third.", "Lettered Lists To achieve the result: This is the first item.", "This is the second.", "This is the third.", "Use the code: To achieve the result: A) This is the first item.", "B) This is the second.", "C) This is the third.", "Nested Lists What is \\(2+2\\)?", "4 8 What is \\(3\\times5\\)?", "14 15 1.", "What is $2+2$?", "**4** b.", "What is $3\\times5$?", "14 b.", "**15** Math Equations Use the dollar signs $x=5$ to write \\(x=5\\) or $z=\\frac{x-\\mu}{\\sigma}$ to write \\(z=\\frac{x-\\mu}{\\sigma}\\).", "For a nicely centered equation use the double dollar signs $$ $$ on separate lines $$ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} $$ to get \\[ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] Or $$ H_0: \\mu_\\text{Group 1} = \\mu_\\text{Group 2} $$ $$ H_a: \\mu_\\text{Group 1} \\neq \\mu_\\text{Group 2} $$ to get \\[ H_0: \\mu_{\\text{Group 1}} = \\mu_{\\text{Group 2}} \\] \\[ H_a: \\mu_{\\text{Group 1}} \\neq \\mu_{\\text{Group 2}} \\] Symbol list: Symbol LaTeX Math Code \\(\\alpha\\) $\\alpha$ \\(\\beta\\) $\\beta$ \\(\\sigma\\) $\\sigma$ \\(\\epsilon\\) $\\epsilon$ \\(\\bar{x}\\) $\\bar{x}$ \\(\\hat{Y}\\) $\\hat{Y}$ \\(=\\) $=$ \\(\\ne\\) $\\ne$ or $\\neq$ \\(>\\) $>$ \\(<\\) $<$ \\(\\ge\\) $\\ge$ \\(\\le\\) $\\le$ \\(\\{ \\}\\) $\\{ \\}$ \\(\\text{Type just text}\\) $\\text{Type just text}$ \\(\\overbrace{Y_i}^\\text{label}\\) $\\overbrace{Y_i}^\\text{label}$ \\(\\underbrace{Y_i}_\\text{label}\\) $\\underbrace{Y_i}_\\text{label}$ Here is a list of all supported LaTeX commands.", "Insert a Picture To add a picture to your document, say some notes you took down on paper from class, Use the code: ![](./Images/insertPictureNotes.jpg) to get… Tables There are many ways to make tables in R Markdown.", "Here is a simple way to make a “pipe” table.", "| Name | Age | Gender | |---------------|---------------|--------------| | Jill | 8 | Female | | Jack | 9 | Male | Name Age Gender Jill 8 Female Jack 9 Male Themes Notice in the YAML (at the top of the RMD file) there is a line that reads: “theme: cerulean” Other possible themes are “default”, “cerulean”, “journal”, “flatly”, “readable”, “spacelab”, “united”, and “cosmo”.", "You can also change the highlighting by adding the line “highlight: tango” to the YAML as follows.", "--- title: \"Markdown Hints\" output: html_document: theme: cerulean highlight: tango --- Other highlighting options are “default”, “tango”, “pygments”, “kate”, “monochrome”, “espresso”, “zenburn”, “haddock”, and “textmate”.", "More Information Go to the rmarkdown.rstudio.com website for more information on how to use R Markdown.", "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus R Markdown Hints Think of an R Markdown File, or Rmd for short, as a command center.", "You write commands, then Knit the file, and an html output file is created according to your commands.", "Overview Click Here to Learn More Carefully read through all parts of this image to learn… Close The above tabs (blue bottons that read “Click Here to Learn More” and “Close”) were created with the code: ## {.tabset .tabset-pills .tabset-fade} ### Click Here to Learn More Carefully read through all parts of this image to learn...", "![](./Images/Rmd2html.png) ### Close ## Creating Links To make a link use the code [Name of Link](addressForLink).", "Linking to parts of your textbook: [Numerical Summaries](NumericalSummaries.html) becomes Numerical Summaries [Boxplots](GraphicalSummaries.html#boxplots) becomes Boxplots [R Commands](RCommands.html) becomes R Commands Linking to outside resources: [R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) becomes R Colors Creating Headers There are six available sizes of headings you can use in an Rmd file (left in image) that show up as shown below (right in image).", "Emphasizing Words To italisize a word use the asterisk (Shift 8) *italisize*.", "To bold a word use the double asterisk **bold**.", "The back tic can be used tohighlightwords by placing back tics on each side of a word: highlight `."],
    "type": "page",
    "page_title": "R Markdown Hints"
  },
  {
    "id": 312,
    "title": "📍 Overview",
    "url": "RMarkdownHints.html#overview",
    "content": "Overview Click Here to Learn More Carefully read through all parts of this image to learn… Close The above tabs (blue bottons that read “Click Here to Learn More” and “Close”) were created with the code: ## {.tabset .tabset-pills .tabset-fade} ### Click Here to Learn More Carefully read through all parts of this image to learn... ![](./Images/Rmd2html.png) ### Close ##",
    "sentences": ["Click Here to Learn More Carefully read through all parts of this image to learn… Close", "The above tabs (blue bottons that read “Click Here to Learn More” and “Close”) were created with the code: ## {.tabset .tabset-pills .tabset-fade} ### Click Here to Learn More Carefully read through all parts of this image to learn...", "![](./Images/Rmd2html.png) ### Close ##"],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Overview"
  },
  {
    "id": 313,
    "title": "📍 Click Here to Learn More",
    "url": "RMarkdownHints.html#clickheretolearnmore",
    "content": "Click Here to Learn More Carefully read through all parts of this image to learn…",
    "sentences": "Carefully read through all parts of this image to learn…",
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Click Here to Learn More"
  },
  {
    "id": 314,
    "title": "📍 Creating Links",
    "url": "RMarkdownHints.html#creatinglinks",
    "content": "Creating Links To make a link use the code [Name of Link](addressForLink). Linking to parts of your textbook: [Numerical Summaries](NumericalSummaries.html) becomes Numerical Summaries [Boxplots](GraphicalSummaries.html#boxplots) becomes Boxplots [R Commands](RCommands.html) becomes R Commands Linking to outside resources: [R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) becomes R Colors",
    "sentences": ["To make a link use the code [Name of Link](addressForLink).", "Linking to parts of your textbook:", "[Numerical Summaries](NumericalSummaries.html) becomes Numerical Summaries [Boxplots](GraphicalSummaries.html#boxplots) becomes Boxplots [R Commands](RCommands.html) becomes R Commands", "Linking to outside resources:", "[R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) becomes R Colors"],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Creating Links"
  },
  {
    "id": 315,
    "title": "📍 Creating Headers",
    "url": "RMarkdownHints.html#creatingheaders",
    "content": "Creating Headers There are six available sizes of headings you can use in an Rmd file (left in image) that show up as shown below (right in image).",
    "sentences": "There are six available sizes of headings you can use in an Rmd file (left in image) that show up as shown below (right in image).",
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Creating Headers"
  },
  {
    "id": 316,
    "title": "📍 Emphasizing Words",
    "url": "RMarkdownHints.html#emphasizingwords",
    "content": "Emphasizing Words To italisize a word use the asterisk (Shift 8) *italisize*. To bold a word use the double asterisk **bold**. The back tic can be used tohighlightwords by placing back tics on each side of a word: highlight `.",
    "sentences": ["To italisize a word use the asterisk (Shift 8) *italisize*.", "To bold a word use the double asterisk **bold**.", "The back tic can be used tohighlightwords by placing back tics on each side of a word: highlight `."],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Emphasizing Words"
  },
  {
    "id": 317,
    "title": "📍 Bullet Points",
    "url": "RMarkdownHints.html#bulletpoints",
    "content": "Bullet Points Simple Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: * This is the first item. * This is the second. * This is the third. Numbered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: 1. This is the first item. This is the second. This is the third.",
    "sentences": ["Simple Lists To achieve the result: This is the first item.", "This is the second.", "This is the third.", "Use the code: To achieve the result: * This is the first item.", "* This is the second.", "* This is the third.", "Numbered Lists To achieve the result: This is the first item.", "This is the second.", "This is the third.", "Use the code: To achieve the result: 1."],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Bullet Points"
  },
  {
    "id": 318,
    "title": "📍 Simple Lists",
    "url": "RMarkdownHints.html#simplelists",
    "content": "Simple Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: * This is the first item. * This is the second. * This is the third.",
    "sentences": ["To achieve the result: This is the first item.", "This is the second.", "This is the third.", "Use the code: To achieve the result: * This is the first item.", "* This is the second.", "* This is the third."],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Simple Lists"
  },
  {
    "id": 319,
    "title": "📍 Numbered Lists",
    "url": "RMarkdownHints.html#numberedlists",
    "content": "Numbered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: 1. This is the first item. This is the second. This is the third.",
    "sentences": ["To achieve the result: This is the first item.", "This is the second.", "This is the third.", "Use the code: To achieve the result: 1.", "This is the first item.", "This is the second.", "This is the third."],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Numbered Lists"
  },
  {
    "id": 320,
    "title": "📍 Lettered Lists",
    "url": "RMarkdownHints.html#letteredlists",
    "content": "Lettered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: A) This is the first item. B) This is the second. C) This is the third.",
    "sentences": ["To achieve the result: This is the first item.", "This is the second.", "This is the third.", "Use the code: To achieve the result: A) This is the first item.", "B) This is the second.", "C) This is the third."],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Lettered Lists"
  },
  {
    "id": 321,
    "title": "📍 Nested Lists",
    "url": "RMarkdownHints.html#nestedlists",
    "content": "Nested Lists What is \\(2+2\\)? 4 8 What is \\(3\\times5\\)? What is $2+2$? **4** b. What is $3\\times5$? 14 b.",
    "sentences": ["What is \\(2+2\\)?", "4 8 What is \\(3\\times5\\)?", "What is $2+2$?", "**4** b.", "What is $3\\times5$?", "14 b."],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Nested Lists"
  },
  {
    "id": 322,
    "title": "📍 Math Equations",
    "url": "RMarkdownHints.html#mathequations",
    "content": "Math Equations Use the dollar signs $x=5$ to write \\(x=5\\) or $z=\\frac{x-\\mu}{\\sigma}$ to write \\(z=\\frac{x-\\mu}{\\sigma}\\). For a nicely centered equation use the double dollar signs $$ $$ on separate lines to get \\[ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] to get \\[ H_0: \\mu_{\\text{Group 1}} = \\mu_{\\text{Group 2}} \\] \\[ H_a: \\mu_{\\text{Group 1}} \\neq \\mu_{\\text{Group 2}} \\] Here is a list of all supported LaTeX commands.",
    "sentences": ["Use the dollar signs $x=5$ to write \\(x=5\\) or $z=\\frac{x-\\mu}{\\sigma}$ to write \\(z=\\frac{x-\\mu}{\\sigma}\\).", "For a nicely centered equation use the double dollar signs $$ $$ on separate lines", "to get \\[ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\]", "to get \\[ H_0: \\mu_{\\text{Group 1}} = \\mu_{\\text{Group 2}} \\] \\[ H_a: \\mu_{\\text{Group 1}} \\neq \\mu_{\\text{Group 2}} \\]", "Here is a list of all supported LaTeX commands."],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Math Equations"
  },
  {
    "id": 323,
    "title": "📍 Insert a Picture",
    "url": "RMarkdownHints.html#insertapicture",
    "content": "Insert a Picture To add a picture to your document, say some notes you took down on paper from class, Use the code: ![](./Images/insertPictureNotes.jpg) to get…",
    "sentences": ["To add a picture to your document, say some notes you took down on paper from class,", "Use the code: ![](./Images/insertPictureNotes.jpg) to get…"],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Insert a Picture"
  },
  {
    "id": 324,
    "title": "📍 Tables",
    "url": "RMarkdownHints.html#tables",
    "content": "Tables There are many ways to make tables in R Markdown. Here is a simple way to make a “pipe” table. | Name | Age | Gender | |---------------|---------------|--------------| | Jill | 8 | Female | | Jack | 9 | Male |",
    "sentences": ["There are many ways to make tables in R Markdown.", "Here is a simple way to make a “pipe” table.", "| Name | Age | Gender | |---------------|---------------|--------------| | Jill | 8 | Female | | Jack | 9 | Male |"],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Tables"
  },
  {
    "id": 325,
    "title": "📍 Themes",
    "url": "RMarkdownHints.html#themes",
    "content": "Themes Notice in the YAML (at the top of the RMD file) there is a line that reads: “theme: cerulean” Other possible themes are “default”, “cerulean”, “journal”, “flatly”, “readable”, “spacelab”, “united”, and “cosmo”. You can also change the highlighting by adding the line “highlight: tango” to the YAML as follows. --- title: \"Markdown Hints\" output: html_document: theme: cerulean highlight: tango --- Other highlighting options are “default”, “tango”, “pygments”, “kate”, “monochrome”, “espresso”, “zenburn”, “haddock”, and “textmate”.",
    "sentences": ["Notice in the YAML (at the top of the RMD file) there is a line that reads:", "“theme: cerulean”", "Other possible themes are", "“default”, “cerulean”, “journal”, “flatly”, “readable”, “spacelab”, “united”, and “cosmo”.", "You can also change the highlighting by adding the line “highlight: tango” to the YAML as follows.", "--- title: \"Markdown Hints\" output: html_document: theme: cerulean highlight: tango ---", "Other highlighting options are", "“default”, “tango”, “pygments”, “kate”, “monochrome”, “espresso”, “zenburn”, “haddock”, and “textmate”."],
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "Themes"
  },
  {
    "id": 326,
    "title": "📍 More Information",
    "url": "RMarkdownHints.html#moreinformation",
    "content": "More Information Go to the rmarkdown.rstudio.com website for more information on how to use R Markdown.",
    "sentences": "Go to the rmarkdown.rstudio.com website for more information on how to use R Markdown.",
    "type": "section",
    "page_title": "R Markdown Hints",
    "section_title": "More Information"
  },
  {
    "id": 327,
    "title": "t Tests",
    "url": "tTests.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus t Tests Much of statistical inference concerns the location of the population mean \\(\\mu\\) for a given parametric distribution. Some of the most common approaches to making inference about \\(\\mu\\) utilize a test statistic that follows a t distribution. One Sample t Test A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable. Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average? Is human body temperature really 98.6&deg; F on average? Do I spend less than $3 a day, on average, purchasing snacks? Requirements This test is only appropriate when both of the following are satisfied. The sample is representative of the population. (Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal. This is a safe assumption when either (a) the population data can be assumed to be normally distributed using a Q-Q Plot or (b) the size of the sample (n) that was taken from the population is large (at least n > 30, but “large” really depends on how badly the data is skewed). If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population. If the requirements are not met, then that doesn’t mean the results of the test are necessarily bad, but there is no guarantee that they are good. Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits R Instructions Console Help Command: ?t.test() t.test(NameOfYourData$Y, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set, like mtcars or KidsFeet. Y must be a “numeric” vector of quantitative data. YourNull is the numeric value from your null hypothesis for \\(\\mu\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(NameOfYourData$Y) Example Code Hover your mouse over the example codes to learn more. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,  ‘mpg’ is Y, a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,  The numeric value from the null hypothesis is 20 meaning \\(\\mu=20\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1−α.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. One Sample t-test EXPLANATION. data: mtcars$mpg EXPLANATION. t = 0.08506, EXPLANATION.  df = 31, EXPLANATION.  p-value = 0.9328 EXPLANATION. alternative hypothesis: true mean is not equal to 20 EXPLANATION. 95 percent confidence interval: EXPLANATION.  17.91768 EXPLANATION.  22.26357 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION.  20.09062 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg) ‘mpg’ is a quantitative variable (numeric vector) from the mtcars dataset.  Click to Show Output  Click to View Output. ## [1] 20 18 Explanation In many cases where it is of interest to test a claim about a single population mean \\(\\mu\\), the one sample t test is used. This is an appropriate decision whenever the sampling distribution of the sample mean can be assumed to be normal and the data represents a simple random sample from the population. In the figure below, the null hypothesis \\(H_0: \\mu = \\mu_0\\) is represented by the normal distribution (gray) centered at \\(\\mu_0\\). Note that \\(\\mu_0\\) is just some specified number. This shows how the null hypothesis represents the assumption about the center of the distribution of the data. After a hypothesis (null) is established and an alternative hypothesis similarly declared, a simple random sample of data of size \\(n\\) is obtained from the population of interest. In the plot above, this is depicted by the points (blue dots) which are centered around their sample mean \\(\\bar{x}\\). Above the points (blue dots) is shown a second normal distribution (blue dashed line) which represents the idea that the alternative hypothesis allows for a normal distribution which is potentially more consistent with the data than the one specified under the null hypothesis. The role of the one sample t test is to measure the probability of a sample mean being as extreme or more extreme from the hypothesized value of \\(\\mu_0\\) than the one observed assuming the null hypothesis is true. This probability is of course the p-value of the test. This works because the sampling distribution of the sample mean has been assumed to be normal. In this case, the distribution of the test statistic t, \\[ t = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\] is known to follow a t distribution with \\(n-1\\) degrees of freedom. (The mathematics that provide this result are phenominal! You can consult any advanced statistical textbook for the details.) The p-value of the one sample t test represents the probability that the test statistic \\(t\\) is as extreme or more extreme than the one observed according to a t-distribution with \\(n-1\\) degrees of freedom. If the probability (the p-value) is close enough to zero (smaller than \\(\\alpha\\)) then it is determined that the most plausible hypothesis is the alternative hypothesis, and thus the null is “rejected” in favor of the alternative. Paired Samples t Test The paired samples t test is used when a value is hypothesized for the popluation mean of the differences, \\(\\mu_d\\), obtained from paired observations. Overview Questions The Paired Samples t Test can be used to answer questions like: From pre-test to post-test is there an improvement on average in the subjects? How much taller are husbands than their wives, on average? Do hospital patients that are carefully matched together according to reason for being in the hospital, age, gender, ethnicity, height, and weight show increased stay times in the hospital when infected with a nosocomial infection compared to those who were not infected? Requirements The test is only appropriate when both of the following are satisfied. The sample of differences is representative of the population differences. The sampling distribution of the sample mean of the differences \\(\\bar{d}\\) (\\(\\bar{x}\\) of the differences) can be assumed to be normal. (This second requirement can be assumed to be satisfied when (a) the differences themselves can be assumed to be normal from a Q-Q Plot, or (b) when the sample size \\(n\\) of the differences is large.) Hypotheses Math Code $$ H_0: \\mu_d = 0 $$ $$ H_a: \\mu_d \\neq 0 $$ \\(H_0: \\mu_d = \\text{some number, but typically 0}\\)\\(H_a: \\mu_d \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepPaired studentPaired R Instructions Console Help Command: ?t.test() Option 1: t.test(NameOfYourData$Y1, NameOfYourData$Y2, paired = TRUE, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set like sleep or mtcars or KidsFeet. Y1 must be a “numeric” vector that represents the quantitative data from the first sample of data. Y2 must be a “numeric” vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y1 - Y2) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the “group1” data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the “group2” data from the sleep data set t.test( ‘t.test’ is an R function that performs one and two sample t-tests. sleep2$extra,  A numeric vector that represents the hours of extra sleep that the group had with drug 2. sleep1$extra,  A numeric vector that represents the hours of extra sleep that the same group had with drug 1. paired=TRUE,  Indicates that this is a paired t-Test. This will cause the subtraction of sleep2$extra - sleep1$extra to be performed to obtain the paired differences. To cause the subtraction to occur in the other order, reverse the order sleep1$extra, sleep2$extra occur in the t.test(…) function. mu = 0,  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. Paired t-test EXPLANATION. data: sleep2$extra and sleep1$extra EXPLANATION. t = 4.0621, EXPLANATION.  df = 9, EXPLANATION.  p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  0.7001142 EXPLANATION.  2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of the differences EXPLANATION.   1.58 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. sleep2$extra The hours of extra sleep that the group had with drug 2.  -  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. ) Closing parenthesis for qqPlot(…) function.  Click to Show Output  Click to View Output. ## [1] 9 5 Option 2: Compute the differences yourself instead of using paired=TRUE. differences = NameOfYourData$Y1 - NameOfYourData$Y2 t.test(differences, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a “numeric” vector that represents the quantitative data from the first sample of data. Y2 must be a “numeric” vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. differences are the resulting differences obtained from subtracting Y1 - Y2. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(differences) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the “group1” data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the “group2” data from the sleep data set differences <-  Saved the computed differences to an object called ‘differences’. sleep2$extra The hours of extra sleep that the group had with drug 2.  -  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. differences,  ‘differences’ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2. mu = 0,  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. One Sample t-test EXPLANATION. data: differences EXPLANATION. t = 4.0621, EXPLANATION.  df = 9, EXPLANATION.  p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  0.7001142 EXPLANATION.  2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION.   1.58 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. differences) ‘differences’ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2.  Click to Show Output  Click to View Output. ## [1] 9 5 Explanation The paired samples t test considers the single mean of all the differences from the paired values. Thus, the paired samples t test essentially becomes a one sample t test on the differences between paired observations. Hence the requirement is that the sampling distribution of the sample mean of the differences, \\(\\bar{d}\\), can be assumed to be normally distri",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus t Tests Much of statistical inference concerns the location of the population mean \\(\\mu\\) for a given parametric distribution.", "Some of the most common approaches to making inference about \\(\\mu\\) utilize a test statistic that follows a t distribution.", "One Sample t Test A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable.", "Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average?", "Is human body temperature really 98.6&deg; F on average?", "Do I spend less than $3 a day, on average, purchasing snacks?", "Requirements This test is only appropriate when both of the following are satisfied.", "The sample is representative of the population.", "(Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal.", "This is a safe assumption when either (a) the population data can be assumed to be normally distributed using a Q-Q Plot or (b) the size of the sample (n) that was taken from the population is large (at least n > 30, but “large” really depends on how badly the data is skewed).", "If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population.", "If the requirements are not met, then that doesn’t mean the results of the test are necessarily bad, but there is no guarantee that they are good.", "Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits R Instructions Console Help Command: ?t.test() t.test(NameOfYourData$Y, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set, like mtcars or KidsFeet.", "Y must be a “numeric” vector of quantitative data.", "YourNull is the numeric value from your null hypothesis for \\(\\mu\\).", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Testing Assumptions library(car) qqPlot(NameOfYourData$Y) Example Code Hover your mouse over the example codes to learn more.", "t.test( ‘t.test’ is an R function that performs one and two sample t-tests.", "mtcars ‘mtcars’ is a dataset.", "Type ‘View(mtcars)’ in R to view the dataset.", "$ The $ allows us to access any variable from the mtcars dataset.", "mpg,  ‘mpg’ is Y, a quantitative variable (numeric vector) from the mtcars dataset.", "mu = 20,  The numeric value from the null hypothesis is 20 meaning \\(\\mu=20\\).", "alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu\\neq20\\).", "conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1−α.", "    Press Enter to run the code if you have typed it in yourself.", "You can also click here to view the output.", " Click to Show Output  Click to View Output.", "One Sample t-test EXPLANATION.", "data: mtcars$mpg EXPLANATION.", "t = 0.08506, EXPLANATION.", " df = 31, EXPLANATION.", " p-value = 0.9328 EXPLANATION.", "alternative hypothesis: true mean is not equal to 20 EXPLANATION.", "95 percent confidence interval: EXPLANATION.", " 17.91768 EXPLANATION.", " 22.26357 EXPLANATION.", "sample estimates: EXPLANATION.", "mean of x EXPLANATION.", " 20.09062 EXPLANATION.", "qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot.", "mtcars ‘mtcars’ is a dataset.", "Type ‘View(mtcars)’ in R to view the dataset.", "$ The $ allows us to access any variable from the mtcars dataset.", "mpg) ‘mpg’ is a quantitative variable (numeric vector) from the mtcars dataset.", " Click to Show Output  Click to View Output.", "## [1] 20 18 Explanation In many cases where it is of interest to test a claim about a single population mean \\(\\mu\\), the one sample t test is used.", "This is an appropriate decision whenever the sampling distribution of the sample mean can be assumed to be normal and the data represents a simple random sample from the population."],
    "type": "page",
    "page_title": "t Tests"
  },
  {
    "id": 328,
    "title": "📍 One Sample t Test",
    "url": "tTests.html#onesamplettest",
    "content": "One Sample t Test A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable. Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average? Is human body temperature really 98.6&deg; F on average? Do I spend less than $3 a day, on average, purchasing snacks? Requirements This test is only appropriate when both of the following are satisfied. The sample is representative of the population. (Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal. This is a safe assumption when either (a) the population data can be assumed to be normally distributed using a Q-Q Plot or (b) the size of the sample (n) that was taken from the population is large (at least n > 30, but “large” really depends on how badly the data is skewed). If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population. If the requirements are not met, then that doesn’t mean the results of the test are necessarily bad, but there is no guarantee that they are good. Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits",
    "sentences": ["A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable.", "Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average?", "Is human body temperature really 98.6&deg; F on average?", "Do I spend less than $3 a day, on average, purchasing snacks?", "Requirements This test is only appropriate when both of the following are satisfied.", "The sample is representative of the population.", "(Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal.", "This is a safe assumption when either (a) the population data can be assumed to be normally distributed using a Q-Q Plot or (b) the size of the sample (n) that was taken from the population is large (at least n > 30, but “large” really depends on how badly the data is skewed).", "If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population.", "If the requirements are not met, then that doesn’t mean the results of the test are necessarily bad, but there is no guarantee that they are good."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "One Sample t Test"
  },
  {
    "id": 329,
    "title": "📍 Overview",
    "url": "tTests.html#overview",
    "content": "Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average? Is human body temperature really 98.6&deg; F on average? Do I spend less than $3 a day, on average, purchasing snacks? Requirements This test is only appropriate when both of the following are satisfied. The sample is representative of the population. (Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal. This is a safe assumption when either (a) the population data can be assumed to be normally distributed using a Q-Q Plot or (b) the size of the sample (n) that was taken from the population is large (at least n > 30, but “large” really depends on how badly the data is skewed). If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population. If the requirements are not met, then that doesn’t mean the results of the test are necessarily bad, but there is no guarantee that they are good. Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits",
    "sentences": ["Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average?", "Is human body temperature really 98.6&deg; F on average?", "Do I spend less than $3 a day, on average, purchasing snacks?", "Requirements This test is only appropriate when both of the following are satisfied.", "The sample is representative of the population.", "(Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal.", "This is a safe assumption when either (a) the population data can be assumed to be normally distributed using a Q-Q Plot or (b) the size of the sample (n) that was taken from the population is large (at least n > 30, but “large” really depends on how badly the data is skewed).", "If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population.", "If the requirements are not met, then that doesn’t mean the results of the test are necessarily bad, but there is no guarantee that they are good.", "Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits"],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "Overview"
  },
  {
    "id": 330,
    "title": "📍 R Instructions",
    "url": "tTests.html#rinstructions",
    "content": "R Instructions Console Help Command: ?t.test() t.test(NameOfYourData$Y, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set, like mtcars or KidsFeet. Y must be a “numeric” vector of quantitative data. YourNull is the numeric value from your null hypothesis for \\(\\mu\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(NameOfYourData$Y) Example Code Hover your mouse over the example codes to learn more. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,  ‘mpg’ is Y, a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,  The numeric value from the null hypothesis is 20 meaning \\(\\mu=20\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1−α.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. One Sample t-test EXPLANATION. data: mtcars$mpg EXPLANATION. t = 0.08506, EXPLANATION.  df = 31, EXPLANATION.  p-value = 0.9328 EXPLANATION. alternative hypothesis: true mean is not equal to 20 EXPLANATION. 95 percent confidence interval: EXPLANATION.  17.91768 EXPLANATION.  22.26357 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION.  20.09062 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg) ‘mpg’ is a quantitative variable (numeric vector) from the mtcars dataset.  Click to Show Output  Click to View Output. ## [1] 20 18",
    "sentences": ["Console Help Command: ?t.test() t.test(NameOfYourData$Y, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set, like mtcars or KidsFeet.", "Y must be a “numeric” vector of quantitative data.", "YourNull is the numeric value from your null hypothesis for \\(\\mu\\).", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Testing Assumptions library(car) qqPlot(NameOfYourData$Y) Example Code Hover your mouse over the example codes to learn more.", "t.test( ‘t.test’ is an R function that performs one and two sample t-tests.", "mtcars ‘mtcars’ is a dataset.", "Type ‘View(mtcars)’ in R to view the dataset."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "R Instructions"
  },
  {
    "id": 331,
    "title": "📍 Explanation",
    "url": "tTests.html#explanation",
    "content": "Explanation In many cases where it is of interest to test a claim about a single population mean \\(\\mu\\), the one sample t test is used. This is an appropriate decision whenever the sampling distribution of the sample mean can be assumed to be normal and the data represents a simple random sample from the population. In the figure below, the null hypothesis \\(H_0: \\mu = \\mu_0\\) is represented by the normal distribution (gray) centered at \\(\\mu_0\\). Note that \\(\\mu_0\\) is just some specified number. This shows how the null hypothesis represents the assumption about the center of the distribution of the data. After a hypothesis (null) is established and an alternative hypothesis similarly declared, a simple random sample of data of size \\(n\\) is obtained from the population of interest. In the plot above, this is depicted by the points (blue dots) which are centered around their sample mean \\(\\bar{x}\\). Above the points (blue dots) is shown a second normal distribution (blue dashed line) which represents the idea that the alternative hypothesis allows for a normal distribution which is potentially more consistent with the data than the one specified under the null hypothesis. The role of the one sample t test is to measure the probability of a sample mean being as extreme or more extreme from the hypothesized value of \\(\\mu_0\\) than the one observed assuming the null hypothesis is true. This probability is of course the p-value of the test. This works because the sampling distribution of the sample mean has been assumed to be normal. In this case, the distribution of the test statistic t, \\[ t = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\] is known to follow a t distribution with \\(n-1\\) degrees of freedom. (The mathematics that provide this result are phenominal! You can consult any advanced statistical textbook for the details.) The p-value of the one sample t test represents the probability that the test statistic \\(t\\) is as extreme or more extreme than the one observed according to a t-distribution with \\(n-1\\) degrees of freedom. If the probability (the p-value) is close enough to zero (smaller than \\(\\alpha\\)) then it is determined that the most plausible hypothesis is the alternative hypothesis, and thus the null is “rejected” in favor of the alternative.",
    "sentences": ["In many cases where it is of interest to test a claim about a single population mean \\(\\mu\\), the one sample t test is used.", "This is an appropriate decision whenever the sampling distribution of the sample mean can be assumed to be normal and the data represents a simple random sample from the population.", "In the figure below, the null hypothesis \\(H_0: \\mu = \\mu_0\\) is represented by the normal distribution (gray) centered at \\(\\mu_0\\).", "Note that \\(\\mu_0\\) is just some specified number.", "This shows how the null hypothesis represents the assumption about the center of the distribution of the data.", "After a hypothesis (null) is established and an alternative hypothesis similarly declared, a simple random sample of data of size \\(n\\) is obtained from the population of interest.", "In the plot above, this is depicted by the points (blue dots) which are centered around their sample mean \\(\\bar{x}\\).", "Above the points (blue dots) is shown a second normal distribution (blue dashed line) which represents the idea that the alternative hypothesis allows for a normal distribution which is potentially more consistent with the data than the one specified under the null hypothesis.", "The role of the one sample t test is to measure the probability of a sample mean being as extreme or more extreme from the hypothesized value of \\(\\mu_0\\) than the one observed assuming the null hypothesis is true.", "This probability is of course the p-value of the test."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "Explanation"
  },
  {
    "id": 332,
    "title": "📍 Paired Samples t Test",
    "url": "tTests.html#pairedsamplesttest",
    "content": "Paired Samples t Test The paired samples t test is used when a value is hypothesized for the popluation mean of the differences, \\(\\mu_d\\), obtained from paired observations. Overview Questions The Paired Samples t Test can be used to answer questions like: From pre-test to post-test is there an improvement on average in the subjects? How much taller are husbands than their wives, on average? Do hospital patients that are carefully matched together according to reason for being in the hospital, age, gender, ethnicity, height, and weight show increased stay times in the hospital when infected with a nosocomial infection compared to those who were not infected? Requirements The test is only appropriate when both of the following are satisfied. The sample of differences is representative of the population differences. The sampling distribution of the sample mean of the differences \\(\\bar{d}\\) (\\(\\bar{x}\\) of the differences) can be assumed to be normal. (This second requirement can be assumed to be satisfied when (a) the differences themselves can be assumed to be normal from a Q-Q Plot, or (b) when the sample size \\(n\\) of the differences is large.) Hypotheses Math Code $$ H_0: \\mu_d = 0 $$ $$ H_a: \\mu_d \\neq 0 $$ \\(H_0: \\mu_d = \\text{some number, but typically 0}\\)\\(H_a: \\mu_d \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepPaired studentPaired R Instructions Console Help Command: ?t.test() Option 1: t.test(NameOfYourData$Y1, NameOfYourData$Y2, paired = TRUE, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set like sleep or mtcars or KidsFeet. Y1 must be a “numeric” vector that represents the quantitative data from the first sample of data. Y2 must be a “numeric” vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y1 - Y2) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the “group1” data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the “group2” data from the sleep data set t.test( ‘t.test’ is an R function that performs one and two sample t-tests. sleep2$extra,  A numeric vector that represents the hours of extra sleep that the group had with drug 2. sleep1$extra,  A numeric vector that represents the hours of extra sleep that the same group had with drug 1. paired=TRUE,  Indicates that this is a paired t-Test. This will cause the subtraction of sleep2$extra - sleep1$extra to be performed to obtain the paired differences. To cause the subtraction to occur in the other order, reverse the order sleep1$extra, sleep2$extra occur in the t.test(…) function. mu = 0,  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. Paired t-test EXPLANATION. data: sleep2$extra and sleep1$extra EXPLANATION. t = 4.0621, EXPLANATION.  df = 9, EXPLANATION.  p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  0.7001142 EXPLANATION.  2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of the differences EXPLANATION.   1.58 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. sleep2$extra The hours of extra sleep that the group had with drug 2.  -  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. ) Closing parenthesis for qqPlot(…) function.  Click to Show Output  Click to View Output. ## [1] 9 5 Option 2: Compute the differences yourself instead of using paired=TRUE. differences = NameOfYourData$Y1 - NameOfYourData$Y2 t.test(differences, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a “numeric” vector that represents the quantitative data from the first sample of data. Y2 must be a “numeric” vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. differences are the resulting differences obtained from subtracting Y1 - Y2. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(differences) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the “group1” data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the “group2” data from the sleep data set differences <-  Saved the computed differences to an object called ‘differences’. sleep2$extra The hours of extra sleep that the group had with drug 2.  -  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. differences,  ‘differences’ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2. mu = 0,  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. One Sample t-test EXPLANATION. data: differences EXPLANATION. t = 4.0621, EXPLANATION.  df = 9, EXPLANATION.  p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  0.7001142 EXPLANATION.  2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION.   1.58 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. differences) ‘differences’ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2.  Click to Show Output  Click to View Output.",
    "sentences": ["The paired samples t test is used when a value is hypothesized for the popluation mean of the differences, \\(\\mu_d\\), obtained from paired observations.", "Overview Questions The Paired Samples t Test can be used to answer questions like: From pre-test to post-test is there an improvement on average in the subjects?", "How much taller are husbands than their wives, on average?", "Do hospital patients that are carefully matched together according to reason for being in the hospital, age, gender, ethnicity, height, and weight show increased stay times in the hospital when infected with a nosocomial infection compared to those who were not infected?", "Requirements The test is only appropriate when both of the following are satisfied.", "The sample of differences is representative of the population differences.", "The sampling distribution of the sample mean of the differences \\(\\bar{d}\\) (\\(\\bar{x}\\) of the differences) can be assumed to be normal.", "(This second requirement can be assumed to be satisfied when (a) the differences themselves can be assumed to be normal from a Q-Q Plot, or (b) when the sample size \\(n\\) of the differences is large.) Hypotheses Math Code $$ H_0: \\mu_d = 0 $$ $$ H_a: \\mu_d \\neq 0 $$ \\(H_0: \\mu_d = \\text{some number, but typically 0}\\)\\(H_a: \\mu_d \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepPaired studentPaired", "R Instructions Console Help Command: ?t.test() Option 1: t.test(NameOfYourData$Y1, NameOfYourData$Y2, paired = TRUE, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set like sleep or mtcars or KidsFeet.", "Y1 must be a “numeric” vector that represents the quantitative data from the first sample of data."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "Paired Samples t Test"
  },
  {
    "id": 333,
    "title": "📍 Overview",
    "url": "tTests.html#overview",
    "content": "Overview Questions The Paired Samples t Test can be used to answer questions like: From pre-test to post-test is there an improvement on average in the subjects? How much taller are husbands than their wives, on average? Do hospital patients that are carefully matched together according to reason for being in the hospital, age, gender, ethnicity, height, and weight show increased stay times in the hospital when infected with a nosocomial infection compared to those who were not infected? Requirements The test is only appropriate when both of the following are satisfied. The sample of differences is representative of the population differences. The sampling distribution of the sample mean of the differences \\(\\bar{d}\\) (\\(\\bar{x}\\) of the differences) can be assumed to be normal. (This second requirement can be assumed to be satisfied when (a) the differences themselves can be assumed to be normal from a Q-Q Plot, or (b) when the sample size \\(n\\) of the differences is large.) Hypotheses Math Code $$ H_0: \\mu_d = 0 $$ $$ H_a: \\mu_d \\neq 0 $$ \\(H_0: \\mu_d = \\text{some number, but typically 0}\\)\\(H_a: \\mu_d \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepPaired studentPaired",
    "sentences": ["Questions The Paired Samples t Test can be used to answer questions like: From pre-test to post-test is there an improvement on average in the subjects?", "How much taller are husbands than their wives, on average?", "Do hospital patients that are carefully matched together according to reason for being in the hospital, age, gender, ethnicity, height, and weight show increased stay times in the hospital when infected with a nosocomial infection compared to those who were not infected?", "Requirements The test is only appropriate when both of the following are satisfied.", "The sample of differences is representative of the population differences.", "The sampling distribution of the sample mean of the differences \\(\\bar{d}\\) (\\(\\bar{x}\\) of the differences) can be assumed to be normal.", "(This second requirement can be assumed to be satisfied when (a) the differences themselves can be assumed to be normal from a Q-Q Plot, or (b) when the sample size \\(n\\) of the differences is large.) Hypotheses Math Code $$ H_0: \\mu_d = 0 $$ $$ H_a: \\mu_d \\neq 0 $$ \\(H_0: \\mu_d = \\text{some number, but typically 0}\\)\\(H_a: \\mu_d \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepPaired studentPaired"],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "Overview"
  },
  {
    "id": 334,
    "title": "📍 R Instructions",
    "url": "tTests.html#rinstructions",
    "content": "R Instructions Console Help Command: ?t.test() Option 1: t.test(NameOfYourData$Y1, NameOfYourData$Y2, paired = TRUE, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set like sleep or mtcars or KidsFeet. Y1 must be a “numeric” vector that represents the quantitative data from the first sample of data. Y2 must be a “numeric” vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y1 - Y2) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the “group1” data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the “group2” data from the sleep data set t.test( ‘t.test’ is an R function that performs one and two sample t-tests. sleep2$extra,  A numeric vector that represents the hours of extra sleep that the group had with drug 2. sleep1$extra,  A numeric vector that represents the hours of extra sleep that the same group had with drug 1. paired=TRUE,  Indicates that this is a paired t-Test. This will cause the subtraction of sleep2$extra - sleep1$extra to be performed to obtain the paired differences. To cause the subtraction to occur in the other order, reverse the order sleep1$extra, sleep2$extra occur in the t.test(…) function. mu = 0,  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. Paired t-test EXPLANATION. data: sleep2$extra and sleep1$extra EXPLANATION. t = 4.0621, EXPLANATION.  df = 9, EXPLANATION.  p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  0.7001142 EXPLANATION.  2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of the differences EXPLANATION.   1.58 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. sleep2$extra The hours of extra sleep that the group had with drug 2.  -  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. ) Closing parenthesis for qqPlot(…) function.  Click to Show Output  Click to View Output. ## [1] 9 5 Option 2: Compute the differences yourself instead of using paired=TRUE. differences = NameOfYourData$Y1 - NameOfYourData$Y2 t.test(differences, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a “numeric” vector that represents the quantitative data from the first sample of data. Y2 must be a “numeric” vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. differences are the resulting differences obtained from subtracting Y1 - Y2. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(differences) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the “group1” data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the “group2” data from the sleep data set differences <-  Saved the computed differences to an object called ‘differences’. sleep2$extra The hours of extra sleep that the group had with drug 2.  -  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. differences,  ‘differences’ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2. mu = 0,  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. One Sample t-test EXPLANATION. data: differences EXPLANATION. t = 4.0621, EXPLANATION.  df = 9, EXPLANATION.  p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  0.7001142 EXPLANATION.  2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION.   1.58 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. differences) ‘differences’ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2.  Click to Show Output  Click to View Output.",
    "sentences": ["Console Help Command: ?t.test() Option 1: t.test(NameOfYourData$Y1, NameOfYourData$Y2, paired = TRUE, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set like sleep or mtcars or KidsFeet.", "Y1 must be a “numeric” vector that represents the quantitative data from the first sample of data.", "Y2 must be a “numeric” vector that represents the quantitative data from the second sample of data.", "This vector must be in the same order as the first sample so that the pairing can take place.", "YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\).", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Testing Assumptions library(car) qqPlot(Y1 - Y2) Example Code Hover your mouse over the example codes to learn more.", "sleep1 <- filter(sleep, group==1) This splits out the “group1” data from the sleep data set."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "R Instructions"
  },
  {
    "id": 335,
    "title": "📍 Explanation",
    "url": "tTests.html#explanation",
    "content": "Explanation The paired samples t test considers the single mean of all the differences from the paired values. Thus, the paired samples t test essentially becomes a one sample t test on the differences between paired observations. Hence the requirement is that the sampling distribution of the sample mean of the differences, \\(\\bar{d}\\), can be assumed to be normally distributed. (It is also required that the obtained differences represent a simple random sample of the full population of possible differences.) The paired samples t test is similar to the independent samples t test scenario, except that there is extra information that allows values from one sample to be paired with a value from the other sample. This pairing of values allows for a more direct analysis of the change or difference individuals experience between the two samples. The points in the plot below demonstrate how points are paired together, and the only thing of interest are the differences between the paired points.",
    "sentences": ["The paired samples t test considers the single mean of all the differences from the paired values.", "Thus, the paired samples t test essentially becomes a one sample t test on the differences between paired observations.", "Hence the requirement is that the sampling distribution of the sample mean of the differences, \\(\\bar{d}\\), can be assumed to be normally distributed.", "(It is also required that the obtained differences represent a simple random sample of the full population of possible differences.) The paired samples t test is similar to the independent samples t test scenario, except that there is extra information that allows values from one sample to be paired with a value from the other sample.", "This pairing of values allows for a more direct analysis of the change or difference individuals experience between the two samples.", "The points in the plot below demonstrate how points are paired together, and the only thing of interest are the differences between the paired points."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "Explanation"
  },
  {
    "id": 336,
    "title": "📍 Independent Samples t\r\nTest",
    "url": "tTests.html#independentsamplesttest",
    "content": "Independent Samples t\r\nTest The independent samples t test is used when a value is hypothesized for the difference between two (possibly) different population means, \\(\\mu_1 - \\mu_2\\). Overview Questions The Independent Samples t Test can be used to answer questions like: Are boys taller than girls on average? Do students who show up to class everyday get higher scores on average than those who don’t? Do you take more steps on average on weekdays or on weekends? Requirements The test is only appropriate when both of the following are satisfied. Both samples are representative of the population. (Simple random samples are the best way to do this.) The sampling distribution of the difference of the sample means \\((\\bar{x}_1 - \\bar{x}_2)\\) can be assumed to be normal. (This is a safe assumption when the sample size of each group is \\(30\\) or greater or when the population data from each group can be assumed to be normal with a Q-Q Plot.) Hypotheses Math Code $$ H_0: \\mu_\\text{label 1} - \\mu_\\text{label 2} = 13.2 $$ $$ H_a: \\mu_\\text{label 1} - \\mu_\\text{label 2} \\neq 13.2 $$ \\(H_0: \\mu_1 - \\mu_2 = \\text{some number, but typically 0}\\) \\(H_a: \\mu_1 - \\mu_2 \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepInd student1 student2 R Instructions Console Help Command: ?t.test() There are two ways to perform the test. Option 1: t.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a “numeric” vector from YourData that represents the data for both samples. X must be a “factor” or “character” vector from YourData that represents the group assignment for each observation. There can only be two groups specified in this column of data. YourNull is the numeric value from your null hypothesis for \\(\\mu_1-\\mu_2\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y ~ X, data=YourData) Example Code Hover your mouse over the example codes to learn more. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. length  ‘length’ is a quantitative variable (numeric vector). ~  ‘~’ is the tilde symbol. sex,  ‘sex’ is a ‘factor’ or ‘character’ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet,  ‘KidsFeet’ is a dataset in library(mosaic). Type View(KidsFeet) to view it. mu = 0,  The numeric value from the null hypothesis for μ1-μ2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = “two.sided”,  The alternative is “two-sided” meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\)     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. Welch Two Sample t-test EXPLANATION. data: length by sex EXPLANATION. t = 1.9174, EXPLANATION.  df = 36.275, EXPLANATION.  p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  -0.04502067 EXPLANATION.  1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean in group B mean in group G EXPLANATION.   25.10500 EXPLANATION.   24.32105 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. length  ‘length’ is a quantitative variable (numeric vector). ~  ‘~’ is the tilde symbol. sex,  ‘sex’ is a “factor” or “character” vector that represents the group assignment for each observation. There are two groups. data=KidsFeet) ‘KidsFeet’ is a dataset in library(mosaic). Type View(KidsFeet) to view it.  Click to Show Output  Click to View Output. Option 2: t.test(NameOfYourData$Y1, NameOfYourData$Y2, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a “numeric” vector that represents the quantitative data from the first sample. Y2 must be a “numeric” vector that represents the quantitative data from the second sample. YourNull is the numeric value from your null hypothesis for the difference of \\(\\mu_1-\\mu_2\\). This is typically zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) par(mfrow=c(1,2)) qqPlot(NameOfYourData$Y1) qqPlot(NameOfYourData$Y2) Example Code Hover your mouse over the example codes to learn more. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. KidsFeet$length[KidsFeet$sex == “B”],  A numeric vector that represents the quantitative data or the foot length for the first sample of data which in this case is the boys. KidsFeet$length[KidsFeet$sex == “G”],  A numeric vector that represents the quantitative data or the foot length for the second sample of data which in this case is the girls. mu = 0,  The numeric value from the null hypothesis for μ1-μ2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = “two.sided”,  The alternative is “two-sided” meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\)     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Welch Two Sample t-test EXPLANATION. data: KidsFeet$length[KidsFeet$sex == “B”] and KidsFeet$length[KidsFeet$sex == “G”] EXPLANATION. t = 1.9174, EXPLANATION.  df = 36.275, EXPLANATION.  p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  -0.04502067 EXPLANATION.  1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean of x mean of y EXPLANATION.   25.10500 EXPLANATION.   24.32105 EXPLANATION. par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow=c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. KidsFeet$length[KidsFeet$sex == “B”]) A numeric vector that represents the quantitative data or the foot length for the first sample of data which in this case is the boys. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. KidsFeet$length[KidsFeet$sex == “G”]) A numeric vector that represents the quantitative data or the foot length for the second sample of data which in this case is the girls.  …  Click to View Output. ## [1] 9 8 ## [1] 18 7",
    "sentences": ["The independent samples t test is used when a value is hypothesized for the difference between two (possibly) different population means, \\(\\mu_1 - \\mu_2\\).", "Overview Questions The Independent Samples t Test can be used to answer questions like: Are boys taller than girls on average?", "Do students who show up to class everyday get higher scores on average than those who don’t?", "Do you take more steps on average on weekdays or on weekends?", "Requirements The test is only appropriate when both of the following are satisfied.", "Both samples are representative of the population.", "(Simple random samples are the best way to do this.) The sampling distribution of the difference of the sample means \\((\\bar{x}_1 - \\bar{x}_2)\\) can be assumed to be normal.", "(This is a safe assumption when the sample size of each group is \\(30\\) or greater or when the population data from each group can be assumed to be normal with a Q-Q Plot.) Hypotheses Math Code $$ H_0: \\mu_\\text{label 1} - \\mu_\\text{label 2} = 13.2 $$ $$ H_a: \\mu_\\text{label 1} - \\mu_\\text{label 2} \\neq 13.2 $$ \\(H_0: \\mu_1 - \\mu_2 = \\text{some number, but typically 0}\\) \\(H_a: \\mu_1 - \\mu_2 \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepInd student1 student2", "R Instructions Console Help Command: ?t.test() There are two ways to perform the test.", "Option 1: t.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a “numeric” vector from YourData that represents the data for both samples."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "Independent Samples t\r\nTest"
  },
  {
    "id": 337,
    "title": "📍 Overview",
    "url": "tTests.html#overview",
    "content": "Overview Questions The Independent Samples t Test can be used to answer questions like: Are boys taller than girls on average? Do students who show up to class everyday get higher scores on average than those who don’t? Do you take more steps on average on weekdays or on weekends? Requirements The test is only appropriate when both of the following are satisfied. Both samples are representative of the population. (Simple random samples are the best way to do this.) The sampling distribution of the difference of the sample means \\((\\bar{x}_1 - \\bar{x}_2)\\) can be assumed to be normal. (This is a safe assumption when the sample size of each group is \\(30\\) or greater or when the population data from each group can be assumed to be normal with a Q-Q Plot.) Hypotheses Math Code $$ H_0: \\mu_\\text{label 1} - \\mu_\\text{label 2} = 13.2 $$ $$ H_a: \\mu_\\text{label 1} - \\mu_\\text{label 2} \\neq 13.2 $$ \\(H_0: \\mu_1 - \\mu_2 = \\text{some number, but typically 0}\\) \\(H_a: \\mu_1 - \\mu_2 \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepInd student1 student2",
    "sentences": ["Questions The Independent Samples t Test can be used to answer questions like: Are boys taller than girls on average?", "Do students who show up to class everyday get higher scores on average than those who don’t?", "Do you take more steps on average on weekdays or on weekends?", "Requirements The test is only appropriate when both of the following are satisfied.", "Both samples are representative of the population.", "(Simple random samples are the best way to do this.) The sampling distribution of the difference of the sample means \\((\\bar{x}_1 - \\bar{x}_2)\\) can be assumed to be normal.", "(This is a safe assumption when the sample size of each group is \\(30\\) or greater or when the population data from each group can be assumed to be normal with a Q-Q Plot.) Hypotheses Math Code $$ H_0: \\mu_\\text{label 1} - \\mu_\\text{label 2} = 13.2 $$ $$ H_a: \\mu_\\text{label 1} - \\mu_\\text{label 2} \\neq 13.2 $$ \\(H_0: \\mu_1 - \\mu_2 = \\text{some number, but typically 0}\\) \\(H_a: \\mu_1 - \\mu_2 \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepInd student1 student2"],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "Overview"
  },
  {
    "id": 338,
    "title": "📍 R Instructions",
    "url": "tTests.html#rinstructions",
    "content": "R Instructions Console Help Command: ?t.test() There are two ways to perform the test. Option 1: t.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a “numeric” vector from YourData that represents the data for both samples. X must be a “factor” or “character” vector from YourData that represents the group assignment for each observation. There can only be two groups specified in this column of data. YourNull is the numeric value from your null hypothesis for \\(\\mu_1-\\mu_2\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y ~ X, data=YourData) Example Code Hover your mouse over the example codes to learn more. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. length  ‘length’ is a quantitative variable (numeric vector). ~  ‘~’ is the tilde symbol. sex,  ‘sex’ is a ‘factor’ or ‘character’ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet,  ‘KidsFeet’ is a dataset in library(mosaic). Type View(KidsFeet) to view it. mu = 0,  The numeric value from the null hypothesis for μ1-μ2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = “two.sided”,  The alternative is “two-sided” meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\)     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. Welch Two Sample t-test EXPLANATION. data: length by sex EXPLANATION. t = 1.9174, EXPLANATION.  df = 36.275, EXPLANATION.  p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  -0.04502067 EXPLANATION.  1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean in group B mean in group G EXPLANATION.   25.10500 EXPLANATION.   24.32105 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. length  ‘length’ is a quantitative variable (numeric vector). ~  ‘~’ is the tilde symbol. sex,  ‘sex’ is a “factor” or “character” vector that represents the group assignment for each observation. There are two groups. data=KidsFeet) ‘KidsFeet’ is a dataset in library(mosaic). Type View(KidsFeet) to view it.  Click to Show Output  Click to View Output. Option 2: t.test(NameOfYourData$Y1, NameOfYourData$Y2, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a “numeric” vector that represents the quantitative data from the first sample. Y2 must be a “numeric” vector that represents the quantitative data from the second sample. YourNull is the numeric value from your null hypothesis for the difference of \\(\\mu_1-\\mu_2\\). This is typically zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) par(mfrow=c(1,2)) qqPlot(NameOfYourData$Y1) qqPlot(NameOfYourData$Y2) Example Code Hover your mouse over the example codes to learn more. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. KidsFeet$length[KidsFeet$sex == “B”],  A numeric vector that represents the quantitative data or the foot length for the first sample of data which in this case is the boys. KidsFeet$length[KidsFeet$sex == “G”],  A numeric vector that represents the quantitative data or the foot length for the second sample of data which in this case is the girls. mu = 0,  The numeric value from the null hypothesis for μ1-μ2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = “two.sided”,  The alternative is “two-sided” meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\)     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Welch Two Sample t-test EXPLANATION. data: KidsFeet$length[KidsFeet$sex == “B”] and KidsFeet$length[KidsFeet$sex == “G”] EXPLANATION. t = 1.9174, EXPLANATION.  df = 36.275, EXPLANATION.  p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  -0.04502067 EXPLANATION.  1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean of x mean of y EXPLANATION.   25.10500 EXPLANATION.   24.32105 EXPLANATION. par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow=c(1,2)) Parameter is being set. The first item inside the combine function c() is the number of rows and the second is the number of columns. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. KidsFeet$length[KidsFeet$sex == “B”]) A numeric vector that represents the quantitative data or the foot length for the first sample of data which in this case is the boys. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. KidsFeet$length[KidsFeet$sex == “G”]) A numeric vector that represents the quantitative data or the foot length for the second sample of data which in this case is the girls.  …  Click to View Output. ## [1] 9 8 ## [1] 18 7",
    "sentences": ["Console Help Command: ?t.test() There are two ways to perform the test.", "Option 1: t.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a “numeric” vector from YourData that represents the data for both samples.", "X must be a “factor” or “character” vector from YourData that represents the group assignment for each observation.", "There can only be two groups specified in this column of data.", "YourNull is the numeric value from your null hypothesis for \\(\\mu_1-\\mu_2\\).", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Testing Assumptions library(car) qqPlot(Y ~ X, data=YourData) Example Code Hover your mouse over the example codes to learn more.", "t.test( ‘t.test’ is an R function that performs one and two sample t-tests."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "R Instructions"
  },
  {
    "id": 339,
    "title": "📍 Explanation",
    "url": "tTests.html#explanation",
    "content": "Explanation The first figure below depicts the scenario where the difference in means of two separate normal distributions is non-zero. In other words, the two distributions have different means, \\(\\mu_1\\) and \\(\\mu_2\\), respectively. It is worth emphasizing that the values of \\(\\mu_1\\) and \\(\\mu_2\\) are unknown to the researcher. The only thing observed are two separate samples of data (blue dots) of sizes \\(n_1\\) and \\(n_2\\), respectively. For the scenario depicted, the null hypothesis that \\(H_0: \\mu_1 - \\mu_2 = 0\\) (i.e., that \\(\\mu_1=\\mu_2\\)) is rejected in favor of the alternative that \\(H_a: \\mu_1 - \\mu_2 \\neq 0\\) based on the sample data observed. This dicision would be correct as the true difference in means, \\(\\mu_1-\\mu_2\\) is non-zero in this case. When the null hypothesis is true, that \\(H_0: \\mu_1 - \\mu_2 = 0\\), then it follows that the test statistic \\(t\\) that is obtained by measuring the distance between the two sample means, \\(\\bar{x}_1-\\bar{x}_2\\), and appropriately standardizing the result follows a \\(t\\) distribution with degrees of freedom less than or equal to \\(n_1+n_2-2\\). Thus, the \\(p\\)-value of the independent samples \\(t\\) test is obtained by using this \\(t\\) distribution to calculate the probability of a test statistic \\(t\\) being as extreme or more extreme than the one observed assuming the null hypothesis is true. \\[ t = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{s_1/n_1+s_2/n_2 }} \\] The plot below demonstrates what data might look like when the null hypothesis is actually true. In other words, when both samples come from the same distribution.",
    "sentences": ["The first figure below depicts the scenario where the difference in means of two separate normal distributions is non-zero.", "In other words, the two distributions have different means, \\(\\mu_1\\) and \\(\\mu_2\\), respectively.", "It is worth emphasizing that the values of \\(\\mu_1\\) and \\(\\mu_2\\) are unknown to the researcher.", "The only thing observed are two separate samples of data (blue dots) of sizes \\(n_1\\) and \\(n_2\\), respectively.", "For the scenario depicted, the null hypothesis that \\(H_0: \\mu_1 - \\mu_2 = 0\\) (i.e., that \\(\\mu_1=\\mu_2\\)) is rejected in favor of the alternative that \\(H_a: \\mu_1 - \\mu_2 \\neq 0\\) based on the sample data observed.", "This dicision would be correct as the true difference in means, \\(\\mu_1-\\mu_2\\) is non-zero in this case.", "When the null hypothesis is true, that \\(H_0: \\mu_1 - \\mu_2 = 0\\), then it follows that the test statistic \\(t\\) that is obtained by measuring the distance between the two sample means, \\(\\bar{x}_1-\\bar{x}_2\\), and appropriately standardizing the result follows a \\(t\\) distribution with degrees of freedom less than or equal to \\(n_1+n_2-2\\).", "Thus, the \\(p\\)-value of the independent samples \\(t\\) test is obtained by using this \\(t\\) distribution to calculate the probability of a test statistic \\(t\\) being as extreme or more extreme than the one observed assuming the null hypothesis is true.", "\\[ t = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{s_1/n_1+s_2/n_2 }} \\] The plot below demonstrates what data might look like when the null hypothesis is actually true.", "In other words, when both samples come from the same distribution."],
    "type": "section",
    "page_title": "t Tests",
    "section_title": "Explanation"
  },
  {
    "id": 340,
    "title": "Wilcoxon Tests",
    "url": "WilcoxonTests.html",
    "content": "Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Wilcoxon Tests Wilcoxon tests allow for the testing of hypotheses about the value of the the median without assuming the test statistic follows any parametric distribution. They are often seen as nonparametric alternatives to the various t tests. However, they can also be used on ordinal data (data that is not quite quantitative, but is ordered) unlike t tests which require quantitative data. Wilcoxon Signed-Rank Test For testing hypotheses about the value of the median of (1) one sample of quantitative data or (2) one set of differences from paired data. Overview The nonparametric equivalent of the paired-samples t test as well as the one-sample t test. Best for smaller sample sizes where the distribution of the data is not normal. The t test is more appropriate when the data is normal or when the sample size is large. While the test will work in most scenarios it suffers slightly when ties (repeated values) are present in the data. If many ties are present in the data, the test is not appropriate. If only a few ties are present, the test is still appropriate. Hypotheses Originally created to test hypotheses about the value of the median, but works as well for the mean when the distribution of the data is symmetrical. One Sample of Data Math Code $$ H_0: \\text{Median} = \\text{(Some Number)} $$ $$ H_a: \\text{Median} \\neq \\text{(Same Number)} $$ \\(H_0: \\text{Median} = \\text{(Some Number)}\\) \\(H_a: \\text{Median} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{(Some Number)}\\) Paired Samples of Data Math Code $$ H_0: \\text{median of differences} = 0 $$ $$ H_a: \\text{median of differences} \\neq 0 $$ \\(H_0: \\text{median of differences} = 0\\) \\(H_a: \\text{median of differences} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ 0\\) Examples: sleep, CornHeights R Instructions Console Help Command: ?wilcox.test() Paired Data wilcox.test(Y1, Y2, mu = YourNull, alternative = YourAlternative, paired = TRUE, conf.level = 0.95) Y1 must be a “numeric” vector. One set of measurements from the pair. Y2 also a “numeric” vector. Other set of measurements from the pair. YourNull is the numeric value from your null hypothesis for the median of differences from the paired data. Usually zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests. sleep$extra[sleep$group==1],  The hours of extra sleep that the group had with drug 2. sleep$extra[sleep$group==2],  The hours of extra sleep that the same group had with drug 1. mu = 0,  The numeric value from the null hypothesis for the median of differences from the paired data is 0 meaning the null hypothesis is \\(\\text{median of differences} = 0\\). paired=TRUE,  This command forces a “paired” samples test to be performed. alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\text{median of differences} \\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Wilcoxon signed rank test with continuity correction The phrase “with continuity correction” implies that instead of using the “exact” distribution of the test statistic a “normal approximation” was used instead to compute the p-value. Further, a small correction was made to allow for the change from the “discrete” exact distribution to the “continuous normal distribution” when calculating the p-value. data: sleep$extra[sleep$group == 1] and sleep$extra[sleep$group == 2] This statement of the output just reminds you of the code you used to perform the test. The important thing is recognizing that the first group listed is “Group 1” and the second group listed is “Group 2.” This is especially important when using alternative hypotheses of “less” or “greater” as the order is always “Group 1” is “less” than “Group 2” or “Group 1” is “greater” than “Group 2.” V = 0, This is the test statistic of the test, i.e., the sum of the ranks from the positive group minus the minimum sum of ranks possible.  p-value = 0.009091 This is the p-value of the test. If no warning is displayed when the test is run, then this is the “exact” p-value from the non-parametric Wilcoxon Test Statistic distribution. Sometimes a message will appear stating “Cannot compute exact p-value with ties” or other similar messages. In those cases, the p-value is still considered valid even though it is obtained through a normal approximation to the exact distribution. alternative hypothesis: true location shift is not equal to 0 This reports that the alternative hypothesis was “two-sided.” If the alternative had been “less” or “greater” the wording would change accordingly. One Sample wilcox.test(object, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object must be a “numeric” vector. YourNull is the numeric value from your null hypothesis for the median (even though it says “mu”). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,  ‘mpg’ is a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,  The numeric value from the null hypothesis is 20 meaning \\(\\mu = 20\\). alternative = “two.sided”,  The alternative is “two.sided” meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1−α.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Wilcoxon signed rank test with continuity correction This reports on the type of test performed. The phrase “with continuity correction” implies the normal approximation was used when calculating the p-value of the test. data: mtcars$mpg This print-out reminds us that the mpg column of the mtcars data was used as “Y” in the test. V = 249, The test statistic of the test.  p-value = 0.7863 The p-value of the test. alternative hypothesis: true location is not equal to 20 The words “not equal” tell us this was a two-sided test. Had it been a one-sided test, either the word “less” or the word “greater” would have appeared instead of “not equal.” Explanation In many cases it is of interest to perform a hypothesis test about the location of the center of a distribution of data. The Wilcoxon Signed Rank Test allows a nonparametric approach to doing this. The Wilcoxon Signed-Rank Test covers two important scenarios. One sample of data from a population. (Not very common.) The differences obtained from paired data. (Very common.) The Wilcoxon methods are most easily explained through examples, beginning with the paired data for which the method was originally created. Scroll down for the One Sample Example if that is what you are really interested in. However, it is still recommended that you read the paired data example first. Paired Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon. Background Height differences “between cross- and self- fertilized corn plants of the same pair” were collected. The experiment hypothesized that the center of the distribution of the height differences would be zero, with the alternative being that the center was not zero. The result of the data collection was 15 height differences: Differences: 14, 56, 60, 16, 6, 8, -48, 49, 24, 28, 29, 41, -67, 23, 75 Step 1 The first step of the Wilcoxon Signed Rank Test is to order the differences from smallest magnitude to largest magnitude. Negative signs are essentially ignored at this point and only magnitudes of the numbers matter. Sorted Differences: 6, 8, 14, 16, 23, 24, 28, 29, 41, -48, 49, 56, 60, -67, 75 Step 2 The next step is to rank the ordered values. Negative signs are attached to the ranks corresponding to negative numbers.   Differences: 6 8 14 16 23 24 28 29 41 -48 49 56 60 -67 75 Ranks: 1 2 3 4 5 6 7 8 9 -10 11 12 13 -14 15 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=15\\). Step 3 The ranks are then put into two groups. Negative Ranks Positive Ranks -10, -14 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15 Step 4 One of the groups is summed, usually the group with the fewest observations. Only the absolute values of the ranks are summed. Sum of Negative Ranks: \\(\\left|-10\\right| + \\left|-14\\right| = 24\\) The sum of the ranks becomes the test statistic of the Wilcoxon Test. The test statistic is sometimes called \\(W\\) or \\(V\\) or \\(U\\). Step 5 The \\(p\\)-value of the test is then obtained by computing the probability of the test statistic being as extreme or more extreme than the one obtained. This is done by first computing the probability of all possible values the test statistic could have obtained using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=15\\) ranks, the possible sums of ranks range from 0 to 120 and hit every integer in between, i.e., \\(1, 2, 3, \\ldots, 120\\). (Note, if summing the negative ranks these sums would technically all be negative.) To verify that \\(120\\) is the largest sum possible for \\(n=15\\) ranks, note that: \\(1+15 = 16\\), \\(2 + 14 = 16\\), \\(3+13 = 16\\), \\(4+12=16\\), \\(5+11=16\\), \\(6+10=16\\), \\(7+9=16\\), and finally that \\(8 = \\frac{16}{2}\\). Thus, there are 7 sums of 16 and one sum of \\(\\frac{16}{2}\\). This could be said in a mathematically equivalent way by stating there are \\(\\frac{14}{2}\\) sums of 16 and one sum of \\(\\frac{16}{2}\\). By multiplication this gives \\[ \\frac{14}{2}\\cdot\\frac{16}{1} + \\frac{1}{1}\\cdot\\frac{16}{2} = \\frac{14\\cdot16 + 1\\cdot16}{2} = \\frac{15\\cdot16}{2} = \\frac{n(n+1)}{2} = 120 \\] The probability of each sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 32,768 total different groups of ranks possible when there are \\(n=15\\) ranks.) For example, a sum of 1 can happen only one way, only the rank of 1 is in the group. A sum of 2 can also only happen 1 way. The sum of 3 however, can happen two ways: we could have the ranks of 1 and 2 in the group, or just the rank of 3 in the group. A similar counting technique is then implemented for each possible sum. After all the calculations are performed, the distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(24\\) (or its opposite of \\(120-24=96\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of (the absolute value of) negative ranks as extreme or more extreme than \\(24\\) is \\(p=0.04126\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would reject the null hypothesis that the center of the distribution of differences is zero. We conclude that the center of the distribution is greater than zero because the sum of negative ranks is much smaller than we expected under the zero center hypothesis (the null). Thus, there is sufficient evidence to conclude that the centers of the distributions of “cross- and self-fertilized corn plants” heights are not equal. One is greater than the other. Notice how the following dot plot shows that the differences are in favor of the cross-fertilized plants (the first group in the subtraction) being taller. This is true even though two self-fertilized plants were much taller than their cross-fertilized counterpart (the two negative differences). Comment If the distribution of differences is symmetric, then the hypotheses can be written as \\[ H_0: \\mu = 0 \\] \\[ H_a: \\mu \\neq 0 \\] If the distribution is skewed, then the hypotheses technically refer to the median instead of the mean and should be written as \\[ H_0: \\text{median} = 0 \\] \\[ H_a: \\text{median} \\neq 0 \\] One Sample Example The idea behind the one sample Wilcoxon Signed Rank test is nearly identical to the paired data. The only change is that the median must be subtracted from all observed values to obtain the differences. Note that the mean is equal to the median when data is symmetric. Background Suppose we are interested in testing to see if the median hourly wage of BYU-Idaho students during their off-track employment is equal to the minimum wage in Idaho, $7.25 an hour as of January 1st, 2015. Five randomly sampled hourly wages from BYU-Idaho Math 221B students provides the following data. Wages: $6.00, $9.00, $8.10, $18.00, $10.45 The differences are then obtained by subtracting the hypothesized value for the median (or mean if the dat",
    "sentences": ["Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Wilcoxon Tests Wilcoxon tests allow for the testing of hypotheses about the value of the the median without assuming the test statistic follows any parametric distribution.", "They are often seen as nonparametric alternatives to the various t tests.", "However, they can also be used on ordinal data (data that is not quite quantitative, but is ordered) unlike t tests which require quantitative data.", "Wilcoxon Signed-Rank Test For testing hypotheses about the value of the median of (1) one sample of quantitative data or (2) one set of differences from paired data.", "Overview The nonparametric equivalent of the paired-samples t test as well as the one-sample t test.", "Best for smaller sample sizes where the distribution of the data is not normal.", "The t test is more appropriate when the data is normal or when the sample size is large.", "While the test will work in most scenarios it suffers slightly when ties (repeated values) are present in the data.", "If many ties are present in the data, the test is not appropriate.", "If only a few ties are present, the test is still appropriate.", "Hypotheses Originally created to test hypotheses about the value of the median, but works as well for the mean when the distribution of the data is symmetrical.", "One Sample of Data Math Code $$ H_0: \\text{Median} = \\text{(Some Number)} $$ $$ H_a: \\text{Median} \\neq \\text{(Same Number)} $$ \\(H_0: \\text{Median} = \\text{(Some Number)}\\) \\(H_a: \\text{Median} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{(Some Number)}\\) Paired Samples of Data Math Code $$ H_0: \\text{median of differences} = 0 $$ $$ H_a: \\text{median of differences} \\neq 0 $$ \\(H_0: \\text{median of differences} = 0\\) \\(H_a: \\text{median of differences} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ 0\\) Examples: sleep, CornHeights R Instructions Console Help Command: ?wilcox.test() Paired Data wilcox.test(Y1, Y2, mu = YourNull, alternative = YourAlternative, paired = TRUE, conf.level = 0.95) Y1 must be a “numeric” vector.", "One set of measurements from the pair.", "Y2 also a “numeric” vector.", "Other set of measurements from the pair.", "YourNull is the numeric value from your null hypothesis for the median of differences from the paired data.", "Usually zero.", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Example Code Hover your mouse over the example codes to learn more.", "wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests.", "sleep$extra[sleep$group==1],  The hours of extra sleep that the group had with drug 2.", "sleep$extra[sleep$group==2],  The hours of extra sleep that the same group had with drug 1.", "mu = 0,  The numeric value from the null hypothesis for the median of differences from the paired data is 0 meaning the null hypothesis is \\(\\text{median of differences} = 0\\).", "paired=TRUE,  This command forces a “paired” samples test to be performed.", "alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\text{median of differences} \\neq0\\).", "conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).", "    Press Enter to run the code if you have typed it in yourself.", "You can also click here to view the output.", " …  Click to View Output.", "Wilcoxon signed rank test with continuity correction The phrase “with continuity correction” implies that instead of using the “exact” distribution of the test statistic a “normal approximation” was used instead to compute the p-value.", "Further, a small correction was made to allow for the change from the “discrete” exact distribution to the “continuous normal distribution” when calculating the p-value.", "data: sleep$extra[sleep$group == 1] and sleep$extra[sleep$group == 2] This statement of the output just reminds you of the code you used to perform the test.", "The important thing is recognizing that the first group listed is “Group 1” and the second group listed is “Group 2.” This is especially important when using alternative hypotheses of “less” or “greater” as the order is always “Group 1” is “less” than “Group 2” or “Group 1” is “greater” than “Group 2.” V = 0, This is the test statistic of the test, i.e., the sum of the ranks from the positive group minus the minimum sum of ranks possible.", " p-value = 0.009091 This is the p-value of the test.", "If no warning is displayed when the test is run, then this is the “exact” p-value from the non-parametric Wilcoxon Test Statistic distribution.", "Sometimes a message will appear stating “Cannot compute exact p-value with ties” or other similar messages.", "In those cases, the p-value is still considered valid even though it is obtained through a normal approximation to the exact distribution.", "alternative hypothesis: true location shift is not equal to 0 This reports that the alternative hypothesis was “two-sided.” If the alternative had been “less” or “greater” the wording would change accordingly.", "One Sample wilcox.test(object, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object must be a “numeric” vector.", "YourNull is the numeric value from your null hypothesis for the median (even though it says “mu”).", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Example Code Hover your mouse over the example codes to learn more.", "wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests.", "mtcars ‘mtcars’ is a dataset.", "Type ‘View(mtcars)’ in R to view the dataset.", "$ The $ allows us to access any variable from the mtcars dataset."],
    "type": "page",
    "page_title": "Wilcoxon Tests"
  },
  {
    "id": 341,
    "title": "📍 Wilcoxon Signed-Rank\r\nTest",
    "url": "WilcoxonTests.html#wilcoxonsignedranktest",
    "content": "Wilcoxon Signed-Rank\r\nTest For testing hypotheses about the value of the median of (1) one sample of quantitative data or (2) one set of differences from paired data. Overview The nonparametric equivalent of the paired-samples t test as well as the one-sample t test. Best for smaller sample sizes where the distribution of the data is not normal. The t test is more appropriate when the data is normal or when the sample size is large. While the test will work in most scenarios it suffers slightly when ties (repeated values) are present in the data. If many ties are present in the data, the test is not appropriate. If only a few ties are present, the test is still appropriate. Hypotheses Originally created to test hypotheses about the value of the median, but works as well for the mean when the distribution of the data is symmetrical. One Sample of Data Math Code $$ H_0: \\text{Median} = \\text{(Some Number)} $$ $$ H_a: \\text{Median} \\neq \\text{(Same Number)} $$ \\(H_0: \\text{Median} = \\text{(Some Number)}\\) \\(H_a: \\text{Median} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{(Some Number)}\\) Paired Samples of Data Math Code $$ H_0: \\text{median of differences} = 0 $$ $$ H_a: \\text{median of differences} \\neq 0 $$ \\(H_0: \\text{median of differences} = 0\\) \\(H_a: \\text{median of differences} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ 0\\) Examples: sleep, CornHeights R Instructions Console Help Command: ?wilcox.test() Paired Data wilcox.test(Y1, Y2, mu = YourNull, alternative = YourAlternative, paired = TRUE, conf.level = 0.95) Y1 must be a “numeric” vector. One set of measurements from the pair. Y2 also a “numeric” vector. Other set of measurements from the pair. YourNull is the numeric value from your null hypothesis for the median of differences from the paired data. Usually zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests. sleep$extra[sleep$group==1],  The hours of extra sleep that the group had with drug 2. sleep$extra[sleep$group==2],  The hours of extra sleep that the same group had with drug 1. mu = 0,  The numeric value from the null hypothesis for the median of differences from the paired data is 0 meaning the null hypothesis is \\(\\text{median of differences} = 0\\). paired=TRUE,  This command forces a “paired” samples test to be performed. alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\text{median of differences} \\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Wilcoxon signed rank test with continuity correction The phrase “with continuity correction” implies that instead of using the “exact” distribution of the test statistic a “normal approximation” was used instead to compute the p-value. Further, a small correction was made to allow for the change from the “discrete” exact distribution to the “continuous normal distribution” when calculating the p-value. data: sleep$extra[sleep$group == 1] and sleep$extra[sleep$group == 2] This statement of the output just reminds you of the code you used to perform the test. The important thing is recognizing that the first group listed is “Group 1” and the second group listed is “Group 2.” This is especially important when using alternative hypotheses of “less” or “greater” as the order is always “Group 1” is “less” than “Group 2” or “Group 1” is “greater” than “Group 2.” V = 0, This is the test statistic of the test, i.e., the sum of the ranks from the positive group minus the minimum sum of ranks possible.  p-value = 0.009091 This is the p-value of the test. If no warning is displayed when the test is run, then this is the “exact” p-value from the non-parametric Wilcoxon Test Statistic distribution. Sometimes a message will appear stating “Cannot compute exact p-value with ties” or other similar messages. In those cases, the p-value is still considered valid even though it is obtained through a normal approximation to the exact distribution. alternative hypothesis: true location shift is not equal to 0 This reports that the alternative hypothesis was “two-sided.” If the alternative had been “less” or “greater” the wording would change accordingly. One Sample wilcox.test(object, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object must be a “numeric” vector. YourNull is the numeric value from your null hypothesis for the median (even though it says “mu”). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,  ‘mpg’ is a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,  The numeric value from the null hypothesis is 20 meaning \\(\\mu = 20\\). alternative = “two.sided”,  The alternative is “two.sided” meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1−α.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Wilcoxon signed rank test with continuity correction This reports on the type of test performed. The phrase “with continuity correction” implies the normal approximation was used when calculating the p-value of the test. data: mtcars$mpg This print-out reminds us that the mpg column of the mtcars data was used as “Y” in the test. V = 249, The test statistic of the test.  p-value = 0.7863 The p-value of the test. alternative hypothesis: true location is not equal to 20 The words “not equal” tell us this was a two-sided test. Had it been a one-sided test, either the word “less” or the word “greater” would have appeared instead of “not equal.”",
    "sentences": ["For testing hypotheses about the value of the median of (1) one sample of quantitative data or (2) one set of differences from paired data.", "Overview The nonparametric equivalent of the paired-samples t test as well as the one-sample t test.", "Best for smaller sample sizes where the distribution of the data is not normal.", "The t test is more appropriate when the data is normal or when the sample size is large.", "While the test will work in most scenarios it suffers slightly when ties (repeated values) are present in the data.", "If many ties are present in the data, the test is not appropriate.", "If only a few ties are present, the test is still appropriate.", "Hypotheses Originally created to test hypotheses about the value of the median, but works as well for the mean when the distribution of the data is symmetrical.", "One Sample of Data Math Code $$ H_0: \\text{Median} = \\text{(Some Number)} $$ $$ H_a: \\text{Median} \\neq \\text{(Same Number)} $$ \\(H_0: \\text{Median} = \\text{(Some Number)}\\) \\(H_a: \\text{Median} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{(Some Number)}\\) Paired Samples of Data Math Code $$ H_0: \\text{median of differences} = 0 $$ $$ H_a: \\text{median of differences} \\neq 0 $$ \\(H_0: \\text{median of differences} = 0\\) \\(H_a: \\text{median of differences} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ 0\\) Examples: sleep, CornHeights", "R Instructions Console Help Command: ?wilcox.test() Paired Data wilcox.test(Y1, Y2, mu = YourNull, alternative = YourAlternative, paired = TRUE, conf.level = 0.95) Y1 must be a “numeric” vector."],
    "type": "section",
    "page_title": "Wilcoxon Tests",
    "section_title": "Wilcoxon Signed-Rank\r\nTest"
  },
  {
    "id": 342,
    "title": "📍 Overview",
    "url": "WilcoxonTests.html#overview",
    "content": "Overview The nonparametric equivalent of the paired-samples t test as well as the one-sample t test. Best for smaller sample sizes where the distribution of the data is not normal. The t test is more appropriate when the data is normal or when the sample size is large. While the test will work in most scenarios it suffers slightly when ties (repeated values) are present in the data. If many ties are present in the data, the test is not appropriate. If only a few ties are present, the test is still appropriate. Hypotheses Originally created to test hypotheses about the value of the median, but works as well for the mean when the distribution of the data is symmetrical. One Sample of Data Math Code $$ H_0: \\text{Median} = \\text{(Some Number)} $$ $$ H_a: \\text{Median} \\neq \\text{(Same Number)} $$ \\(H_0: \\text{Median} = \\text{(Some Number)}\\) \\(H_a: \\text{Median} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{(Some Number)}\\) Paired Samples of Data Math Code $$ H_0: \\text{median of differences} = 0 $$ $$ H_a: \\text{median of differences} \\neq 0 $$ \\(H_0: \\text{median of differences} = 0\\) \\(H_a: \\text{median of differences} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ 0\\) Examples: sleep, CornHeights",
    "sentences": ["The nonparametric equivalent of the paired-samples t test as well as the one-sample t test.", "Best for smaller sample sizes where the distribution of the data is not normal.", "The t test is more appropriate when the data is normal or when the sample size is large.", "While the test will work in most scenarios it suffers slightly when ties (repeated values) are present in the data.", "If many ties are present in the data, the test is not appropriate.", "If only a few ties are present, the test is still appropriate.", "Hypotheses Originally created to test hypotheses about the value of the median, but works as well for the mean when the distribution of the data is symmetrical.", "One Sample of Data Math Code $$ H_0: \\text{Median} = \\text{(Some Number)} $$ $$ H_a: \\text{Median} \\neq \\text{(Same Number)} $$ \\(H_0: \\text{Median} = \\text{(Some Number)}\\) \\(H_a: \\text{Median} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{(Some Number)}\\) Paired Samples of Data Math Code $$ H_0: \\text{median of differences} = 0 $$ $$ H_a: \\text{median of differences} \\neq 0 $$ \\(H_0: \\text{median of differences} = 0\\) \\(H_a: \\text{median of differences} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ 0\\) Examples: sleep, CornHeights"],
    "type": "section",
    "page_title": "Wilcoxon Tests",
    "section_title": "Overview"
  },
  {
    "id": 343,
    "title": "📍 R Instructions",
    "url": "WilcoxonTests.html#rinstructions",
    "content": "R Instructions Console Help Command: ?wilcox.test() Paired Data wilcox.test(Y1, Y2, mu = YourNull, alternative = YourAlternative, paired = TRUE, conf.level = 0.95) Y1 must be a “numeric” vector. One set of measurements from the pair. Y2 also a “numeric” vector. Other set of measurements from the pair. YourNull is the numeric value from your null hypothesis for the median of differences from the paired data. Usually zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests. sleep$extra[sleep$group==1],  The hours of extra sleep that the group had with drug 2. sleep$extra[sleep$group==2],  The hours of extra sleep that the same group had with drug 1. mu = 0,  The numeric value from the null hypothesis for the median of differences from the paired data is 0 meaning the null hypothesis is \\(\\text{median of differences} = 0\\). paired=TRUE,  This command forces a “paired” samples test to be performed. alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\text{median of differences} \\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Wilcoxon signed rank test with continuity correction The phrase “with continuity correction” implies that instead of using the “exact” distribution of the test statistic a “normal approximation” was used instead to compute the p-value. Further, a small correction was made to allow for the change from the “discrete” exact distribution to the “continuous normal distribution” when calculating the p-value. data: sleep$extra[sleep$group == 1] and sleep$extra[sleep$group == 2] This statement of the output just reminds you of the code you used to perform the test. The important thing is recognizing that the first group listed is “Group 1” and the second group listed is “Group 2.” This is especially important when using alternative hypotheses of “less” or “greater” as the order is always “Group 1” is “less” than “Group 2” or “Group 1” is “greater” than “Group 2.” V = 0, This is the test statistic of the test, i.e., the sum of the ranks from the positive group minus the minimum sum of ranks possible.  p-value = 0.009091 This is the p-value of the test. If no warning is displayed when the test is run, then this is the “exact” p-value from the non-parametric Wilcoxon Test Statistic distribution. Sometimes a message will appear stating “Cannot compute exact p-value with ties” or other similar messages. In those cases, the p-value is still considered valid even though it is obtained through a normal approximation to the exact distribution. alternative hypothesis: true location shift is not equal to 0 This reports that the alternative hypothesis was “two-sided.” If the alternative had been “less” or “greater” the wording would change accordingly. One Sample wilcox.test(object, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object must be a “numeric” vector. YourNull is the numeric value from your null hypothesis for the median (even though it says “mu”). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,  ‘mpg’ is a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,  The numeric value from the null hypothesis is 20 meaning \\(\\mu = 20\\). alternative = “two.sided”,  The alternative is “two.sided” meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1−α.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Wilcoxon signed rank test with continuity correction This reports on the type of test performed. The phrase “with continuity correction” implies the normal approximation was used when calculating the p-value of the test. data: mtcars$mpg This print-out reminds us that the mpg column of the mtcars data was used as “Y” in the test. V = 249, The test statistic of the test.  p-value = 0.7863 The p-value of the test. alternative hypothesis: true location is not equal to 20 The words “not equal” tell us this was a two-sided test. Had it been a one-sided test, either the word “less” or the word “greater” would have appeared instead of “not equal.”",
    "sentences": ["Console Help Command: ?wilcox.test() Paired Data wilcox.test(Y1, Y2, mu = YourNull, alternative = YourAlternative, paired = TRUE, conf.level = 0.95) Y1 must be a “numeric” vector.", "One set of measurements from the pair.", "Y2 also a “numeric” vector.", "Other set of measurements from the pair.", "YourNull is the numeric value from your null hypothesis for the median of differences from the paired data.", "Usually zero.", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Example Code Hover your mouse over the example codes to learn more."],
    "type": "section",
    "page_title": "Wilcoxon Tests",
    "section_title": "R Instructions"
  },
  {
    "id": 344,
    "title": "📍 Explanation",
    "url": "WilcoxonTests.html#explanation",
    "content": "Explanation In many cases it is of interest to perform a hypothesis test about the location of the center of a distribution of data. The Wilcoxon Signed Rank Test allows a nonparametric approach to doing this. The Wilcoxon Signed-Rank Test covers two important scenarios. One sample of data from a population. (Not very common.) The differences obtained from paired data. (Very common.) The Wilcoxon methods are most easily explained through examples, beginning with the paired data for which the method was originally created. Scroll down for the One Sample Example if that is what you are really interested in. However, it is still recommended that you read the paired data example first. Paired Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon. Background Height differences “between cross- and self- fertilized corn plants of the same pair” were collected. The experiment hypothesized that the center of the distribution of the height differences would be zero, with the alternative being that the center was not zero. The result of the data collection was 15 height differences: Differences: 14, 56, 60, 16, 6, 8, -48, 49, 24, 28, 29, 41, -67, 23, 75 Step 1 The first step of the Wilcoxon Signed Rank Test is to order the differences from smallest magnitude to largest magnitude. Negative signs are essentially ignored at this point and only magnitudes of the numbers matter. Sorted Differences: 6, 8, 14, 16, 23, 24, 28, 29, 41, -48, 49, 56, 60, -67, 75 Step 2 The next step is to rank the ordered values. Negative signs are attached to the ranks corresponding to negative numbers.   Differences: 6 8 14 16 23 24 28 29 41 -48 49 56 60 -67 75 Ranks: 1 2 3 4 5 6 7 8 9 -10 11 12 13 -14 15 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=15\\). Step 3 The ranks are then put into two groups. Negative Ranks Positive Ranks -10, -14 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15 Step 4 One of the groups is summed, usually the group with the fewest observations. Only the absolute values of the ranks are summed. Sum of Negative Ranks: \\(\\left|-10\\right| + \\left|-14\\right| = 24\\) The sum of the ranks becomes the test statistic of the Wilcoxon Test. The test statistic is sometimes called \\(W\\) or \\(V\\) or \\(U\\). Step 5 The \\(p\\)-value of the test is then obtained by computing the probability of the test statistic being as extreme or more extreme than the one obtained. This is done by first computing the probability of all possible values the test statistic could have obtained using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=15\\) ranks, the possible sums of ranks range from 0 to 120 and hit every integer in between, i.e., \\(1, 2, 3, \\ldots, 120\\). (Note, if summing the negative ranks these sums would technically all be negative.) To verify that \\(120\\) is the largest sum possible for \\(n=15\\) ranks, note that: \\(1+15 = 16\\), \\(2 + 14 = 16\\), \\(3+13 = 16\\), \\(4+12=16\\), \\(5+11=16\\), \\(6+10=16\\), \\(7+9=16\\), and finally that \\(8 = \\frac{16}{2}\\). Thus, there are 7 sums of 16 and one sum of \\(\\frac{16}{2}\\). This could be said in a mathematically equivalent way by stating there are \\(\\frac{14}{2}\\) sums of 16 and one sum of \\(\\frac{16}{2}\\). By multiplication this gives \\[ \\frac{14}{2}\\cdot\\frac{16}{1} + \\frac{1}{1}\\cdot\\frac{16}{2} = \\frac{14\\cdot16 + 1\\cdot16}{2} = \\frac{15\\cdot16}{2} = \\frac{n(n+1)}{2} = 120 \\] The probability of each sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 32,768 total different groups of ranks possible when there are \\(n=15\\) ranks.) For example, a sum of 1 can happen only one way, only the rank of 1 is in the group. A sum of 2 can also only happen 1 way. The sum of 3 however, can happen two ways: we could have the ranks of 1 and 2 in the group, or just the rank of 3 in the group. A similar counting technique is then implemented for each possible sum. After all the calculations are performed, the distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(24\\) (or its opposite of \\(120-24=96\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of (the absolute value of) negative ranks as extreme or more extreme than \\(24\\) is \\(p=0.04126\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would reject the null hypothesis that the center of the distribution of differences is zero. We conclude that the center of the distribution is greater than zero because the sum of negative ranks is much smaller than we expected under the zero center hypothesis (the null). Thus, there is sufficient evidence to conclude that the centers of the distributions of “cross- and self-fertilized corn plants” heights are not equal. One is greater than the other. Notice how the following dot plot shows that the differences are in favor of the cross-fertilized plants (the first group in the subtraction) being taller. This is true even though two self-fertilized plants were much taller than their cross-fertilized counterpart (the two negative differences). Comment If the distribution of differences is symmetric, then the hypotheses can be written as \\[ H_0: \\mu = 0 \\] \\[ H_a: \\mu \\neq 0 \\] If the distribution is skewed, then the hypotheses technically refer to the median instead of the mean and should be written as \\[ H_0: \\text{median} = 0 \\] \\[ H_a: \\text{median} \\neq 0 \\] One Sample Example The idea behind the one sample Wilcoxon Signed Rank test is nearly identical to the paired data. The only change is that the median must be subtracted from all observed values to obtain the differences. Note that the mean is equal to the median when data is symmetric. Background Suppose we are interested in testing to see if the median hourly wage of BYU-Idaho students during their off-track employment is equal to the minimum wage in Idaho, $7.25 an hour as of January 1st, 2015. Five randomly sampled hourly wages from BYU-Idaho Math 221B students provides the following data. Wages: $6.00, $9.00, $8.10, $18.00, $10.45 The differences are then obtained by subtracting the hypothesized value for the median (or mean if the data is symmetric) from all observations. Differences: -1.25, 1.75, 0.85, 10.75, 3.20 Note: from this point down, the wording of this example is identical to the paired data example (above) with the numbers changed to match \\(n=5\\). It is useful to continue reading to reinforce the idea of the Wilcoxon Signed Rank Test, but no new knowledge will be presented. Step 1 The first step of the Wilcoxon Signed Rank Test is to order the differences from smallest magnitude to largest magnitude. Negative signs are essentially ignored at this point and only magnitudes of the numbers matter. Sorted Differences: 0.85, -1.25, 1.75, 3.20, 10.75 Step 2 The next step is to rank the ordered values. Negative signs are attached to the ranks corresponding to negative numbers. Ranks: 1, -2, 3, 4, 5 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=5\\). Step 3 The ranks are then put into two groups. Negative Ranks Positive Ranks -2 1, 3, 4, 5 Step 4 One of the groups is summed, usually the group with the fewest observations. Sum of Negative Ranks: \\(\\left|-2\\right| = 2\\) Step 5 The \\(p\\)-value of the test is then obtained by computing the probabilities of all possible sums using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=5\\) ranks, the possible sums of ranks range from 0 to 15 and hit every integer in between, i.e., \\(1, 2, 3, \\ldots, 15\\). (Note, if summing the negative ranks these sums would technically all be negative.) To verify that \\(15\\) is the largest sum possible for \\(n=5\\) ranks, note that: \\(1+5 = 6\\), \\(2 + 4 = 6\\), and finally \\(3 = \\frac{6}{2}\\). Thus, there are 2 sums of 6 and one sum of \\(\\frac{6}{2}\\). This could be said in a mathematically equivalent way by stating there are \\(\\frac{4}{2}\\) sums of 6 and one sum of \\(\\frac{6}{2}\\). By multiplication this gives \\[ \\frac{4}{2}\\cdot\\frac{6}{1} + \\frac{1}{1}\\cdot\\frac{6}{2} = \\frac{4\\cdot6 + 1\\cdot6}{2} = \\frac{5\\cdot6}{2} = \\frac{n(n+1)}{2} = 15 \\] The probability of each sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 32 total different groups of ranks possible when there are \\(n=5\\) ranks.) For example, a sum of 1 can happen only one way, only the rank of 1 is in the group. A sum of 2 can also only happen 1 way. The sum of 3 however, can happen two ways: we could have the ranks of 1 and 2 in the group, or just the rank of 3 in the group. A similar counting technique is then implemented for each possible sum. After all the calculations are performed, the distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(2\\) (or its opposite of \\(15-2=13\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of negative ranks as extreme or more extreme than \\(-2\\) is \\(p=0.1875\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would fail to reject the null hypothesis that the center of the distribution of differences is zero. We will continue to assume the null hypothesis was true, that the median off-track hourly wage of BYU-Idaho students is the same as the Idaho minimum wage. Final Comment Notice that when the sample size is large the distribution of the test statistic can be approximated by a normal distribution. Most software applications use this approximation when the sample size is over \\(50\\), i.e., for \\(n\\geq50\\) because computing all the possible sums of ranks becomes incredibly time consuming. Thus, for large sample sizes the results will be almost identical whether the Wilcoxon Tests or a t Test is used.",
    "sentences": ["In many cases it is of interest to perform a hypothesis test about the location of the center of a distribution of data.", "The Wilcoxon Signed Rank Test allows a nonparametric approach to doing this.", "The Wilcoxon Signed-Rank Test covers two important scenarios.", "One sample of data from a population.", "(Not very common.) The differences obtained from paired data.", "(Very common.) The Wilcoxon methods are most easily explained through examples, beginning with the paired data for which the method was originally created.", "Scroll down for the One Sample Example if that is what you are really interested in.", "However, it is still recommended that you read the paired data example first.", "Paired Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon.", "Background Height differences “between cross- and self- fertilized corn plants of the same pair” were collected."],
    "type": "section",
    "page_title": "Wilcoxon Tests",
    "section_title": "Explanation"
  },
  {
    "id": 345,
    "title": "📍 Wilcoxon Rank Sum\r\n(Mann-Whitney) Test",
    "url": "WilcoxonTests.html#wilcoxonranksummannwhitneytest",
    "content": "Wilcoxon Rank Sum\r\n(Mann-Whitney) Test For testing the equality of the medians of two (possibly different) distributions of a quantitative variable. Overview The nonparametric equivalent of the Independent Samples t Test. Can also be used when data is ordered (ordinal) but does not have an exact measurement. For example, first place, second place, and so on. The Independent Samples t Test is more appropriate when the distributions are normal, or when the sample size for each sample is large. The test is negatively affected when there are ties (repeated values) present in the data, but the results are still useful if there are relatively few ties. Hypotheses Originally designed to test for the equality of medians from two identically shaped distributions. Math Code $$ H_0: \\text{difference in medians} = 0 $$ $$ H_a: \\text{difference in medians} \\neq 0 $$ \\(H_0: \\text{difference in medians} = 0\\) \\(H_a: \\text{difference in medians} \\neq 0\\) However, the test also allows for the more general hypotheses that one distribution is stochastically greater than the other. \\(H_0: \\text{the distributions are stochastically equal}\\) \\(H_a: \\text{one distribution is stochastically greater than the other}\\) If these hypotheses are used, then the distributions do not have to be identically distributed. (Note: Men’s heights are stochastically greater than women’s heights because men are generally taller than women, but not all men are taller than all women.) Examples: BugSpray, MoralIntegration",
    "sentences": ["For testing the equality of the medians of two (possibly different) distributions of a quantitative variable.", "Overview The nonparametric equivalent of the Independent Samples t Test.", "Can also be used when data is ordered (ordinal) but does not have an exact measurement.", "For example, first place, second place, and so on.", "The Independent Samples t Test is more appropriate when the distributions are normal, or when the sample size for each sample is large.", "The test is negatively affected when there are ties (repeated values) present in the data, but the results are still useful if there are relatively few ties.", "Hypotheses Originally designed to test for the equality of medians from two identically shaped distributions.", "Math Code $$ H_0: \\text{difference in medians} = 0 $$ $$ H_a: \\text{difference in medians} \\neq 0 $$ \\(H_0: \\text{difference in medians} = 0\\) \\(H_a: \\text{difference in medians} \\neq 0\\) However, the test also allows for the more general hypotheses that one distribution is stochastically greater than the other.", "\\(H_0: \\text{the distributions are stochastically equal}\\) \\(H_a: \\text{one distribution is stochastically greater than the other}\\) If these hypotheses are used, then the distributions do not have to be identically distributed.", "(Note: Men’s heights are stochastically greater than women’s heights because men are generally taller than women, but not all men are taller than all women.) Examples: BugSpray, MoralIntegration"],
    "type": "section",
    "page_title": "Wilcoxon Tests",
    "section_title": "Wilcoxon Rank Sum\r\n(Mann-Whitney) Test"
  },
  {
    "id": 346,
    "title": "📍 Overview",
    "url": "WilcoxonTests.html#overview",
    "content": "Overview The nonparametric equivalent of the Independent Samples t Test. Can also be used when data is ordered (ordinal) but does not have an exact measurement. For example, first place, second place, and so on. The Independent Samples t Test is more appropriate when the distributions are normal, or when the sample size for each sample is large. The test is negatively affected when there are ties (repeated values) present in the data, but the results are still useful if there are relatively few ties. Hypotheses Originally designed to test for the equality of medians from two identically shaped distributions. Math Code $$ H_0: \\text{difference in medians} = 0 $$ $$ H_a: \\text{difference in medians} \\neq 0 $$ \\(H_0: \\text{difference in medians} = 0\\) \\(H_a: \\text{difference in medians} \\neq 0\\) However, the test also allows for the more general hypotheses that one distribution is stochastically greater than the other. \\(H_0: \\text{the distributions are stochastically equal}\\) \\(H_a: \\text{one distribution is stochastically greater than the other}\\) If these hypotheses are used, then the distributions do not have to be identically distributed. (Note: Men’s heights are stochastically greater than women’s heights because men are generally taller than women, but not all men are taller than all women.) Examples: BugSpray, MoralIntegration",
    "sentences": ["The nonparametric equivalent of the Independent Samples t Test.", "Can also be used when data is ordered (ordinal) but does not have an exact measurement.", "For example, first place, second place, and so on.", "The Independent Samples t Test is more appropriate when the distributions are normal, or when the sample size for each sample is large.", "The test is negatively affected when there are ties (repeated values) present in the data, but the results are still useful if there are relatively few ties.", "Hypotheses Originally designed to test for the equality of medians from two identically shaped distributions.", "Math Code $$ H_0: \\text{difference in medians} = 0 $$ $$ H_a: \\text{difference in medians} \\neq 0 $$ \\(H_0: \\text{difference in medians} = 0\\) \\(H_a: \\text{difference in medians} \\neq 0\\) However, the test also allows for the more general hypotheses that one distribution is stochastically greater than the other.", "\\(H_0: \\text{the distributions are stochastically equal}\\) \\(H_a: \\text{one distribution is stochastically greater than the other}\\) If these hypotheses are used, then the distributions do not have to be identically distributed.", "(Note: Men’s heights are stochastically greater than women’s heights because men are generally taller than women, but not all men are taller than all women.) Examples: BugSpray, MoralIntegration"],
    "type": "section",
    "page_title": "Wilcoxon Tests",
    "section_title": "Overview"
  },
  {
    "id": 347,
    "title": "📍 R Instructions",
    "url": "WilcoxonTests.html#rinstructions",
    "content": "R Instructions Console Help Command: ?wilcox.test() There are two ways to perform the test. Option 1: wilcox.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a “numeric” vector from YourData that represents the data for both samples. X must be a “factor” or “character” vector from YourData that represents the group assignment for each observation. There can only be two groups specified in this column of data. YourNull is the numeric value from your null hypothesis for the difference in medians from the two groups. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests. length  ‘length’ is a quantitative variable (numeric vector). ~  ‘~’ is the tilde symbol. sex,  ‘sex’ is a ‘factor’ or ‘character’ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet,  ‘KidsFeet’ is a dataset in library(mosaic). Type View(KidsFeet) to view it. mu = 0,  The numeric value from the null hypothesis for the difference in medians from the two groups is 0 meaning the null hypothesis is \\(\\text{difference in medians} = 0\\) alternative = “two.sided”,  The alternative is “two-sided” meaning the alternative hypothesis is \\(\\text{difference in medians} \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\)     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Wilcoxon rank sum test with continuity correction This states the test that was performed and that a normal approximation to the test statistic was used instead of the exact distribution. data: length by sex This states that the length column was split into the two groups found in the “sex” column. Unfortunately, it forgets to remind us that the test used the KidsFeet data set. V = 252, The test statistic of the test. In this case, the sum of the ranks from the alphabetically first group minus the minimum sum of ranks possible.  p-value = 0.0836 The p-value of the test. alternative hypothesis: true location shift is not equal to 0 This reports that the test used an alternative hypothesis of “not equal” (a two-sided test). Further, the phrase “true location shift” emphasizes that the Wilcoxon Rank Sum Test is testing to see if one distribution is shifted higher or lower than the other. Option 2: wilcox.test(object1, object2, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object1 must be a “numeric” vector that represents the first sample of data. obejct2 must be a “numeric” vector that represents the second sample of data. YourNull is the numeric value from your null hypothesis for the difference in medians from the two groups. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests. KidsFeet$length[KidsFeet$sex == “B”],  A numeric vector of foot length for the first sample of data or for the group of boys. KidsFeet$length[KidsFeet$sex == “G”],  A numeric vector of foot length for the second sample of data or for the group of girls. mu = 0,  The numeric value from the null hypothesis for the difference in medians from the two groups is 0 meaning the null hypothesis is \\(\\text{difference in medians} = 0\\) alternative = “two.sided”,  The alternative is “two-sided” meaning the alternative hypothesis is \\(\\text{difference in medians} \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\)     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Wilcoxon rank sum test with continuity correction This states the type of test performed. data: KidsFeet$length[KidsFeet$sex == “B”] and KidsFeet$length[KidsFeet$sex == “G”] Reminds you what data was used for the test and points out that the first set of data listed is “Group 1” and the second set of data listed is “Group 2.” V = 252, The test statistic of the test.  p-value = 0.0836 The p-value of the test. alternative hypothesis: true location shift is not equal to 0 The alternative hypothesis of the test. The phrase “not equal” implies a “two-sided” alternative was used.",
    "sentences": ["Console Help Command: ?wilcox.test() There are two ways to perform the test.", "Option 1: wilcox.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a “numeric” vector from YourData that represents the data for both samples.", "X must be a “factor” or “character” vector from YourData that represents the group assignment for each observation.", "There can only be two groups specified in this column of data.", "YourNull is the numeric value from your null hypothesis for the difference in medians from the two groups.", "YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis.", "The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99.", "It should correspond to \\(1-\\alpha\\).", "Example Code Hover your mouse over the example codes to learn more.", "wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests."],
    "type": "section",
    "page_title": "Wilcoxon Tests",
    "section_title": "R Instructions"
  },
  {
    "id": 348,
    "title": "📍 Explanation",
    "url": "WilcoxonTests.html#explanation",
    "content": "Explanation In many cases it is of interest to perform a hypothesis test concerning the equality of the centers of two (possibly different) distributions. In other words, an independent samples test. The Wilcoxon Rank Sum Test allows a nonparametric approach to doing this. It is often considered the nonparametric equivalent of the independent samples t test. The method is most easily explained through an example. The theory behind it is very similar to the theory behind the Wilcoxon Signed-Rank Test. Independent Samples Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon. Background The percent of flies (bugs) killed from two different concentrations of a certain spray were recorded from 16 different trials, 8 trials per treatment concentration. The experiment hypothesized that the center of the distributions of the percent killed by either concentration were the same. In other words, that both treatments were equally effective. The alternative hypothesis was that the treatments differed in their effectiveness. Spray Concentration Percent Killed A 68, 68, 59, 72, 64, 67, 70, 74 B 60, 67, 61, 62, 67, 63, 56, 58 Step 1 The first step of the Wilcoxon Rank Sum Test is to order all the data from smallest magnitude to largest magnitude, while keeping track of the group. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Step 2 The next step is to rank the ordered values. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=16\\). Any ranks that are tied need to have the average rank assigned to each of those that are tied. Sorted Data Percent Killed 56 58 59 60 61 62 63 64 67 67 67 68 68 70 72 74 Concentration B B A B B B B A A B B A A A A A Rank 1 2 3 4 5 6 7 8 10 10 10 12.5 12.5 14 15 16 Step 3 The ranks are then returned to their original groups. Ranks of Spray A Ranks of Spray B 3, 8, 10, 12.5, 12.5, 14, 15, 16 1, 2, 4, 5, 6, 7, 10, 10 Step 4 The ranks are summed for one of the groups. (It does not matter which group.) Sum of Ranks for Spray A: \\[ 3+8+10+12.5+12.5+14+15+16 = 91 \\] Step 5 The \\(p\\)-value of the test is then obtained by computing the probabilities of all possible sums that one group can achieve using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=16\\) ranks, with just \\(8\\) of the ranks assigned to one group, the possible sums of ranks range from \\(36\\) to \\(100\\) and include every integer in between, i.e., \\(36, 37, 38, \\ldots, 100\\). Note that the smallest sum would be obtained if the ranks 1-8 were in the group. The largest sum would be obtained if the ranks 9-16 were in the group. The probability of each possible sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 12,870 total different sets of ranks possible when there are \\(n=16\\) ranks and \\(8\\) are assigned to one group.) The distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(91\\) (or its opposite of \\(136-91=45\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of negative ranks as extreme or more extreme than \\(91\\) is \\(p=0.01476\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would reject the null hypothesis that the difference in the center of the distributions is zero. We conclude that the Spray Concentration A is more effective at killing flies (bugs). Comment The hypotheses for the Wilcoxon Rank Sum Test are difficult to write out in simple mathematical statements. The test is often referred to as a test of medians, but goes deeper than this. Technically, it allows us to determine if one distribution is stochastically larger than another. In other words, if one distribution typically gives larger values than does another distribution. If the distributions are identically shaped and have the same spread, then this implies the medians (and means) are different. Thus, the hypotheses for the test can be written mathematically as \\[ H_0: \\text{difference in medians} = 0 \\] \\[ H_a: \\text{difference in medians} \\neq 0 \\] or even as \\[ H_0: \\mu_1-\\mu_2 = 0 \\] \\[ H_a: \\mu_1-\\mu_2 \\neq 0 \\] However, it is important to remember that the estimated difference in location parameters that results from the test does not provide a measurement on either of these things. However, the \\(p\\)-value can lead us to determine whether to reject, or fail to reject the null, whichever of the above hypotheses is used. As stated in the R help file for this test ?wilcox.test() “the estimator for the difference in location parameters does not estimate the difference in medians (a common misconception) but rather the median of the difference between a sample from [the first population] and a sample from [the second population].” These are technical details that most people ignore without encountering too much difficulty. However, it does remind us that the t test is more easily interpreted whenever it is appropriate to use that test. Final Comment Notice that when the sample size is large the distribution of the test statistic can be approximated by a normal distribution. Most software applications use this approximation when the sample size is over \\(50\\), i.e., for \\(n\\geq50\\) because computing all the possible sums of ranks becomes incredibly time consuming. Thus, for large sample sizes the results will be almost identical whether the Wilcoxon Tests or a t Test is used.",
    "sentences": ["In many cases it is of interest to perform a hypothesis test concerning the equality of the centers of two (possibly different) distributions.", "In other words, an independent samples test.", "The Wilcoxon Rank Sum Test allows a nonparametric approach to doing this.", "It is often considered the nonparametric equivalent of the independent samples t test.", "The method is most easily explained through an example.", "The theory behind it is very similar to the theory behind the Wilcoxon Signed-Rank Test.", "Independent Samples Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon.", "Background The percent of flies (bugs) killed from two different concentrations of a certain spray were recorded from 16 different trials, 8 trials per treatment concentration.", "The experiment hypothesized that the center of the distributions of the percent killed by either concentration were the same.", "In other words, that both treatments were equally effective."],
    "type": "section",
    "page_title": "Wilcoxon Tests",
    "section_title": "Explanation"
  }
]
