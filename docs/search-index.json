[{"title":"Chicken Weight – ANOVA Block Design","url":"325Analyses/ANOVA/Examples/ChickWeightANOVABlock.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } Chicken Weight – ANOVA Block Design Background An experiment was conducted “on the effect of diet on early growth of chicks.” A total of 50 chicks were assigned to one of four possible diets. Diet Number of Chicks Assigned 1 20 2 10 3 10 4 10 Weight measurements on each chick were taken (in grams) at birth (day 0) as well as on days 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, and 21. Five chicks do not have measurements at all times (chicks 8, 15, 16, 18, and 44). The dataset ChickWeight contains the data from this experiment. The goal of the study is to identify the diet that produces the greatest increase in chick weights. Even though measurements were made at many time intervals, the growth of the chicks over time is not of direct interest. Thus, we will treat time as a blocking factor. This is justified because there is reason to believe that within each time period measurements will be similar although they should differ across time periods. Since our main interest is in the effect of Diet on growth, using time as a blocking factor will allow us to decide if any Diet affects growth consistently over time. Analysis The mathematical model for this study is written as \\[ Y_{li} = \\mu + B_l + \\alpha_i + \\epsilon_{li} \\] where \\(B_l\\) represents the blocking factor of time so that \\(l=1,\\ldots,11\\); \\(\\alpha_i\\) represents the different Diets with \\(i=1,\\ldots,4\\). The null hypothesis is written as \\[ H_0: \\alpha_1 = \\alpha_2 = \\alpha_3 = \\alpha_4 = 0 \\] \\[ H_a: \\alpha_i \\neq 0 \\ \\text{for at least one}\\ i \\] cw.aov <- aov(weight ~ as.factor(Time) + Diet, data=ChickWeight) summary(cw.aov) Df Sum Sq Mean Sq F value Pr(>F) as.factor(Time) 11 2067050 187914 147.4 <2e-16 *** Diet 3 129721 43240 33.9 <2e-16 *** Residuals 563 717785 1275 --- Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 While the blocking factor of Time is significant, that is not really of interest to us. We just included time in the model so that it would account for some of the variation (or a LOT of the variation as witnessed by the size of the “Mean Sq” which is just the variation due to Time). This gives a clearer picture on how Diet affects the weight of the chicks over all time periods. Clearly Diet is a significant factor \\((p < 2x10^{-16})\\). We conclude that at least one Diet has a different average weight than the others. However, a further look at the residuals show that the ANOVA design is really not appropriate for these data. Neither the constant variance assumption (left plot) nor the normality assupmtion (right plot) are satisfied. par(mfrow=c(1,2)) plot(cw.aov, which=1:2) It behooves us to try a different approach to analyzing the data. While the ANOVA Block design would have been the best approach, the Kruskal-Wallis test could be used to see if Diet has a different average over all Time periods. kruskal.test(weight ~ Diet, data=ChickWeight) Kruskal-Wallis rank sum test data: weight by Diet Kruskal-Wallis chi-squared = 24.45, df = 3, p-value = 2.012e-05 The conclusion of the test is that yes, at least one Diet shows a distribution of weights that is different from the others. Interpretation xyplot(weight ~ Diet, data=ChickWeight, type=c(\"p\",\"a\")) The above plot shows Diets 3 and 4 carry the highest average weights of chicks over all time periods, with a potentially slight advantage to Diet 3. // add bootstrap table styles to pandoc tables $(document).ready(function () { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Chick Weights – One-way ANOVA","url":"325Analyses/ANOVA/Examples/chickwtsOneWayANOVA.html","content":"Code Show All Code Hide All Code Chick Weights – One-way ANOVA Background This experiment comes from the R help file ?chickwts and the data is correspondingly recorded in the chickwts data set. “An experiment was conducted to measure and compare the effectiveness of various feed supplements on the growth rate of chickens. Newly hatched chicks were randomly allocated into six groups, and each group was given a different feed supplement. Their weights in grams after six weeks are given along with feed types.” library(car) library(mosaic) library(pander) library(tidyverse) The six feed types were casein, horsebean, linseed, meatmeal, soybean, and sunflower. Analysis Applying a one-way ANOVA to this study, we have the null hypothesis that the average weight of grown chickens, represented by \\(\\mu\\), is equal for each feed type. This is formally written as follows. \\[ H_0: \\mu_\\text{casein} = \\mu_\\text{horsebean} = \\mu_\\text{linseed} = \\mu_\\text{meatmeal} = \\mu_\\text{soybean} = \\mu_\\text{sunflower} = \\mu \\] The alternative hypotheses claims that at least one of the feed types produces a significantly different average weight in the grown chickens. \\[ H_a: \\text{at least one mean differs} \\] These hypotheses will allow us the ability to answer the question, is the average chick weight the same for each feed type? Or is at least one feed type different on average than the others? We will use a signifance level of \\(\\alpha = 0.05\\) for this study. To perform the analysis we compute the following ANOVA. chick.aov <- aov(weight ~ feed, data=chickwts) summary(chick.aov) %>% pander() Analysis of Variance Model   Df Sum Sq Mean Sq F value Pr(>F) feed 5 231129 46226 15.36 5.936e-10 Residuals 65 195556 3009 NA NA The \\(p\\)-value of the test is highly significant \\((p = 5.9\\times10^{-10})\\), therefore we conclude the alternative, that at least one feed type results in a different average weight of the grown chickens. The following plots demonstrate that the assumptions of the ANOVA were satisfied. This can be seen as the residuals versus fitted values plot shows roughly constant variance within each vertical group of dots, and the Q-Q Plot shows normality of the residuals as all dots stay within the boundary lines. par(mfrow=c(1,2)) plot(chick.aov, which=1, pch=16) qqPlot(chick.aov$residuals, id=FALSE) The following graphic provides an explanation of which feed types are producing the highest average weights. (Note, this plot was created using mPlot(chickwts) and then editing the code slightly.) xyplot(weight ~ feed, data=chickwts, main=\"Experiment on how Feed Type impacts Chicken Growth\", ylab=\"Adult Weight of Chickens (in grams)\", xlab=\"Type of Feed Chickens were Fed \\n (Blue line shows average weights per feed type.)\", type=c(\"p\",\"a\")) The averages depicted by the blue line in the plot above are summarized in the table below as well as the standard deviation and sample size for the final chicken weights within each feed type. pander(favstats(weight ~ feed, data=chickwts)[,c(\"feed\",\"mean\",\"sd\",\"n\")]) feed mean sd n casein 323.6 64.43 12 horsebean 160.2 38.63 10 linseed 218.8 52.24 12 meatmeal 276.9 64.9 11 soybean 246.4 54.13 14 sunflower 328.9 48.84 12 Interpretation It appears that horsebean and linseed should not be used as feed types, as they have the lowest average weights at \\(160.2\\) and \\(218.8\\), respectively. The best two feed types appear to be casein and sunflower at \\(323.6\\) and \\(328.9\\), respectively. At this point, a cost analysis would also be important in deciding which feed type would provide the greatest average weight at the lowest cost. If significantly cheaper, meatmeal may be worth considering at an average weight of 276.9. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"CO2 – Three-way ANOVA","url":"325Analyses/ANOVA/Examples/CO2ThreeWayANOVA.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } button.code-folding-btn:focus { outline: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); CO2 – Three-way ANOVA Background This example is taken from an experiment listed in the R help files under ?CO2. “An experiment on the cold tolerance of the grass species Echinochloa crus-galli was conducted. The CO2 uptake of six plants from Quebec and six plants from Mississippi was measured at several levels of ambient CO2 concentration. Half the plants of each type were chilled overnight before the experiment was conducted.” Plants were considered tolerant to the cold if they were still able to acheive high CO2 uptake values after being chilled. knitr::kable(head(CO2)) Plant Type Treatment conc uptake Qn1 Quebec nonchilled 95 16.0 Qn1 Quebec nonchilled 175 30.4 Qn1 Quebec nonchilled 250 34.8 Qn1 Quebec nonchilled 350 37.2 Qn1 Quebec nonchilled 500 35.3 Qn1 Quebec nonchilled 675 39.2 Ignoring the Plant ID for a moment, there are three factors that possibly effect the uptake of a plant. These include Type, Treatment, and conc. The factor Type has two levels, Quebec and Mississippi. The factor Treatment has two levels chilled and nonchilled and the factor conc has seven levels 95, 175, 250, 350, 500, 675, and 1000. An ANOVA could be performed on this data using the model \\[ y_{ijkl} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijkl} \\ \\text{where} \\ \\epsilon_{ijkl}\\sim N(0,\\sigma) \\] This analysis would be performed in R by running the code CO2.aov <- aov(uptake ~ Type + Treatment + as.factor(conc), data=CO2) summary(CO2.aov) ## Df Sum Sq Mean Sq F value Pr(>F) ## Type 1 3366 3366 196.50 <2e-16 *** ## Treatment 1 988 988 57.69 7e-11 *** ## as.factor(conc) 6 4069 678 39.59 <2e-16 *** ## Residuals 75 1285 17 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Notice that in the summary output the factors of Type, Treatment, and conc all show significant p-values. This claims that each of these factors significantly affect the uptake level. In other words, if you recall the hypotheses of ANOVA, the conclusion is that at least one level of each significant factor (in this case all factors are significant) has a different average value of uptake from the other levels of that factor. We can demonstrate the results of the ANOVA test by showing the plots for each factor. # Plot demonstrating Type: xyplot( uptake ~ Type, data=CO2, main=\"\", jitter.x=TRUE, pch=16) # Plot demonstrating Treatment: xyplot( uptake ~ Treatment, data=CO2, main=\"\", jitter.x=TRUE, pch=16) # Plot demonstrating conc: xyplot( uptake ~ conc, data=CO2, main=\"\", jitter.x=TRUE, pch=16) Of course, these results are only meaningful if the ANOVA is appropriate. To assess the appropriateness of performing the current ANOVA on these data we need to check the residuals. This is done as shown below. The two assumptions are certainly questionable. The left plot shows that the constant variance may not be satisfied and the right plot shows that normality is uncertain. Official tests of these requirements confirm the difficulty of using ANOVA on these data by showing that at the 0.05 level, normality should not be assumed while the constant variance technically could be. par(mfrow=c(1,2)) plot(CO2.aov, which=1:2) # Test for constant variance: library(lmtest) ## Loading required package: zoo ## ## Attaching package: 'zoo' ## The following objects are masked from 'package:base': ## ## as.Date, as.Date.numeric bptest(CO2.aov) ## ## studentized Breusch-Pagan test ## ## data: CO2.aov ## BP = 15.926, df = 8, p-value = 0.04345 # Test for normality shapiro.test(CO2.aov$residuals) ## ## Shapiro-Wilk normality test ## ## data: CO2.aov$residuals ## W = 0.98025, p-value = 0.2247 This is certainly a frustration and needs to be resolved before conclusions about the data can be reached. A common reason that the assumptions of an ANOVA are not satisfied is that there are important interactions that have not been included in the ANOVA model. Consider how the plot (made previously but repeated here for convenience) of uptake ~ conc shows the possibility of an interaction due to the separate groups of data (a low group and a high group). # Repeated plot demonstrating conc: xyplot( uptake ~ as.factor(conc), data=CO2, main=\"\", jitter.x=TRUE, pch=16) Thus, we could perform an ANOVA that expands the model to include all possible interactions between Type, Treatment, and Uptake. CO2int.aov <- aov(uptake ~ Type * Treatment * as.factor(conc), data=CO2) summary(CO2int.aov) ## Df Sum Sq Mean Sq F value Pr(>F) ## Type 1 3366 3366 399.758 < 2e-16 *** ## Treatment 1 988 988 117.368 2.32e-15 *** ## as.factor(conc) 6 4069 678 80.548 < 2e-16 *** ## Type:Treatment 1 226 226 26.812 3.15e-06 *** ## Type:as.factor(conc) 6 374 62 7.412 7.24e-06 *** ## Treatment:as.factor(conc) 6 101 17 1.999 0.0811 . ## Type:Treatment:as.factor(conc) 6 112 19 2.216 0.0547 . ## Residuals 56 471 8 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 As shown in the output summary above, the interactions of Type:Treatment and Type:conc are significant. However, the interactions of Treatment:conc and Type:Treatment:conc are not significant. Notice that the residual plots still show some difficulties. There is still more to this story that will need to be resolved. plot(CO2int.aov, which=1:2) // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Day Care Fines","url":"325Analyses/ANOVA/Examples/DayCare.html","content":"Code Show All Code Hide All Code Day Care Fines library(mosaic) library(DT) library(pander) library(car) library(tidyverse) # To get the \"late\" dataset into your R Console, # you need to go to your file menu of R-Studio and select # \"Session, Set working directory, To source file location\" # Then click the green \"play\" arrow in # the top right corner of this gray R-Chunk. ## Wide data from original article: late <- read.csv(\"../../../Data/late.csv\", header=TRUE) ## Change to \"long\" data for use in R: Late <- pivot_longer(late, #Start with wide data set late cols = starts_with(\"Week\"), #Find columns to gather from long data names_to = \"Week\", #New column name of Weeks in long data names_prefix = \"Week\", #Remove prefix of Week from Week1, Week2, ... names_transform = list(Week= ~as.numeric(.x)), #Make Week a numeric column of data values_to = \"NumberofLateChildren\") #Name of data column in long data Background Study Details This background is quoted directly from the article “A Fine is a Price”. There are two types of day-care centers in Israel: private and public. A study was conducted in 10 private day-care centers in the city of Haifa from January to June 1998. All of these centers are located in the same part of town, and there is no important difference among them. During the day children are organized into groups according to age, from 1 to 4 years old. Each day-care center is allowed to hold a maximum of 35 children. In some exceptional cases a few additional children are allowed. The fee for each child is NIS 1,400 per month. (The NIS is the New Israeli Shekel.) At the time of the study, a U.S. dollar was worth approximately NIS 3.68, so the fee was about $380 at that time. The contract signed at the beginning of the year states that the day-care center operates between 0730 and 1600. There is no mention of what happens if parents come late to pick up their children. In particular, before the beginning of the study, there was no fine for coming late. When parents did not come on time, one of the teachers had to wait with the children concerned. Teachers would rotate in this task, which is considered part of the job of a teacher, a fact that is clearly explained when a teacher is hired. Parents rarely came after 1630. A natural option [to fix the problem of parents showing up late] is to introduce a fine: every time a parent comes late, [they] will have to pay a fine. Will that reduce the number of parents who come late? If the fine is removed, will things revert back to the way they were originally? The overall period of the study was 20 weeks. In the first 4 weeks we simply recorded the number of parents who arrived late each week. At the beginning of the fifth week, we introduced a fine in six of the 10 day-care centers, which had been selected randomly. The announcement of the fine was made with a note posted on the bulletin board of the day-care center. Parents tend to look at this board every day, since important announcements are posted there. The announcement specified that the fine would be NIS 10 for a delay of 10 minutes or more. The fine was per child; thus, if parents had two children in the center and they came late, they had to pay NIS 20. Payment was made to the principal of the day-care center at the end of the month. Since monthly payments are made to the owner during the year, the fines were added to those amounts. The money was paid to the owner, rather then to the teacher who was staying late (and did not get any additional money). The teachers were informed of the fine but not of the study. Registering the names of parents who came late was a common practice in any case. At the beginning of the seventeenth (17th) week, the fine was removed with no explanation. Notice of the cancellation was posted on the board. If parents asked why the fines were removed, the principals were instructed to reply that the fine had been a trial for a limited time and that the results of this trial were now being evaluated. A comparison with other fines in Israel may give an idea of the size of the penalty that was introduced. A fine of NIS 10 is relatively small but not insignificant. In comparison, the fine for illegal parking is NIS 75; the fine for driving through a red light is NIS 1,000 plus penalties; the fine for not collecting the droppings of a dog is NIS 360. For many of these violations, however, detection and enforcement are low or, as in the case of dog dirt, nonexistent in practice. A baby-sitter earns between NIS 15 and NIS 20 per hour. The average gross salary per month in Israel at the time of the study was NIS 5,595. The Data (Wide) The late Day Care Center data is shown here in the “wide data format” that was provided by the authors of the original research article. But this format is not terribly useful for an ANOVA analysis. #Show the full width of the \"Wide\" version of the late data: pander(late, split.tables = Inf) Treatment Center No.ofChildren Week1 Week2 Week3 Week4 Week5 Week6 Week7 Week8 Week9 Week10 Week11 Week12 Week13 Week14 Week15 Week16 Week17 Week18 Week19 Week20 Fine 1 37 8 8 7 6 8 9 9 12 13 13 15 13 14 16 14 15 16 13 15 17 Fine 2 35 6 7 3 5 2 11 14 9 16 12 10 14 14 16 12 17 14 10 14 15 Fine 3 35 8 9 8 9 3 5 15 18 16 14 20 18 25 22 27 19 20 23 23 22 Fine 4 34 10 3 14 9 6 24 8 22 22 19 25 18 23 22 24 17 15 23 25 18 Fine 5 33 13 12 9 13 15 10 27 28 35 10 24 32 29 29 26 31 26 35 29 28 Fine 6 28 5 8 7 5 5 9 12 14 19 17 14 13 10 15 14 16 6 12 17 13 Control 7 35 7 10 12 6 4 13 7 8 5 12 3 5 6 13 7 4 7 10 4 6 Control 8 34 12 9 14 18 10 11 6 15 14 13 7 12 9 9 17 8 5 11 8 13 Control 9 34 3 4 9 3 3 5 9 5 2 7 6 6 9 4 9 2 3 8 3 5 Control 10 32 15 13 13 12 10 9 15 15 15 10 17 12 13 11 14 17 12 9 15 13 The Data (Long) The Late Day Care Center data is shown here in the “long data format”, a more useable format for analysis. pander(Late) Treatment Center No.ofChildren Week NumberofLateChildren Fine 1 37 1 8 Fine 1 37 2 8 Fine 1 37 3 7 Fine 1 37 4 6 Fine 1 37 5 8 Fine 1 37 6 9 Fine 1 37 7 9 Fine 1 37 8 12 Fine 1 37 9 13 Fine 1 37 10 13 Fine 1 37 11 15 Fine 1 37 12 13 Fine 1 37 13 14 Fine 1 37 14 16 Fine 1 37 15 14 Fine 1 37 16 15 Fine 1 37 17 16 Fine 1 37 18 13 Fine 1 37 19 15 Fine 1 37 20 17 Fine 2 35 1 6 Fine 2 35 2 7 Fine 2 35 3 3 Fine 2 35 4 5 Fine 2 35 5 2 Fine 2 35 6 11 Fine 2 35 7 14 Fine 2 35 8 9 Fine 2 35 9 16 Fine 2 35 10 12 Fine 2 35 11 10 Fine 2 35 12 14 Fine 2 35 13 14 Fine 2 35 14 16 Fine 2 35 15 12 Fine 2 35 16 17 Fine 2 35 17 14 Fine 2 35 18 10 Fine 2 35 19 14 Fine 2 35 20 15 Fine 3 35 1 8 Fine 3 35 2 9 Fine 3 35 3 8 Fine 3 35 4 9 Fine 3 35 5 3 Fine 3 35 6 5 Fine 3 35 7 15 Fine 3 35 8 18 Fine 3 35 9 16 Fine 3 35 10 14 Fine 3 35 11 20 Fine 3 35 12 18 Fine 3 35 13 25 Fine 3 35 14 22 Fine 3 35 15 27 Fine 3 35 16 19 Fine 3 35 17 20 Fine 3 35 18 23 Fine 3 35 19 23 Fine 3 35 20 22 Fine 4 34 1 10 Fine 4 34 2 3 Fine 4 34 3 14 Fine 4 34 4 9 Fine 4 34 5 6 Fine 4 34 6 24 Fine 4 34 7 8 Fine 4 34 8 22 Fine 4 34 9 22 Fine 4 34 10 19 Fine 4 34 11 25 Fine 4 34 12 18 Fine 4 34 13 23 Fine 4 34 14 22 Fine 4 34 15 24 Fine 4 34 16 17 Fine 4 34 17 15 Fine 4 34 18 23 Fine 4 34 19 25 Fine 4 34 20 18 Fine 5 33 1 13 Fine 5 33 2 12 Fine 5 33 3 9 Fine 5 33 4 13 Fine 5 33 5 15 Fine 5 33 6 10 Fine 5 33 7 27 Fine 5 33 8 28 Fine 5 33 9 35 Fine 5 33 10 10 Fine 5 33 11 24 Fine 5 33 12 32 Fine 5 33 13 29 Fine 5 33 14 29 Fine 5 33 15 26 Fine 5 33 16 31 Fine 5 33 17 26 Fine 5 33 18 35 Fine 5 33 19 29 Fine 5 33 20 28 Fine 6 28 1 5 Fine 6 28 2 8 Fine 6 28 3 7 Fine 6 28 4 5 Fine 6 28 5 5 Fine 6 28 6 9 Fine 6 28 7 12 Fine 6 28 8 14 Fine 6 28 9 19 Fine 6 28 10 17 Fine 6 28 11 14 Fine 6 28 12 13 Fine 6 28 13 10 Fine 6 28 14 15 Fine 6 28 15 14 Fine 6 28 16 16 Fine 6 28 17 6 Fine 6 28 18 12 Fine 6 28 19 17 Fine 6 28 20 13 Control 7 35 1 7 Control 7 35 2 10 Control 7 35 3 12 Control 7 35 4 6 Control 7 35 5 4 Control 7 35 6 13 Control 7 35 7 7 Control 7 35 8 8 Control 7 35 9 5 Control 7 35 10 12 Control 7 35 11 3 Control 7 35 12 5 Control 7 35 13 6 Control 7 35 14 13 Control 7 35 15 7 Control 7 35 16 4 Control 7 35 17 7 Control 7 35 18 10 Control 7 35 19 4 Control 7 35 20 6 Control 8 34 1 12 Control 8 34 2 9 Control 8 34 3 14 Control 8 34 4 18 Control 8 34 5 10 Control 8 34 6 11 Control 8 34 7 6 Control 8 34 8 15 Control 8 34 9 14 Control 8 34 10 13 Control 8 34 11 7 Control 8 34 12 12 Control 8 34 13 9 Control 8 34 14 9 Control 8 34 15 17 Control 8 34 16 8 Control 8 34 17 5 Control 8 34 18 11 Control 8 34 19 8 Control 8 34 20 13 Control 9 34 1 3 Control 9 34 2 4 Control 9 34 3 9 Control 9 34 4 3 Control 9 34 5 3 Control 9 34 6 5 Control 9 34 7 9 Control 9 34 8 5 Control 9 34 9 2 Control 9 34 10 7 Control 9 34 11 6 Control 9 34 12 6 Control 9 34 13 9 Control 9 34 14 4 Control 9 34 15 9 Control 9 34 16 2 Control 9 34 17 3 Control 9 34 18 8 Control 9 34 19 3 Control 9 34 20 5 Control 10 32 1 15 Control 10 32 2 13 Control 10 32 3 13 Control 10 32 4 12 Control 10 32 5 10 Control 10 32 6 9 Control 10 32 7 15 Control 10 32 8 15 Control 10 32 9 15 Control 10 32 10 10 Control 10 32 11 17 Control 10 32 12 12 Control 10 32 13 13 Control 10 32 14 11 Control 10 32 15 14 Control 10 32 16 17 Control 10 32 17 12 Control 10 32 18 9 Control 10 32 19 15 Control 10 32 20 13 Analysis Hypotheses Two-way ANOVA Graphics & Numerical Summaries Conclusions Diagnostic Plots // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Breaks in Warps while Weaving: Two-way ANOVA","url":"325Analyses/ANOVA/Examples/warpbreaksTwoWayANOVA.html","content":"Code Show All Code Hide All Code Breaks in Warps while Weaving: Two-way ANOVA library(mosaic) library(DT) library(pander) library(tidyverse) Background This data appeared in one of the first textbooks ever written on statistics by L. H. C. Tippet, published in 1935. Thus, it concerns a rather old fashioned idea of rather extreme importance, warp breaks. A warp is the yarn held in tension by the loom, and when it breaks, it causes extreme difficulties. (See wikipedia on Warp (weaving) for details.) This analysis looks at data from an experiment that was conducted to determine which wool types and loom tensions produced the fewest warp breaks on average. This image comes from the U.S. National Archives. The original caption to this image read as follows. “Paterson, New Jersey - Textiles. The exhausted warp. The operator is preparing to remove the beam end harness in order to set the loom in operation again. The ends of a new warp must be entered through the harness or twisted on to the from ends remaining in the harness, March 1937.” Hide Data Show Data The data for this experiment comes from the warpbreaks R data set. As found on the help file ?warpbreaks, “This data set gives the number of warp breaks per loom, where a loom corresponds to a fixed length of yarn. There are measurements on 9 looms for each of the six types of warp (AL, AM, AH, BL, BM, BH).” datatable(warpbreaks) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\"],[26,30,54,25,70,52,51,26,67,18,21,29,17,12,18,35,30,36,36,21,24,18,10,43,28,15,26,27,14,29,19,29,31,41,20,44,42,26,19,16,39,28,21,39,29,20,21,24,17,13,15,15,16,28],[\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\"],[\"L\",\"L\",\"L\",\"L\",\"L\",\"L\",\"L\",\"L\",\"L\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"L\",\"L\",\"L\",\"L\",\"L\",\"L\",\"L\",\"L\",\"L\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\"]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>breaks<\\/th>\\n <th>wool<\\/th>\\n <th>tension<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":1},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Analysis Hypotheses There are two choices on how to present the hypotheses for a two-way ANOVA. You can use either the “Means Model” route, or the more advanced, but more mathematically correct, “Effects Model.” Within a given analysis, only one method need be stated. Both are shown here for educational purposes only. Means Model This analysis will use a two-way ANOVA with the factors of wool and tension and their interaction. Thus, we have three sets of hypotheses that need to be stated in order to understand the effect of each on the average number of warpbreaks. Does the type of wool affect the avereage number of breaks? Factor: wool with levels \\(A\\) and \\(B\\). \\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] Does the tension affect the average number of breaks? Factor: tension with levels \\(L\\), \\(M\\), and \\(H\\). \\[ H_0: \\mu_L = \\mu_M = \\mu_H = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one}\\ i\\in\\{1=L,2=M,3=H\\} \\] Does the effect of tension change for different types of wool? (Does the effect of wool change for different levels of tension?) In other words, is there an interaction between wool and tension? \\[ H_0: \\text{The effect of tension is the same for all types of wool.} \\] \\[ H_a: \\text{The effect of tension is not the same for all types of wool.} \\] A significance level of \\(\\alpha = 0.05\\) will be used for this study. Effects Model Applying a Two-way ANOVA with an interaction term to this study, we have the model for the number of warp breaks given by \\[ \\underbrace{Y_{ijk}}_\\text{Warp breaks} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] where \\(\\mu\\) is the grand mean, \\(\\alpha_i\\) is the wool factor with levels \\(A=1\\) and \\(B=2\\), \\(\\beta_j\\) is the tension factor with levels \\(L=1\\), \\(M=2\\), and \\(H=3\\), \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors which has \\(2\\times3=6\\) levels, and \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) is the normally distributed error term. This model allows us to ask the following questions and hypotheses. Does the type of wool affect the avereage number of breaks? Factor: wool with levels \\(A\\) and \\(B\\). \\[ H_0: \\alpha_A = \\alpha_B = 0 \\] \\[ H_a: \\alpha_i \\neq 0 \\ \\text{for at least one}\\ i\\in\\{1=A,2=B\\} \\] Does the tension affect the average number of breaks? Factor: tension with levels \\(L\\), \\(M\\), and \\(H\\). \\[ H_0: \\beta_L = \\beta_M = \\beta_H = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j\\in\\{1=L,2=M,3=H\\} \\] Does the effect of tension change for different types of wool? (Does the effect of wool change for different levels of tension?) In other words, is there an interaction between wool and tension? \\[ H_0: \\alpha\\beta_{ij} = 0 \\ \\text{for all } i,j \\] \\[ H_a: \\alpha\\beta_{ij} \\neq 0 \\ \\text{for at least one } i,j \\] A significance level of \\(\\alpha = 0.05\\) will be used for this study. Two-way ANOVA To perform the analysis we compute the following Two-way ANOVA. warp.aov <- aov(breaks ~ wool + tension + wool:tension, data=warpbreaks) summary(warp.aov) %>% pander() Analysis of Variance Model   Df Sum Sq Mean Sq F value Pr(>F) wool 1 450.7 450.7 3.765 0.05821 tension 2 2034 1017 8.498 0.0006926 wool:tension 2 1003 501.4 4.189 0.02104 Residuals 48 5745 119.7 NA NA The ANOVA table listed in the above output contains three p-values, one for each hypothesis test that was stated previously. The conclusions are that wool is not a significant factor \\((p=0.05821)\\), but tension does have a significant effect on the number of breaks \\((p=0.00069)\\), and the effect of tension seems to depend on the type of wool because the interaction term is also significant \\((p=0.02104)\\). Diagnostic Plots The appropriateness of the above ANOVA is somewhat questionable as demonstrated in the plots below. Notice that while the normality of the error terms appears to be satisfied (Normal Q-Q Plot on the right) the constant variance assumption is questionable (Residuals vs Fitted values pont on the left). This is because the spread of the four sets of points seems to get larger as the groups move forward. However, the change in variance is not substantial enough to discredit the ANOVA. The results of the test can be considered valid. par(mfrow=c(1,2)) plot(warp.aov, which=1:2, pch=16) Graphical Summaries & Conclusions The following graphics emphasize the results of each of the three hypothesis tests. Wool Type This first graphic is colored gray to emphasize that the wool factor is not significant. In other words, the average number of breaks does not seem to be effected by the type of wool used in the loom. However, if a choice must be made on the wool type, we could gamble on Type B being the better choice with an average of 25.26 breaks while Type A wool resulted in 31.04 breaks on average. We just don’t have statistical evidence to actually make the conclusion that this pattern would hold for future studies (p-value = 0.05821). xyplot xyplot(breaks ~ wool, data=warpbreaks, type=c(\"p\",\"a\"), main=\"You Can Use Either Type of Wool\", col='gray', xlab=\"Type of Wool\", ylab=\"Number of Warps that Broke\") ggplot ggplot(warpbreaks, aes(x=wool, y=breaks, group=1)) + geom_point(color=\"gray\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"You Can Use Either Type of Wool\", x=\"Type of Wool\", y=\"Number of Warps that Broke\") + theme_bw() warpbreaks %>% group_by(wool) %>% summarise(`Mean Warp Breaks`=mean(breaks)) %>% pander(caption=\"Mean Warp Breaks according to Wool Type\") Mean Warp Breaks according to Wool Type wool Mean Warp Breaks A 31.04 B 25.26 Tension Level This second graphic demonstrates that in general, the higher the tension the fewer number of breaks. This is somewhat counter-intuitive, but the result is significant and shows that the average breaks drops to 22 at the highest level of tension, but is as much as 36 at the lowest level of tension. Whether or not there is an advantage of High Tension over Medium Tension is unclear currently. Further testing would be required to reach a firm conclusion. xyplot xyplot(breaks ~ tension, data=warpbreaks, type=c(\"p\",\"a\"), main=\"Higher Tension Seems Best\", xlab=\"Tension Level (Low, Medium, High)\", ylab=\"Number of Warps that Broke\") ggplot ggplot(warpbreaks, aes(x=tension, y=breaks, group=1)) + geom_point(color=\"steelblue\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Tension Seems Best\", x=\"Tension Level (Low, Medium, High)\", y=\"Number of Warps that Broke\") + theme_bw() warpbreaks %>% group_by(tension) %>% summarise(`Mean Warp Breaks`=mean(breaks), .groups=\"drop\") %>% pander(caption=\"Mean Warp Breaks according to Tension Level\") Mean Warp Breaks according to Tension Level tension Mean Warp Breaks L 36.39 M 26.39 H 21.67 Tension Choices Depending on Wool Type As stated previously, the type of wool used is really up to the manufacturer. Neither wool type (A or B) shows significant advantage over the other. In our particular study, Type B was lower, but not significantly so. However, it turns out that the choice of Wool used does have a determining factor on which Tension level should be selected. As shown in the graph below, the Medium or High Tension levels are equally acceptable when using Type A Wool. But when using Type B Wool, it is recommended that only the High Tension level is used. The overall fewest breaks are likely achieved by the High Tension, Type B Wool situation, which results in only 18.78 breaks on average. However, if Type A Wool is desired, then either Medium or High Tension levels achieved similar results with 24 to 25 breaks on average. Both of these values came in lower than the Type B Wool on either Medium or Low Tension settings (28 to 29 breaks on average). xyplot xyplot(breaks ~ tension, data=warpbreaks, groups=wool, type=c(\"p\",\"a\"), main=\"Significance of the Interaction\", auto.key=list(corner=c(1,1))) ggplot ggplot(warpbreaks, aes(x=tension, y=breaks, group=wool, color=wool)) + geom_point() + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Tension Seems Best\", x=\"Tension Level (Low, Medium, High)\", y=\"Number of Warps that Broke\") + theme_bw() warpbreaks %>% group_by(wool, tension) %>% summarise(ave=mean(breaks), .groups=\"drop\") %>% spread(tension, ave) %>% pander(caption=\"Mean Warp Breaks according to Wool Type (A,B) and Tension Level (Low, Medium, High)\") Mean Warp Breaks according to Wool Type (A,B) and Tension Level (Low, Medium, High) wool L M H A 44.56 24 24.56 B 28.22 28.78 18.78 Final Thoughts In general, we recommend using higher tension levels rather than lower tension levels. While the type of wool used does not seem to matter, we wish to note that the best reduction in warp breakage seems to be through the Type B Wool–High Tension combination at 18.78 breaks on average. We suggest avoiding the Low Tension–Type A Wool combination as that results in the highest breakage of 44.56 breaks on average. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":" Two-way ANOVA: Final Beginning Algebra Grades ","url":"325Analyses/ANOVA/MyTwoWayANOVA.html","content":"Code Show All Code Hide All Code Two-way ANOVA: Final Beginning Algebra Grades library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) # Record your data from your own mini experiment in Excel. # Save the data as a .csv file in the Data folder of the Statistics-Notebook. # Read in the data BAFinalGrades <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/BeginningAlgebraFinalGrades.csv\") Background Beginning Algebra is a course at BYU-Idaho recommended for students “needing basic algebra before taking progressively higher math courses,” according to the BYU-Idaho Mathematics Course Page1. This course is available every semester within BYU-Idaho’s three-track system: Fall, Winter, and Spring. Students taking the course use Canvas, a web-based learning management system, to upload and complete assignments. Canvas tracks various aspects of student performance, including assignments, grades, attendance, and participation in the course. For this study, we collected final grades from Beginning Algebra students through Canvas across all three semesters. We also recorded each student’s participation level as tracked by Canvas. According to the Canvas Course Analytics for Faculty 2, Canvas measures participation based on student interaction within the Beginning Algebra course module. It monitors specific actions that influence participation scores, such as the frequency of on-time or late work submissions. Based on these factors, Canvas assigns each student a participation rating of Low, Moderate, or High. With this data, we will conduct a Two-Way ANOVA test to determine which semester and participation produces students with the highest final grades. A table listing the data of each student is listed below. Each row represents a single student. Then, each column shows the student’s participation, final grade, and the semester they took Beginning Algebra. datatable(BAFinalGrades, options=list(lengthMenu = c(3,10,30))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"202\",\"203\",\"204\"],[\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Spring\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Fall\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\",\"Winter\"],[\"Moderate\",\"Low\",\"Moderate\",\"High\",\"Moderate\",\"High\",\"High\",\"High\",\"Moderate\",\"High\",\"Low\",\"Moderate\",\"High\",\"Moderate\",\"Moderate\",\"Moderate\",\"High\",\"Low\",\"Low\",\"Moderate\",\"Moderate\",\"Moderate\",\"Low\",\"Moderate\",\"High\",\"High\",\"Moderate\",\"High\",\"Moderate\",\"Moderate\",\"Moderate\",\"Moderate\",\"Low\",\"Low\",\"High\",\"Low\",\"Moderate\",\"Moderate\",\"High\",\"Low\",\"High\",\"Moderate\",\"Moderate\",\"High\",\"Low\",\"Moderate\",\"High\",\"Moderate\",\"Moderate\",\"High\",\"High\",\"Moderate\",\"Low\",\"Moderate\",\"High\",\"Moderate\",\"Moderate\",\"Moderate\",\"Low\",\"Low\",\"Moderate\",\"Low\",\"Moderate\",\"Low\",\"Moderate\",\"Moderate\",\"High\",\"Moderate\",\"High\",\"Low\",\"Moderate\",\"Low\",\"Moderate\",\"Low\",\"Moderate\",\"Low\",\"Moderate\",\"Low\",\"Moderate\",\"High\",\"High\",\"Moderate\",\"Moderate\",\"Moderate\",\"Moderate\",\"Moderate\",\"Low\",\"Moderate\",\"Moderate\",\"Moderate\",\"Moderate\",\"High\",\"Low\",\"Low\",\"High\",\"Moderate\",\"Moderate\",\"Moderate\",\"High\",\"Low\",\"High\",\"Moderate\",\"High\",\"Moderate\",\"Moderate\",\"High\",\"Low\",\"Moderate\",\"Moderate\",\"Moderate\",\"High\",\"Moderate\",\"Moderate\",\"High\",\"High\",\"Moderate\",\"Moderate\",\"Moderate\",\"Low\",\"High\",\"Low\",\"Moderate\",\"Moderate\",\"High\",\"Moderate\",\"High\",\"Low\",\"Low\",\"Moderate\",\"Moderate\",\"High\",\"Moderate\",\"Moderate\",\"Moderate\",\"Moderate\",\"High\",\"Moderate\",\"Low\",\"High\",\"Moderate\",\"Moderate\",\"Moderate\",\"High\",\"High\",\"Moderate\",\"High\",\"Moderate\",\"High\",\"Low\",\"High\",\"Moderate\",\"High\",\"High\",\"Moderate\",\"Moderate\",\"High\",\"Moderate\",\"Moderate\",\"Moderate\",\"Moderate\",\"Moderate\",\"Moderate\",\"High\",\"Low\",\"Moderate\",\"High\",\"Moderate\",\"High\",\"Low\",\"Moderate\",\"Moderate\",\"High\",\"Moderate\",\"Moderate\",\"Low\",\"Moderate\",\"Moderate\",\"Low\",\"High\",\"High\",\"Moderate\",\"Moderate\",\"Moderate\",\"Low\",\"Moderate\",\"Moderate\",\"High\",\"Moderate\",\"Low\",\"High\",\"Moderate\",\"Low\",\"Moderate\",\"Moderate\",\"Moderate\",\"Low\",\"Moderate\",\"Low\",\"High\",\"Low\",\"Moderate\",\"Low\",\"Low\",\"Moderate\"],[89.11,86.31,84.25,86.59999999999999,87.84999999999999,85.66,94.81999999999999,96.59,88.65000000000001,96.54000000000001,75.89,93.48,102.95,83.5,91.70999999999999,97.55,98.76000000000001,68.81,66.56999999999999,87.97,86.5,97.81999999999999,78.36,64.8,95.90000000000001,97.39,94.09,97.3,89.48999999999999,95.44,96.34,90.72,91.15000000000001,53.95,95.48999999999999,85.91,69.86,93.98,95.67,63.59,86.02,89.33,101.64,96.45,51,101.11,99.45999999999999,83.27,99.76000000000001,102.9,80.15000000000001,90.70999999999999,48.81,103.54,95.7,92.75,82.86,95.98,83.61,48.72,75.25,87.7,98.09999999999999,82.54000000000001,101.09,93.42,93.16,94.94,97.84,87.69,97.37,56.02,79.65000000000001,79.70999999999999,78.27,16.89,98.5,69.55,77.92,98.08,101.52,101.6,86.36,85.93000000000001,90.7,84.3,49.91,92.2,91.45,86.26000000000001,75.37,95.84,52.25,48.71,94.84,87.69,72.63,93.13,92.83,38.82,77.27,86.84,101.77,98.16,82.25,91.62,75.06,89.8,92.28,84.29000000000001,97.26000000000001,98.17,100.58,93.81,96.64,84.72,92.72,82.84999999999999,82.48,104.34,42.24,102.42,103.06,86.37,48.36,100.97,63.96,71.03,80.79000000000001,84.87,99.2,96.09,88.39,87.29000000000001,72.09999999999999,94.53,81.76000000000001,68.87,93.40000000000001,96.29000000000001,95.56,90.25,87.14,90.75,89.7,79.58,84.76000000000001,87.11,83.53,91.48,74.42,99.92,100.72,81.8,93.59999999999999,97,83.41,90.88,97.73,70.72,94.42,93.34,95.56,16.13,95.58,85.7,86.20999999999999,94.75,68.26000000000001,94.09999999999999,94.45999999999999,98.97,97.13,95.2,76.45,96.52,95.23999999999999,43.81,102.84,103.02,88.92,89.95,95.87,82.64,73.52,78.2,100.33,76.69,47.3,97.20999999999999,96.98999999999999,62.85,85.37,91.91,83.05,10.11,83.62,49.99,94.34999999999999,78.75,88.75,57.58,49.52,82.06999999999999]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>Semester<\\/th>\\n <th>Participation<\\/th>\\n <th>FinalGrade<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"lengthMenu\":[3,10,30],\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":3},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Semester\",\"targets\":1},{\"name\":\"Participation\",\"targets\":2},{\"name\":\"FinalGrade\",\"targets\":3}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Hypothesis By using a Two-Way ANOVA test, we will analyze how the two variables, semester and participation, effect the outcome of final grades. The model of this Two-Way ANOVA test will be shown with the following equation: \\[ \\underbrace{Y_{ijk}}_\\text{Final Grades} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] To further clarify the use of this equation, here is what each variable indicates: μ : the grand mean (which is the average Y-value ignoring all information contained in the factors) \\(\\alpha_i\\) : the first factor, Semesters, with levels Fall, Winter, and Spring \\(\\beta_i\\) : the second factor, Participation, with levels Low, Moderate, and High \\(\\alpha\\beta_{ij}\\) : the interaction of the two factors which has 3×3=9 levels \\(\\epsilon_{ijk}\\sim N(0,\\sigma^2)\\) = the normally distributed error term With our model shown and explained, we can now ask the following questions through these 3 hypotheses: Does the amount a student participates in the course affect the average final Beginning Algebra grades? Factor: Participation with levels Low (L), Moderate (M), and High (H) \\[ H_0:α_L =α_M=α_H = 0 \\] \\[ Ha:α_i ≠\\text{ 0 for at least one i ∈ {1 = L, 2 = M, 3 = H} } \\] Does the semester in which a student takes Beginning Algebra affect their average final grade? Factor: Semester wth levels Fall (F), Winter (W), and Spring (S) \\[ H_0:β_F = β_W = β_S =0 \\] \\[ H_a : β_j ≠ \\text{0 for at least one i ∈ { 1 = F, 2 = W, 3 = S}} \\] Does a student’s participation level vary depending on the semester? (Alternatively, does the semester influence a student’s participation?) Is there an interaction between participation and a specific semester? \\[H0:αβ_{ij} = \\text{0 for all i, j}\\] \\[ Ha: αβ_{ij} \\neq \\text{0 at least one i, j} \\] We will also use the following significance level throughout this study: \\[ \\alpha = 0.05 \\] Analysis Two-Way ANOVA The table below shows the Two-Way ANOVA test on the Beginning Algebra Final Grades: BAanova <- aov(FinalGrade ~ Semester + Participation + Semester:Participation, data=BAFinalGrades) summary(BAanova) %>% pander() Analysis of Variance Model   Df Sum Sq Mean Sq F value Pr(>F) Semester 2 825.4 412.7 3.232 0.04161 Participation 2 28142 14071 110.2 9.544e-33 Semester:Participation 4 1611 402.8 3.154 0.01533 Residuals 195 24901 127.7 NA NA With the three p-values that are computed by our ANOVA test, we can answer the hypotheses that we stated at the beginning of the study. \\[ \\text{Semester p-value} = 0.04161 < \\alpha \\] \\[ \\text{Participation p-value} = 9.544e^{-33} < \\alpha \\] \\[ \\text{Interaction p-value} = 0.01533 < \\alpha \\] Comparing each p-value to the significance level, both factors, Semester and Participation, have a significant effect on a student’s final grade in Beginning Algebra as they both produce small p-values. The Semester’s p-value is barely significant, being only 0.01 points below the level of significance, while the Participation’s p-value is highly significant, falling even farther below. Moreover, the specific semester appears to significantly influence student participation in the course, as evidenced by the p-value for the interaction between these two factors being considerably lower than the established significance level. To better illustrate the impact of each factor, we will now create graphical representations and construct a numerical summary to demonstrate how these factors affect final grades as well as check the fulfillment of the ANOVA requirements. Graphical & Numerical Summaries To visually support the findings of our Two-way ANOVA, we will present the results of each hypothesis through the following graphics. Additionally, a table showing the specific average final grade values are shown directly after each graphic. Continue through each tab to see the individual factors and their interaction BYU-Idaho Semesters The first factor, Semester, is shown to be slightly significant according to the Two-Way ANOVA test with a p-value of 0.04161. The following graph displays the higher average of final grades appear to be seen in the Spring semester, followed by the Winter and then Fall with both displaying nearly the same lower averages. Therefore, if a student has to decide which semester they should schedule their Beginning Algebra course, the desired semester would be the Spring semester. BAFinalGrades$Semester <- factor(BAFinalGrades$Semester, levels = c(\"Fall\",\"Winter\",\"Spring\")) ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=1)) + geom_point(color=\"firebrick1\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Spring Semester Seems to Have Higher Final Grades\", x=\"Semester\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades %>% group_by(Semester) %>% summarise(`Mean Final Grades`=mean(FinalGrade), .groups=\"drop\") %>% pander(caption=\"Mean Final Grades By Semester\") Mean Final Grades By Semester Semester Mean Final Grades Fall 83.38 Winter 83.69 Spring 87.75 Canvas Participation The second factor, Participation, proves to be extremely significant according to the Two-Way ANOVA test’s p-value of 9.544e-33. Our graph clearly shows that the highest final grade averages correlate with high participation. Consequently, students aiming for better grades in Beginning Algebra—or any class—should consistently complete and submit their assignments on time. BAFinalGrades$Participation <- factor(BAFinalGrades$Participation, levels = c(\"Low\",\"Moderate\",\"High\")) ggplot(BAFinalGrades, aes(x=Participation, y=FinalGrade, group=1)) + geom_point(color=\"darkred\") + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Higher Participation Seems to Lead to Higher Grades\", x=\"Participation\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades %>% group_by(Participation) %>% summarise(`Mean Final Grades`=mean(FinalGrade), .groups=\"drop\") %>% pander(caption=\"Mean Final Grades by Participation\") Mean Final Grades by Participation Participation Mean Final Grades Low 62.86 Moderate 88.88 High 94.85 BYU-Idaho Semesters Based on Canvas Participation According to the previous graphics and Two-Way ANOVA test, both factors, Semester and Participation, are significant with their p-values displaying below 0.05. While one factor was more significant than the other, the interaction between the two factors is highly significant with a p-value of 0.01533. The following graphic illustrates that high participation in the fall semester yields the highest average final grade in Beginning Algebra, at 95.31. Interestingly, the Spring semester produces the highest final grade averages for Low and Moderate participation levels, with the Low participation average for Spring being 72.54. Grades resulting in the 70s range is considered to be a passing grade resulting in C+, C, or C-. In contrast, the other two semesters’ low participation final grade averages range in the 50s, which is considered to be a failing grade resulting in the letter grade F. This suggests that the Spring semester results in more students passing Beginning Algebra compared to those in Fall and Winter. ggplot(BAFinalGrades, aes(x=Semester, y=FinalGrade, group=Participation, color=Participation)) + geom_point() + stat_summary(fun=\"mean\", geom=\"line\") + labs(title=\"Beginning Algebra Seems Better during the Spring Semester\", x=\"BYU-Idaho Semesters\", y=\"Beginning Algebra Final Grades\") + theme_minimal() BAFinalGrades %>% group_by(Participation, Semester) %>% summarise(ave=mean(FinalGrade), .groups=\"drop\") %>% spread(Participation, ave) %>% pander(caption=\"Mean Final Grades by Semester and Participation\") Mean Final Grades by Semester and Participation Semester Low Moderate High Fall 58.25 87.29 95.31 Winter 55.92 88.75 94.5 Spring 72.54 90.79 94.77 Diagnostic Plots Before we trust the results of our Two-Way ANOVA test, we will first check to see if the requirements for our ANOVA test are fulfilled. There two requirements that we must check when using an ANOVA test: Constant (equal) Variance Normal Error Terms (Residuals) To check each requirement, we will make a graph for each requirement as follows: par(mfrow=c(1,2)) plot(BAanova, which=1:2, pch=16) The residual plot shows that the constant (equal) variance assumption raises some concern as the points within each group have varying spread. The spread of each group seems to gets closer and closer as we go from left to right as well as some straying point from the more clustered groups. Thus, showing the variance of the residuals are not constant and do not accurately show a relationship between each variable. For the Q-Q plot, it shows that the residuals of the data is not quite normally distributed. There are visible points at the lower and upper ends of the graph deviating away form the diagonal line, which represents a normal distribution. Thus, showing us that the residuals of the data are not normal distributed. Although both graphs indicate that the data fails to meet both requirements, this primarily means our findings may be less reliable due to the abnormalities in the data. As a result, we should interpret the results found within the ANOVA test, graphical summaries, and numerical summaries cautiously. Interpretation Overall, both the semester in which a student takes Beginning Algebra and their level of participation in the course affect their final grade. The main recommendation would be to take Beginning Algebra in the Spring semester, as students are more likely to pass the course regardless of their participation level with a average final grade range of 72.54 (low participation) - 94.77 (high participation). While other semesters also yield high final grade averages, students risk failing if they don’t maintain moderate or high participation throughout as the grades of students with a low participation average between a final grade of 55.92-58.25. It’s important to note that our data failed to meet both ANOVA requirements, so these findings and interpretations should be viewed with caution. To statistically confirm these results, further studies will be necessary. Sources: Data Collection Huge thank you to Carol Ashcraft, my lovely boss, for taking the time to go through her Canvas page for me to collect all those final grades and participation scores from her many semesters of teaching Beginning Algebra classes! Formatting Inspiration Breaks in Warps while Weaving: Two-way ANOVA BYU-Idaho Mathematics Course Catalog↩︎ Canvas Course Analytics for Faculty↩︎ // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdo"},{"title":"Hair Eye Color / Chi-squared Test","url":"325Analyses/Chi Squared Tests/Examples/HairEyeColorChiSquaredTest.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } Hair Eye Color / Chi-squared Test Background The hair and eye color as well as gender was recorded for 592 statistics students from the University of Deleware. Questions & Hypotheses There are a few different questions that could be asked to these data. To show several examples of how a chi-squared test could be implemented, each of the following three questions will be answered with a separate chi-squared test. Any one of these questions would be sufficient for a single analysis. Is hair color associated with eye color regardless of gender? \\[ H_{01}:\\ \\text{Hair color and eye color are not associated.} \\] \\[ H_{a1}:\\ \\text{Hair color and eye color are associated.} \\] Is hair color associated with gender? \\[ H_{02}:\\ \\text{Hair color and gender are not associated.} \\] \\[ H_{a2}:\\ \\text{Hair color and gender are associated.} \\] Is eye color associated with gender? \\[ H_{03}:\\ \\text{Eye color and gender are not associated.} \\] \\[ H_{a3}:\\ \\text{Eye color and gender are associated.} \\] Data Analysis # Test H_{01}: HEC1 <- HairEyeColor[,,\"Male\"] + HairEyeColor[,,\"Female\"] chi.HEC1 <- chisq.test(HEC1) chi.HEC1 ## ## Pearson's Chi-squared test ## ## data: HEC1 ## X-squared = 138.3, df = 9, p-value < 2.2e-16 chi.HEC1$expected > 5 ## Eye ## Hair Brown Blue Hazel Green ## Black TRUE TRUE TRUE TRUE ## Brown TRUE TRUE TRUE TRUE ## Red TRUE TRUE TRUE TRUE ## Blond TRUE TRUE TRUE TRUE All expected counts are greater than 5, so the requirements are met. (If this failed, it will still be appropriate as long as all expected counts are at least 1 and the average expected count is at least 5.) # Test H_{02}: MH <- apply(HairEyeColor[,,\"Male\"],1,sum) FH <- apply(HairEyeColor[,,\"Female\"],1,sum) HEC2 <- cbind(MH,FH) chi.HEC2 <- chisq.test(HEC2) chi.HEC2 ## ## Pearson's Chi-squared test ## ## data: HEC2 ## X-squared = 7.994, df = 3, p-value = 0.04613 chi.HEC2$expected > 5 ## MH FH ## Black TRUE TRUE ## Brown TRUE TRUE ## Red TRUE TRUE ## Blond TRUE TRUE All expected counts are greater than 5, so the requirements are met. # Test H_{03}: ME <- apply(HairEyeColor[,,\"Male\"],2,sum) FE <- apply(HairEyeColor[,,\"Female\"],2,sum) HEC3 <- cbind(ME,FE) chi.HEC3 <- chisq.test(HEC3) chi.HEC3 ## ## Pearson's Chi-squared test ## ## data: HEC3 ## X-squared = 1.53, df = 3, p-value = 0.6754 chi.HEC3$expected > 5 ## ME FE ## Brown TRUE TRUE ## Blue TRUE TRUE ## Hazel TRUE TRUE ## Green TRUE TRUE All expected counts are greater than 5, so the requirements are met. Graphics barplot(HEC1, beside=TRUE, legend.text=TRUE, xlab=\"Eye Color\", main=\"Eye Color vs. Hair Color\") The Chi-squared test above showed a significant relationship exists between Hair Color and Eye Color \\((p<0.0001)\\). This is seen in the above plot by noting that the pattern of heights on the bars is changed across the different colors of hair. For Brown eyes, the most common hair color is brown, followed by black, then red, then blond. For Blue eyes, the most common hair color is blond, then brown with black and red being much more rare. For Hazel eyes, most people also seem to have brown hair with black, then red, then blond ranking as much less common. People with green eyes also have brown hair as the most common result, with blond, red, and black following behind. If there was no relationship between hair color and eye color, then the pattern of hair color would have remained much the same across the levels of eye color. barplot(HEC2, beside=TRUE, legend.text=TRUE, xlab=\"Gender\", main=\"Gender vs. Hair Color\", names.arg=c(\"Male\",\"Female\")) The relationship between Gender and Hair Color is stated to be significant according to the chi-squared test \\((p=0.04613)\\). As can be seen in the above plot, the pattern of brown hair being the most common and red hair being the least common is consistent across both genders. However, the reversal of the pattern is seen in that for males, black hair is more common than blond while for women, blond hair is more common than black. barplot(HEC3, beside=TRUE, legend.text=TRUE, xlab=\"Gender\", main=\"Gender vs. Eye Color\", names.arg=c(\"Male\",\"Female\")) That there is no relationship between Gender and Eye Color \\((p=0.6754)\\) is seen by the fact that the general pattern of eye color is relatively consistent across genders. Brown and blue are the most common colors and hazel and green are the least common. While the sample showed some evidence that perhaps brown is more common than blue in women and blue is more common than brown in males, there is insufficient evidence to make such a conclusion about the population. Thus we conclude that eye color is not associated with gender. It is interesting to note that these plots do support the conclusion that brown eyes and blue eyes are much more common that hazel and green in the general population. Interpretation (See captions below each plot.) Note If all three tests were actually performed simultaneously on the same data, then only the first test would be considered significant because each test would need to be tested at the \\(\\alpha=0.05/3 \\approx 0.0167\\) level to account for the multiplicity of tests. The three tests performed here were simply to give three different examples in a concise way. // add bootstrap table styles to pandoc tables $(document).ready(function () { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Chi-Squared Analysis","url":"325Analyses/Chi Squared Tests/Examples/Math325StudentRatings.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Chi-Squared Analysis Background Students in Math 325: Intermediate Statistics anonymously answered a survey to indicate how they feel about certain aspects of the course. They rate aspects on a scale from 0-3, 0 indicating they feel they have lost learning from an aspect and 3 indicating they feel they have learned from the aspect. We will look at these rating as Unsatisfied (0 to 1), Satisfied (1 to 2), and Pleased (2 to 3). The two aspects we will analyze are Critiquing others’ work in the course and Being Critiqued by others in the course. Hypothesis Our question for our data is: Do students who like/dislike critiquing have the same feelings towards being critiqied? In other words, are the satisfaction levels of critiquing other students’ work and being critiqued by other students associated? Our null and alternative hypothesis are as follows: \\[ H_0: \\text{The Critiquing satisfaction and Being Critiqued satisfaction are independent}. \\] \\[ H_a: \\text{The Critiquing satisfaction and Being Critiqued satisfaction are associated (not independent)}. \\] We will be using a level of significance \\(\\alpha=0.05\\). Analysis First, let’s look at a table of counts for Critquing (rows) and Being Critiqued (columns). We do see that the some of the smallest counts are for being pleased with one thing and unsatisfied with the other. If students are pleased with Being Critiqued, they are not likely to be unstatisfied with Critiquing with a count of 3. Also, some of the largest counts (84 and 82) are matched on levels of satisfaction for Critiquing and Being Critiqued. So, we should expect to have some association with Critiquing and Being Critiqued. qs2<-qs qs2$Critiquing<-cut(qs2$Critiquing, c(-1,1.5,2.5,3),c(\"Unsatisfied\",\"Satisfied\",\"Pleased\"),ordered_result = TRUE) qs2$BeingCritiqued<-cut(qs2$BeingCritiqued, c(-1,1.5,2.5,3),c(\"Unsatisfied\",\"Satisfied\",\"Pleased\"),ordered_result = TRUE) survtable<-table(qs2$Critiquing,qs2$BeingCritiqued) pander(survtable)   Unsatisfied Satisfied Pleased Unsatisfied 25 12 3 Satisfied 30 84 46 Pleased 18 53 82 We see our barplot shows some of our conjectures are strong. However, we must run a test to get a \\(p\\)-value to determine if our conjectures are correct. barplot(survtable, beside=TRUE, col=c(\"springgreen\",\"springgreen2\",\"springgreen3\"),legend.text=TRUE,args.legend = list(x=\"topleft\",bty=\"n\",title=\"Critiquing\"),xlab=\"Being Critiqued\",main=\"Student Satisfaction \\n Math 325: Intermediate Statistics\") Now, we will run a Chi-Squared Test to determine if we reject our null hypothesis and conclude that the satisfaction levels of Critiquing and Being Critiqued are associated. We will also look at our expected counts to determine if our Chi-Squared Test requirements are met. qs2Chi<-chisq.test(survtable) pander(qs2Chi) Pearson’s Chi-squared test: survtable Test statistic df P value 71.17 4 1.287e-14 * * * pander(qs2Chi$expected)   Unsatisfied Satisfied Pleased Unsatisfied 8.272 16.88 14.84 Satisfied 33.09 67.54 59.38 Pleased 31.64 64.58 56.78 All expected counts are greater than 5, so the requirements are met. Our test results show a \\(p\\)-value of \\(p=1.287e-14\\), which is much smaller than our level of significance \\(\\alpha=0.05\\), so we reject our null hypothesis and conclude that the satisfaction levels of Critiquing and Being Critiqued are associated. Let us look at our residuals to see how much our observed counts differ from the expected counts if our null hypothesis were true. pander(qs2Chi$residuals)   Unsatisfied Satisfied Pleased Unsatisfied 5.816 -1.189 -3.074 Satisfied -0.5368 2.003 -1.736 Pleased -2.425 -1.441 3.347 We see the greatest difference between what was observed and what would be expected is where students are unsatisfied with both aspects of Critiquing and Being Critiqued, with a difference of 5.816 more counts. This is probably what contributed most to our test statistic and resulting \\(p\\)-value. Interpretation We can interpret how our alternative hypothesis applys to the students in Math 325. We see that students who are satisfied/unsatisfied with one aspect (Critiquing and Being Critiqued) will most likely feel the same way about the other aspect. Maybe this is because the students who are unsatisfied find both aspects of the course to be time consuming and unaffective. Those who are satisfied, and likely satisfied with both, may find these aspects helpful and a learning experience that is worth the time. Another survey could be conducted to see why students are satisfied/unsatisfied, but for now, we can see that they will have similar satisfaction levels for Critiquing others’ work and Being Critiqued by others. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"ConsultingChi-squared.utf8","url":"325Analyses/Chi Squared Tests/Examples/movies.html","content":"Code Show All Code Hide All Code Decade of Movies Chi-Squared Test Background It’s fair to say that through the last decade as most people prepare to see a feature film, either at home or at the local theater, they often consult an online movie rating source to find out if the movie in question is good or not. As they scour through the various websites and blogs they generally come across, and enter, the IMDb website. IMDb which started in the 90’s has accumulated and quantified almost every bit of information available on movies and tv shows from the past century. Through time it’s become a reputable source for all entertainment information, and continues to be a standard for free information sharing for the public to access. Interestingly what I personally use IMDb for the most, is their ratings on movies and shows. I search and read what these reviewers think about these movies and shows so that I know what to expect. I’m not alone in this. Because the team at IMDb has built and extensive list and data set of most movies made in the past century, questions can answered using the data recorded for each movie. In this analysis the IMDb data set will be used to see of the Age rating of each movie and the decade that each movie was made are associated or not. Hypotheses Question Does the total number of each Age rating (all, 7+, 13+, 16+, 18+) in a decade change in the past three Decades? Or, in other words, has the entertainment industry pumped out more or less movies of a certain age rating (associated) or are they the same decade to decade? (not associated) Hypotheses \\[ H_0: \\text{The movie Age rating and the movie Decade are independent of one another.} \\] \\[ H_0: \\text{The movie Age rating and the movie Decade are associated.} \\] \\[ \\text{level of significance: } a = .05 \\] Table Totals from each group Below are the total counts of each Age count per Decade. The decades in question are the past three, the nineties, two-thousands, and the most recent decade the twenty-tens. Looking at the table it’s apparent that there were a larger total sum of movies made in the past decade than the other two, so the question will become how much have they all changed? This test will look into the difference between these observed counts and the expected counts for each cell. #Filter area decades <- filter(mov, Year >= 1990) decades <- decades %>% drop_na(Age) decades <- decades %>% mutate( Age = factor(Age, levels= c(\"all\", \"7+\", \"13+\", \"16+\", \"18+\"), ordered= TRUE), Decade = case_when( Year >= 2010 ~ 'Twenty-Tens', Year >= 2000 & Year < 2010 ~ 'Two-Thousands', Year >= 1990 & Year < 2000 ~ 'Nineties' ), Decade = factor( Decade, levels= c('Nineties', 'Two-Thousands', 'Twenty-Tens')), ordered= TRUE ) decades <- table(decades$Age , decades$Decade) pander(decades)   Nineties Two-Thousands Twenty-Tens all 83 225 305 7+ 124 275 724 13+ 96 303 768 16+ 7 37 272 18+ 340 804 1770 Graph In this bar plot of the data table the trends are much more obvious. From a glance, there seems to be double the number of each group each decade that has passed in most groups. The exception is that all movies and 16+ movies seem to avoid this doubling trend. This may be due to those ratings being less common than the others. But a Chi-Squared test would support these findings statistically. barplot(decades, col= brewer.pal(n=5, name= 'RdGy'),beside=TRUE, legend.text=TRUE, main = \"Age Ratings by Decade\", xlab = \"Decade\", ylab = \"Count of Movies\", args.legend=list(x = \"topleft\", bty=\"n\")) pander(decades, caption=\"Totals\") Totals   Nineties Two-Thousands Twenty-Tens all 83 225 305 7+ 124 275 724 13+ 96 303 768 16+ 7 37 272 18+ 340 804 1770 \\(X^2\\) Test Statistical Test To try and reject the \\(H_0\\) of our test we need to determine a \\(X^2\\) test statistic and an associated p-value. The test on Decade and Age rating is as follows: KingCHI <- chisq.test(decades) pander(KingCHI) Pearson’s Chi-squared test: decades Test statistic df P value 136.1 8 1.499e-25 * * * With a p-value so minuscule there is no doubt that the columns and rows are associated, but to what degree and what does this mean? Interpretation Expected Counts When considering the fit and the interpretation of the test, the expected count for each column and row must be greater than five, and while looking below, they all are. pander(KingCHI$expected)   Nineties Two-Thousands Twenty-Tens all 64.97 164.3 383.7 7+ 119 301 703 13+ 123.7 312.8 730.5 16+ 33.49 84.71 197.8 18+ 308.8 781.1 1824 Observed Counts This is again the observed counts for every group. Some of which are close to one another and others pretty distant. pander(decades)   Nineties Two-Thousands Twenty-Tens all 83 225 305 7+ 124 275 724 13+ 96 303 768 16+ 7 37 272 18+ 340 804 1770 Residuals Knowing that the p-value is as low as it is it would behoove us to look at the residuals to see how far the expected counts were from the observed counts. This is the distance of each value of the above two values. The higher further the value is from \\(0\\) the the larger the difference. These Pearson residuals are calculated by finding the (observed - expected)/sqrt(expected) of each cell. For reference this looks like: \\(\\frac{(O_i-E_i)}{\\sqrt{E_i}}\\). Out of all the Pearson residuals below there are some that are higher than expected, and quite drastically so. The all age rating category has some of highest residuals. With values ranging between -4.018 and 4.734 it’s obvious that the count changed drastically from decade to decade. The other age rating that changed drastically with a range of -4.578 and 5.276 was the 16+ category. These two groups were assumed to make the test statistic as high as it is because of its residual variability. The other three groups however don’t have much variability, thay range from around -2.578 and 1.773. Which is significantly less than the aforementioned groups. pander(KingCHI$residuals)   Nineties Two-Thousands Twenty-Tens all 2.237 4.734 -4.018 7+ 0.4565 -1.5 0.7939 13+ -2.489 -0.5554 1.388 16+ -4.578 -5.183 5.276 18+ 1.773 0.8186 -1.265 The sum of the squares of all the residual differences was enough to produce a p-value of 1.499e-25 which is a massive indicator that the groups are associated. As the results currently stand we can reject the \\(H_0\\) and claim that Age and Decade are associated. This means that as decades pass the number of movies with each Age rating also changes. This means that the media industry is changing and will always change as far as it’s standards are concerned. Trends do change though, although the number Age ratings fluctuated from decade to decade there isn’t a complete pattern that can predict what would happen in decades to come. All that is known is the count changes. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open') }); }); $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":" Does Your View on Disciplining Children Make You a Potential Murderer? ","url":"325Analyses/Chi Squared Tests/MyChiSquaredTest.html","content":"Code Show All Code Hide All Code Does Your View on Disciplining Children Make You a Potential Murderer? library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) Background The GSS Data Explorer Page is a publicly available national source displaying the surveyed results of American Society1. Within this data, I was interested in the SPANKING and POLMURDR columns. The SPANKING column questioned individuals if they favored spanking to discipline their child, the options being : Strongly Agree (1), Agree (2), Disagree (3), Strongly Disagree (4). The POLMURDR column questioned individuals if they have ever been questioned as a potential murder suspect, their options being: Yes(1) or No(2). In this study, we will be using the previous columns provided by GSS Data Explorer to answer the question: Does your views on disciplining children make you a potential murderer? To determine this, we will be using a Chi-Squared Test to determine if the preference to discipline your child is associated or not associated with being suspected as a potential murder. The data for this study is shown below: gss2021 <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/gss2021.csv\") childmurdr <- table(gss2021$SPANKING,gss2021$POLMURDR) rownames(childmurdr) <- c(\"Strongly Agree\", \"Agree\", \"Disagree\", \"Strongly Disagree\") #these replace the 1, 2, and 3 colnames(childmurdr) <- c(\"Yes\", \"No\") #these replace the 1 and 2 chillM <- gss2021 %>% select(SPANKING, POLMURDR) datatable(chillM, options = list(c(3,10,30))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"223\",\"224\",\"225\",\"226\",\"227\",\"228\",\"229\",\"230\",\"231\",\"232\",\"233\",\"234\",\"235\",\"236\",\"237\",\"238\",\"239\",\"240\",\"241\",\"242\",\"243\",\"244\",\"245\",\"246\",\"247\",\"248\",\"249\",\"250\",\"251\",\"252\",\"253\",\"254\",\"255\",\"256\",\"257\",\"258\",\"259\",\"260\",\"261\",\"262\",\"263\",\"264\",\"265\",\"266\",\"267\",\"268\",\"269\",\"270\",\"271\",\"272\",\"273\",\"274\",\"275\",\"276\",\"277\",\"278\",\"279\",\"280\",\"281\",\"282\",\"283\",\"284\",\"285\",\"286\",\"287\",\"288\",\"289\",\"290\",\"291\",\"292\",\"293\",\"294\",\"295\",\"296\",\"297\",\"298\",\"299\",\"300\",\"301\",\"302\",\"303\",\"304\",\"305\",\"306\",\"307\",\"308\",\"309\",\"310\",\"311\",\"312\",\"313\",\"314\",\"315\",\"316\",\"317\",\"318\",\"319\",\"320\",\"321\",\"322\",\"323\",\"324\",\"325\",\"326\",\"327\",\"328\",\"329\",\"330\",\"331\",\"332\",\"333\",\"334\",\"335\",\"336\",\"337\",\"338\",\"339\",\"340\",\"341\",\"342\",\"343\",\"344\",\"345\",\"346\",\"347\",\"348\",\"349\",\"350\",\"351\",\"352\",\"353\",\"354\",\"355\",\"356\",\"357\",\"358\",\"359\",\"360\",\"361\",\"362\",\"363\",\"364\",\"365\",\"366\",\"367\",\"368\",\"369\",\"370\",\"371\",\"372\",\"373\",\"374\",\"375\",\"376\",\"377\",\"378\",\"379\",\"380\",\"381\",\"382\",\"383\",\"384\",\"385\",\"386\",\"387\",\"388\",\"389\",\"390\",\"391\",\"392\",\"393\",\"394\",\"395\",\"396\",\"397\",\"398\",\"399\",\"400\",\"401\",\"402\",\"403\",\"404\",\"405\",\"406\",\"407\",\"408\",\"409\",\"410\",\"411\",\"412\",\"413\",\"414\",\"415\",\"416\",\"417\",\"418\",\"419\",\"420\",\"421\",\"422\",\"423\",\"424\",\"425\",\"426\",\"427\",\"428\",\"429\",\"430\",\"431\",\"432\",\"433\",\"434\",\"435\",\"436\",\"437\",\"438\",\"439\",\"440\",\"441\",\"442\",\"443\",\"444\",\"445\",\"446\",\"447\",\"448\",\"449\",\"450\",\"451\",\"452\",\"453\",\"454\",\"455\",\"456\",\"457\",\"458\",\"459\",\"460\",\"461\",\"462\",\"463\",\"464\",\"465\",\"466\",\"467\",\"468\",\"469\",\"470\",\"471\",\"472\",\"473\",\"474\",\"475\",\"476\",\"477\",\"478\",\"479\",\"480\",\"481\",\"482\",\"483\",\"484\",\"485\",\"486\",\"487\",\"488\",\"489\",\"490\",\"491\",\"492\",\"493\",\"494\",\"495\",\"496\",\"497\",\"498\",\"499\",\"500\",\"501\",\"502\",\"503\",\"504\",\"505\",\"506\",\"507\",\"508\",\"509\",\"510\",\"511\",\"512\",\"513\",\"514\",\"515\",\"516\",\"517\",\"518\",\"519\",\"520\",\"521\",\"522\",\"523\",\"524\",\"525\",\"526\",\"527\",\"528\",\"529\",\"530\",\"531\",\"532\",\"533\",\"534\",\"535\",\"536\",\"537\",\"538\",\"539\",\"540\",\"541\",\"542\",\"543\",\"544\",\"545\",\"546\",\"547\",\"548\",\"549\",\"550\",\"551\",\"552\",\"553\",\"554\",\"555\",\"556\",\"557\",\"558\",\"559\",\"560\",\"561\",\"562\",\"563\",\"564\",\"565\",\"566\",\"567\",\"568\",\"569\",\"570\",\"571\",\"572\",\"573\",\"574\",\"575\",\"576\",\"577\",\"578\",\"579\",\"580\",\"581\",\"582\",\"583\",\"584\",\"585\",\"586\",\"587\",\"588\",\"589\",\"590\",\"591\",\"592\",\"593\",\"594\",\"595\",\"596\",\"597\",\"598\",\"599\",\"600\",\"601\",\"602\",\"603\",\"604\",\"605\",\"606\",\"607\",\"608\",\"609\",\"610\",\"611\",\"612\",\"613\",\"614\",\"615\",\"616\",\"617\",\"618\",\"619\",\"620\",\"621\",\"622\",\"623\",\"624\",\"625\",\"626\",\"627\",\"628\",\"629\",\"630\",\"631\",\"632\",\"633\",\"634\",\"635\",\"636\",\"637\",\"638\",\"639\",\"640\",\"641\",\"642\",\"643\",\"644\",\"645\",\"646\",\"647\",\"648\",\"649\",\"650\",\"651\",\"652\",\"653\",\"654\",\"655\",\"656\",\"657\",\"658\",\"659\",\"660\",\"661\",\"662\",\"663\",\"664\",\"665\",\"666\",\"667\",\"668\",\"669\",\"670\",\"671\",\"672\",\"673\",\"674\",\"675\",\"676\",\"677\",\"678\",\"679\",\"680\",\"681\",\"682\",\"683\",\"684\",\"685\",\"686\",\"687\",\"688\",\"689\",\"690\",\"691\",\"692\",\"693\",\"694\",\"695\",\"696\",\"697\",\"698\",\"699\",\"700\",\"701\",\"702\",\"703\",\"704\",\"705\",\"706\",\"707\",\"708\",\"709\",\"710\",\"711\",\"712\",\"713\",\"714\",\"715\",\"716\",\"717\",\"718\",\"719\",\"720\",\"721\",\"722\",\"723\",\"724\",\"725\",\"726\",\"727\",\"728\",\"729\",\"730\",\"731\",\"732\",\"733\",\"734\",\"735\",\"736\",\"737\",\"738\",\"739\",\"740\",\"741\",\"742\",\"743\",\"744\",\"745\",\"746\",\"747\",\"748\",\"749\",\"750\",\"751\",\"752\",\"753\",\"754\",\"755\",\"756\",\"757\",\"758\",\"759\",\"760\",\"761\",\"762\",\"763\",\"764\",\"765\",\"766\",\"767\",\"768\",\"769\",\"770\",\"771\",\"772\",\"773\",\"774\",\"775\",\"776\",\"777\",\"778\",\"779\",\"780\",\"781\",\"782\",\"783\",\"784\",\"785\",\"786\",\"787\",\"788\",\"789\",\"790\",\"791\",\"792\",\"793\",\"794\",\"795\",\"796\",\"797\",\"798\",\"799\",\"800\",\"801\",\"802\",\"803\",\"804\",\"805\",\"806\",\"807\",\"808\",\"809\",\"810\",\"811\",\"812\",\"813\",\"814\",\"815\",\"816\",\"817\",\"818\",\"819\",\"820\",\"821\",\"822\",\"823\",\"824\",\"825\",\"826\",\"827\",\"828\",\"829\",\"830\",\"831\",\"832\",\"833\",\"834\",\"835\",\"836\",\"837\",\"838\",\"839\",\"840\",\"841\",\"842\",\"843\",\"844\",\"845\",\"846\",\"847\",\"848\",\"849\",\"850\",\"851\",\"852\",\"853\",\"854\",\"855\",\"856\",\"857\",\"858\",\"859\",\"860\",\"861\",\"862\",\"863\",\"864\",\"865\",\"866\",\"867\",\"868\",\"869\",\"870\",\"871\",\"872\",\"873\",\"874\",\"875\",\"876\",\"877\",\"878\",\"879\",\"880\",\"881\",\"882\",\"883\",\"884\",\"885\",\"886\",\"887\",\"888\",\"889\",\"890\",\"891\",\"892\",\"893\",\"894\",\"895\",\"896\",\"897\",\"898\",\"899\",\"900\",\"901\",\"902\",\"903\",\"904\",\"905\",\"906\",\"907\",\"908\",\"909\",\"910\",\"911\",\"912\",\"913\",\"914\",\"915\",\"916\",\"917\",\"918\",\"919\",\"920\",\"921\",\"922\",\"923\",\"924\",\"925\",\"926\",\"927\",\"928\",\"929\",\"930\",\"931\",\"932\",\"933\",\"934\",\"935\",\"936\",\"937\",\"938\",\"939\",\"940\",\"941\",\"942\",\"943\",\"944\",\"945\",\"946\",\"947\",\"948\",\"949\",\"950\",\"951\",\"952\",\"953\",\"954\",\"955\",\"956\",\"957\",\"958\",\"959\",\"960\",\"961\",\"962\",\"963\",\"964\",\"965\",\"966\",\"967\",\"968\",\"969\",\"970\",\"971\",\"972\",\"973\",\"974\",\"975\",\"976\",\"977\",\"978\",\"979\",\"980\",\"981\",\"982\",\"983\",\"984\",\"985\",\"986\",\"987\",\"988\",\"989\",\"990\",\"991\",\"992\",\"993\",\"994\",\"995\",\"996\",\"997\",\"998\",\"999\",\"1000\",\"1001\",\"1002\",\"1003\",\"1004\",\"1005\",\"1006\",\"1007\",\"1008\",\"1009\",\"1010\",\"1011\",\"1012\",\"1013\",\"1014\",\"1015\",\"1016\",\"1017\",\"1018\",\"1019\",\"1020\",\"1021\",\"1022\",\"1023\",\"1024\",\"1025\",\"1026\",\"1027\",\"1028\",\"1029\",\"1030\",\"1031\",\"1032\",\"1033\",\"1034\",\"1035\",\"1036\",\"1037\",\"1038\",\"1039\",\"1040\",\"1041\",\"1042\",\"1043\",\"1044\",\"1045\",\"1046\",\"1047\",\"1048\",\"1049\",\"1050\",\"1051\",\"1052\",\"1053\",\"1054\",\"1055\",\"1056\",\"1057\",\"1058\",\"1059\",\"1060\",\"1061\",\"1062\",\"1063\",\"1064\",\"1065\",\"1066\",\"1067\",\"1068\",\"1069\",\"1070\",\"1071\",\"1072\",\"1073\",\"1074\",\"1075\",\"1076\",\"1077\",\"1078\",\"1079\",\"1080\",\"1081\",\"1082\",\"1083\",\"1084\",\"1085\",\"1086\",\"1087\",\"1088\",\"1089\",\"1090\",\"1091\",\"1092\",\"1093\",\"1094\",\"1095\",\"1096\",\"1097\",\"1098\",\"1099\",\"1100\",\"1101\",\"1102\",\"1103\",\"1104\",\"1105\",\"1106\",\"1107\",\"1108\",\"1109\",\"1110\",\"1111\",\"1112\",\"1113\",\"1114\",\"1115\",\"1116\",\"1117\",\"1118\",\"1119\",\"1120\",\"1121\",\"1122\",\"1123\",\"1124\",\"1125\",\"1126\",\"1127\",\"1128\",\"1129\",\"1130\",\"1131\",\"1132\",\"1133\",\"1134\",\"1135\",\"1136\",\"1137\",\"1138\",\"1139\",\"1140\",\"1141\",\"1142\",\"1143\",\"1144\",\"1145\",\"1146\",\"1147\",\"1148\",\"1149\",\"1150\",\"1151\",\"1152\",\"1153\",\"1154\",\"1155\",\"1156\",\"1157\",\"1158\",\"1159\",\"1160\",\"1161\",\"1162\",\"1163\",\"1164\",\"1165\",\"1166\",\"1167\",\"1168\",\"1169\",\"1170\",\"1171\",\"1172\",\"1173\",\"1174\",\"1175\",\"1176\",\"1177\",\"1178\",\"1179\",\"1180\",\"1181\",\"1182\",\"1183\",\"1184\",\"1185\",\"1186\",\"1187\",\"1188\",\"1189\",\"1190\",\"1191\",\"1192\",\"1193\",\"1194\",\"1195\",\"1196\",\"1197\",\"1198\",\"1199\",\"1200\",\"1201\",\"1202\",\"1203\",\"1204\",\"1205\",\"1206\",\"1207\",\"1208\",\"1209\",\"1210\",\"1211\",\"1212\",\"1213\",\"1214\",\"1215\",\"1216\",\"1217\",\"1218\",\"1219\",\"1220\",\"1221\",\"1222\",\"1223\",\"1224\",\"1225\",\"1226\",\"1227\",\"1228\",\"1229\",\"1230\",\"1231\",\"1232\",\"1233\",\"1234\",\"1235\",\"1236\",\"1237\",\"1238\",\"1239\",\"1240\",\"1241\",\"1242\",\"1243\",\"1244\",\"1245\",\"1246\",\"1247\",\"1248\",\"1249\",\"1250\",\"1251\",\"1252\",\"1253\",\"1254\",\"1255\",\"1256\",\"1257\",\"1258\",\"1259\",\"1260\",\"1261\",\"1262\",\"1263\",\"1264\",\"1265\",\"1266\",\"1267\",\"1268\",\"1269\",\"1270\",\"1271\",\"1272\",\"1273\",\"1274\",\"1275\",\"1276\",\"1277\",\"1278\",\"1279\",\"1280\",\"1281\",\"1282\",\"1283\",\"1284\",\"1285\",\"1286\",\"1287\",\"1288\",\"1289\",\"1290\",\"1291\",\"1292\",\"1293\",\"1294\",\"1295\",\"1296\",\"1297\",\"1298\",\"1299\",\"1300\",\"1301\",\"1302\",\"1303\",\"1304\",\"1305\",\"1306\",\"1307\",\"1308\",\"1309\",\"1310\",\"1311\",\"1312\",\"1313\",\"1314\",\"1315\",\"1316\",\"1317\",\"1318\",\"1319\",\"1320\",\"1321\",\"1322\",\"1323\",\"1324\",\"1325\",\"1326\",\"1327\",\"1328\",\"1329\",\"1330\",\"1331\",\"1332\",\"1333\",\"1334\",\"1335\",\"1336\",\"1337\",\"1338\",\"1339\",\"1340\",\"1341\",\"1342\",\"1343\",\"1344\",\"1345\",\"1346\",\"1347\",\"1348\",\"1349\",\"1350\",\"1351\",\"1352\",\"1353\",\"1354\",\"1355\",\"1356\",\"1357\",\"1358\",\"1359\",\"1360\",\"1361\",\"1362\",\"1363\",\"1364\",\"1365\",\"1366\",\"1367\",\"1368\",\"1369\",\"1370\",\"1371\",\"1372\",\"1373\",\"1374\",\"1375\",\"1376\",\"1377\",\"1378\",\"1379\",\"1380\",\"1381\",\"1382\",\"1383\",\"1384\",\"1385\",\"1386\",\"1387\",\"1388\",\"1389\",\"1390\",\"1391\",\"1392\",\"1393\",\"1394\",\"1395\",\"1396\",\"1397\",\"1398\",\"1399\",\"1400\",\"1401\",\"1402\",\"1403\",\"1404\",\"1405\",\"1406\",\"1407\",\"1408\",\"1409\",\"1410\",\"1411\",\"1412\",\"1413\",\"1414\",\"1415\",\"1416\",\"1417\",\"1418\",\"1419\",\"1420\",\"1421\",\"1422\",\"1423\",\"1424\",\"1425\",\"1426\",\"1427\",\"1428\",\"1429\",\"1430\",\"1431\",\"1432\",\"1433\",\"1434\",\"1435\",\"1436\",\"1437\",\"1438\",\"1439\",\"1440\",\"1441\",\"1442\",\"1443\",\"1444\",\"1445\",\"1446\",\"1447\",\"1448\",\"1449\",\"1450\",\"1451\",\"1452\",\"1453\",\"1454\",\"1455\",\"1456\",\"1457\",\"1458\",\"1459\",\"1460\",\"1461\",\"1462\",\"1463\",\"1464\",\"1465\",\"1466\",\"1467\",\"1468\",\"1469\",\"1470\",\"1471\",\"1472\",\"1473\",\"1474\",\"1475\",\"1476\",\"1477\",\"1478\",\"1479\",\"1480\",\"1481\",\"1482\",\"1483\",\"1484\",\"1485\",\"1486\",\"1487\",\"1488\",\"1489\",\"1490\",\"1491\",\"1492\",\"1493\",\"1494\",\"1495\",\"1496\",\"1497\",\"1498\",\"1499\",\"1500\",\"1501\",\"1502\",\"1503\",\"1504\",\"1505\",\"1506\",\"1507\",\"1508\",\"1509\",\"1510\",\"1511\",\"1512\",\"1513\",\"1514\",\"1515\",\"1516\",\"1517\",\"1518\",\"1519\",\"1520\",\"1521\",\"1522\",\"1523\",\"1524\",\"1525\",\"1526\",\"1527\",\"1528\",\"1529\",\"1530\",\"1531\",\"1532\",\"1533\",\"1534\",\"1535\",\"1536\",\"1537\",\"1538\",\"1539\",\"1540\",\"1541\",\"1542\",\"1543\",\"1544\",\"1545\",\"1546\",\"1547\",\"1548\",\"1549\",\"1550\",\"1551\",\"1552\",\"1553\",\"1554\",\"1555\",\"1556\",\"1557\",\"1558\",\"1559\",\"1560\",\"1561\",\"1562\",\"1563\",\"1564\",\"1565\",\"1566\",\"1567\",\"1568\",\"1569\",\"1570\",\"1571\",\"1572\",\"1573\",\"1574\",\"1575\",\"1576\",\"1577\",\"1578\",\"1579\",\"1580\",\"1581\",\"1582\",\"1583\",\"1584\",\"1585\",\"1586\",\"1587\",\"1588\",\"1589\",\"1590\",\"1591\",\"1592\",\"1593\",\"1594\",\"1595\",\"1596\",\"1597\",\"1598\",\"1599\",\"1600\",\"1601\",\"1602\",\"1603\",\"1604\",\"1605\",\"1606\",\"1607\",\"1608\",\"1609\",\"1610\",\"1611\",\"1612\",\"1613\",\"1614\",\"1615\",\"1616\",\"1617\",\"1618\",\"1619\",\"1620\",\"1621\",\"1622\",\"1623\",\"1624\",\"1625\",\"1626\",\"1627\",\"1628\",\"1629\",\"1630\",\"1631\",\"1632\",\"1633\",\"1634\",\"1635\",\"1636\",\"1637\",\"1638\",\"1639\",\"1640\",\"1641\",\"1642\",\"1643\",\"1644\",\"1645\",\"1646\",\"1647\",\"1648\",\"1649\",\"1650\",\"1651\",\"1652\",\"1653\",\"1654\",\"1655\",\"1656\",\"1657\",\"1658\",\"1659\",\"1660\",\"1661\",\"1662\",\"1663\",\"1664\",\"1665\",\"1666\",\"1667\",\"1668\",\"1669\",\"1670\",\"1671\",\"1672\",\"1673\",\"1674\",\"1675\",\"1676\",\"1677\",\"1678\",\"1679\",\"1680\",\"1681\",\"1682\",\"1683\",\"1684\",\"1685\",\"1686\",\"1687\",\"1688\",\"1689\",\"1690\",\"1691\",\"1692\",\"1693\",\"1694\",\"1695\",\"1696\",\"1697\",\"1698\",\"1699\",\"1700\",\"1701\",\"1702\",\"1703\",\"1704\",\"1705\",\"1706\",\"1707\",\"1708\",\"1709\",\"1710\",\"1711\",\"1712\",\"1713\",\"1714\",\"1715\",\"1716\",\"1717\",\"1718\",\"1719\",\"1720\",\"1721\",\"1722\",\"1723\",\"1724\",\"1725\",\"1726\",\"1727\",\"1728\",\"1729\",\"1730\",\"1731\",\"1732\",\"1733\",\"1734\",\"1735\",\"1736\",\"1737\",\"1738\",\"1739\",\"1740\",\"1741\",\"1742\",\"1743\",\"1744\",\"1745\",\"1746\",\"1747\",\"1748\",\"1749\",\"1750\",\"1751\",\"1752\",\"1753\",\"1754\",\"1755\",\"1756\",\"1757\",\"1758\",\"1759\",\"1760\",\"1761\",\"1762\",\"1763\",\"1764\",\"1765\",\"1766\",\"1767\",\"1768\",\"1769\",\"1770\",\"1771\",\"1772\",\"1773\",\"1774\",\"1775\",\"1776\",\"1777\",\"1778\",\"1779\",\"1780\",\"1781\",\"1782\",\"1783\",\"1784\",\"1785\",\"1786\",\"1787\",\"1788\",\"1789\",\"1790\",\"1791\",\"1792\",\"1793\",\"1794\",\"1795\",\"1796\",\"1797\",\"1798\",\"1799\",\"1800\",\"1801\",\"1802\",\"1803\",\"1804\",\"1805\",\"1806\",\"1807\",\"1808\",\"1809\",\"1810\",\"1811\",\"1812\",\"1813\",\"1814\",\"1815\",\"1816\",\"1817\",\"1818\",\"1819\",\"1820\",\"1821\",\"1822\",\"1823\",\"1824\",\"1825\",\"1826\",\"1827\",\"1828\",\"1829\",\"1830\",\"1831\",\"1832\",\"1833\",\"1834\",\"1835\",\"1836\",\"1837\",\"1838\",\"1839\",\"1840\",\"1841\",\"1842\",\"1843\",\"1844\",\"1845\",\"1846\",\"1847\",\"1848\",\"1849\",\"1850\",\"1851\",\"1852\",\"1853\",\"1854\",\"1855\",\"1856\",\"1857\",\"1858\",\"1859\",\"1860\",\"1861\",\"1862\",\"1863\",\"1864\",\"1865\",\"1866\",\"1867\",\"1868\",\"1869\",\"1870\",\"1871\",\"1872\",\"1873\",\"1874\",\"1875\",\"1876\",\"1877\",\"1878\",\"1879\",\"1880\",\"1881\",\"1882\",\"1883\",\"1884\",\"1885\",\"1886\",\"1887\",\"1888\",\"1889\",\"1890\",\"1891\",\"1892\",\"1893\",\"1894\",\"1895\",\"1896\",\"1897\",\"1898\",\"1899\",\"1900\",\"1901\",\"1902\",\"1903\",\"1904\",\"1905\",\"1906\",\"1907\",\"1908\",\"1909\",\"1910\",\"1911\",\"1912\",\"1913\",\"1914\",\"1915\",\"1916\",\"1917\",\"1918\",\"1919\",\"1920\",\"1921\",\"1922\",\"1923\",\"1924\",\"1925\",\"1926\",\"1927\",\"1928\",\"1929\",\"1930\",\"1931\",\"1932\",\"1933\",\"1934\",\"1935\",\"1936\",\"1937\",\"1938\",\"1939\",\"1940\",\"1941\",\"1942\",\"1943\",\"1944\",\"1945\",\"1946\",\"1947\",\"1948\",\"1949\",\"1950\",\"1951\",\"1952\",\"1953\",\"1954\",\"1955\",\"1956\",\"1957\",\"1958\",\"1959\",\"1960\",\"1961\",\"1962\",\"1963\",\"1964\",\"1965\",\"1966\",\"1967\",\"1968\",\"1969\",\"1970\",\"1971\",\"1972\",\"1973\",\"1974\",\"1975\",\"1976\",\"1977\",\"1978\",\"1979\",\"1980\",\"1981\",\"1982\",\"1983\",\"1984\",\"1985\",\"1986\",\"1987\",\"1988\",\"1989\",\"1990\",\"1991\",\"1992\",\"1993\",\"1994\",\"1995\",\"1996\",\"1997\",\"1998\",\"1999\",\"2000\",\"2001\",\"2002\",\"2003\",\"2004\",\"2005\",\"2006\",\"2007\",\"2008\",\"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\",\"2021\",\"2022\",\"2023\",\"2024\",\"2025\",\"2026\",\"2027\",\"2028\",\"2029\",\"2030\",\"2031\",\"2032\",\"2033\",\"2034\",\"2035\",\"2036\",\"2037\",\"2038\",\"2039\",\"2040\",\"2041\",\"2042\",\"2043\",\"2044\",\"2045\",\"2046\",\"2047\",\"2048\",\"2049\",\"2050\",\"2051\",\"2052\",\"2053\",\"2054\",\"2055\",\"2056\",\"2057\",\"2058\",\"2059\",\"2060\",\"2061\",\"2062\",\"2063\",\"2064\",\"2065\",\"2066\",\"2067\",\"2068\",\"2069\",\"2070\",\"2071\",\"2072\",\"2073\",\"2074\",\"2075\",\"2076\",\"2077\",\"2078\",\"2079\",\"2080\",\"2081\",\"2082\",\"2083\",\"2084\",\"2085\",\"2086\",\"2087\",\"2088\",\"2089\",\"2090\",\"2091\",\"2092\",\"2093\",\"2094\",\"2095\",\"2096\",\"2097\",\"2098\",\"2099\",\"2100\",\"2101\",\"2102\",\"2103\",\"2104\",\"2105\",\"2106\",\"2107\",\"2108\",\"2109\",\"2110\",\"2111\",\"2112\",\"2113\",\"2114\",\"2115\",\"2116\",\"2117\",\"2118\",\"2119\",\"2120\",\"2121\",\"2122\",\"2123\",\"2124\",\"2125\",\"2126\",\"2127\",\"2128\",\"2129\",\"2130\",\"2131\",\"2132\",\"2133\",\"2134\",\"2135\",\"2136\",\"2137\",\"2138\",\"2139\",\"2140\",\"2141\",\"2142\",\"2143\",\"2144\",\"2145\",\"2146\",\"2147\",\"2148\",\"2149\",\"2150\",\"2151\",\"2152\",\"2153\",\"2154\",\"2155\",\"2156\",\"2157\",\"2158\",\"2159\",\"2160\",\"2161\",\"2162\",\"2163\",\"2164\",\"2165\",\"2166\",\"2167\",\"2168\",\"2169\",\"2170\",\"2171\",\"2172\",\"2173\",\"2174\",\"2175\",\"2176\",\"2177\",\"2178\",\"2179\",\"2180\",\"2181\",\"2182\",\"2183\",\"2184\",\"2185\",\"2186\",\"2187\",\"2188\",\"2189\",\"2190\",\"2191\",\"2192\",\"2193\",\"2194\",\"2195\",\"2196\",\"2197\",\"2198\",\"2199\",\"2200\",\"2201\",\"2202\",\"2203\",\"2204\",\"2205\",\"2206\",\"2207\",\"2208\",\"2209\",\"2210\",\"2211\",\"2212\",\"2213\",\"2214\",\"2215\",\"2216\",\"2217\",\"2218\",\"2219\",\"2220\",\"2221\",\"2222\",\"2223\",\"2224\",\"2225\",\"2226\",\"2227\",\"2228\",\"2229\",\"2230\",\"2231\",\"2232\",\"2233\",\"2234\",\"2235\",\"2236\",\"2237\",\"2238\",\"2239\",\"2240\",\"2241\",\"2242\",\"2243\",\"2244\",\"2245\",\"2246\",\"2247\",\"2248\",\"2249\",\"2250\",\"2251\",\"2252\",\"2253\",\"2254\",\"2255\",\"2256\",\"2257\",\"2258\",\"2259\",\"2260\",\"2261\",\"2262\",\"2263\",\"2264\",\"2265\",\"2266\",\"2267\",\"2268\",\"2269\",\"2270\",\"2271\",\"2272\",\"2273\",\"2274\",\"2275\",\"2276\",\"2277\",\"2278\",\"2279\",\"2280\",\"2281\",\"2282\",\"2283\",\"2284\",\"2285\",\"2286\",\"2287\",\"2288\",\"2289\",\"2290\",\"2291\",\"2292\",\"2293\",\"2294\",\"2295\",\"2296\",\"2297\",\"2298\",\"2299\",\"2300\",\"2301\",\"2302\",\"2303\",\"2304\",\"2305\",\"2306\",\"2307\",\"2308\",\"2309\",\"2310\",\"2311\",\"2312\",\"2313\",\"2314\",\"2315\",\"2316\",\"2317\",\"2318\",\"2319\",\"2320\",\"2321\",\"2322\",\"2323\",\"2324\",\"2325\",\"2326\",\"2327\",\"2328\",\"2329\",\"2330\",\"2331\",\"2332\",\"2333\",\"2334\",\"2335\",\"2336\",\"2337\",\"2338\",\"2339\",\"2340\",\"2341\",\"2342\",\"2343\",\"2344\",\"2345\",\"2346\",\"2347\",\"2348\",\"2349\",\"2350\",\"2351\",\"2352\",\"2353\",\"2354\",\"2355\",\"2356\",\"2357\",\"2358\",\"2359\",\"2360\",\"2361\",\"2362\",\"2363\",\"2364\",\"2365\",\"2366\",\"2367\",\"2368\",\"2369\",\"2370\",\"2371\",\"2372\",\"2373\",\"2374\",\"2375\",\"2376\",\"2377\",\"2378\",\"2379\",\"2380\",\"2381\",\"2382\",\"2383\",\"2384\",\"2385\",\"2386\",\"2387\",\"2388\",\"2389\",\"2390\",\"2391\",\"2392\",\"2393\",\"2394\",\"2395\",\"2396\",\"2397\",\"2398\",\"2399\",\"2400\",\"2401\",\"2402\",\"2403\",\"2404\",\"2405\",\"2406\",\"2407\",\"2408\",\"2409\",\"2410\",\"2411\",\"2412\",\"2413\",\"2414\",\"2415\",\"2416\",\"2417\",\"2418\",\"2419\",\"2420\",\"2421\",\"2422\",\"2423\",\"2424\",\"2425\",\"2426\",\"2427\",\"2428\",\"2429\",\"2430\",\"2431\",\"2432\",\"2433\",\"2434\",\"2435\",\"2436\",\"2437\",\"2438\",\"2439\",\"2440\",\"2441\",\"2442\",\"2443\",\"2444\",\"2445\",\"2446\",\"2447\",\"2448\",\"2449\",\"2450\",\"2451\",\"2452\",\"2453\",\"2454\",\"2455\",\"2456\",\"2457\",\"2458\",\"2459\",\"2460\",\"2461\",\"2462\",\"2463\",\"2464\",\"2465\",\"2466\",\"2467\",\"2468\",\"2469\",\"2470\",\"2471\",\"2472\",\"2473\",\"2474\",\"2475\",\"2476\",\"2477\",\"2478\",\"2479\",\"2480\",\"2481\",\"2482\",\"2483\",\"2484\",\"2485\",\"2486\",\"2487\",\"2488\",\"2489\",\"2490\",\"2491\",\"2492\",\"2493\",\"2494\",\"2495\",\"2496\",\"2497\",\"2498\",\"2499\",\"2500\",\"2501\",\"2502\",\"2503\",\"2504\",\"2505\",\"2506\",\"2507\",\"2508\",\"2509\",\"2510\",\"2511\",\"2512\",\"2513\",\"2514\",\"2515\",\"2516\",\"2517\",\"2518\",\"2519\",\"2520\",\"2521\",\"2522\",\"2523\",\"2524\",\"2525\",\"2526\",\"2527\",\"2528\",\"2529\",\"2530\",\"2531\",\"2532\",\"2533\",\"2534\",\"2535\",\"2536\",\"2537\",\"2538\",\"2539\",\"2540\",\"2541\",\"2542\",\"2543\",\"2544\",\"2545\",\"2546\",\"2547\",\"2548\",\"2549\",\"2550\",\"2551\",\"2552\",\"2553\",\"2554\",\"2555\",\"2556\",\"2557\",\"2558\",\"2559\",\"2560\",\"2561\",\"2562\",\"2563\",\"2564\",\"2565\",\"2566\",\"2567\",\"2568\",\"2569\",\"2570\",\"2571\",\"2572\",\"2573\",\"2574\",\"2575\",\"2576\",\"2577\",\"2578\",\"2579\",\"2580\",\"2581\",\"2582\",\"2583\",\"2584\",\"2585\",\"2586\",\"2587\",\"2588\",\"2589\",\"2590\",\"2591\",\"2592\",\"2593\",\"2594\",\"2595\",\"2596\",\"2597\",\"2598\",\"2599\",\"2600\",\"2601\",\"2602\",\"2603\",\"2604\",\"2605\",\"2606\",\"2607\",\"2608\",\"2609\",\"2610\",\"2611\",\"2612\",\"2613\",\"2614\",\"2615\",\"2616\",\"2617\",\"2618\",\"2619\",\"2620\",\"2621\",\"2622\",\"2623\",\"2624\",\"2625\",\"2626\",\"2627\",\"2628\",\"2629\",\"2630\",\"2631\",\"2632\",\"2633\",\"2634\",\"2635\",\"2636\",\"2637\",\"2638\",\"2639\",\"2640\",\"2641\",\"2642\",\"2643\",\"2644\",\"2645\",\"2646\",\"2647\",\"2648\",\"2649\",\"2650\",\"2651\",\"2652\",\"2653\",\"2654\",\"2655\",\"2656\",\"2657\",\"2658\",\"2659\",\"2660\",\"2661\",\"2662\",\"2663\",\"2664\",\"2665\",\"2666\",\"2667\",\"2668\",\"2669\",\"2670\",\"2671\",\"2672\",\"2673\",\"2674\",\"2675\",\"2676\",\"2677\",\"2678\",\"2679\",\"2680\",\"2681\",\"2682\",\"2683\",\"2684\",\"2685\",\"2686\",\"2687\",\"2688\",\"2689\",\"2690\",\"2691\",\"2692\",\"2693\",\"2694\",\"2695\",\"2696\",\"2697\",\"2698\",\"2699\",\"2700\",\"2701\",\"2702\",\"2703\",\"2704\",\"2705\",\"2706\",\"2707\",\"2708\",\"2709\",\"2710\",\"2711\",\"2712\",\"2713\",\"2714\",\"2715\",\"2716\",\"2717\",\"2718\",\"2719\",\"2720\",\"2721\",\"2722\",\"2723\",\"2724\",\"2725\",\"2726\",\"2727\",\"2728\",\"2729\",\"2730\",\"2731\",\"2732\",\"2733\",\"2"},{"title":"Pig Birth Weights – Kruskal-Wallis","url":"325Analyses/Kruskal-Wallis Test/Examples/pigweightsKruskal.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } button.code-folding-btn:focus { outline: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Pig Birth Weights – Kruskal-Wallis function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } Background This experiment was included in the original research paper by Kruskal and Wallis (Use of Ranks in One-Criterion Variance Analysis, 1952) to demonstrate the Kruskal-Wallis test. “Snedecor’s data on the birth weight of pigs [Snedecor, George W., Statistical Methods, Ames, Iowa State College Press, 1937 and later, Table 10.12] are shown in” the table below. Data Table (click to view) pigweights <- data.frame(Subject=1:56, Litter=factor(rep(1:8, c(10,8,10,8,6,4,6,4))), Weight=c(2.0,2.8,3.3,3.2,4.4,3.6,1.9,3.3,2.8,1.1, 3.5,2.8,3.2,3.5,2.3,2.4,2.0,1.6, 3.3,3.6,2.6,3.1,3.2,3.3,2.9,3.4,3.2,3.2, 3.2,3.3,3.2,2.9,3.3,2.5,2.6,2.8, 2.6,2.6,2.9,2.0,2.0,2.1, 3.1,2.9,3.1,2.5, 2.6,2.2,2.2,2.5,1.2,1.2, 2.5,2.4,3.0,1.4)) knitr::kable(pigweights, align='l') Subject Litter Weight 1 1 2.0 2 1 2.8 3 1 3.3 4 1 3.2 5 1 4.4 6 1 3.6 7 1 1.9 8 1 3.3 9 1 2.8 10 1 1.1 11 2 3.5 12 2 2.8 13 2 3.2 14 2 3.5 15 2 2.3 16 2 2.4 17 2 2.0 18 2 1.6 19 3 3.3 20 3 3.6 21 3 2.6 22 3 3.1 23 3 3.2 24 3 3.3 25 3 2.9 26 3 3.4 27 3 3.2 28 3 3.2 29 4 3.2 30 4 3.3 31 4 3.2 32 4 2.9 33 4 3.3 34 4 2.5 35 4 2.6 36 4 2.8 37 5 2.6 38 5 2.6 39 5 2.9 40 5 2.0 41 5 2.0 42 5 2.1 43 6 3.1 44 6 2.9 45 6 3.1 46 6 2.5 47 7 2.6 48 7 2.2 49 7 2.2 50 7 2.5 51 7 1.2 52 7 1.2 53 8 2.5 54 8 2.4 55 8 3.0 56 8 1.4 The research question surrounding the data was weather or not the various weights of pigs in the different litters were comparable or not. In other words, does a given mother always give birth to piglets that come from the same weight distribution? Or is there evidence that at least one litter came from a different distribution of weights than the others? Analysis Note that the assumptions of an ANOVA test do not appear to be violated for these data. However, in keeping with the presentation of the original authors of the Kruskal-Wallis test, this data will be analyzed with a Kruskal-Wallis Test. The null hypothesis is that the various litters represent samples of weights from the same population. The alternative hypothesis is that at least one of the samples is from a different population, with similar shape, but shifted either higher or lower than the others. #Note that this R-Chunk began with: ```{r, comment=NA} kruskal.test(Weight ~ Litter, data=pigweights) Kruskal-Wallis rank sum test data: Weight by Litter Kruskal-Wallis chi-squared = 18.565, df = 7, p-value = 0.009663 It appears that at least one litter represents a sample of weights that are from a different population than the others \\((p=0.009663)\\). The following graphic displays the data. boxplot(Weight ~ Litter, data=pigweights, col=\"lightgray\", xlab=\"Litter Number\", main=\"Weights of Piglets for 8 Different Litters from One Mother\", ylab=\"Piglet Weight\") stripchart(Weight ~ Litter, data=pigweights, vertical=TRUE, pch=16, add=TRUE, col=\"skyblue3\") pander(favstats(Weight ~ Litter, data=pigweights)[,-10]) Litter min Q1 median Q3 max mean sd n 1 1.1 2.2 3 3.3 4.4 2.84 0.9536 10 2 1.6 2.225 2.6 3.275 3.5 2.663 0.705 8 3 2.6 3.125 3.2 3.3 3.6 3.18 0.2741 10 4 2.5 2.75 3.05 3.225 3.3 2.975 0.3196 8 5 2 2.025 2.35 2.6 2.9 2.367 0.383 6 6 2.5 2.8 3 3.1 3.1 2.9 0.2828 4 7 1.2 1.45 2.2 2.425 2.6 1.983 0.6274 6 8 1.4 2.15 2.45 2.625 3 2.325 0.6702 4 Interpretation While the results of the test show that not all litters are coming from the same weight distribution, it is difficult to decipher what is going on. Litter 3 had the heaviest distribution of piglets. Litter 7 had the lowest median weight of piglets even though Litter 1 has the single piglet with the smallest weight (lowest minimum). However, the Kruskal-Wallis test does not provide information as to why the null hypothesis was rejected, only that at least one of these litters is different from the others. At this point it would be beneficial to understand more about the circumstances surrounding each litter so that the full story can be pieced together. Was the diet of the mother different prior to delivering each litter? Was the sire of the pigs different for each litter? This data actually produces more questions than it answers, but questions lead to future research, so the study was useful in that aspect. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Student Wages – Kruskal-Wallis","url":"325Analyses/Kruskal-Wallis Test/Examples/wagesKruskal.html","content":"Code Show All Code Hide All Code Student Wages – Kruskal-Wallis Background Five different Math 221 courses at BYU-Idaho were given a brief start of semester survey. Two of the variables collected gave information about each student’s class rank (Freshman, Sophomore, Junior, and Senior) and their off-track hourly wage (in U.S. Dollars). The survey data will be used to answer the following questions. Note that the responses from two students were removed from the Class Survey data because they did not specify their class rank. Also, one other student’s responses were removed because they claimed an hourly wage of $100 an hour, which was likely a typo that was supposed to be $10 an hour. ClassSurvey <- read.csv(\"../../../Data/ClassSurvey.csv\", header=TRUE) # Note that to get the ClassSurvey data into your Console # you should go to \"Import Dataset\" under the \"Environment\" tab # in the top right of your workspace. # There are a few problems in the data that need to be fixed # before we work with the data. Run the following code to # xyplot(Wage ~ Rank, data=ClassSurvey, type=c(\"p\")) # see that there are two observations that don't have a class # rank recorded and that one person supposedly earned $100 an # hour, which is probably a typo and should be deleted. # Filter out the outlier: ClassSurvey <- filter(ClassSurvey, Wage<100) # xyplot(Wage ~ Rank, data=ClassSurvey, type=c(\"p\")) # Filter out the missing Rank values: ClassSurvey <- filter(ClassSurvey, Rank %in% c(\"FR\",\"JR\",\"SO\",\"SR\")) %>% droplevels() %>% mutate(Rank = factor(Rank, levels=c(\"FR\",\"SO\",\"JR\",\"SR\"))) # xyplot(Wage ~ Rank, data=ClassSurvey, type=c(\"p\")) How much do BYU-Idaho students make hourly during their off-track? Do they earn more as they gain more education? Analysis Side-by-side boxplots show that in the sample, the Freshman have the highest median wage, while the Sophomores and Seniors have fairly right-skewed distributions with some very high outliers. This suggest that the shape of the distribution of wages is potentially different for the various class ranks. It may be the case that Freshman hourly wages are left skewed, Junior wages are fairly normal, and Sophomore and Senior wages are right-skewed. boxplot(Wage ~ Rank, data=ClassSurvey, col='grey', ylab=\"Hourly Wage\", main=\"Math 221 Students\", xlab=\"Class Rank\") \\[ H_0: \\text{All samples represent a sample of data from the same distribution.} \\] \\[ H_a: \\text{At least one distribution is stochastically different than the others.} \\] \\[ \\alpha = 0.05 \\] According to the original authors, what the alternative to the Kruskal-Wallis test really is, “is a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly. In many cases, this is practically equivalent to the mean of at least one population differing from the others.” kruskal.test(Wage ~ Rank, data=ClassSurvey) Kruskal-Wallis rank sum test data: Wage by Rank Kruskal-Wallis chi-squared = 2.3262, df = 3, p-value = 0.5075 favstats(Wage ~ Rank, data=ClassSurvey)[,-10] %>% pander(caption=\"Math 221 Student Wage Summaries by Class Rank\") Math 221 Student Wage Summaries by Class Rank Rank min Q1 median Q3 max mean sd n FR 1 9.03 11.1 13.12 15 10.47 4.475 8 SO 7.25 8.175 9 11.48 30 10.53 4.2 71 JR 3.25 8.25 9.375 12 17 10.09 2.727 56 SR 4.62 8.135 8.875 10.34 26.25 10.22 3.898 38 There is insufficient evidence to reject the null hypothesis that each of these represents a sample from the same distribution. We will continue to assume that the hourly wage distributions are the same across all class ranks. Interpretation Since all samples can be assumed to be from the same distribution, Class Rank really has no apparent effect on hourly wages. So students in general aren’t earning more or less during their off track as they progress through their college education. It sufficies then to simply understand the off-track hourly wage of BYU-Idaho students as a whole since Class Rank has no apparent effect on hourly wages. The following histogram summarizes the relevant information. Oddly, some students are reporting earning $5 or less an hour. Most state minimum wages are above $7.25 an hour. So this is surprising. Typically students are earning around $9 (the median) an hour, although some are doing quite well, all the way up to $30 an hour! hist(ClassSurvey$Wage, breaks=15, col='sandybrown', xlab=\"Hourly Wage\", main=\"BYU-Idaho Math 221 Students\") favstats(ClassSurvey$Wage)[-9] %>% pander(caption=\"Summaries of BYU-I Students' Hourly Wages\") Summaries of BYU-I Students’ Hourly Wages min Q1 median Q3 max mean sd n 1 8.2 9 11.67 30 10.32 3.698 173 // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":" College Student’s Food Perception ","url":"325Analyses/Kruskal-Wallis Test/Food.html","content":"Code Show All Code Hide All Code College Student’s Food Perception # From your file menu (at the top of R-Studio) select: # \"Session -> Set working directory -> To source file location\" # Then play this chunk to get the data into R. library(mosaic) library(car) library(DT) library(pander) library(readr) library(knitr) library(kableExtra) library(formattable) library(ggplot2) food <- read_csv(\"../../Data/food.csv\") #food.csv is in the Data folder... Background The following experiment conducted consisted of a survey that went around a particular college campus asking students about their GPA and Weight as well as a variety of questions regarding food. There were questions asking about their comfort food, the likely hood of eating a specific cuisine, and if a health option or a junk food option came to mind when asking about a specific food. In this study, we will be specifically examining the student’s weight as well as their perception of food to answer the following question: Does the perception of food among students at this college result in a change in their weight? Alternatively: If a student thinks healthier, do they have a lower weight? Or, if a student thinks unhealthy do they have a higher weight? For the data table of the following experiment and more information on the data table, click Show Data Table: Hide Data Table Show Data Table The survey showed individuals two pictures to choose from when it came to associating foods with a specific words. The table below shows the different questions the survey asked in regards to word food association: questions <- c(\"Which one of these pictures do you associate the word 'breakfast'?\", \"Which picture do you associate with the word 'drink'?\", \"Which of these pictures you associate with word 'fries'?\", \"Which of the two pictures you associate with the word 'soup'?\") foodoptions <- c(\"Cereal / Donut\", \"1- Orange Juice / 2- Soda\", \"1- McDonald's Fries / 2- Home Fries\", \"1- Veggie Soup / 2- Creamy Soup\") collegestusurvey <- data.frame( Question = questions, Options = foodoptions, stringsAsFactors = FALSE) kable(collegestusurvey, col.names = c(\"Questions\", \"Food Options\"), caption =\"Food Survey\") Food Survey Questions Food Options Which one of these pictures do you associate the word ‘breakfast’? Cereal / Donut Which picture do you associate with the word ‘drink’? 1- Orange Juice / 2- Soda Which of these pictures you associate with word ‘fries’? 1- McDonald’s Fries / 2- Home Fries Which of the two pictures you associate with the word ‘soup’? 1- Veggie Soup / 2- Creamy Soup Thus, their answer was either expressed as a number or the specific word, one option being healthier than the other. If they chose the junk food option out of the two, they are rewarded a point that would then be counted for in their Food Perception Score. This would then result in scores ranging from 0-4, 0 being the healthy thinking and 4 being junk food thinking. The Food Perception Score column is highlighted in green in the table to display each student’s score. Foood <- food %>% select(weight, breakfast, fries, drink, soup) %>% filter(!is.na(suppressWarnings(as.numeric(weight))) + is.na(drink)) %>% mutate(`Food Perception Score` = ifelse(breakfast == \"Cereal\", 0, ifelse(breakfast == \"Donut\", 1, 0)) + ifelse(fries == \"1\", 1, ifelse(fries == \"2\", 0, 0)) + ifelse(drink == \"orange juice\", 0, ifelse(drink == \"soda\", 1, 0)) + ifelse(soup == \"1\", 0, ifelse(soup == \"2\", 1, 0))) %>% mutate(weight = as.numeric(weight)) datatable(Foood, options=list(pageLength = 10)) %>% formatStyle('Food Perception Score', backgroundColor = 'lightgreen') {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\"],[187,155,190,190,180,137,180,125,116,110,264,123,185,180,145,170,135,165,175,185,185,105,125,160,175,180,167,115,205,128,150,150,150,170,150,175,140,120,135,100,170,113,168,145,155,150,169,185,200,265,165,192,175,140,155,155,135,118,210,140,112,125,145,130,140,140,140,200,120,150,200,135,145,130,190,170,127,167,140,190,155,175,129,260,135,190,165,175,184,210,155,185,165,125,160,135,130,230,125,130,165,128,200,160,170,129,170,138,150,170,113,140,185,156,180,120,135,135],[\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Donut\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Donut\",\"Cereal\",\"Donut\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Donut\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Donut\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Donut\",\"Cereal\",\"Cereal\",\"Donut\",\"Cereal\",\"Cereal\",\"Donut\",\"Donut\",\"Cereal\",\"Cereal\",\"Cereal\",\"Donut\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Donut\",\"Cereal\",\"Cereal\",\"Donut\",\"Donut\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\",\"Cereal\"],[2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1,1,2,2,1,1,2,1,1,1,1,1,1,2,2,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[\"orange juice\",\"soda\",\"soda\",\"soda\",\"orange juice\",\"soda\",\"orange juice\",\"orange juice\",\"soda\",\"orange juice\",\"soda\",\"soda\",\"soda\",\"soda\",\"orange juice\",\"soda\",\"orange juice\",\"soda\",\"soda\",\"orange juice\",\"orange juice\",\"soda\",\"orange juice\",\"orange juice\",\"soda\",\"orange juice\",\"soda\",\"orange juice\",\"soda\",\"orange juice\",\"orange juice\",\"orange juice\",\"soda\",\"soda\",\"orange juice\",\"soda\",\"soda\",\"orange juice\",\"soda\",\"orange juice\",\"orange juice\",\"orange juice\",\"soda\",\"orange juice\",\"soda\",\"orange juice\",\"orange juice\",\"orange juice\",\"orange juice\",\"soda\",\"orange juice\",\"soda\",\"orange juice\",\"orange juice\",\"soda\",\"soda\",\"soda\",\"orange juice\",\"orange juice\",\"orange juice\",\"orange juice\",\"soda\",\"soda\",\"soda\",\"orange juice\",\"orange juice\",\"soda\",\"soda\",\"orange juice\",\"soda\",\"soda\",\"orange juice\",\"soda\",\"soda\",\"soda\",\"soda\",\"orange juice\",\"soda\",\"soda\",\"soda\",\"soda\",\"soda\",\"orange juice\",\"soda\",\"soda\",\"soda\",\"soda\",\"soda\",\"soda\",\"soda\",\"soda\",\"soda\",\"orange juice\",\"orange juice\",\"soda\",\"soda\",\"orange juice\",\"soda\",\"soda\",\"orange juice\",\"orange juice\",\"soda\",\"soda\",\"orange juice\",\"soda\",\"orange juice\",\"soda\",\"soda\",\"soda\",\"orange juice\",\"orange juice\",\"orange juice\",\"orange juice\",\"soda\",\"orange juice\",\"orange juice\",\"soda\",\"orange juice\"],[1,1,1,1,1,1,2,1,1,1,2,1,1,2,1,1,2,1,1,1,1,2,1,2,1,1,1,2,2,1,1,1,2,1,1,1,1,1,1,2,1,1,1,1,2,2,1,1,2,1,1,1,1,2,1,2,1,2,1,2,1,1,1,2,1,1,1,1,1,2,2,1,1,1,2,1,1,1,1,1,1,1,1,2,1,1,1,1,2,1,1,1,1,1,1,1,1,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1],[0,2,2,2,1,2,2,1,2,1,3,2,2,4,1,2,2,2,2,0,2,3,2,2,2,1,2,2,3,1,1,1,3,2,1,2,2,1,2,2,1,1,2,1,4,2,1,1,2,2,0,3,1,1,1,3,2,1,1,2,1,2,2,4,0,0,3,2,1,4,4,0,2,2,4,2,1,2,2,2,2,3,0,3,3,3,2,2,3,2,2,2,1,1,2,2,1,3,3,1,1,2,2,1,2,1,2,2,2,1,1,1,1,2,1,1,3,1]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>weight<\\/th>\\n <th>breakfast<\\/th>\\n <th>fries<\\/th>\\n <th>drink<\\/th>\\n <th>soup<\\/th>\\n <th>Food Perception Score<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":10,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,3,5,6]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"weight\",\"targets\":1},{\"name\":\"breakfast\",\"targets\":2},{\"name\":\"fries\",\"targets\":3},{\"name\":\"drink\",\"targets\":4},{\"name\":\"soup\",\"targets\":5},{\"name\":\"Food Perception Score\",\"targets\":6}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"rowCallback\":\"function(row, data, displayNum, displayIndex, dataIndex) {\\nvar value=data[6]; $(this.api().cell(row, 6).node()).css({'background-color':'lightgreen'});\\n}\"}},\"evals\":[\"options.rowCallback\"],\"jsHooks\":[]} Hypothesis To find the answers for our question above, we will be using a Kruskal-Wallis Rank Sum Test. Thus, we will use the following hypotheses: \\[H_0 : \\text{All samples are from the same distribution.}\\] \\[H_a : \\text{At least one sample's distribution is stochastically different.}\\] In more depth, the null hypothesis states that the weights of college students, regardless of their food perception (healthy or unhealthy), come from the same distribution at this specific college. While the alternative hypothesis suggests that at least one type of food perception (healthy or unhealthy) results in a different weight distribution among college students at this particular school. Additionally, our level of significance will be: \\[ \\alpha = 0.05 \\] Analysis Graphical/ Numerical Summaries The following box plot helps to show us which distribution is the highest out of all the Food Perception Scores. The different shades of green show us the range in Food Perception Scores, 0 being the healthy thinking and 4 being junk food thinking. According the box plot, college students with a Food Perception Score of 3 have the highest distribution in our sample. Additionally, the five number summary of each box plot is given below the box plot. boxplot(weight ~ `Food Perception Score`, data=Foood, col=c(\"darkseagreen1\",\"palegreen\",\"palegreen3\",\"palegreen4\",\"mediumseagreen\"), xlab=\"Food Perception Score\", main=\"Weight of Surveyed College Students\", ylab=\"Weight (lbs)\") stripchart(weight ~ `Food Perception Score`, data=Foood, pch=16,vertical=TRUE, add=TRUE, col=\"gray28\") Foodie <- favstats(weight ~ `Food Perception Score`, data = Foood)[,-10] Foodie2 <- as.data.frame(Foodie) kable(Foodie2) Food Perception Score min Q1 median Q3 max mean sd n 0 129 137.50 140.0 175.0 187 154.4286 24.31637 7 1 110 125.00 145.0 169.0 210 146.5946 25.90952 37 2 100 138.00 160.0 175.0 265 160.1887 28.97881 53 3 105 137.50 175.0 198.5 264 176.3333 48.17330 15 4 130 151.25 167.5 187.5 200 167.5000 26.78619 6 Kruskal-Wallis Rank Sum Test According to our Kruskal-Wallis test shown below, we have a p-value of 0.06521. kruskal.test(weight ~ `Food Perception Score`, data=Foood) %>% pander() Kruskal-Wallis rank sum test: weight by Food Perception Score Test statistic df P value 8.841 4 0.06521 \\[ p-value = 0.06521 > \\alpha \\] Due to our high p-value, we are able to conclude that we have insufficient evidence to reject the null hypothesis. Interpretation Even though the Kruskal-Wallis test shows us one thing, the visuals provided by the box plot tell us another. Student’s with a Food Perception Score of 3 had the highest distribution as well as the highest median of 175 in our sample. Therefore, appearing to show us that there is at least one distribution that differs from the rest as stated in our alternative hypothesis. However, with a p-value of 0.06521 slightly above our significance level (0.05), we have insufficient evidence to reject the null hypothesis for the surveyed school’s student population. This suggests that the way a student perceives food may not significantly affect the student’s weight distribution at this particular institution. Sources: ChatGPT for helping me to figure out how to put in cool stylistic things such as the colored column in my data table Formatting Inspiration Student Wages - Kruskal-Wallis Example Pig Birth Weights - Kruskal-Wallis Example // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Body Weight – Simple Linear Regression","url":"325Analyses/Linear Regression/Examples/BodyWeightSLR.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Body Weight – Simple Linear Regression library(car) library(DT) library(tidyverse) library(pander) library(mosaic) Background Do individuals that engage in regular exercise know and report their true weight? Reported weights and actual weights were collected from 200 regularly exercising individuals in order to answer this question (as well as other questions). A mixture of males and females were included in the data. One outlier was removed, observation number 12, due to their body weight being dramatically heavier that the other 199 observations. The reported weight for this individual was one third of their actual weight, thus either an error in reporting or calculation seems likely. Hide Data Show Data The data for this analysis comes from the Davis data set found in library(car). Original Data datatable(Davis) {\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\"],[\"M\",\"F\",\"F\",\"M\",\"F\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"F\",\"F\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"M\",\"F\",\"F\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"M\",\"F\",\"F\",\"M\",\"F\",\"M\",\"M\",\"M\",\"F\",\"M\",\"F\",\"M\",\"M\",\"F\",\"M\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"M\",\"M\",\"M\",\"F\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"F\",\"M\",\"F\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"M\",\"M\",\"F\",\"M\",\"F\",\"F\",\"F\",\"M\",\"M\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"M\",\"M\",\"F\",\"M\",\"F\",\"F\",\"F\",\"F\",\"M\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"M\",\"M\",\"F\",\"M\",\"F\",\"F\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"F\",\"F\",\"M\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"M\",\"F\",\"F\",\"M\",\"F\",\"F\",\"F\",\"M\",\"M\",\"M\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"M\",\"F\",\"F\",\"F\",\"F\",\"F\",\"M\",\"M\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"M\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"M\",\"M\",\"M\",\"F\",\"M\",\"M\",\"M\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"M\",\"F\",\"F\",\"F\",\"M\",\"M\",\"M\",\"M\",\"M\"],[77,58,53,68,59,76,76,69,71,65,70,166,51,64,52,65,92,62,76,61,119,61,65,66,54,50,63,58,39,101,71,75,79,52,68,64,56,69,88,65,54,80,63,78,85,54,73,49,54,75,82,56,74,102,64,65,66,73,75,57,68,71,71,78,97,60,64,64,52,80,62,66,55,56,50,50,50,63,69,69,61,55,53,60,56,59,62,53,57,57,70,56,84,69,88,56,103,50,52,55,55,63,47,45,62,53,52,57,64,59,84,79,55,67,76,62,83,96,75,65,78,69,68,55,67,52,47,45,68,44,62,87,56,50,83,53,64,62,90,85,66,52,53,54,64,55,55,59,70,88,57,47,47,55,48,54,69,59,58,57,51,54,53,59,56,59,63,66,96,53,76,54,61,82,62,71,60,66,81,68,80,43,82,63,70,56,60,58,76,50,88,89,59,51,62,74,83,81,90,79],[182,161,161,177,157,170,167,186,178,171,175,57,161,168,163,166,187,168,197,175,180,170,175,173,171,166,169,166,157,183,166,178,173,164,169,176,166,174,178,187,164,178,163,183,179,160,180,161,174,162,182,165,169,185,177,176,170,183,172,173,165,177,180,173,189,162,165,164,158,178,175,173,165,163,166,171,160,160,182,183,165,168,169,167,170,182,178,165,163,162,173,161,184,180,189,165,185,169,159,155,164,178,163,163,175,164,152,167,166,166,183,179,174,179,167,168,184,184,169,178,178,167,178,165,179,169,153,157,171,157,166,185,160,148,177,162,172,167,188,191,175,163,165,176,171,160,165,157,173,184,168,162,150,162,163,169,172,170,169,167,163,161,162,172,163,159,170,166,191,158,169,163,170,176,168,178,174,170,178,174,176,154,181,165,173,162,172,169,183,158,185,173,164,156,164,175,180,175,181,177],[77,51,54,70,59,76,77,73,71,64,75,56,52,64,57,66,101,62,75,61,124,61,66,70,59,50,61,60,41,100,71,73,76,52,63,65,54,69,86,67,53,80,59,80,82,55,null,null,56,75,85,57,73,107,null,64,65,74,70,58,69,71,76,75,98,59,63,62,51,76,61,66,54,57,50,null,55,64,70,70,60,56,52,55,56,61,66,53,59,56,68,56,86,71,87,57,101,50,52,null,55,63,47,45,63,51,51,55,64,55,90,79,57,67,77,62,83,94,76,66,77,73,68,55,null,56,null,45,68,44,61,89,53,47,84,53,62,null,91,83,68,53,55,55,66,55,55,55,67,86,58,47,45,null,44,58,68,null,null,56,50,54,52,58,58,59,62,66,95,50,75,null,61,null,64,68,null,67,82,68,78,null,null,59,70,56,55,54,75,49,93,86,59,51,61,71,80,null,91,81],[180,159,158,175,155,165,165,180,175,170,174,163,158,165,160,165,185,165,200,171,178,170,173,170,168,165,168,160,153,180,165,175,173,161,170,175,165,171,175,188,160,178,159,180,175,158,null,null,173,158,183,163,170,185,null,172,null,180,169,170,165,170,175,169,185,160,163,161,155,175,171,175,163,159,161,null,150,158,180,183,163,170,175,163,170,183,175,165,160,160,170,161,183,180,185,160,182,165,153,154,163,175,160,160,173,160,150,164,165,163,183,171,171,179,165,163,181,183,165,178,175,165,175,163,null,null,154,153,169,155,163,185,158,148,175,160,168,null,185,188,175,160,163,176,171,155,165,158,170,183,165,160,152,null,160,165,174,null,null,165,160,160,158,171,161,155,168,165,188,155,165,null,170,null,168,178,null,165,175,173,175,null,null,160,173,160,168,166,180,155,188,173,165,158,161,175,180,null,178,178]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>sex<\\/th>\\n <th>weight<\\/th>\\n <th>height<\\/th>\\n <th>repwt<\\/th>\\n <th>repht<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3,4,5]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Outlier Removed Data Observation #12 was removed from the data (as seen on page 2 of this data set) because the weight and height of this person seem to have been recorded backwards. # Code to remove the outlier davis <- Davis[-12, ] datatable(davis) {\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\"],[\"M\",\"F\",\"F\",\"M\",\"F\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"F\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"M\",\"F\",\"F\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"M\",\"F\",\"F\",\"M\",\"F\",\"M\",\"M\",\"M\",\"F\",\"M\",\"F\",\"M\",\"M\",\"F\",\"M\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"M\",\"M\",\"M\",\"F\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"F\",\"M\",\"F\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"M\",\"M\",\"F\",\"M\",\"F\",\"F\",\"F\",\"M\",\"M\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"M\",\"M\",\"F\",\"M\",\"F\",\"F\",\"F\",\"F\",\"M\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"M\",\"M\",\"F\",\"M\",\"F\",\"F\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"F\",\"F\",\"M\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"M\",\"F\",\"F\",\"M\",\"F\",\"F\",\"F\",\"M\",\"M\",\"M\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"M\",\"F\",\"F\",\"F\",\"F\",\"F\",\"M\",\"M\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"M\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"M\",\"M\",\"M\",\"F\",\"M\",\"M\",\"M\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"M\",\"F\",\"F\",\"F\",\"M\",\"M\",\"M\",\"M\",\"M\"],[77,58,53,68,59,76,76,69,71,65,70,51,64,52,65,92,62,76,61,119,61,65,66,54,50,63,58,39,101,71,75,79,52,68,64,56,69,88,65,54,80,63,78,85,54,73,49,54,75,82,56,74,102,64,65,66,73,75,57,68,71,71,78,97,60,64,64,52,80,62,66,55,56,50,50,50,63,69,69,61,55,53,60,56,59,62,53,57,57,70,56,84,69,88,56,103,50,52,55,55,63,47,45,62,53,52,57,64,59,84,79,55,67,76,62,83,96,75,65,78,69,68,55,67,52,47,45,68,44,62,87,56,50,83,53,64,62,90,85,66,52,53,54,64,55,55,59,70,88,57,47,47,55,48,54,69,59,58,57,51,54,53,59,56,59,63,66,96,53,76,54,61,82,62,71,60,66,81,68,80,43,82,63,70,56,60,58,76,50,88,89,59,51,62,74,83,81,90,79],[182,161,161,177,157,170,167,186,178,171,175,161,168,163,166,187,168,197,175,180,170,175,173,171,166,169,166,157,183,166,178,173,164,169,176,166,174,178,187,164,178,163,183,179,160,180,161,174,162,182,165,169,185,177,176,170,183,172,173,165,177,180,173,189,162,165,164,158,178,175,173,165,163,166,171,160,160,182,183,165,168,169,167,170,182,178,165,163,162,173,161,184,180,189,165,185,169,159,155,164,178,163,163,175,164,152,167,166,166,183,179,174,179,167,168,184,184,169,178,178,167,178,165,179,169,153,157,171,157,166,185,160,148,177,162,172,167,188,191,175,163,165,176,171,160,165,157,173,184,168,162,150,162,163,169,172,170,169,167,163,161,162,172,163,159,170,166,191,158,169,163,170,176,168,178,174,170,178,174,176,154,181,165,173,162,172,169,183,158,185,173,164,156,164,175,180,175,181,177],[77,51,54,70,59,76,77,73,71,64,75,52,64,57,66,101,62,75,61,124,61,66,70,59,50,61,60,41,100,71,73,76,52,63,65,54,69,86,67,53,80,59,80,82,55,null,null,56,75,85,57,73,107,null,64,65,74,70,58,69,71,76,75,98,59,63,62,51,76,61,66,54,57,50,null,55,64,70,70,60,56,52,55,56,61,66,53,59,56,68,56,86,71,87,57,101,50,52,null,55,63,47,45,63,51,51,55,64,55,90,79,57,67,77,62,83,94,76,66,77,73,68,55,null,56,null,45,68,44,61,89,53,47,84,53,62,null,91,83,68,53,55,55,66,55,55,55,67,86,58,47,45,null,44,58,68,null,null,56,50,54,52,58,58,59,62,66,95,50,75,null,61,null,64,68,null,67,82,68,78,null,null,59,70,56,55,54,75,49,93,86,59,51,61,71,80,null,91,81],[180,159,158,175,155,165,165,180,175,170,174,158,165,160,165,185,165,200,171,178,170,173,170,168,165,168,160,153,180,165,175,173,161,170,175,165,171,175,188,160,178,159,180,175,158,null,null,173,158,183,163,170,185,null,172,null,180,169,170,165,170,175,169,185,160,163,161,155,175,171,175,163,159,161,null,150,158,180,183,163,170,175,163,170,183,175,165,160,160,170,161,183,180,185,160,182,165,153,154,163,175,160,160,173,160,150,164,165,163,183,171,171,179,165,163,181,183,165,178,175,165,175,163,null,null,154,153,169,155,163,185,158,148,175,160,168,null,185,188,175,160,163,176,171,155,165,158,170,183,165,160,152,null,160,165,174,null,null,165,160,160,158,171,161,155,168,165,188,155,165,null,170,null,168,178,null,165,175,173,175,null,null,160,173,160,168,166,180,155,188,173,165,158,161,175,180,null,178,178]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>sex<\\/th>\\n <th>weight<\\/th>\\n <th>height<\\/th>\\n <th>repwt<\\/th>\\n <th>repht<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3,4,5]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Analysis This analysis attempts to model the actual weight of individuals according to their reported weight using a linear regression. Specifically, \\[ \\underbrace{Y_i}_\\text{Actual Weight} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Reported Weight} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] The hypotheses for our study concern the slope of the regression model, \\(\\beta_1\\). If the slope is zero, then there is not a meaningful relationship between the average actual weight of individuals and the reported weight of individuals. \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] \\[ \\alpha = 0.05 \\] A scatterplot of the relationship between weight and reported weights shows a strong positive trend, the correlation being \\(0.986\\). The dots are very tightly clustered around the line. plot(weight ~ repwt, data=davis, pch=21, bg=\"steelblue\", col=\"gray\", xlab=\"Each Individual's Self Reported Weight (kg)\", ylab=\"Their Actual Measured Weight (kg)\", main=\"Weight Perceptions of 182 Exercising Individuals\", las=1) mtext(side=3, text=\"These individuals seem to know their weight fairly accurately.\", cex=0.8) davis.lm <- lm(weight ~ repwt, data=davis) abline(davis.lm, lwd=3, col=rgb(.4,.4,.4,.2)) abline(v=seq(40,120,20), h=seq(40,120,20), lty=2, col=rgb(.6,.6,.6,.2)) # This code run in Console to obtain the correlation # davis %>% summarise(cor(weight, repwt, use=\"complete.obs\")) The equation of the estimated regression equation from the scatterplot above is given by \\[ \\underbrace{\\hat{Y}_i}_\\text{Mean Actual Wt.} = 2.734 + 0.9584 \\underbrace{X_i}_\\text{Reported Wt.} \\] The estimated slope of this regression is 0.9584, which is significantly different from zero \\((p = 1.385e-141)\\). Follows is a complete table showing the summary of the full regression results. davis.lm <- lm(weight ~ repwt, data=davis) pander(summary(davis.lm))   Estimate Std. Error t value Pr(>|t|) (Intercept) 2.734 0.8148 3.355 0.0009672 repwt 0.9584 0.01214 78.93 1.385e-141 Fitting linear model: weight ~ repwt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 182 2.254 0.9719 0.9718 For the interested reader, notice the that value of \\(R^2\\) is the square of the correlation of the scatterplot. It shows that 97.19% of the variation in Actual Measured Weights is explained by this regression model, which is pretty impressive. Interpretation The estimated value of the slope is 0.958 which shows that on average, individuals weigh just a little less than what they reported. It shows that the average actual weight of individuals is 95.84% of the reported weight plus a baseline weight of 2.734 kg. This shows individuals undereporting their weight by about 4% on average. If the slope had been 1 and the y-intercept 0, then the average actual weight would be exactly that of the reported weight. Appropriateness of the above Regression It is arguable whether or not the above analysis is fully appropriate because of the lack of normality of the residuals shown in the Normal Q-Q plot. However, as shown by the residuals versus fitted values plot, there are no real concerns with linearity (the red line appears essentially flat) and the vertical variation in the residuals seems constant enough for all fitted-values to assume constant variance of the errors. There are some odd patterns visible in this plot which are due to the integer values of the data, but this is okay. In the end, with the only problem being a lack of normality in the residuals (the least important assumption to violate in a regression) the evidence is not overwhelming enough to discredit the above analysis. The residuals vs. order plot shows that error terms do not appear to be correlated because there is no pattern in the plot. # Note that this r-chunk uses ```{r, fig.height=3} # to reduce the size of the plots. This makes the plots # look a little nicer and also emphasizes that they are not # as important as the main graphic (scatterplot) of the analysis. par(mfrow=c(1,3)) plot(davis.lm, which=1:2) plot(davis.lm$residuals, main=\"Residuals vs Order\", xlab=\"\", ylab=\"Residuals\") // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Price of Cadillacs – Multiple Linear Regression","url":"325Analyses/Linear Regression/Examples/cadillacsMLR.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Price of Cadillacs – Multiple Linear Regression Note: This “analysis” is meant more as an instructional document than a full example of an analysis. Background The Cad data set consists of 80 observations on Cadillacs. For each vehicle the selling price, mileage, and model was recorded. The first six observations are shown below. Cad <- read.table(\"../../../Data/Cadillac.txt\", header=TRUE, quote=\"\\\"\") knitr::kable(head(Cad)) Price Mileage Model 51154.05 2202 CST-V 49248.16 6685 CST-V 46747.67 15343 CST-V 44130.62 21341 CST-V 44084.91 21367 CST-V 43892.47 23371 CST-V We would like to come up with a model that is useful for accurately predicting the price of a given Cadillac. Analysis Notice a few things about the Cad dataset. First, the response variable Price is a quantitative variable. This is required for both simple and multiple linear regression. Second, notice that the explanatory variables can be either quantitative, like Mileage, or qualitative, like Model. Third, even though there is only one qualitative variable, Model, there are several “levels” to this variable. As shown in the following bar chart, there are 10 of every type of Model other than the “Deville” for which there are 30 observations. This final comment is important when building the mathematical model for the multiple regression. The “levels” of a qualitative variable are not numbers, so how do we include them in a mathematical model? plot(Cad$Model, col='skyblue') For the Cad dataset, the mathematical model for the Price of a vehicle, \\(Y_i\\), is written (rather complicatedly) as \\[ Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\beta_4 X_{i4} + \\beta_5 X_{i5} + \\beta_6 X_{i6} + \\epsilon_i \\] where we still assume \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), but the coefficients and \\(X_i\\) are as follows: Coefficient X-Variable Meaning \\(\\beta_0\\) The average cost of a Cadillac CST with zero miles \\(\\beta_1\\) \\(X_{i1} =\\) vehicle Mileage Effect of each mile on average price \\(\\beta_2\\) \\(X_{i2} = \\left\\{\\begin{array}{ll} 1, & \\text{if Model} = \\text{CTS} \\\\ 0, & \\text{if Model}\\neq \\text{CTS} \\end{array}\\right.\\) How much more or less a Cadillac CTS costs on average over a Cadillac CST \\(\\beta_3\\) \\(X_{i3} = \\left\\{\\begin{array}{ll} 1, & \\text{if Model} = \\text{Deville} \\\\ 0, & \\text{if Model}\\neq \\text{Deville} \\end{array}\\right.\\) How much more or less a Cadillac Deville costs on average over a Cadillac CST \\(\\beta_4\\) \\(X_{i4} = \\left\\{\\begin{array}{ll} 1, & \\text{if Model} = \\text{STS-V6} \\\\ 0, & \\text{if Model}\\neq \\text{STS-V6} \\end{array}\\right.\\) How much more or less a Cadillac STS-V6 costs on average over a Cadillac CST \\(\\beta_5\\) \\(X_{i5} = \\left\\{\\begin{array}{ll} 1, & \\text{if Model} = \\text{STS-V8} \\\\ 0, & \\text{if Model}\\neq \\text{STS-V8} \\end{array}\\right.\\) How much more or less a Cadillac STS-V8 costs on average over a Cadillac CST \\(\\beta_6\\) \\(X_{i6} = \\left\\{\\begin{array}{ll} 1, & \\text{if Model} = \\text{XLR-V8} \\\\ 0, & \\text{if Model}\\neq \\text{XLR-V8} \\end{array}\\right.\\) How much more or less a Cadillac XLR-V8 costs on average over a Cadillac CST We won’t concern ourselves with the details of how to compute the estimates for each parameter, \\(\\beta_j\\). We will let software take care of the maximum likelihood estimation for us. This is done for the current dataset by Cad.lm <- lm(Price ~ Mileage + Model, data=Cad) summary(Cad.lm) ## ## Call: ## lm(formula = Price ~ Mileage + Model, data = Cad) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5027.5 -552.2 281.7 1215.7 3750.2 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 5.156e+04 7.593e+02 67.904 < 2e-16 *** ## Mileage -3.379e-01 2.400e-02 -14.080 < 2e-16 *** ## ModelCTS -1.549e+04 8.523e+02 -18.172 < 2e-16 *** ## ModelDeville -8.636e+03 6.939e+02 -12.446 < 2e-16 *** ## ModelSTS-V6 -7.826e+03 8.499e+02 -9.208 7.59e-14 *** ## ModelSTS-V8 -2.357e+03 8.501e+02 -2.773 0.00705 ** ## ModelXLR-V8 1.772e+04 8.499e+02 20.852 < 2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 1900 on 73 degrees of freedom ## Multiple R-squared: 0.9667, Adjusted R-squared: 0.964 ## F-statistic: 353.3 on 6 and 73 DF, p-value: < 2.2e-16 Notice how simple it was to put the model into R, but how complicated the output is. This is because R recognizes that the variable Model is qualitative and creates several “dummy” variables automatically. These dummy variables are 1 if Model = the given model and 0 if the Model is not the given model. Notice how similar the R summary() output looks to the table of Coefficients, \\(X\\), and Meaning given previously. Knowing how to interpret the coefficents in a multiple regression is important. While the interpretation is similiar to that of simple linear regression, an extra statement is necessary. For example, \\(\\beta_1\\) denotes the change in the expected value of Price \\((E\\{Y\\})\\) for a unit change in Mileage, \\(X_{i1}\\), holding all other variables constant. This last statement is the only difference in interpretation of the coefficients between simple linear regression and multiple linear regression, but it is very important. It states that to consider the effect of one variable on Price we have to consider all the other variables to be held constant. For example, the effect of Mileage is to decrease the average Price of a Cadillac by $0.34 \\((b_1 = -3.379e-01)\\) for any fixed value of Model. Said differently, choose any value of Model you want, within that Model of Cadillac, the effect of Mileage is to decrease the average price of that Model by 34 cents. While there is usually no way to graphically show a multiple regression model like you can in simple regression using a scatterplot, we can depict the current multiple regression as follows. This is because one variable is quantitative and one variable is qualitative. This allows us to place the quantitative variable on the x-axis and make the different colored lines represent the different levels of the qualitative variable. palette(c(\"skyblue4\",\"firebrick\",\"skyblue\",\"sienna1\",\"gray\",\"sienna4\")) plot(Price ~ Mileage, data=Cad, pch=16, col=Cad$Model, xlim=c(0,50000)) abline(Cad.lm$coef[1] , Cad.lm$coef[2], col=palette()[1]) abline(Cad.lm$coef[1]+Cad.lm$coef[3], Cad.lm$coef[2], col=palette()[2]) abline(Cad.lm$coef[1]+Cad.lm$coef[4], Cad.lm$coef[2], col=palette()[3]) abline(Cad.lm$coef[1]+Cad.lm$coef[5], Cad.lm$coef[2], col=palette()[4]) abline(Cad.lm$coef[1]+Cad.lm$coef[6], Cad.lm$coef[2], col=palette()[5]) abline(Cad.lm$coef[1]+Cad.lm$coef[7], Cad.lm$coef[2], col=palette()[6]) legend(\"topright\",Cad.lm$xlevels$Model, lty=1, lwd=5, col=palette(), cex=0.7) Finally, in multiple linear regression it is important to note that the expected value, \\(E\\{Y_i\\}\\), is now given by \\[ E\\{Y_i\\} = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\beta_4 X_{i4} + \\beta_5 X_{i5} + \\beta_6 X_{i6} \\] and that the estimated regression line (technically a hyperplane now) \\(\\hat{Y}_i\\) is given by \\[ \\hat{Y}_i = b_0 + b_1 X_{i1} + b_2 X_{i2} + b_3 X_{i3} + b_4 X_{i4} + b_5 X_{i5} + b_6 X_{i6} \\] Checking for departures from the model assumptions in multiple linear regression is carried out using the residuals in precisely the same way as was done for simple linear regression. Refer to the Simple Linear Regression document for the details. The only difference is that it is more typical to have extra explanatory variables in multiple regression that may or may not be of benefit to include in the model. Thus, it is possible in multiple regression to check for departures from Assumption 6, that one or several important predictor variables have been omitted from the model, by trying to add additional variables into the model. This could be done by going back and finding out other information about each vehicle. The current data uses all known information in the model. To check the appropriateness of the multiple linear regression model we run the following commands. # Check for departures 1, 2, 3 and 4: par(mfrow=c(1,2), mai=c(1,1,1,.2)) plot(Cad.lm, which=1:2) # Check for departure 5: par(mai=c(.8,1,0.1,.2)) plot(Cad.lm$residuals, ylab=\"Residuals\", las=1, cex.axis=.8) The normality of the residuals is questionable. There is also some evidence of a potentially non-linear trend in the residuals versus fitted values plot. However, the violations are not overly dramatic, thus this regression could well be presented as valid and useful for predicting the price of a Cadillac. Interactions Notice that if interactions between each dummy variable for model and the mileage are included in the model, then each set of points gets its own slope. In other words, the slope of the lines are not all forced to be the same anymore. However, the phrase “holding all other variables constant” is no longer applicable. Thus, interpretation of the model is severely decreased. Notice, that with this expanded model the residuals seem to be doing much better. The single difficulty now is that the Deville Model seems to have two distinctly different types of vehicles that our model does poorly at capturing. # Check for departures 1, 2, 3 and 4: par(mfrow=c(1,2), mai=c(1,1,1,.2)) plot(Cad.lm, which=1:2) # Check for departure 5: par(mai=c(.8,1,0.1,.2)) plot(Cad.lm$residuals, ylab=\"Residuals\", las=1, cex.axis=.8) A final important note to consider is that while the model appears to be visibly improved by allowing the different slopes, no interaction term is actually significant. Thus, even though there appears to be some advantage to the more coplex model, the statistics show we are likely overfitting the data, trying to say too much with the data we have. Thus, for these data it is recommended to stay with the simpler and more interpretable model presented previously. It would also be recommended to go and figure out what makes for two different classes of the Deville model. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Stopping Distances of Cars – Simple Linear Regression","url":"325Analyses/Linear Regression/Examples/carsSLR.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Stopping Distances of Cars – Simple Linear Regression library(pander) library(car) library(DT) library(mosaic) library(tidyverse) Poor Brakes Every licensed driver knows that when you press your brakes, the car stops. But how quickly? And how much does the stopping distance depend on the speed of the vehicle? This analysis, though quite outdated unless you are driving around a vintage 1920’s vehicle, bears a solution on this question with the equation \\[ \\text{Stopping Distance} = -17.6 + 3.9 \\times \\text{(Speed of Vehicle (mph))} \\] This shows that (on average) for every additional mile per hour increase in the speed the vehicle is going, the car will take an additional 3.9 ft (essentially 4 more feet) to stop. Keep in mind however that actual stopping distances at higher speeds vary as much as \\(\\pm 30\\) feet from this general estimate. plot(dist ~ speed, data=cars, pch=20, col=\"firebrick\", cex=1.2, las=1, xlab=\"Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied\", ylab=\"Distance (ft) it took the Vehicle to Stop\", main=\"Don't Step in front of a Moving 1920's Vehicle...\") mtext(side=3, text=\"...they take a few feet to stop.\", cex=0.7, line=.5) legend(\"topleft\", legend=\"Stopping Distance Experiment\", bty=\"n\") cars.lm <- lm(dist ~ speed, data=cars) abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3)) abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2)) Analysis Details A regression was performed in R to obtain the above results. The code and output of the regression is included below. The model assumed by the regression analysis was \\[ \\underbrace{Y_i}_\\text{Distance} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Speed} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] Summary of the Regression cars.lm <- lm(dist ~ speed, data=cars) pander(summary(cars.lm))   Estimate Std. Error t value Pr(>|t|) (Intercept) -17.58 6.758 -2.601 0.01232 speed 3.932 0.4155 9.464 1.49e-12 Fitting linear model: dist ~ speed Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 50 15.38 0.6511 0.6438 There does appear to be some evidence of a relationship. Assuming the relationship is linear, the equation of the fitted line shown in the plot above is \\[ \\underbrace{\\hat{Y}_i}_\\text{Ave. Dist.} = -17.58 + 3.93 \\underbrace{X_i}_\\text{Speed} \\] A test of the hypotheses that \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] shows that the slope is significant \\((p = 1.49e-12)\\). Hence the relationship is meaningful. Assuming the model is appropriate, then we estimate that increasing the speed by 1 mph will result in an extra 3.93 feet of stopping distance (95% CI: (3.1, 4.77) feet). As a general rule of thumb we suggest “4 extra feet of stopping distance for every 1 mph increase in speed.” Note that the baseline of 0 mph actually has a negative stopping distance. Thus we restrict the interpretability of the model to speeds of 5 or more mph. At a speed of 5 mph or less, the stopping distance is essentially zero. (Although, we do note that one vehicle took 10 feet to stop at a speed of 4 mph. Bad driver perhaps? At a minimum the vehicle needs new brakes.) Validation of the Model Appropriateness A check on the appropriateness of the model shows some question about whether the relation should be assumed to be linear or not. The residuals do not depart too dramatically from normality, but case #23 does go out of bounds. We considered removing outliers 23 and 49, but decided to leave them in and deal with the difficulties this causes. Efforts were made in an attempt to correct the potential difficulty with linearity (see the Transformations section below). However, the above model and interpretation was determined to be the most simple, although somewhat questionable, for these data. par(mfrow=c(1,3)) plot(cars.lm, which=1) qqPlot(cars.lm$residuals, id=FALSE) plot(cars.lm$residuals, main=\"Residuals vs Order\") The Data Here is the data from the experiment for those interested. datatable(cars) {\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\"],[4,4,7,7,8,9,10,10,10,11,11,12,12,12,12,13,13,13,13,14,14,14,14,15,15,15,16,16,17,17,17,18,18,18,18,19,19,19,20,20,20,20,20,22,23,24,24,24,24,25],[2,10,4,22,16,10,18,26,34,17,28,14,20,24,28,26,34,34,46,26,36,60,80,20,26,54,32,40,32,40,50,42,56,76,84,36,46,68,32,48,52,56,64,66,54,70,92,93,120,85]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>speed<\\/th>\\n <th>dist<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Honda Civic vs. Toyota Corolla","url":"325Analyses/Linear Regression/Examples/CivicVsCorollaMLR.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Honda Civic vs. Toyota Corolla library(mosaic) library(car) library(DT) library(pander) ccCars <- read.csv(\"../../../Data/CivicCorolla.csv\", header=TRUE) The Comparison Two very popular cars on the road today are the Honda Civic and the Toyota Corolla. This analysis looks to see which car costs more when brand new and how well each vehicle holds its value over time. The goal is to see which one is the better buy from a purely monetary standpoint. It assumes the driving experience, pride of ownership, and other aspects are equal between the two vehicles. The Data To make the comparison as fair as possible, only the Civic LX and Corolla LE models are used in this analysis. Data was collected from KSL Classifieds on October 30th, 2018 with two separate searches. The first used “Honda, Civic, LX” and the second used “Toyota, Corolla, LE”. In both cases, the mileage was also specified to be less than 50,000 miles and from 2015 or newer (up to 2019). Under those search criterion, the listing price and current mileage was recorded for 23 different Civic LX’s and 25 different Corolla LE’s. The data is shown below. While the year the vehicle was manufactured was also recorded, it is not being considered in this analysis. datatable(ccCars) {\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\"],[\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Civic\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\",\"Corolla\"],[\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LX\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\",\"LE\"],[2013,2017,2016,2017,2015,2015,2015,2016,2017,2015,2015,2016,2015,2017,2018,2017,2016,2016,2015,2018,2019,2018,2018,2016,2017,2016,2017,2017,2016,2015,2017,2017,2015,2017,2017,2017,2018,2018,2018,2017,2017,2015,2017,2017,2019,2019,2018,2019],[52911,27966,21639,21893,39868,26416,29031,37437,23999,26611,45110,15355,24988,5656,9942,17378,16296,35719,46813,11,7,7,5,29384,49359,35135,42574,42785,29404,19000,28605,10729,19673,8938,10729,44202,11363,6276,17573,28605,43609,41436,46522,45211,15,11,15,11],[9900,15495,14995,15595,11498,14000,12495,15000,16971,14000,8499,17466,12696,16570,18350,17277,12499,14995,8499,19429,20187,19430,19427,14994,14577,13700,15266,15266,14976,11000,15368,16367,14550,16990,16367,13956,15995,15495,18499,15368,14280,14995,14720,13991,18269,18410,19190,18320]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>Model<\\/th>\\n <th>Trim<\\/th>\\n <th>Year<\\/th>\\n <th>Mileage<\\/th>\\n <th>Price<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[3,4,5]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} The Model A multiple linear regression model was applied to the data to obtain two regression lines, one for each vehicle. To be precise, the (slightly involved) model is given by: \\[ \\underbrace{Y_i}_{\\text{Price}} = \\overbrace{\\beta_0 + \\beta_1 \\underbrace{X_{i1}}_{\\text{Mileage}}}^{\\text{Civic Line}} + \\overbrace{\\beta_2 \\underbrace{X_{i2}}_{\\text{1 if Corolla}} + \\beta_3 \\underbrace{X_{i1} X_{i2}}_{\\text{Interaction}}}^{\\text{Corolla Adjustments to Line}} + \\epsilon_i \\] where \\(\\epsilon_i\\sim N(0,\\sigma^2)\\) and \\(X_{i2} = 0\\) when the vehicle is a Civic and \\(X_{i2} = 1\\) when the vehicle is a Corolla. This forced 0, 1 encoding for \\(X_{i2}\\) produces the following models. Vehicle Value of \\(X_{i2}\\) Resulting Model Civic \\(X_{i2} = 0\\) \\(Y_i = \\beta_0 + \\beta_1 X_{i1} + \\epsilon_i\\) Corolla \\(X_{i2} = 1\\) \\(Y_i = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) X_{i1} + \\epsilon_i\\) Showing these separated models for each vehicle is to demonstrate that \\(\\beta_2\\) is the difference between the y-intercepts for the Civic and Corolla. Similarly, \\(\\beta_3\\) is the difference in the slopes for the two models. Since it is logical to assume that the average cost (y-intercept) of these vehicles is greater than zero, and that all vehicles lose value over time, there is no need to test the coefficients of \\(\\beta_0\\) or \\(\\beta_1\\) for differences from zero. These are the baseline intercept and slope for the Civic vehicles, and we will simply assume they are different from zero. What is of interest however, is whether the regression lines for the Civic and Corolla have different y-intercepts and different slopes. Thus, there is interest in testing \\(\\beta_2\\) and \\(\\beta_3\\) for differences from zero. Test for Equal y-Intercepts If \\(\\beta_2\\) is zero in the combined regression model, then the y-intercepts, which represent the average cost of a brand new vehicle, are the same for the Civic and Corolla. If \\(\\beta_2\\) is greater than zero, then the Corolla costs more on average than the Civic when brand new, and if \\(\\beta_2\\) is less than zero, then the Corolla costs less. These hypotheses will be judged at the \\(\\alpha=0.05\\) significance level. \\[ H_0: \\beta_2 = 0 \\quad \\text{(Equal average cost when brand new)} \\\\ H_a: \\beta_2 \\neq 0 \\quad \\text{(Non-equal average cost when brand new)} \\] Test for Equal slopes If \\(\\beta_3\\) is zero, then the slopes of the two lines are the same. This would imply that the rate of depreciation is the same for both the Civic and the Corolla. However, if the slopes differ, i.e., \\(\\beta_3 \\neq 0\\), then one vehicle loses its value faster than the other. These hypotheses will be judged at the \\(\\alpha=0.05\\) significance level. \\[ H_0: \\beta_3 = 0 \\quad \\text{(Equal rates of depreciation)} \\\\ H_a: \\beta_3 \\neq 0 \\quad \\text{(Non-equal rates of depreciation)} \\] The Results The scatterplot below of Price vs. Mileage of Civics and Corollas, and the corresponding regression summary output below the plot, show that the Toyota Corolla costs $2,002 less that the Honda Civic on average when brand new (p-value = 0.01448), and holds its value better over time than the Honda Civic. More specifically, the Civic depreciates $0.12 (12 cents) more on average each mile driven than does the Corolla (p-value = 0.00007969). To demonstrate how much this could impact the owner of the vehicle monetarily, assume that both vehicles were purchased at their average new prices of $19,408 (Civic) and $17,406 (Corolla). Then, assume each vehicle was driven for 50,000 miles then sold for their average 50,000 miles prices of $9,779 (Civic) and $13,786 (Corolla). This would result in a loss of $9,629 for the Civic as compared to just $3,620 for the Corolla. That implies that over the first 50,000 miles, you could expect to save around $6,000 owning the Corolla rather than the Civic. That’s quite a bit of cash! ## Code for the linear regression cars_lm <- lm(Price ~ Mileage + Model + Mileage:Model, data=ccCars) ## Code to obtain 50,000 mileage average prices: (only run in Console) # predict(cars_lm, data.frame(Mileage=c(50000,50000), Model=c(\"Civic\",\"Corolla\"))) ## Code for the plot: # Declare the color palette for the plot: palette(c(\"gray25\",\"#86CA48\")) # Set some plotting arrangment commands: par(fg=\"darkgray\", #colors of the bounding box. col.lab=\"gray35\", #color of the xlab and ylab. mai=c(1,1,.8,.4), #controls outside margins: bottom, left, top, right. mgp=c(3.5,1,0)) #moves the xlab and ylab out to 3.5 lines. # Draw plot with default axis labels turned off (xaxt, yaxt): plot(Price ~ Mileage, data=ccCars, col=Model, pch=16, main=\"Cost and Depreciation Comparison \\n Honda Civic vs. Toyota Corolla\", ylab=\"Listing Price of the Vehicle\", xlab=\"Mileage of the Vehicle\", yaxt=\"n\", xaxt=\"n\", xlim=c(0,60000), ylim=c(8000,20000)) # Add grid lines: abline(v=c(0,10000, 20000, 30000, 40000, 50000), h=c(10000,12000,14000,16000,18000), col=rgb(.8,.8,.8,.4), lty=2) # Add manually specified axes: axis(1, at=c(10000,20000,30000,40000,50000), labels=c(\"10,000\", \"20,000\", \"30,000\", \"40,000\", \"50,000\"), col.axis=\"gray55\") axis(2, at=c(10000, 12000, 14000, 16000, 18000), labels=c(\"$10k\",\"$12k\",\"$14k\",\"$16k\",\"$18k\"), col.axis=\"gray55\", las=2) # Add legend: legend(\"topright\", col=palette(), pch=21, legend=c(\"Honda Civic LX\", \"Toyota Corolla LE\"), bty=\"n\", text.col = palette()) # Add the regression lines: abline(19408, -0.1926, col=\"gray55\") #abline(intercept, slope, ...) abline(19408-2002, -0.1926+0.1202, col=palette()[2]) # Add text and dot for y-intercepts: text(1300,19408, \"$19,408\", cex=0.6, col=\"gray55\", pos=3, offset=.25) points(0,19408, pch=16, col=\"gray55\", cex=0.5) text(900,19408-2002, \"$17,406\", cex=0.6, col=\"#86CA48\", pos=3, offset=.25) points(0,19408-2002, pch=16, col=\"#86CA48\", cex=0.5) # Add text for depreciation rates: text(50000, 13900, \"Drops $0.07 per mile\", cex=0.6, col=\"#86CA48\", pos=4) text(46000, 8010, \"Drops $0.19 per mile\", cex=0.6, col=\"gray55\", pos=4) The following table provides the actual numbers used in the conclusions given above. ## Print regression summary output pander(cars_lm, caption=\"Regression Summary output for Price on Mileage according to Model Type\") Regression Summary output for Price on Mileage according to Model Type   Estimate Std. Error t value Pr(>|t|) (Intercept) 19408 571.7 33.95 3.358e-33 Mileage -0.1926 0.02078 -9.268 6.606e-12 ModelCorolla -2002 786.6 -2.546 0.01448 Mileage:ModelCorolla 0.1202 0.02763 4.35 7.969e-05 Note that in the table above, “Mileage” is the coefficient estimate for \\(\\beta_1\\), “ModelCorolla” is the coefficient estimate for \\(\\beta_2\\), “Mileage:ModelCorolla” is the coefficient estimate for \\(\\beta_3\\), and “(Intercept)” is the coefficient estimate for \\(\\beta_0\\). Thus, “ModelCorolla” is the difference between the two y-intercepts and “Mileage:ModelCorolla” is the difference between the two slopes. The Limitations The results of this analysis should be taken with a grain of salt for a few reasons. First, it should be remembered that these vehicles were only sampled from the Utah based Classifieds “KSL Classifieds”. Also, the data was sampled on a single day, so these results don’t actually show pricing trends over time. They just show the current value of various mileages. Most importantly the regression model assumptions are not fully satisfied as detailed in the next paragraph. This leaves some question as to validity of the specific detailed results of this analysis, though the general pattern in the data seems to demonstrate the Corolla costing less brand new and holding its value better over time. The linearity of the data looks to be okay because of the relative flatness of the red lowess line in the residuals vs fitted plot. Also, the vertical variability of the dots in the residuals vs. fitted plot seems to be roughly constant across all fitted values, so constant variance can be assumed. There are two potential outliers shown in the residuals vs fitted plot (observations 17 and 30 in the original dataset) that could be unduly influencing the regression. It may be worth removing these to see how they are effecting the results of the regression. The most important violation of the model assumptions to note is the lack of normality of the residuals shown in the Q-Q Plot of the residuals. This is seen by how much the dots depart the green dashed lines bounding the “zone of normality” where data would typically land if it truly was normally distributed. There does not appear to be any independence issues in the data however, as witnessed by the random pattern in the residuals vs. order plot. # This chunk uses ```{r, fig.height=3} to shrink the heigh of the graphs. par(mfrow=c(1,3)) plot(cars_lm, which=1) qqPlot(cars_lm$residuals) ## [1] 30 17 mtext(side=3,text=\"Q-Q Plot of Residuals\") plot(cars_lm$residuals, type=\"b\") mtext(side=3, text=\"Residuals vs. Order\") // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Weather Analysis Lesson","url":"325Analyses/Linear Regression/MulitpleLinearRegressionLesson.html","content":"Code Show All Code Hide All Code Weather Analysis Lesson Analysis Goal Now write up an analysis that studies something like how well the MIN temperature predicts the MAX temperature of the day in the two cities you selected. Of course, you are welcome to do any multiple regression you want with the data, but the recommendation would be to just run a “two-lines” model with the x=MIN, y=MAX, and color of the dots the NAME column. Getting the Data 1) Install GSODR Open the Weather.Rmd file in your Statistics Notebook in the Linear Regression folder and then make sure you are able to do the following: install.packages(\"GSODR\") library(GSODR) load(system.file(\"extdata\", \"isd_history.rda\", package = \"GSODR\")) View(isd_history) # install.packages(\"GSODR\") library(GSODR) library(mosaic) library(tidyverse) library(pander) library(car) library(DT) #run: install.packages(\"GSODR\") # to get the GSODR package. You'll need this package to pull in your weather data. load(system.file(\"extdata\", \"isd_history.rda\", package = \"GSODR\")) #Run this in your console to see the Country Names you can pick from: View(isd_history) #GOAL: select the STNID (station ID) for two different weather stations. #For example, Rexburg is STNID == \"726818-94194\" #Once you have two STNID values selected, go to the next R-chunk. #To see what columns mean, go here: https://cran.r-project.org/web/packages/GSODR/vignettes/GSODR.html#appendices 2) Picking two cities PART A) View isd_history Look and choose two cities in the isd_history data set. Then run a similar code to get your station information for your weather stations. Note: You might have to change the for your cities if one of them doesn’t have data available that year! FORMAT: city1 <- get_GSOD(years = 2023, station = \"STNID #\") city2 <- get_GSOD(years = 2023, station = \"STNID #\") ### EXAMPLE: REXBURG <- get_GSOD(years = 2021, station = \"726818-94194\") SALTLAKE <- get_GSOD(years = 2021, station = \"725720-24127\") PART B) Two into one! Finally, join your two datasets together into one dataset: FORMAT: weather <- rbind(city1,city2) ### EXAMPLE: weather2 <- rbind(REXBURG, SALTLAKE) PART C) Choosing Variables Choose TWO quantitative variables (for y and x1) and the city names to be your ONE categorical variable (for x2) FORMAT weather2 <- weather %>% select(c(NAME, MAX, MIN) weather3 <- weather2 %>% select(c(NAME, MAX, MIN)) datatable(weather3, options(list=c(3,10,30))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"223\",\"224\",\"225\",\"226\",\"227\",\"228\",\"229\",\"230\",\"231\",\"232\",\"233\",\"234\",\"235\",\"236\",\"237\",\"238\",\"239\",\"240\",\"241\",\"242\",\"243\",\"244\",\"245\",\"246\",\"247\",\"248\",\"249\",\"250\",\"251\",\"252\",\"253\",\"254\",\"255\",\"256\",\"257\",\"258\",\"259\",\"260\",\"261\",\"262\",\"263\",\"264\",\"265\",\"266\",\"267\",\"268\",\"269\",\"270\",\"271\",\"272\",\"273\",\"274\",\"275\",\"276\",\"277\",\"278\",\"279\",\"280\",\"281\",\"282\",\"283\",\"284\",\"285\",\"286\",\"287\",\"288\",\"289\",\"290\",\"291\",\"292\",\"293\",\"294\",\"295\",\"296\",\"297\",\"298\",\"299\",\"300\",\"301\",\"302\",\"303\",\"304\",\"305\",\"306\",\"307\",\"308\",\"309\",\"310\",\"311\",\"312\",\"313\",\"314\",\"315\",\"316\",\"317\",\"318\",\"319\",\"320\",\"321\",\"322\",\"323\",\"324\",\"325\",\"326\",\"327\",\"328\",\"329\",\"330\",\"331\",\"332\",\"333\",\"334\",\"335\",\"336\",\"337\",\"338\",\"339\",\"340\",\"341\",\"342\",\"343\",\"344\",\"345\",\"346\",\"347\",\"348\",\"349\",\"350\",\"351\",\"352\",\"353\",\"354\",\"355\",\"356\",\"357\",\"358\",\"359\",\"360\",\"361\",\"362\",\"363\",\"364\",\"365\",\"366\",\"367\",\"368\",\"369\",\"370\",\"371\",\"372\",\"373\",\"374\",\"375\",\"376\",\"377\",\"378\",\"379\",\"380\",\"381\",\"382\",\"383\",\"384\",\"385\",\"386\",\"387\",\"388\",\"389\",\"390\",\"391\",\"392\",\"393\",\"394\",\"395\",\"396\",\"397\",\"398\",\"399\",\"400\",\"401\",\"402\",\"403\",\"404\",\"405\",\"406\",\"407\",\"408\",\"409\",\"410\",\"411\",\"412\",\"413\",\"414\",\"415\",\"416\",\"417\",\"418\",\"419\",\"420\",\"421\",\"422\",\"423\",\"424\",\"425\",\"426\",\"427\",\"428\",\"429\",\"430\",\"431\",\"432\",\"433\",\"434\",\"435\",\"436\",\"437\",\"438\",\"439\",\"440\",\"441\",\"442\",\"443\",\"444\",\"445\",\"446\",\"447\",\"448\",\"449\",\"450\",\"451\",\"452\",\"453\",\"454\",\"455\",\"456\",\"457\",\"458\",\"459\",\"460\",\"461\",\"462\",\"463\",\"464\",\"465\",\"466\",\"467\",\"468\",\"469\",\"470\",\"471\",\"472\",\"473\",\"474\",\"475\",\"476\",\"477\",\"478\",\"479\",\"480\",\"481\",\"482\",\"483\",\"484\",\"485\",\"486\",\"487\",\"488\",\"489\",\"490\",\"491\",\"492\",\"493\",\"494\",\"495\",\"496\",\"497\",\"498\",\"499\",\"500\",\"501\",\"502\",\"503\",\"504\",\"505\",\"506\",\"507\",\"508\",\"509\",\"510\",\"511\",\"512\",\"513\",\"514\",\"515\",\"516\",\"517\",\"518\",\"519\",\"520\",\"521\",\"522\",\"523\",\"524\",\"525\",\"526\",\"527\",\"528\",\"529\",\"530\",\"531\",\"532\",\"533\",\"534\",\"535\",\"536\",\"537\",\"538\",\"539\",\"540\",\"541\",\"542\",\"543\",\"544\",\"545\",\"546\",\"547\",\"548\",\"549\",\"550\",\"551\",\"552\",\"553\",\"554\",\"555\",\"556\",\"557\",\"558\",\"559\",\"560\",\"561\",\"562\",\"563\",\"564\",\"565\",\"566\",\"567\",\"568\",\"569\",\"570\",\"571\",\"572\",\"573\",\"574\",\"575\",\"576\",\"577\",\"578\",\"579\",\"580\",\"581\",\"582\",\"583\",\"584\",\"585\",\"586\",\"587\",\"588\",\"589\",\"590\",\"591\",\"592\",\"593\",\"594\",\"595\",\"596\",\"597\",\"598\",\"599\",\"600\",\"601\",\"602\",\"603\",\"604\",\"605\",\"606\",\"607\",\"608\",\"609\",\"610\",\"611\",\"612\",\"613\",\"614\",\"615\",\"616\",\"617\",\"618\",\"619\",\"620\",\"621\",\"622\",\"623\",\"624\",\"625\",\"626\",\"627\",\"628\",\"629\",\"630\",\"631\",\"632\",\"633\",\"634\",\"635\",\"636\",\"637\",\"638\",\"639\",\"640\",\"641\",\"642\",\"643\",\"644\",\"645\",\"646\",\"647\",\"648\",\"649\",\"650\",\"651\",\"652\",\"653\",\"654\",\"655\",\"656\",\"657\",\"658\",\"659\",\"660\",\"661\",\"662\",\"663\",\"664\",\"665\",\"666\",\"667\",\"668\",\"669\",\"670\",\"671\",\"672\",\"673\",\"674\",\"675\",\"676\",\"677\",\"678\",\"679\",\"680\",\"681\",\"682\",\"683\",\"684\",\"685\",\"686\",\"687\",\"688\",\"689\",\"690\",\"691\",\"692\",\"693\",\"694\",\"695\",\"696\",\"697\",\"698\",\"699\",\"700\",\"701\",\"702\",\"703\",\"704\",\"705\",\"706\",\"707\",\"708\",\"709\",\"710\",\"711\",\"712\",\"713\",\"714\",\"715\",\"716\",\"717\",\"718\",\"719\",\"720\",\"721\",\"722\",\"723\",\"724\",\"725\",\"726\",\"727\",\"728\",\"729\",\"730\"],[\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"REXBURG-MADISON COUNTY APT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY INTERNATIONAL AIRPORT\",\"SALT LAKE CITY IN"},{"title":" Simple Linear Regression : BYU-Idaho Math Tutoring ","url":"325Analyses/Linear Regression/MySimpleLinearRegression.html","content":"Code Show All Code Hide All Code Simple Linear Regression : BYU-Idaho Math Tutoring library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) ExamsTutor <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv\") Background Brigham Young University-Idaho offers free tutoring for all students in various subjects through personal tutors, group tutoring, or drop-in labs. The introductory math classes (Math 100A, 100B, and 101) at BYU-Idaho collaborate with the university’s Math Study Center by assigning “Tutor Visits.” These assignments require students to visit the free Math Study Center for help with challenge questions. Completing these tutor visits contributes to the Tutoring category, which accounts for 10% of the final grade. The purpose of these assignments is twofold: to encourage students to utilize available resources and to enhance their learning. Ultimately, tutoring aims to improve performance in other areas, such as assignments and tests. In this study, we will address the following question: Does the amount of tutoring a student receives correlate with their chapter exam scores? To see the data, click the tab below Hide Data Show Data In this study, we will be using students from Math 100B (Beginning Algebra). The rows represents a student, and the columns show the students’ Tutoring Final Score and their Chapter Exam Final Score. Tutoring Final Scores are calculated by how many Tutor Visit assignments the student completed and then averaged to produce a final score, and Chapter Exam Final Scores are given by averaging their Chapter Exam Scores throughout the semester into an overall Exam Final Score. datatable(ExamsTutor, options(list=c(3,10,30))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\"],[94.43000000000001,89.43000000000001,99.43000000000001,96.86,84.29000000000001,93.56999999999999,83.56999999999999,92,84.29000000000001,94.14,88,95.29000000000001,90.70999999999999,10.29,85.70999999999999,77.43000000000001,90.43000000000001,91.86,79.14,81.43000000000001,92.86,96.29000000000001,72.86,98.86,81.29000000000001,77.43000000000001,93.14,78.29000000000001,84.56999999999999,83.43000000000001,95,97.29000000000001,91.43000000000001,66.29000000000001,76.56999999999999,85.56999999999999,93.29000000000001,64.86,86.56999999999999,89,85.29000000000001,87.14,88.29000000000001,94.70999999999999,95.43000000000001,92.70999999999999,98.86,91.29000000000001,78.56999999999999,91.56999999999999,92.29000000000001,93.14,97.86,89,97.14,97.86,95,96.70999999999999,76.56999999999999,97.86,86.86,73.14,92.43000000000001,93.86,70.29000000000001,91,85.70999999999999,100,93.56999999999999,60,96.29000000000001],[96.15000000000001,76.92,96.15000000000001,100,96.15000000000001,100,84.62,76.92,96.15000000000001,100,96.15000000000001,100,19.23,15.38,96.15000000000001,100,100,50,92.31,69.23,92.31,100,88.45999999999999,100,92.31,100,100,57.69,100,96.15000000000001,100,100,100,65.38,92.31,100,96.15000000000001,11.54,100,100,23.08,80.77,100,96.15000000000001,100,76.92,100,96.15000000000001,76.92,100,100,96.15000000000001,88.45999999999999,84.62,100,100,100,88.45999999999999,26.92,100,92.31,57.69,96.15000000000001,100,65.38,96.15000000000001,96.15000000000001,100,100,26.92,100]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>Exams Final Score<\\/th>\\n <th>Tutoring Final Score<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Exams Final Score\",\"targets\":1},{\"name\":\"Tutoring Final Score\",\"targets\":2}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Hypothesis In this study, the following model show the relationship between a student’s Final Chapter Exam Score through a linear regression. \\[\\underbrace{Y_i}_\\text{Exam Final Scores} = \\overbrace{\\beta_0}^\\text{Y- intercept} + \\overbrace{\\beta_1}^\\text{Slope} \\underbrace{X_i}_\\text{Final Tutoring Scores} + \\epsilon_i \\space where \\space \\epsilon_i \\sim N(0,\\sigma^2)\\] In linear regression, the key points of interest are the y-intercept and the slope. The y-intercept isn’t particularly helpful in this case, as it only tells us the average final exam score for students who don’t receive tutoring. Instead, we’ll focus on the slope, which reveals how exam scores change relative to the percentage of tutoring a student receives. Our hypotheses will be based on this relationship: \\[H_0 : \\beta_1 = 0\\] \\[H_a :\\beta_1 \\neq 0\\] Additionally, our level of significance will be: \\[\\alpha = 0.05\\] Analysis A scatter plot of Tutoring Final Scores vs. Exam Final Scores below shows a moderately positive relationship. The correlation coefficient of 0.6323 indicates a moderate linear association between these variables. Hover over each dot to see specific scores TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) + geom_point(size=1.5, color = \"darkolivegreen\", alpha =0.5) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"darkgreen\")+ labs(title=\"BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests\") + theme_minimal() ggplotly(TTExams) {\"x\":{\"data\":[{\"x\":[96.150000000000006,76.920000000000002,96.150000000000006,100,96.150000000000006,100,84.620000000000005,76.920000000000002,96.150000000000006,100,96.150000000000006,100,19.23,15.380000000000001,96.150000000000006,100,100,50,92.310000000000002,69.230000000000004,92.310000000000002,100,88.459999999999994,100,92.310000000000002,100,100,57.689999999999998,100,96.150000000000006,100,100,100,65.379999999999995,92.310000000000002,100,96.150000000000006,11.539999999999999,100,100,23.079999999999998,80.769999999999996,100,96.150000000000006,100,76.920000000000002,100,96.150000000000006,76.920000000000002,100,100,96.150000000000006,88.459999999999994,84.620000000000005,100,100,100,88.459999999999994,26.920000000000002,100,92.310000000000002,57.689999999999998,96.150000000000006,100,65.379999999999995,96.150000000000006,96.150000000000006,100,100,26.920000000000002,100],\"y\":[94.430000000000007,89.430000000000007,99.430000000000007,96.859999999999999,84.290000000000006,93.569999999999993,83.569999999999993,92,84.290000000000006,94.140000000000001,88,95.290000000000006,90.709999999999994,10.289999999999999,85.709999999999994,77.430000000000007,90.430000000000007,91.859999999999999,79.140000000000001,81.430000000000007,92.859999999999999,96.290000000000006,72.859999999999999,98.859999999999999,81.290000000000006,77.430000000000007,93.140000000000001,78.290000000000006,84.569999999999993,83.430000000000007,95,97.290000000000006,91.430000000000007,66.290000000000006,76.569999999999993,85.569999999999993,93.290000000000006,64.859999999999999,86.569999999999993,89,85.290000000000006,87.140000000000001,88.290000000000006,94.709999999999994,95.430000000000007,92.709999999999994,98.859999999999999,91.290000000000006,78.569999999999993,91.569999999999993,92.290000000000006,93.140000000000001,97.859999999999999,89,97.140000000000001,97.859999999999999,95,96.709999999999994,76.569999999999993,97.859999999999999,86.859999999999999,73.140000000000001,92.430000000000007,93.859999999999999,70.290000000000006,91,85.709999999999994,100,93.569999999999993,60,96.290000000000006],\"text\":[\"Tutoring Final Score: 96.15<br />Exams Final Score: 94.43\",\"Tutoring Final Score: 76.92<br />Exams Final Score: 89.43\",\"Tutoring Final Score: 96.15<br />Exams Final Score: 99.43\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 96.86\",\"Tutoring Final Score: 96.15<br />Exams Final Score: 84.29\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 93.57\",\"Tutoring Final Score: 84.62<br />Exams Final Score: 83.57\",\"Tutoring Final Score: 76.92<br />Exams Final Score: 92.00\",\"Tutoring Final Score: 96.15<br />Exams Final Score: 84.29\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 94.14\",\"Tutoring Final Score: 96.15<br />Exams Final Score: 88.00\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 95.29\",\"Tutoring Final Score: 19.23<br />Exams Final Score: 90.71\",\"Tutoring Final Score: 15.38<br />Exams Final Score: 10.29\",\"Tutoring Final Score: 96.15<br />Exams Final Score: 85.71\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 77.43\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 90.43\",\"Tutoring Final Score: 50.00<br />Exams Final Score: 91.86\",\"Tutoring Final Score: 92.31<br />Exams Final Score: 79.14\",\"Tutoring Final Score: 69.23<br />Exams Final Score: 81.43\",\"Tutoring Final Score: 92.31<br />Exams Final Score: 92.86\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 96.29\",\"Tutoring Final Score: 88.46<br />Exams Final Score: 72.86\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 98.86\",\"Tutoring Final Score: 92.31<br />Exams Final Score: 81.29\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 77.43\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 93.14\",\"Tutoring Final Score: 57.69<br />Exams Final Score: 78.29\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 84.57\",\"Tutoring Final Score: 96.15<br />Exams Final Score: 83.43\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 95.00\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 97.29\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 91.43\",\"Tutoring Final Score: 65.38<br />Exams Final Score: 66.29\",\"Tutoring Final Score: 92.31<br />Exams Final Score: 76.57\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 85.57\",\"Tutoring Final Score: 96.15<br />Exams Final Score: 93.29\",\"Tutoring Final Score: 11.54<br />Exams Final Score: 64.86\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 86.57\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 89.00\",\"Tutoring Final Score: 23.08<br />Exams Final Score: 85.29\",\"Tutoring Final Score: 80.77<br />Exams Final Score: 87.14\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 88.29\",\"Tutoring Final Score: 96.15<br />Exams Final Score: 94.71\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 95.43\",\"Tutoring Final Score: 76.92<br />Exams Final Score: 92.71\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 98.86\",\"Tutoring Final Score: 96.15<br />Exams Final Score: 91.29\",\"Tutoring Final Score: 76.92<br />Exams Final Score: 78.57\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 91.57\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 92.29\",\"Tutoring Final Score: 96.15<br />Exams Final Score: 93.14\",\"Tutoring Final Score: 88.46<br />Exams Final Score: 97.86\",\"Tutoring Final Score: 84.62<br />Exams Final Score: 89.00\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 97.14\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 97.86\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 95.00\",\"Tutoring Final Score: 88.46<br />Exams Final Score: 96.71\",\"Tutoring Final Score: 26.92<br />Exams Final Score: 76.57\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 97.86\",\"Tutoring Final Score: 92.31<br />Exams Final Score: 86.86\",\"Tutoring Final Score: 57.69<br />Exams Final Score: 73.14\",\"Tutoring Final Score: 96.15<br />Exams Final Score: 92.43\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 93.86\",\"Tutoring Final Score: 65.38<br />Exams Final Score: 70.29\",\"Tutoring Final Score: 96.15<br />Exams Final Score: 91.00\",\"Tutoring Final Score: 96.15<br />Exams Final Score: 85.71\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 100.00\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 93.57\",\"Tutoring Final Score: 26.92<br />Exams Final Score: 60.00\",\"Tutoring Final Score: 100.00<br />Exams Final Score: 96.29\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(85,107,47,1)\",\"opacity\":0.5,\"size\":5.6692913385826778,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(85,107,47,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[11.539999999999999,12.659746835443038,13.779493670886076,14.899240506329113,16.018987341772153,17.138734177215191,18.258481012658226,19.378227848101265,20.497974683544303,21.617721518987342,22.73746835443038,23.857215189873415,24.976962025316453,26.096708860759492,27.21645569620253,28.336202531645569,29.455949367088607,30.575696202531645,31.695443037974684,32.815189873417722,33.934936708860761,35.054683544303799,36.174430379746838,37.294177215189876,38.413924050632914,39.533670886075953,40.653417721518991,41.77316455696203,42.892911392405061,44.0126582278481,45.132405063291138,46.252151898734176,47.371898734177215,48.491645569620253,49.611392405063292,50.73113924050633,51.850886075949369,52.970632911392407,54.090379746835445,55.210126582278484,56.329873417721522,57.449620253164554,58.569367088607592,59.689113924050631,60.808860759493669,61.928607594936707,63.048354430379746,64.168101265822784,65.287848101265823,66.407594936708861,67.5273417721519,68.647088607594938,69.766835443037976,70.886582278481015,72.006329113924053,73.126075949367092,74.245822784810116,75.365569620253154,76.485316455696193,77.605063291139231,78.72481012658227,79.844556962025308,80.964303797468347,82.084050632911385,83.203797468354423,84.323544303797462,85.4432911392405,86.563037974683539,87.682784810126577,88.802531645569616,89.922278481012654,91.042025316455693,92.161772151898731,93.281518987341769,94.401265822784808,95.521012658227846,96.640759493670885,97.760506329113923,98.880253164556962,100],\"y\":[61.214856891890953,61.603902936667964,61.992948981444968,62.381995026221972,62.771041070998983,63.160087115775987,63.549133160552998,63.938179205330002,64.327225250107006,64.71627129488401,65.105317339661028,65.494363384438032,65.883409429215035,66.272455473992039,66.661501518769043,67.050547563546061,67.439593608323065,67.828639653100069,68.217685697877073,68.606731742654077,68.995777787431095,69.384823832208099,69.773869876985103,70.162915921762107,70.55196196653911,70.941008011316129,71.330054056093132,71.719100100870136,72.10814614564714,72.497192190424144,72.886238235201148,73.275284279978166,73.66433032475517,74.053376369532174,74.442422414309178,74.831468459086182,75.2205145038632,75.609560548640204,75.998606593417207,76.387652638194211,76.776698682971215,77.165744727748233,77.554790772525237,77.943836817302241,78.332882862079245,78.721928906856249,79.110974951633267,79.500020996410271,79.889067041187275,80.278113085964279,80.667159130741283,81.056205175518301,81.445251220295305,81.834297265072308,82.223343309849312,82.612389354626316,83.00143539940332,83.390481444180324,83.779527488957342,84.168573533734346,84.55761957851135,84.946665623288354,85.335711668065358,85.724757712842376,86.11380375761938,86.502849802396383,86.891895847173387,87.280941891950391,87.669987936727409,88.059033981504413,88.448080026281417,88.837126071058421,89.226172115835425,89.615218160612443,90.004264205389447,90.393310250166451,90.782356294943469,91.171402339720458,91.560448384497477,91.94949442927448],\"text\":[\"Tutoring Final Score: 11.54000<br />Exams Final Score: 61.21486\",\"Tutoring Final Score: 12.65975<br />Exams Final Score: 61.60390\",\"Tutoring Final Score: 13.77949<br />Exams Final Score: 61.99295\",\"Tutoring Final Score: 14.89924<br />Exams Final Score: 62.38200\",\"Tutoring Final Score: 16.01899<br />Exams Final Score: 62.77104\",\"Tutoring Final Score: 17.13873<br />Exams Final Score: 63.16009\",\"Tutoring Final Score: 18.25848<br />Exams Final Score: 63.54913\",\"Tutoring Final Score: 19.37823<br />Exams Final Score: 63.93818\",\"Tutoring Final Score: 20.49797<br />Exams Final Score: 64.32723\",\"Tutoring Final Score: 21.61772<br />Exams Final Score: 64.71627\",\"Tutoring Final Score: 22.73747<br />Exams Final Score: 65.10532\",\"Tutoring Final Score: 23.85722<br />Exams Final Score: 65.49436\",\"Tutoring Final Score: 24.97696<br />Exams Final Score: 65.88341\",\"Tutoring Final Score: 26.09671<br />Exams Final Score: 66.27246\",\"Tutoring Final Score: 27.21646<br />Exams Final Score: 66.66150\",\"Tutoring Final Score: 28.33620<br />Exams Final Score: 67.05055\",\"Tutoring Final Score: 29.45595<br />Exams Final Score: 67.43959\",\"Tutoring Final Score: 30.57570<br />Exams Final Score: 67.82864\",\"Tutoring Final Score: 31.69544<br />Exams Final Score: 68.21769\",\"Tutoring Final Score: 32.81519<br />Exams Final Score: 68.60673\",\"Tutoring Final Score: 33.93494<br />Exams Final Score: 68.99578\",\"Tutoring Final Score: 35.05468<br />Exams Final Score: 69.38482\",\"Tutoring Final Score: 36.17443<br />Exams Final Score: 69.77387\",\"Tutoring Final Score: 37.29418<br />Exams Final Score: 70.16292\",\"Tutoring Final Score: 38.41392<br />Exams Final Score: 70.55196\",\"Tutoring Final Score: 39.53367<br />Exams Final Score: 70.94101\",\"Tutoring Final Score: 40.65342<br />Exams Final Score: 71.33005\",\"Tutoring Final Score: 41.77316<br />Exams Final Score: 71.71910\",\"Tutoring Final Score: 42.89291<br />Exams Final Score: 72.10815\",\"Tutoring Final Score: 44.01266<br />Exams Final Score: 72.49719\",\"Tutoring Final Score: 45.13241<br />Exams Final Score: 72.88624\",\"Tutoring Final Score: 46.25215<br />Exams Final Score: 73.27528\",\"Tutoring Final Score: 47.37190<br />Exams Final Score: 73.66433\",\"Tutoring Final Score: 48.49165<br />Exams Final Score: 74.05338\",\"Tutoring Final Score: 49.61139<br />Exams Final Score: 74.44242\",\"Tutoring Final Score: 50.73114<br />Exams Final Score: 74.83147\",\"Tutoring Final Score: 51.85089<br />Exams Final Score: 75.22051\",\"Tutoring Final Score: 52.97063<br />Exams Final Score: 75.60956\",\"Tutoring Final Score: 54.09038<br />Exams Final Score: 75.99861\",\"Tutoring Final Score: 55.21013<br />Exams Final Score: 76.38765\",\"Tutoring Final Score: 56.32987<br />Exams Final Score: 76.77670\",\"Tutoring Final Score: 57.44962<br />Exams Final Score: 77.16574\",\"Tutoring Final Score: 58.56937<br />Exams Final Score: 77.55479\",\"Tutoring Final Score: 59.68911<br />Exams Final Score: 77.94384\",\"Tutoring Final Score: 60.80886<br />Exams Final Score: 78.33288\",\"Tutoring Final Score: 61.92861<br />Exams Final Score: 78.72193\",\"Tutoring Final Score: 63.04835<br />Exams Final Score: 79.11097\",\"Tutoring Final Score: 64.16810<br />Exams Final Score: 79.50002\",\"Tutoring Final Score: 65.28785<br />Exams Final Score: 79.88907\",\"Tutoring Final Score: 66.40759<br />Exams Final Score: 80.27811\",\"Tutoring Final Score: 67.52734<br />Exams Final Score: 80.66716\",\"Tutoring Final Score: 68.64709<br />Exams Final Score: 81.05621\",\"Tutoring Final Score: 69.76684<br />Exams Final Score: 81.44525\",\"Tutoring Final Score: 70.88658<br />Exams Final Score: 81.83430\",\"Tutoring Final Score: 72.00633<br />Exams Final Score: 82.22334\",\"Tutoring Final Score: 73.12608<br />Exams Final Score: 82.61239\",\"Tutoring Final Score: 74.24582<br />Exams Final Score: 83.00144\",\"Tutoring Final Score: 75.36557<br />Exams Final Score: 83.39048\",\"Tutoring Final Score: 76.48532<br />Exams Final Score: 83.77953\",\"Tutoring Final Score: 77.60506<br />Exams Final Score: 84.16857\",\"Tutoring Final Score: 78.72481<br />Exams Final Score: 84.55762\",\"Tutoring Final Score: 79.84456<br />Exams Final Score: 84.94667\",\"Tutoring Final Score: 80.96430<br />Exams Final Score: 85.33571\",\"Tutoring Final Score: 82.08405<br />Exams Final Score: 85.72476\",\"Tutoring Final Score: 83.20380<br />Exams Final Score: 86.11380\",\"Tutoring Final Score: 84.32354<br />Exams Final Score: 86.50285\",\"Tutoring Final Score: 85.44329<br />Exams Final Sco"},{"title":" Heatwaves in Japanese Navy Air Bases ","url":"325Analyses/Linear Regression/Weather.html","content":"Code Show All Code Hide All Code Heatwaves in Japanese Navy Air Bases library(GSODR) library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(sf) library(ggspatial) library(leaflet.extras) #run: install.packages(\"GSODR\") # to get the GSODR package. You'll need this package to pull in your weather data. load(system.file(\"extdata\", \"isd_history.rda\", package = \"GSODR\")) #Run this in your console to see the Country Names you can pick from: View(isd_history) #Search \"United States\" in the search bar of the top-right corner of the data Viewer that pops up. #Or search for any other country you are interested in. #Goal, select the STNID (station ID) for two different weather stations. #For example, Rexburg is STNID == \"726818-94194\" #Once you have two STNID values selected, go to the next R-chunk. #rexburg <- get_GSOD(years = 2023, station = \"726818-94194\") #Run: View(rexburg) #To see what columns mean, go here: https://cran.r-project.org/web/packages/GSODR/vignettes/GSODR.html#appendices Background Japan houses some of the United States finest Navy Bases. I was born on one of them. While Japan is home to many of our exceptional Navy men, Japan is also known for the miserably scorching heatwaves the happen every year. The two criteria for an unmerciful heatwave being: high temperature and high humidity.1 In this study, we will be studying the weather in the following two Japanese cities, Misawa and Atsugi, who residence U.S. Navy bases. Additionally, we will use a Multiple Linear regression to answer this question: Is one Navy base more prone to heatwaves than the other? The map below depicts the two cities with several features: Legend : the top right legend changes the style of the map when clicking on a specific option , showing us how condense the cities are population wise vs. geographically. To inspect further,Zoom in to see each city with more detail Markers : Click on each marker for a general description about the weather patterns that take place during the hottest times of the year in the Atsugi2 and Misawa3 Air Bases.* bases <- data.frame( NAME = c(\"ATSUGI U.S. NAVAL AIR STATION\", \"MISAWA NAVAL AIR STATION\"), LAT = c(35.456, 40.703), LON = c(139.449, 141.368), COLOR = c(\"red\", \"darkred\"), POPUP = c(\"<u>Average High Temperature:<\/u><br> The hot season lasts for <b>2.7 months<\/b>, from June 26 to September 18, with an average daily high temperature <b>above 26.11°C<\/b>.\", \"<u>Average High Temperature:<\/u> <br> The warm season lasts for <b>3.0 months<\/b>, from June 27 to September 26, with an average daily high temperature <b>above 20.56°C<\/b>.\")) sickicons <- awesomeIcons( icon = 'flag', library = 'fa', markerColor=bases$COLOR ) leaflet() %>% addProviderTiles(providers$Esri.WorldTopoMap, group = \"World Map\") %>% addProviderTiles(providers$Esri.WorldImagery, group = \"Terrain Map\") %>% setView(lng = 140.5, lat = 38, zoom = 5) %>% addAwesomeMarkers( lng = bases$LON, lat = bases$LAT, popup = bases$POPUP, label = bases$NAME, labelOptions = labelOptions( noHide = TRUE, direction = \"right\", textOnly = TRUE, style = list(\"font-weight\" = \"bold\", \"font-size\" = \"12px\") ), icon = sickicons)%>% addLayersControl( baseGroups = c(\"World Map\", \"Terrain Map\"), options = layersControlOptions(collapsed = FALSE) ) {\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addProviderTiles\",\"args\":[\"Esri.WorldTopoMap\",null,\"World Map\",{\"errorTileUrl\":\"\",\"noWrap\":false,\"detectRetina\":false}]},{\"method\":\"addProviderTiles\",\"args\":[\"Esri.WorldImagery\",null,\"Terrain Map\",{\"errorTileUrl\":\"\",\"noWrap\":false,\"detectRetina\":false}]},{\"method\":\"addAwesomeMarkers\",\"args\":[[35.456,40.703],[139.449,141.368],{\"icon\":\"flag\",\"markerColor\":[\"red\",\"darkred\"],\"iconColor\":\"white\",\"spin\":false,\"squareMarker\":false,\"iconRotate\":0,\"font\":\"monospace\",\"prefix\":\"fa\"},null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},[\"<u>Average High Temperature:<\\/u><br> The hot season lasts for <b>2.7 months<\\/b>, from June 26 to September 18, with an average daily high temperature <b>above 26.11°C<\\/b>.\",\"<u>Average High Temperature:<\\/u> <br> The warm season lasts for <b>3.0 months<\\/b>, from June 27 to September 26, with an average daily high temperature <b>above 20.56°C<\\/b>.\"],null,null,null,[\"ATSUGI U.S. NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\"],{\"interactive\":false,\"permanent\":true,\"direction\":\"right\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":true,\"style\":{\"font-weight\":\"bold\",\"font-size\":\"12px\"},\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addLayersControl\",\"args\":[[\"World Map\",\"Terrain Map\"],[],{\"collapsed\":false,\"autoZIndex\":true,\"position\":\"topright\"}]}],\"setView\":[[38,140.5],5,[]],\"limits\":{\"lat\":[35.456,40.703],\"lng\":[139.449,141.368]}},\"evals\":[],\"jsHooks\":[]} Click the tab to learn more about the data Hide Data Show Data The data table below shows what we will be using to answer our question that was stated above. Our data comes from GSODR’s internal database of station locations4. From that database, we were able to filter down the data to our two desired cities with their Maximum Temperature, Humidity, and the month the data was recorded. #Then run a similar code to get your station information for your weather stations. #(If you want to use rexburg, then just use one of the following codes) MISAWA <- get_GSOD(years = 2023, station = \"475800-44402\") ATSUGI <- get_GSOD(years = 2023, station = \"476790-43319\") #Finally, join your two datasets together into one dataset: weather <- rbind(MISAWA,ATSUGI) weathery <- weather %>% select(c(NAME, MAX, RH, MONTH)) %>% rename( HUMIDITY = RH, 'AIR NAVY BASE' = NAME, MAXTEMP = MAX) datatable(weathery, options(list=c(3,10,30))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"223\",\"224\",\"225\",\"226\",\"227\",\"228\",\"229\",\"230\",\"231\",\"232\",\"233\",\"234\",\"235\",\"236\",\"237\",\"238\",\"239\",\"240\",\"241\",\"242\",\"243\",\"244\",\"245\",\"246\",\"247\",\"248\",\"249\",\"250\",\"251\",\"252\",\"253\",\"254\",\"255\",\"256\",\"257\",\"258\",\"259\",\"260\",\"261\",\"262\",\"263\",\"264\",\"265\",\"266\",\"267\",\"268\",\"269\",\"270\",\"271\",\"272\",\"273\",\"274\",\"275\",\"276\",\"277\",\"278\",\"279\",\"280\",\"281\",\"282\",\"283\",\"284\",\"285\",\"286\",\"287\",\"288\",\"289\",\"290\",\"291\",\"292\",\"293\",\"294\",\"295\",\"296\",\"297\",\"298\",\"299\",\"300\",\"301\",\"302\",\"303\",\"304\",\"305\",\"306\",\"307\",\"308\",\"309\",\"310\",\"311\",\"312\",\"313\",\"314\",\"315\",\"316\",\"317\",\"318\",\"319\",\"320\",\"321\",\"322\",\"323\",\"324\",\"325\",\"326\",\"327\",\"328\",\"329\",\"330\",\"331\",\"332\",\"333\",\"334\",\"335\",\"336\",\"337\",\"338\",\"339\",\"340\",\"341\",\"342\",\"343\",\"344\",\"345\",\"346\",\"347\",\"348\",\"349\",\"350\",\"351\",\"352\",\"353\",\"354\",\"355\",\"356\",\"357\",\"358\",\"359\",\"360\",\"361\",\"362\",\"363\",\"364\",\"365\",\"366\",\"367\",\"368\",\"369\",\"370\",\"371\",\"372\",\"373\",\"374\",\"375\",\"376\",\"377\",\"378\",\"379\",\"380\",\"381\",\"382\",\"383\",\"384\",\"385\",\"386\",\"387\",\"388\",\"389\",\"390\",\"391\",\"392\",\"393\",\"394\",\"395\",\"396\",\"397\",\"398\",\"399\",\"400\",\"401\",\"402\",\"403\",\"404\",\"405\",\"406\",\"407\",\"408\",\"409\",\"410\",\"411\",\"412\",\"413\",\"414\",\"415\",\"416\",\"417\",\"418\",\"419\",\"420\",\"421\",\"422\",\"423\",\"424\",\"425\",\"426\",\"427\",\"428\",\"429\",\"430\",\"431\",\"432\",\"433\",\"434\",\"435\",\"436\",\"437\",\"438\",\"439\",\"440\",\"441\",\"442\",\"443\",\"444\",\"445\",\"446\",\"447\",\"448\",\"449\",\"450\",\"451\",\"452\",\"453\",\"454\",\"455\",\"456\",\"457\",\"458\",\"459\",\"460\",\"461\",\"462\",\"463\",\"464\",\"465\",\"466\",\"467\",\"468\",\"469\",\"470\",\"471\",\"472\",\"473\",\"474\",\"475\",\"476\",\"477\",\"478\",\"479\",\"480\",\"481\",\"482\",\"483\",\"484\",\"485\",\"486\",\"487\",\"488\",\"489\",\"490\",\"491\",\"492\",\"493\",\"494\",\"495\",\"496\",\"497\",\"498\",\"499\",\"500\",\"501\",\"502\",\"503\",\"504\",\"505\",\"506\",\"507\",\"508\",\"509\",\"510\",\"511\",\"512\",\"513\",\"514\",\"515\",\"516\",\"517\",\"518\",\"519\",\"520\",\"521\",\"522\",\"523\",\"524\",\"525\",\"526\",\"527\",\"528\",\"529\",\"530\",\"531\",\"532\",\"533\",\"534\",\"535\",\"536\",\"537\",\"538\",\"539\",\"540\",\"541\",\"542\",\"543\",\"544\",\"545\",\"546\",\"547\",\"548\",\"549\",\"550\",\"551\",\"552\",\"553\",\"554\",\"555\",\"556\",\"557\",\"558\",\"559\",\"560\",\"561\",\"562\",\"563\",\"564\",\"565\",\"566\",\"567\",\"568\",\"569\",\"570\",\"571\",\"572\",\"573\",\"574\",\"575\",\"576\",\"577\",\"578\",\"579\",\"580\",\"581\",\"582\",\"583\",\"584\",\"585\",\"586\",\"587\",\"588\",\"589\",\"590\",\"591\",\"592\",\"593\",\"594\",\"595\",\"596\",\"597\",\"598\",\"599\",\"600\",\"601\",\"602\",\"603\",\"604\",\"605\",\"606\",\"607\",\"608\",\"609\",\"610\",\"611\",\"612\",\"613\",\"614\",\"615\",\"616\",\"617\",\"618\",\"619\",\"620\",\"621\",\"622\",\"623\",\"624\",\"625\",\"626\",\"627\",\"628\",\"629\",\"630\",\"631\",\"632\",\"633\",\"634\",\"635\",\"636\",\"637\",\"638\",\"639\",\"640\",\"641\",\"642\",\"643\",\"644\",\"645\",\"646\",\"647\",\"648\",\"649\",\"650\",\"651\",\"652\",\"653\",\"654\",\"655\",\"656\",\"657\",\"658\",\"659\",\"660\",\"661\",\"662\",\"663\",\"664\",\"665\",\"666\",\"667\",\"668\",\"669\",\"670\",\"671\",\"672\",\"673\",\"674\",\"675\",\"676\",\"677\",\"678\",\"679\",\"680\",\"681\",\"682\",\"683\",\"684\",\"685\",\"686\",\"687\",\"688\",\"689\",\"690\",\"691\",\"692\",\"693\",\"694\",\"695\",\"696\",\"697\",\"698\",\"699\",\"700\",\"701\",\"702\",\"703\",\"704\",\"705\",\"706\",\"707\",\"708\",\"709\",\"710\",\"711\",\"712\",\"713\",\"714\",\"715\",\"716\",\"717\",\"718\",\"719\"],[\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MISAWA NAVAL AIR STATION\",\"MIS"},{"title":"Challenger O-Ring Data – Logistic Regression","url":"325Analyses/Logistic Regression/Examples/challengerLogisticReg.html","content":"Code Show All Code Hide All Code Challenger O-Ring Data – Logistic Regression Background The Space Shuttle Challenger exploded 73 second after liftoff on January 28th, 1986. The disaster claimed the lives of all seven astronauts on board, including school teacher Christa McAuliffe.1 The details surrounding this disaster were very involved. If you are interested in learning more, watch this 18-minute video documentary on PBS.org. For the purposes of this analysis, it is sufficient to point out that engineers that manufactured the large boosters that launched the rocket were aware of the possible failures that could happen during cold temperatures. They tried to prevent the launch, but were ultimately ignored and disaster ensued. Data from Previous Launches The main concern of engineers in launching the Challenger was the evidence that the large O-rings sealing the several sections of the boosters could fail in cold temperatures. The lowest temperature of any of the 23 prior launches (before the Challenger explosion) was 53° F 2. This is evident in the data set shown below. Engineers prior to the Challenger launch suggested that the launch not be attempted below 53°. The “evidence” that the o-rings could fail below 53° was based on a simple conclusion that since the launch at 53° experienced two o-ring failures, it seemed unwise to launch below that temperature. In the following analysis we demonstrate more fully how dangerous it was to launch on this specific day where the outside temperature at the time of the launch was 31°. The “fail” column in the data set below records how many O-rings experienced failures during that particular launch. The “temp” column lists the outside temperature at the time of launch. library(alr4) library(mosaic) library(DT) library(pander) datatable(Challeng) {\"x\":{\"filter\":\"none\",\"data\":[[\"4/12/81\",\"11/12/81\",\"3/22/82\",\"11/11/82\",\"4/4/83\",\"6/18/83\",\"8/30/83\",\"11/28/83\",\"2/3/84\",\"4/6/84\",\"8/30/84\",\"10/5/84\",\"11/8/84\",\"1/24/85\",\"4/12/85\",\"4/29/85\",\"6/17/85\",\"7/29/85\",\"8/27/85\",\"10/3/85\",\"10/30/85\",\"11/26/85\",\"1/12/86\"],[66,70,69,68,67,72,73,70,57,63,70,78,67,53,67,75,70,81,76,79,75,76,58],[50,50,50,50,50,50,50,100,100,200,200,200,200,200,200,200,200,200,200,200,200,200,200],[0,1,0,0,0,0,0,0,1,1,1,0,0,2,0,0,0,0,0,0,2,0,1],[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],[0,1,0,0,0,0,0,0,1,1,1,0,0,3,0,0,0,0,0,0,0,0,1],[0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,0,0],[0,4,0,0,0,0,0,0,4,2,4,0,0,11,0,0,0,0,0,0,4,0,4]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>temp<\\/th>\\n <th>pres<\\/th>\\n <th>fail<\\/th>\\n <th>n<\\/th>\\n <th>erosion<\\/th>\\n <th>blowby<\\/th>\\n <th>damage<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Logistic Model The probability of at least one o-ring failing during a shuttle launch based on the known outside temperature at the time of launch is given by the following logistic regression model. \\[ P(Y_i = 1|x_i) = \\frac{e^{\\beta_0+\\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] In this model, for each previous shuttle launch \\(i\\): \\(Y_i = 1\\) denotes at least one o-ring failing for the given launch, \\(Y_i=0\\) denotes no o-rings failing (successful launch), and \\(x_i\\) denotes the outside temperature in degrees Fahrenheit at the time of the launch. If \\(\\beta_1\\) is zero in the above model, then \\(x_i\\) (temperature) provides no insight about the probability of a failed O-ring. If not zero however, then temperature plays an important role in the probability of o-rings failing. Using a significance level of \\(\\alpha = 0.05\\) we will test the below hypotheses about \\(\\beta_1\\). \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] Fitting the Model The estimates of the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) for the above logistic regression model and data are shown below. chall.glm <- glm(fail>0 ~ temp, data=Challeng, family=binomial) summary(chall.glm) %>% pander()   Estimate Std. Error z value Pr(>|z|) (Intercept) 15.04 7.379 2.039 0.04148 temp -0.2322 0.1082 -2.145 0.03196 (Dispersion parameter for binomial family taken to be 1 ) Null deviance: 28.27 on 22 degrees of freedom Residual deviance: 20.32 on 21 degrees of freedom This gives the estimated model for \\(\\pi_i\\) as \\[ P(Y_i = 1|x_i) \\approx \\frac{e^{15.043-0.232 x_i}}{1+e^{15.043 - 0.232 x_i}} = \\hat{\\pi}_i \\] where \\(b_0 = 15.043\\) is the value of the (Intercept) which estimates \\(\\beta_0\\) and \\(b_1 = -0.232\\) is the value of temp which estimates \\(\\beta_1\\). Importantly, the \\(p\\)-value for the test of temp shows a significant result \\((p = 0.0320)\\) giving sufficient evidence to conclude that \\(\\beta_1 \\neq 0\\). The outside temperature at the time of launch effects the probability of at least one o-ring failure during the launch. Visualizing the Model The following plot shows how much colder it was on the day of the Challenger launch (31°F, shown by the vertical dashed gray line) compared to all 23 previous shuttle launches (black dots in the graph). plot( fail>0 ~ temp, data=Challeng, xlab=\"Outside Temperature at Time of Launch (Fahrenheit)\", ylab='Probability of At least One O-ring Failing', pch=16, main=\"NASA Shuttle Launch Data from 1981 to 1985\", xlim=c(30,85)) curve(exp(15.043-0.232*x)/(1+exp(15.043-0.232*x)), add=TRUE) abline(v=31, lty=2, col=\"lightgray\") text(31,0.3,\"Outside Temp on Jan. 28, 1986 was 31\", pos=4, cex=0.7, col=\"lightgray\") abline(h=c(0,1), lty=1, col=rgb(.2,.2,.2,.2)) legend(\"right\", bty=\"n\", legend=c(\"Previous Launches\"), pch=16, col=\"black\", cex=0.7) Diagnosing the Model To demonstrate that the logistic regression is a good fit to these data we apply the Hosmer-Lemeshow goodness of fit test (since there are only a couple repeated \\(x\\)-values) from the library(ResourceSelection). library(ResourceSelection) hoslem.test(chall.glm$y, chall.glm$fitted, g=6) %>% pander() Hosmer and Lemeshow goodness of fit (GOF) test: chall.glm$y, chall.glm$fitted Test statistic df P value 7.412 4 0.1157 # Note: doesn't give a p-value for g >= 7, default is g=10. # Larger g is usually better than smaller g. Since the null hypothesis is that the logistic regression is a good fit for the data, we claim that the logistic regression is appropriate (p-value = 0.1157). Conclusion Since the temperature being zero is not really realistic for this model, the value of \\(e^{b_0}\\) is not interpretable. However, the value of \\(e^{b_1} = e^{-0.232} \\approx 0.79\\) shows that the odds of the o-rings failing for a given launch decreases by a factor of 0.79 for every 1° F increase in temperature. Said differently, the odds of an o-ring failure during launch decrease by 21% (1-0.79) for every 1° F increase in temperature. (Also, from the reverse perspective, every 1° F decrease in temperature increases the odds of a failed o-ring by a factor of \\(e^{0.232} \\approx 1.26\\).) The Challenger shuttle was launched at a temperature of 31° F. By waiting until 53° F, the odds of failure would have been decreased by a factor of \\(e^{-0.232(53-31)}\\approx 0.006\\), a 99.4% reduction in the odds of an o-ring failure! To state it more clearly, for a temperature of 31° F our model puts the probability of a failure at \\[ P(Y_i = 1|x_i) \\approx \\frac{e^{15.043-0.232\\cdot 31}}{1+e^{15.043 - 0.232 \\cdot 31}} = \\hat{\\pi}_i \\] pred <- predict(chall.glm, data.frame(temp=31), type='response') #The inline code was used below to put this \"pred\" value into the text: #$\\hat{\\pi_i} \\approx `r round(pred,5)`$ which, using R to do this calculation we get \\(\\hat{\\pi_i} \\approx 0.99961\\). This shows that an O-ring failure was sure to happen when the launch temperature was that cold (31°F). See the article on Britannica.com: https://www.britannica.com/event/Challenger-disaster↩︎ See this article at ics.uci.edu: http://www.ics.uci.edu/~staceyah/111-202/handouts/Dalal_etal_1989-Challenger.pdf↩︎ // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open') }); }); $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Logistic Regression Examples","url":"325Analyses/Logistic Regression/Examples/GSSMultipleLogisticReg.html","content":"Code Show All Code Hide All Code Logistic Regression Examples library(mosaic) library(tidyverse) library(ResourceSelection) library(pander) Background “The General Social Survey (GSS) conducts basic scientific research on the structure and development of American society with a data-collection program designed to both monitor societal change within the United States and to compare the United States to other nations.”\\(^1\\) It is a cooperative effort to survey the American people every couple years asking a wide variety of questions and has been going on since 1972. The 2012 data file is contained in the file GSS2012.csv. #The GSS2012.csv file is actually a \"tab\" instead of \"comma\" separated file, so we have to use fancy code to read it in: GSS <- read.table(\"../../../Data/GSS2012.csv\", sep=\"\\t\", header=TRUE) Each column name in the GSS dataset corresponds to a variable name in the General Social Survey. All variable names can be browsed in the 2012 General Social Survey Browser. Hypotheses & Questions Many questions could be answered with the GSS data. There are 1,974 individuals that responded to the 2012 survey and 818 variables were recorded for at least some of these individuals. Data for all 818 variables was not collected on every individual. To see a list of all the variable names that were included in the 2012 survey type the command colnames(GSS) into your R Console after “Importing the Dataset” into R. This analysis will explore the answer to the question, Does a person’s frequency of prayer predict whether they believe divorce should be easier or more difficult to obtain than it is now? Age could be a confounding factor, as older people may tend to feel differently about divorce laws than younger people, so it will be included in the model as a covariate. The variables divlaw, pray, and age will be used to analyze this question. Summary of Variables Used Looking the variables of divlaw, pray, and age up in the online GSS variable browser we gather the following information. divlaw divlaw: Should divorce in this country be easier or more difficult to obtain than it is now? Response Meaning 1 Easier 2 More difficult 3 Stay Same 0 IAP, question is inapplicable to this person for some reason. 8 DK, Don’t know 9 NA, question not asked to this individual.   barplot(table(GSS$divlaw), xlab=\"divlaw\", col=c(\"gray\",\"skyblue\",\"skyblue\",\"gray\",\"gray\",\"gray\",\"gray\")) This barchart shows that of the people that responded to the question (answers 1, 2, and 3) the feelings are pretty strong that divorce laws should either be stricter (2) or less strict (1) although there are still a fair number that think it is just right (3) or are unsure how they feel (8). Only individuals that responded as a 1 or 2 will be used in this analysis. pray pray: How often does respondant pray? Response Meaning 1 Several times a day 2 Once a day 3 Several times a week 4 Once a week 5 Less than once a week 6 Never 0 IAP, question is inapplicable to this person for some reason. 8 DK, Don’t know 9 NA, question not asked to this individual.   barplot(table(GSS$pray), xlab=\"pray\", col=c(rep(\"skyblue\",6),\"gray\",\"gray\")) This barchart shows that most people that responded to this question claim they pray each day, with many praying several times a day (values of 1 and 2). Fewer pray seldomly (3-5) than those that never pray (6). Idividuals with a response of 8 or 9 will be omitted from the analysis. age age: Age of respondent. Response Meaning 18-88 Age in years 89 89 years old or older 98 DK, Don’t know 99 NA, question not asked to this individual.   hist(GSS$age, col='skyblue') This histogram shows the distribution of ages of those that answered all survey questions of interest. A good range of values is represented with the majority being middle aged. Indiviuals with values of 98 and 99 will be omitted from this analysis. Multiple Logistic Regression Model To answer our question, we will use a multiple logistic regression of the form \\[ P(Y_i = 1|\\, x_{i1},x_{i2},x_{i3},x_{i4},x_{i5},x_{i6}) = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i3} + \\beta_4 x_{i4} + \\beta_5 x_{i5} + \\beta_6 x_{i6}}}{1+e^{\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i3} + \\beta_4 x_{i4} + \\beta_5 x_{i5} + \\beta_6 x_{i6}}} = \\pi_i \\] where Variable Value Explanation \\(x_{i1}\\) 18 to 88 the age of the individual \\(x_{i2}\\) 1 if pray == 2, 0 otherwise individual prays once a day \\(x_{i3}\\) 1 if pray == 3, 0 o.w. individual prays several times a week \\(x_{i4}\\) 1 if pray == 4, 0 o.w. individual prays once a week \\(x_{i5}\\) 1 ifpray == 5, 0 o.w. individual prays less than once a week \\(x_{i6}\\) 1 ifpray == 6, 0 o.w. individual never prays \\(Y_i=1\\) will denote a person feels it should be more difficult to get a divorce than it is now. \\(Y_i=0\\) will denote a person that feels it should be easier than it is now to get a divorce. The hypotheses for this study will concern the coefficients of the various responses for pray as well as the age covariate. Each will be tested at the \\(\\alpha = 0.05\\) level. \\[ H_0: \\beta_1 = 0 \\text{ (age has no effect)} \\\\ H_a: \\beta_1 \\neq 0 \\text{ (age has an effect)} \\] \\[ H_0: \\beta_2 = 0 \\text{ (praying once a day has no effect)}\\\\ H_a: \\beta_2 \\neq 0 \\text{ (praying once a day has an effect)} \\] \\[ H_0: \\beta_3 = 0 \\text{ (praying several times a week has no effect)}\\\\ H_a: \\beta_3 \\neq 0 \\text{ (praying several times a week has an effect)} \\] \\[ H_0: \\beta_4 = 0 \\text{ (praying once a week has no effect)}\\\\ H_a: \\beta_4 \\neq 0 \\text{ (praying once a week has an effect)} \\] \\[ H_0: \\beta_5 = 0 \\text{ (praying less than once a week has no effect)}\\\\ H_a: \\beta_5 \\neq 0 \\text{ (praying less than once a week has an effect)} \\] \\[ H_0: \\beta_6 = 0 \\text{ (never praying has no effect)}\\\\ H_a: \\beta_6 \\neq 0 \\text{ (never praying has an effect)} \\] Data Analysis Because there are some missing values (NA, DK, Can’t choose, and IAP) in the data, we need to filter the data before analyzing it. (This is sometimes called data cleaning.) Filter the Data Click open the code button at the right to see how the data was cleaned for this study. # Cleaned version of GSS GSSc <- GSS %>% select(divlaw,age,pray) %>% #choose only 3 variables of interest filter(pray %in% 1:6,#keep only good values divlaw %in% c(1,2), age <= 89) %>% mutate(pray = as.factor(pray)) Logistic Regression Test Now for the multiple logistic regression using GSSc. GSSc.glm <- glm( (divlaw == 2) ~ age + pray, data=GSSc, family=binomial) summary(GSSc.glm) %>% pander(caption=\"Multiple Logistic Regression Summary\")   Estimate Std. Error z value Pr(>|z|) (Intercept) -0.3032 0.2322 -1.306 0.1917 age 0.01713 0.00398 4.305 1.672e-05 pray2 -0.3424 0.1724 -1.986 0.04699 pray3 -0.7159 0.254 -2.819 0.004816 pray4 -0.4237 0.2781 -1.524 0.1275 pray5 -0.3964 0.2533 -1.565 0.1176 pray6 -0.9507 0.231 -4.115 3.875e-05 (Dispersion parameter for binomial family taken to be 1 ) Null deviance: 1280 on 926 degrees of freedom Residual deviance: 1231 on 920 degrees of freedom See the end of this document for the interpretation on the results of this logistic regression. Goodness-of-Fit Test The Hosmer-Lemeshow test will be used to test the goodness of fit of this logistic regression model. The null assumes the logistic regression is a good fit. As shown below, there is insufficient evidence to reject the null \\((p = 0.7823)\\) so we will conclude a good logistic fit on these data. hoslem.test(GSSc.glm$y, GSSc.glm$fitted) %>% pander(caption=\"Hosmer-Lemeshow Goodness-of-Fit Test\") Hosmer-Lemeshow Goodness-of-Fit Test Test statistic df P value 4.766 8 0.7823 Visualization of Results par(mai=c(1,1.3,.8,.1)) palette(rev(c(\"firebrick3\",\"firebrick2\",\"orange\",\"wheat\",\"skyblue\",\"skyblue4\"))) plot((divlaw == 2) + as.numeric(as.character(pray))*.01-.03 ~ age, data=GSSc, pch=16, cex=0.5, xlim=c(18,120), col=as.factor(pray), ylab=\"Probability of Favoring Stricter \\n Divorce Laws\", main=\"Age and Faith Lead to Greater Opposition of Divorce\") curve(exp(-0.30321 + 0.01713*x)/(1+exp(-0.30321 + 0.01713*x)), from=18, to=88, add=TRUE, col=palette()[1]) curve(exp(-0.30321-0.34240 + 0.01713*x)/(1+exp(-0.30321-0.34240 + 0.01713*x)), from=18, to=88, add=TRUE, col=palette()[2]) curve(exp(-0.30321-0.71595 + 0.01713*x)/(1+exp(-0.30321-0.71595 + 0.01713*x)), from=18, to=88, add=TRUE, col=palette()[3]) curve(exp(-0.30321-0.42372 + 0.01713*x)/(1+exp(-0.30321-0.42372 + 0.01713*x)), from=18, to=88, add=TRUE, col=palette()[4]) curve(exp(-0.30321-0.39636 + 0.01713*x)/(1+exp(-0.30321-0.39636 + 0.01713*x)), from=18, to=88, add=TRUE, col=palette()[5]) curve(exp(-0.30321-0.95067 + 0.01713*x)/(1+exp(-0.30321-0.95067 + 0.01713*x)), from=18, to=88, add=TRUE, col=palette()[6]) legend(\"right\", legend=c(\"Many times a Day\", \"Once a Day\", \"Sev. Times a Week\", \"Once a Week\", \"Rarely\", \"Never\"), title=\"Prays\", col=palette(), lty=1, bty=\"n\", cex=0.8) Interpretation Although age was not of direct interest in this study, it can be seen that for any level of pray the effect of age on the odds is \\(e^{0.01713} = 1.0172776\\) for every one year increase in age. Thus, the odds of supporting stricter laws on divorce is higher for the older population than it is for the younger population. This is the reason age was included as a covariate. It seemed logical from the beginning that age would have such an effect, so including it in the model allowed us to focus on the effect of pray while accounting for this assumedly known effect of age. We interpret the pray1 level, which is the intercept in the current model, as the odds that a 0-year old person (illogical), would support stricter divorce laws, \\(e^{-0.30321} = 0.738444\\). Note that this interpretation isn’t especially meaningful. This is because pray1 is the baseline level to which all other references will be made. We interpret the pray2 variable as the effect praying once a day (as opposed to several times a day) will have on the odds that a person will favor stricter divorce laws, \\(e^{-0.34240} = 0.7100641\\). This means there is a 29% drop in the odds of favoring stricter divorce laws for those that pray once a day instead of several times a day. The interpretation of the other variables is similar. For example, the coefficient of the pray3 variable is interpreted as the change in the odds that a person will favor stricter divorce laws given they pray several times a week instead of several times a day. This change is \\(e^{-0.71595} = 0.4887276\\), showing just over a 50% drop in the odds. The most substantial drop in the odds is for those that never pray, pray6. Which comes out to be \\(e^{-0.95067} = 0.386482\\), or roughly a 62% drop in the odds. The final conclusion is that there is sufficient evidence to conclude that those that pray several times a day have higher odds of supporting stricter divorce laws than those that never pray. (The answer is not as clear for the various levels of frequency of prayer, but is very clear for these two categories.) Here are a few interesting situations to consider. probs <- predict(GSSc.glm, data.frame(age=c(20,50,80), pray=as.factor(c(6,1,1))), type=\"response\") Age Frequency of Prayer Probability of Favoring Strictor Divorce Laws 20 Never 0.2867533 50 Several Times a Day 0.6349388 80 Several Times a Day 0.7441144 This shows that roughly 3 out of 4 of those 80 years old that pray frequently (several times a day) will favor stricter divorce laws while those who are in their twenties and never pray sit closer to 1 in 4 who favor the stricter divorce laws. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open') }); }); $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"My Simple Logistic Regression","url":"325Analyses/Logistic Regression/Examples/mouseLogisticReg.html","content":"Code Show All Code Hide All Code My Simple Logistic Regression function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } function openTab(evt, tabName) { var i, tabcontent, tablinks; tabcontent = document.getElementsByClassName(\"tabcontent\"); for (i = 0; i < tabcontent.length; i++) { tabcontent[i].style.display = \"none\"; } tablinks = document.getElementsByClassName(\"tablinks\"); for (i = 0; i < tablinks.length; i++) { tablinks[i].className = tablinks[i].className.replace(\" active\", \"\"); } document.getElementById(tabName).style.display = \"block\"; evt.currentTarget.className += \" active\"; } Background Overview In scientific research, animals are frequently used for testing certain hypotheses and getting answers to important questions. Many times an animal will have to undergo a surgery or a stereotaxic injection to get insight into an existing problem. When that happens, the Animal Welfare Act (AWA)1 requires the use of anesthesia during such procedures. Not only it is a morally good idea for an animal to not feel pain, but it is also often imperative that an animal does not move during the procedure so that certain precision could be achieved. However, as with any medical procedures, certain risks come with the use of anesthesia that could vary from mild swelling at the injection site or to a severe shock or death. One of the many reasons an animal can die from an anesthetic is an overdose. An animal will not always be knocked out by the first dosage of an analgesic drug, either due to some tolerance to the drug or due to poor quality of the anesthetic itself (sometimes the quality is not good even though the expiration date or a manufacturer does not indicate so). As such, one of the most widely used anesthetic agents on animals is a mix of Ketamine2 and Xylozine3. If an animal is not knocked out after the first dosage of the Ketamine-Xylozine mix (which is suggested to be 75‐150 mg/kg and 16‐20 mg/kg respectively)4, an additional 25-50% of the initial Ketamine dose needs to be administrated as is suggested by JAALAS. However, the question remains, when should one stop after the first dosage was injected to administer the drug to prevent death if it is obvious that it is not working on that animal? The data coming from my notes during my internship in a research lab in Ukraine was taken to answer that question (click on “The Data” tab for further details). The Data In the lab, we used the suggested amount of Ketamine and Xylazine based on a bodyweight of a mouse to knock it out before the neurosurgical procedures. A mouse would not always get unconscious by the first dosage of analgesia, so a recommended 50% of the initial Ketamine–Xylazine dose would be administered again after some time. However, there were a few instances when a mouse would die either right after the exposure of the first dose of anesthetic or right after the additional administrations. Original Data Note that because we are interested only in deaths caused by analgesia, the ‘Death’ column is composed of only those deaths that were observed right after the exposure of anesthetic. Thus, in the “Death” column, ‘0’ indicates no death is observed right after the administration of Ketamine/Xylazine cocktail and ‘1’ represents the observed death right after the exposure to the anesthetic. “CofficientK” column represents the coefficient of Ketamine by which the weight of a mouse was multiplied when getting the amount of Ketamine to be injected into a mouse. This coefficient would almost always start at 2 (the first dosage) and then go up by about 50% of the initial dosage with each additional dose in case the mouse was not knocked out after the first injection. “CofficientX” column represents the coefficient of Xylozine by which the weight of a mouse was multiplied when getting the amount of Xylozine to be injected into a mouse. datatable(Analysis) {\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\"],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56],[19,27,29,26,24,29,20,20,27,24,21,26,28,24,25,21,23,26,25,19,19,27,21,22,26,24,25,26,27,28,23,21,23,33,32,28,32,31,28,32,30,26,28,33,25,28,29,25,24,26,29,24,31,25,26,36],[39.9,48.6,78.3,78,96,87,40,60,81,96,42,78,56,48,120,56,46,78,50,57,57,54,42,44,52,96,50,52,54,74.7,46,42,46,99,96,56,64,62,56,64,60,52,56,66,90,84,87,50,48,78,87,72,93,75,52,108],[13.3,18.9,16.3,14.6,24,17.4,12,12,16.2,14.4,12.6,15.6,16.8,14.4,15,12.6,13.8,15.6,17.5,13.3,13.3,18.9,14.7,13.2,18.2,14.4,15,15.6,16.2,16.8,13.8,12.6,13.8,19.8,19.2,16.8,19.2,18.6,16.8,19.2,18,15.6,16.8,19.8,15,16.8,17.4,15,14.4,15.6,17.4,14.4,18.6,15,15.6,21.6],[2.1,1.8,2.7,3,4,3,2,3,3,4,2,3,2,2,4.8,2.666666667,2,3,2,3,3,2,2,2,2,4,2,2,2,2.667857143,2,2,2,3,3,2,2,2,2,2,2,2,2,2,3.6,3,3,2,2,3,3,3,3,3,2,3],[0.7,0.7,0.5620689655,0.5615384615,1,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.7,0.7,0.7,0.7,0.7,0.6,0.7,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6],[0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,1,0]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>Mouse<\\/th>\\n <th>Weight(g)<\\/th>\\n <th>Ketamine(mul)<\\/th>\\n <th>Xylozine(mul)<\\/th>\\n <th>CoefficientK<\\/th>\\n <th>CoefficientX<\\/th>\\n <th>Death<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Filtered Data Since in this analysis my question is not about the correct initial amount dosage but rather what is a probability of death of a mouse after additional dosages, all of the mice which had only the first dosage administered (CoefficientK = 2) were filtered out. ‘CoefficientX’ was not taken into account either. Since Xylozine is used for muscle relaxation and not for analgesia, even if additional administration would cause the mouse to not move, it would not prevent it from feeling pain. For this reason, it would not be added repeatedly, in contrast to Ketamine, after the first dosage amount of the Ketamine/Xylazine cocktail has already been administered. Analysis2 <- filter(Analysis, CoefficientK > 2) Analysis2<- select(Analysis2, \"Mouse\", \"CoefficientK\", \"Death\") pander(Analysis2) Mouse CoefficientK Death 1 2.1 0 3 2.7 0 4 3 0 5 4 0 6 3 0 8 3 0 9 3 0 10 4 0 12 3 0 15 4.8 1 16 2.667 0 18 3 0 20 3 0 21 3 0 26 4 1 30 2.668 0 34 3 0 35 3 0 45 3.6 0 46 3 0 47 3 0 50 3 0 51 3 0 52 3 0 53 3 1 54 3 0 56 3 0 Hypotheses Since in this study I am interested in how the probability (\\(\\pi_i\\)) of death changes given the observed Ketamine dosage coefficient, the following Logistic Regression model is going to be used: Note: “Simple Logistic Regression is used when the response variable is binary (\\(Y_i=0\\) or \\(1\\)), and there is a single explanatory variable \\(X\\) that is typically quantitative but could be qualitative (if \\(X\\) is binary or ordinal)”(Garrett Saunders). In our case mouse’s death is the binary response variable and the Ketamine dosage coefficient is the quantitative explanatory variable. \\[ P(Y_i = 1|x_i) = \\frac{e^{\\beta_0+\\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i \\] where for observation \\(i\\): \\(Y_i = 1\\) denotes death for a given Ketamine dosage coefficient, \\(Y_i=0\\) denotes no death for a given Ketamine dosage coefficient, and \\(x_i\\) denotes the Ketamine dosage coefficient administered to a mouse. If \\(\\beta_1\\) in the above model equals zero, that would mean that \\(x_i\\) gives us no insight about \\(\\pi_i\\) (0*\\(x_i\\) = 0, which drops \\(x_i\\) coefficient from the equation completely). Thus we can set the following hypotheses: \\[ H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\] where, the alternative hypothesis suggests that Ketamine dosage coefficient (\\(x_i\\)) can help in calculating the probability of the death of a mouse, and the null that it cannot. The level of significance is set at \\(\\alpha\\) = 0.05 for this study. Analysis Let’s first see if our data resembles the Logistic Regression mathematical model and can be used for this test to get an insight into the aforementioned question. Here is the plot of our data: plot(Death~CoefficientK, data = Analysis2, pch=16, main = \"Institute of Physiology\\nAnesthesia of Adult Mice\", xlab = \"Ketamine Dosage Coefficient for a Body of Weight of a Mouse\", ylab = \"Probability of Mouse's Death\", col =\"firebrick\") curve(exp(-2.911+0.4286*x)/(1+exp(-2.911+0.4286*x)), add = TRUE) From the plot above we can see that the dots at the bottom of the plot seem to be closer to the beginning of the x-axis values, whereas the dots at the top of the plot seem to be shifted more towards the end values of the x-axis. This somewhat reminds us of a logistic regression model. To further confirm that the data is appropriate for the Logistic Regression Test, let’s run the Hosmer-Lemeshow Goodness of Fit Test (since we have only a few replicated values of each \\(x_i\\)) with g (number of groups to run the goodness of fit test on) = 3 (since our sample size is not big (n=27)). pander(hoslem.test(myglm$y, myglm$fitted, g=3)) Hosmer and Lemeshow goodness of fit (GOF) test: myglm$y, myglm$fitted Test statistic df P value 0.09406 1 0.7591 Since p-value > \\(\\alpha\\), we have insufficient evidence to reject the null or the statement that the data is a good fit for the logistic regression test. This means that we can conclude that this data is a good fit for this type of test and will give us some valid results. With that, let’s run the Simple Logistic Regression Test: Simple Logistic Regression Test on Ketamine Dosage for Anesthesia in Adult Mice   Estimate Std. Error z value Pr(>|z|) (Intercept) -11.33 4.678 -2.422 0.01542 CoefficientK 2.725 1.29 2.112 0.03466 Since our p-value = 0.03466<\\(\\alpha\\), we can conclude that \\(\\beta_1 \\neq 0\\). This means that there is a certain association between the probability of a mouse’s death and the amount of Ketamine that was injected. In other words, the probability of a mouse’s death is higher for certain values of the Ketamine dosage coefficient than for other values of the Ketamine dosage coefficient. In addition, the true probability of death from the additional Ketamine dosages for the whole adult mice population can be computed with the following equation: \\[ P(Y_i = 1|x_i)\\approx \\frac{e^{-11.33+2.725 x_i}}{1+e^{-11.33 + 2.725 x_i}} = \\hat{\\pi}_i \\] where, \\(b_0\\) = -11.33 is an estimate of \\(\\beta_0\\) of the true equation for the whole population; \\(b_1\\) = 2.725 is an estimate of \\(\\beta_1\\) of the true equation for the whole population; To get a better idea of what this all means exactly, with the help of some algebra, let’s rewrite the above equation slightly: \\[ \\underbrace{\\frac{\\pi_i}{1-\\pi_i}}_{\\text{Odds for}\\ Y_i=1} = e^{-11.33}e^{2.725 x_i} \\] Show how the above equation was found(click to view) Initial equation: \\[ \\hat{\\pi}_i= \\frac{e^{-11.33+2.725 x_i}}{1+e^{-11.33 + 2.725 x_i}} \\] Multiply both sides by a factor of \\({1+e^{-11.33 + 2.725 x_i}}\\): \\[ \\hat{\\pi}_i({1+e^{-11.33 + 2.725 x_i}})= {e^{-11.33+2.725 x_i}} \\] Use the distributive property to get rid of the parentheses: \\[ \\hat{\\pi}_i+\\hat{\\pi}_ie^{-11.33 + 2.725 x_i}= {e^{-11.33+2.725 x_i}} \\] Rearrange to be able to factor out the common term of \\(e^{-11.33 + 2.725 x_i}\\): \\[ \\hat{\\pi}_i = {e^{-11.33+2.725 x_i}} -\\hat{\\pi}_ie^{-11.33 + 2.725 x_i} \\] Factor out \\({e^{-11.33+2.725 x_i}}\\): \\[ \\hat{\\pi}_i = {e^{-11.33+2.725 x_i}}(1 -\\hat{\\pi}_i) \\] Divide both sides by \\(1 -\\hat{\\pi}_i\\): \\[ \\frac{\\hat{\\pi}_i}{1 -\\hat{\\pi}_i} = {e^{-11.33+2.725 x_i}} \\] Use the ‘Product Rule’ of exponents: \\[ \\frac{\\hat{\\pi}_i}{1 -\\hat{\\pi}_i} = e^{-11.33}e^{2.725 x_i} \\] This equation brings us to a concept of ‘odds’, so that now we could better interpret what \\(b_0\\) = -11.33 and \\(b_1\\) = 2.725 represent in the above equation. Show explanation on ‘odds’(click to view) Since it is common to have p-value or the probability as a means of describing certain things in statistics, it might be helpful to explain the difference between the probability and the odds. Probability is usually found by dividing the number of “successes” (or, in other words, the number of desirable events that were observed) by the whole sample size (the number of all events that occurred). The ‘odds’ is the same as probability, except, the number of “successes” is divided by the number of “failures”, instead of the total sample size. For example, if a coin was flipped 10 times and 7 of those times it was head, the probability of a flipping head is going to be 7/10; whereas, the odds = 7/3 (3 times we failed to get head). This brings us to the idea that we could calculate odds from probability and vice versa. Let’s say the probability of a certain event happening is: \\[ {\\pi_i}= \\frac{success}{n} \\] Then the odds could be written as: \\[ {odds}= \\frac{success}{n-success} \\] To find the formula for odds using only a known probability (or the p-value that Logistic Regression test calculates), let’s divide both the numerator and the denominator of the last equation by n: \\[ {odds}= \\frac{(\\frac{success}{n})}{(\\frac{n-success}{n})} \\] Now that we have written the equation, let’s proceed and perform the division of each factor of the denominator by n: \\[ {odds}= \\frac{(\\frac{success}{n})}{(\\frac{n}{n}-\\frac{success}{n})} \\] Which simplifies to: \\[ {odds}= \\frac{(\\frac{success}{n})}{(1-\\frac{success}{n})} \\] Notice at this point \\(\\frac{success}{n}\\) is actually the \\({\\pi_i}\\). So, now let’s substitute this into the equation to get: \\[ {odds}= \\frac{\\pi_i}{1-\\pi_i} \\] Using the rewritten Linear Regression equation, if \\(x_i\\) = 0 (meaning no Ketamine was injected into a mouse), the odds of a mouse to die are almost zero: \\[ odds = e^{-11.33}e^{2.725 *0} = e^{-11.33}e^{0}=e^{-11.33} *1= 0.000012007 \\] However, if, for example, the Ketamine dosage coefficient equals 1 then the odds of a mouse’s death grow to 0.00018: \\[ odds = e^{-11.33}e^{2.725 *1} = e^{-11.33}e^{2.725}=0.000012007 *15.25641= 0.0001831837 \\] This means that the odds of a mouse’s death are increasing by a factor of \\(e^{2.725}\\)=15.25641 when the Ketamine dosage coefficient goes up by 1. In other words, every one unit increase in \\(x_i\\) increases the odds of death by a factor of 15.25641. So each 1 unit increase in the Ketamine dosage makes a mouse 15 more times likely to die. Conclusion To answer our initial question of when one should stop injecting Ketamine after the first dosage was not enough to knock out a mouse, let’s use the Logistic Equation that we found earlier. In my opinion, as soon as the probability exceeded 50%, that’s probably the point when we should have stopped injecting more Ketamine and have preserved the animal for other purposes to prevent its pointless death. So, using \\(\\hat{\\pi}_i\\) = 0.5, we will get the following equation: \\[ \\frac{0.5}{1 -0.5} = {e^{-11.33+2.725 x_i}} \\] Show the calculations(click to view) Take natural logarithm (log) of both sides: \\[ log(\\frac{0.5}{1 -0.5}) = log({e^{-11.33+2.725 x_i}}) \\] Using properties of logarithms, we get: \\[ log(\\frac{0.5}{1 -0.5}) = {-11.33+2.725 x_i} \\] Move -11.33 to the left: \\[ log(\\frac{0.5}{1 -0.5}) +11.33={2.725 x_i} \\] Divide both sides by 2.725: \\[ \\frac{log(\\frac{0.5}{1 -0.5}) +11.33}{2.725}={x_i} \\] which will give us \\(x_i\\approx\\) 4.16. If we use R to predict the probability of a mouse’s death when \\(x_i\\) = 4.16, we get the following: predict(myglm, data.frame(CoefficientK=4.16), type='response') ## 1 ## 0.5006428 Which confirms the calculations we did earlier. From our data, we used a coefficient of 4 or greater only on 4 mice out of 56 total. Only 2 out of those died: pander(Analysis3) Mouse CoefficientK Death 5 4 0 10 4 0 15 4.8 1 26 4 1 If I would not have proceeded with the 3rd repeated injection (CoefficientK=4), I would have been able to save those two. However, I would have put away the other two (mice 5 and 10), although they would have been just fine with the 3rd repeated injection. So, in sum, we are saving two mice’s lives and then losing Ketamine we have used for another two in vain in our experiment. However, if the coefficient is slightly greater (4.16>4.0), this puts us at a 100% death rate in our data (mouse 15). So, having a Coefficient that exceeds 4.16 as a stopping point seems to be a good rule of thumb. Even though this coefficient would have saved just 1 mouse out of 56 during the time of my internship, in the long run, using this rule by all researchers could potentially save thousands, if not millions. Instead of losing both Ketamine and a mouse’s life in vain, it would only be Ketamine, which is way cheaper and way less upsetting than losing a mouse too. https://fbresearch.org/animal-care/animal-testing-regulations/↩︎ https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4258981/↩︎ https://en.wikipedia.org/wiki/Xylazine↩︎ https://animalcare.ubc.ca/sites/default/files/documents/Guideline%20-%20Rodent%20Anesthesia%20Analgesia%20Formulary%20%282016%29.pdf↩︎ // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open') }); }); $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Logistic Regression - Cat Adoptions","url":"325Analyses/Logistic Regression/MyLogisticRegression.html","content":"Code Show All Code Hide All Code Logistic Regression - Cat Adoptions Background A few months ago, I adopted two male kittens from the Idaho Falls Animal Shelter. From seeing the listing to actually getting to take them home felt like a life time of waiting. According to the Animal Human Society1, most cats are in new homes within 14 days from intake to adoption on average. Additionally, the East Dallas Veterinary Clinic2 states that male cats tend to be more sociable and playful in comparison to their female counter parts. Based off these assumptions, we will be using a simple logistic regression to answer the following question: What is the probability that the cat is a male given how many days it took for them to get adopted? Click the Show Data tab below to learn more about the data and its collection. Hide Data Show Data This data was gathered off the Idaho Falls Animal Shelter’s Facebook page. On their page, they post various cats along with information such as their age (Months), gender (1 = Male, 0 = Female), the day they are available for adoption, as well as the day those said cats get adopted. Therefore, I recorded these characteristics in the table below. I mutated the classification of gender into binary values in order to conduct the logistic regression test, as well as adding in a column that subtracted the day they were posted from the day they were adopted to get the waiting period from listing to homing. adoptions <- catadoptions %>% mutate(Gender = ifelse(Sex == \"M\", 1, 0)) %>% select(Age, Gender, Posted, Adopted, DaysListed) datatable(adoptions, options=list(lengthMenu=c(3,10,30))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\"],[2,2,2,2,2,2,2,2,1,1,1,1,1,1,36,3,2,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,36,120,24,12,2,2,2,2,2,36,6,2,2,2,2,2,2,1,120,2,48,3,1,1,1,1,1,2,2,1,1,1,1,1,3,2,2,2,2,2,2,2,3,3,3,36,3,3,3,108,84,2,2,168,96,3,2,3,3,2,12,144,3,3,3,3,36,36,4,3,2,2,2,2,36,3,3,4,4,4,4,4,3,96,1,4,4,2,96,72,24,96,3,12,2,2,2,2,2,2,48,192,192,3,3,3,3,3,3,36,2,2,2,2,2],[1,0,0,0,0,1,1,0,1,0,1,1,1,1,1,0,0,0,0,0,0,1,0,1,0,0,0,0,1,1,1,0,1,1,1,1,0,0,0,1,1,0,1,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,1,1,1,0,0,1,0,0,1,0,0,0,0,1,1,1,1,0,0,0,0,1,1,0,0,1,1,1,1,1,0,0,1,1,0,1,1,0,1,1,1,0,0,0,1,0,0,1,0,0,0,1,1,1,1,1,0,1,1,1,0,1,0,1,1,0,1,1,1,1,1,0,1,1,1,1,1,0,0,0,1,0,0,1,0],[\"22-Nov\",\"22-Nov\",\"22-Nov\",\"19-Nov\",\"19-Nov\",\"19-Nov\",\"15-Nov\",\"14-Nov\",\"14-Nov\",\"14-Nov\",\"14-Nov\",\"14-Nov\",\"14-Nov\",\"14-Nov\",\"8-Nov\",\"5-Nov\",\"5-Nov\",\"5-Nov\",\"5-Nov\",\"5-Nov\",\"5-Nov\",\"4-Nov\",\"4-Nov\",\"31-Oct\",\"31-Oct\",\"31-Oct\",\"31-Oct\",\"31-Oct\",\"31-Oct\",\"31-Oct\",\"31-Oct\",\"31-Oct\",\"30-Oct\",\"28-Oct\",\"28-Oct\",\"25-Oct\",\"25-Oct\",\"22-Oct\",\"22-Oct\",\"22-Oct\",\"21-Oct\",\"16-Oct\",\"16-Oct\",\"16-Oct\",\"16-Oct\",\"12-Oct\",\"8-Oct\",\"8-Oct\",\"8-Oct\",\"8-Oct\",\"8-Oct\",\"8-Oct\",\"8-Oct\",\"3-Oct\",\"1-Oct\",\"21-Sep\",\"24-Oct\",\"3-Oct\",\"3-Oct\",\"3-Oct\",\"3-Oct\",\"3-Oct\",\"3-Oct\",\"1-Oct\",\"1-Oct\",\"1-Oct\",\"1-Oct\",\"1-Oct\",\"1-Oct\",\"29-Sep\",\"29-Sep\",\"27-Sep\",\"27-Sep\",\"27-Sep\",\"27-Sep\",\"27-Sep\",\"27-Sep\",\"27-Sep\",\"27-Sep\",\"27-Sep\",\"23-Sep\",\"20-Sep\",\"20-Sep\",\"20-Sep\",\"17-Sep\",\"12-Sep\",\"11-Sep\",\"11-Sep\",\"9-Sep\",\"6-Sep\",\"5-Sep\",\"4-Sep\",\"4-Sep\",\"4-Sep\",\"4-Sep\",\"30-Aug\",\"30-Aug\",\"30-Aug\",\"30-Aug\",\"30-Aug\",\"30-Aug\",\"29-Aug\",\"29-Aug\",\"28-Aug\",\"27-Aug\",\"25-Aug\",\"25-Aug\",\"25-Aug\",\"25-Aug\",\"24-Aug\",\"23-Aug\",\"22-Aug\",\"21-Aug\",\"21-Aug\",\"21-Aug\",\"21-Aug\",\"20-Aug\",\"16-Aug\",\"15-Aug\",\"15-Aug\",\"12-Aug\",\"12-Aug\",\"12-Aug\",\"8-Aug\",\"8-Aug\",\"6-Aug\",\"6-Aug\",\"5-Aug\",\"5-Aug\",\"31-Jul\",\"31-Jul\",\"30-Jul\",\"30-Jul\",\"30-Jul\",\"30-Jul\",\"29-Jul\",\"29-Jul\",\"29-Jul\",\"29-Jul\",\"29-Jul\",\"29-Jul\",\"29-Jul\",\"29-Jul\",\"29-Jul\",\"29-Jul\",\"29-Jul\",\"29-Jul\",\"29-Jul\",\"29-Jul\",\"29-Jul\"],[\"23-Nov\",\"23-Nov\",\"23-Nov\",\"20-Nov\",\"22-Nov\",\"20-Nov\",\"15-Nov\",\"15-Nov\",\"18-Nov\",\"18-Nov\",\"15-Nov\",\"15-Nov\",\"15-Nov\",\"15-Nov\",\"8-Nov\",\"10-Nov\",\"6-Nov\",\"7-Nov\",\"8-Nov\",\"8-Nov\",\"15-Nov\",\"7-Nov\",\"5-Nov\",\"1-Nov\",\"1-Nov\",\"4-Nov\",\"1-Nov\",\"5-Nov\",\"8-Nov\",\"9-Nov\",\"5-Nov\",\"6-Nov\",\"31-Oct\",\"28-Oct\",\"28-Oct\",\"26-Oct\",\"9-Nov\",\"25-Oct\",\"26-Oct\",\"24-Oct\",\"23-Oct\",\"21-Oct\",\"21-Oct\",\"21-Oct\",\"21-Oct\",\"21-Oct\",\"22-Oct\",\"21-Oct\",\"22-Oct\",\"11-Oct\",\"11-Oct\",\"21-Oct\",\"11-Oct\",\"4-Nov\",\"21-Oct\",\"25-Oct\",\"24-Oct\",\"16-Oct\",\"11-Oct\",\"11-Oct\",\"16-Oct\",\"11-Oct\",\"21-Oct\",\"3-Oct\",\"3-Oct\",\"3-Oct\",\"3-Oct\",\"3-Oct\",\"3-Oct\",\"3-Oct\",\"1-Oct\",\"30-Sep\",\"28-Sep\",\"30-Sep\",\"28-Sep\",\"30-Sep\",\"30-Sep\",\"27-Sep\",\"27-Sep\",\"27-Sep\",\"5-Oct\",\"21-Sep\",\"21-Sep\",\"21-Sep\",\"23-Sep\",\"11-Oct\",\"12-Sep\",\"17-Sep\",\"25-Sep\",\"10-Sep\",\"9-Sep\",\"8-Sep\",\"6-Sep\",\"4-Sep\",\"9-Sep\",\"4-Sep\",\"9-Sep\",\"3-Sep\",\"31-Aug\",\"3-Sep\",\"1-Sep\",\"30-Aug\",\"30-Aug\",\"6-Sep\",\"3-Sep\",\"27-Aug\",\"26-Aug\",\"27-Aug\",\"26-Aug\",\"29-Aug\",\"27-Aug\",\"29-Aug\",\"27-Aug\",\"27-Aug\",\"27-Aug\",\"27-Aug\",\"21-Aug\",\"26-Aug\",\"26-Aug\",\"15-Aug\",\"19-Aug\",\"17-Aug\",\"15-Aug\",\"10-Aug\",\"10-Aug\",\"12-Sep\",\"19-Aug\",\"19-Aug\",\"7-Aug\",\"5-Aug\",\"7-Aug\",\"6-Aug\",\"6-Aug\",\"1-Aug\",\"2-Aug\",\"30-Jul\",\"26-Aug\",\"3-Aug\",\"30-Jul\",\"30-Jul\",\"8-Aug\",\"15-Aug\",\"10-Aug\",\"10-Aug\",\"15-Aug\",\"1-Aug\",\"7-Aug\",\"5-Aug\",\"29-Jul\",\"29-Jul\"],[1,1,1,1,3,1,0,1,4,4,1,1,1,1,0,5,1,2,3,3,10,3,1,1,1,4,1,5,8,9,5,6,1,0,0,1,15,3,4,2,2,5,5,5,5,9,14,13,14,3,3,13,3,32,20,34,0,13,8,8,13,8,18,2,2,2,2,2,2,4,2,3,1,3,1,3,3,0,0,0,12,1,1,1,6,29,1,6,16,4,4,4,2,0,5,5,10,4,1,4,2,1,1,9,7,2,1,2,1,5,4,7,6,6,6,6,1,10,11,0,7,5,3,2,2,37,13,14,2,5,7,7,7,2,3,1,28,5,1,1,10,17,12,12,17,3,9,7,0,0]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>Age<\\/th>\\n <th>Gender<\\/th>\\n <th>Posted<\\/th>\\n <th>Adopted<\\/th>\\n <th>DaysListed<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"lengthMenu\":[3,10,30],\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,5]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Age\",\"targets\":1},{\"name\":\"Gender\",\"targets\":2},{\"name\":\"Posted\",\"targets\":3},{\"name\":\"Adopted\",\"targets\":4},{\"name\":\"DaysListed\",\"targets\":5}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Logistic Model The probabiltity of the gender of the cat being a male based on how many days it took for them to get adopted is shown by the logistic regression model below: \\[P(Y_i = 1 | x_i ) = \\frac{e^{\\beta_0+\\beta_1\\text{Days Listed}}}{1+e^{\\beta_0+\\beta_1\\text{Days Listed}}} = \\pi_i\\] Part What it means \\(Y_i\\) (Responsive variable) Describes the probability the \\(Y_i = \\text{Male (1) or Female (0)}\\) for a given scenario. \\(x_i\\) (Explanatory variable) Describes the observed value of the number of days an individual cat was listed. Our \\(\\beta_1\\) value focuses on the proportional change in the odds that \\(Y_i = 1\\) (or that the cat adopted is male) for every day that goes by. Thus, our hypotheses are stated as so to capture if the slope is or is not significant to effecting the probability that the gender of the cat is male: \\[H_0 : \\beta_1 = 0\\] \\[H_a : \\beta_1 \\neq 0\\] Additionally, our level of significance will be: \\[\\alpha = 0.05\\] Analysis To visualize the wait a cat has depending on their gender, we will use a scatter plot. The values listed as 1 are the male cats and the 0 are the female cats. The gradient of the dots shows just how long a specific cat had to wait from getting listed to being adopted. The lighter the dot the shorter the wait (0-5 days), and the darker the dot the longer the wait(>20 days). Looking at the slope of the line, it seems thatas the days go on the probability of the cat being male decreases. However, the curve of the plot does not approach 0 or 1 within the range of the data. Therefore, the logistic regression is not very useful for this data. catgraph <- ggplot(data=adoptions, aes(x=DaysListed, y=Gender, color = DaysListed)) + geom_point(size =4, alpha = 0.8, aes( text = paste( \"Age (Months):\", Age, \"<br>\", \"Listed:\", DaysListed, \"days<br>\"))) + geom_smooth(method = \"glm\", method.args = list(family=\"binomial\"), se=FALSE, color = \"steelblue1\")+ scale_color_gradient(low = \"steelblue1\", high = \"royalblue4\") + labs(title = \"Idaho Falls Animals Shelter Cat Adoptions\", x = \"Days Listed\", y = \"Gender (Male = 1, Female = 0\") + theme_minimal() + theme( panel.background = element_rect(fill = \"white\", color = NA), plot.background = element_rect(fill = \"white\", color = NA) ) ggplotly(catgraph, tooltip=\"text\") {\"x\":{\"data\":[{\"x\":[1,1,1,1,3,1,0,1,4,4,1,1,1,1,0,5,1,2,3,3,10,3,1,1,1,4,1,5,8,9,5,6,1,0,0,1,15,3,4,2,2,5,5,5,5,9,14,13,14,3,3,13,3,32,20,34,0,13,8,8,13,8,18,2,2,2,2,2,2,4,2,3,1,3,1,3,3,0,0,0,12,1,1,1,6,29,1,6,16,4,4,4,2,0,5,5,10,4,1,4,2,1,1,9,7,2,1,2,1,5,4,7,6,6,6,6,1,10,11,0,7,5,3,2,2,37,13,14,2,5,7,7,7,2,3,1,28,5,1,1,10,17,12,12,17,3,9,7,0,0],\"y\":[1,0,0,0,0,1,1,0,1,0,1,1,1,1,1,0,0,0,0,0,0,1,0,1,0,0,0,0,1,1,1,0,1,1,1,1,0,0,0,1,1,0,1,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,1,1,1,0,0,1,0,0,1,0,0,0,0,1,1,1,1,0,0,0,0,1,1,0,0,1,1,1,1,1,0,0,1,1,0,1,1,0,1,1,1,0,0,0,1,0,0,1,0,0,0,1,1,1,1,1,0,1,1,1,0,1,0,1,1,0,1,1,1,1,1,0,1,1,1,1,1,0,0,0,1,0,0,1,0],\"text\":[\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 2 <br> Listed: 3 days<br>\",\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 2 <br> Listed: 0 days<br>\",\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 1 <br> Listed: 4 days<br>\",\"Age (Months): 1 <br> Listed: 4 days<br>\",\"Age (Months): 1 <br> Listed: 1 days<br>\",\"Age (Months): 1 <br> Listed: 1 days<br>\",\"Age (Months): 1 <br> Listed: 1 days<br>\",\"Age (Months): 1 <br> Listed: 1 days<br>\",\"Age (Months): 36 <br> Listed: 0 days<br>\",\"Age (Months): 3 <br> Listed: 5 days<br>\",\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 3 <br> Listed: 2 days<br>\",\"Age (Months): 2 <br> Listed: 3 days<br>\",\"Age (Months): 2 <br> Listed: 3 days<br>\",\"Age (Months): 2 <br> Listed: 10 days<br>\",\"Age (Months): 2 <br> Listed: 3 days<br>\",\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 2 <br> Listed: 4 days<br>\",\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 2 <br> Listed: 5 days<br>\",\"Age (Months): 2 <br> Listed: 8 days<br>\",\"Age (Months): 2 <br> Listed: 9 days<br>\",\"Age (Months): 2 <br> Listed: 5 days<br>\",\"Age (Months): 2 <br> Listed: 6 days<br>\",\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 2 <br> Listed: 0 days<br>\",\"Age (Months): 2 <br> Listed: 0 days<br>\",\"Age (Months): 36 <br> Listed: 1 days<br>\",\"Age (Months): 120 <br> Listed: 15 days<br>\",\"Age (Months): 24 <br> Listed: 3 days<br>\",\"Age (Months): 12 <br> Listed: 4 days<br>\",\"Age (Months): 2 <br> Listed: 2 days<br>\",\"Age (Months): 2 <br> Listed: 2 days<br>\",\"Age (Months): 2 <br> Listed: 5 days<br>\",\"Age (Months): 2 <br> Listed: 5 days<br>\",\"Age (Months): 2 <br> Listed: 5 days<br>\",\"Age (Months): 36 <br> Listed: 5 days<br>\",\"Age (Months): 6 <br> Listed: 9 days<br>\",\"Age (Months): 2 <br> Listed: 14 days<br>\",\"Age (Months): 2 <br> Listed: 13 days<br>\",\"Age (Months): 2 <br> Listed: 14 days<br>\",\"Age (Months): 2 <br> Listed: 3 days<br>\",\"Age (Months): 2 <br> Listed: 3 days<br>\",\"Age (Months): 2 <br> Listed: 13 days<br>\",\"Age (Months): 1 <br> Listed: 3 days<br>\",\"Age (Months): 120 <br> Listed: 32 days<br>\",\"Age (Months): 2 <br> Listed: 20 days<br>\",\"Age (Months): 48 <br> Listed: 34 days<br>\",\"Age (Months): 3 <br> Listed: 0 days<br>\",\"Age (Months): 1 <br> Listed: 13 days<br>\",\"Age (Months): 1 <br> Listed: 8 days<br>\",\"Age (Months): 1 <br> Listed: 8 days<br>\",\"Age (Months): 1 <br> Listed: 13 days<br>\",\"Age (Months): 1 <br> Listed: 8 days<br>\",\"Age (Months): 2 <br> Listed: 18 days<br>\",\"Age (Months): 2 <br> Listed: 2 days<br>\",\"Age (Months): 1 <br> Listed: 2 days<br>\",\"Age (Months): 1 <br> Listed: 2 days<br>\",\"Age (Months): 1 <br> Listed: 2 days<br>\",\"Age (Months): 1 <br> Listed: 2 days<br>\",\"Age (Months): 1 <br> Listed: 2 days<br>\",\"Age (Months): 3 <br> Listed: 4 days<br>\",\"Age (Months): 2 <br> Listed: 2 days<br>\",\"Age (Months): 2 <br> Listed: 3 days<br>\",\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 2 <br> Listed: 3 days<br>\",\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 2 <br> Listed: 3 days<br>\",\"Age (Months): 2 <br> Listed: 3 days<br>\",\"Age (Months): 3 <br> Listed: 0 days<br>\",\"Age (Months): 3 <br> Listed: 0 days<br>\",\"Age (Months): 3 <br> Listed: 0 days<br>\",\"Age (Months): 36 <br> Listed: 12 days<br>\",\"Age (Months): 3 <br> Listed: 1 days<br>\",\"Age (Months): 3 <br> Listed: 1 days<br>\",\"Age (Months): 3 <br> Listed: 1 days<br>\",\"Age (Months): 108 <br> Listed: 6 days<br>\",\"Age (Months): 84 <br> Listed: 29 days<br>\",\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 2 <br> Listed: 6 days<br>\",\"Age (Months): 168 <br> Listed: 16 days<br>\",\"Age (Months): 96 <br> Listed: 4 days<br>\",\"Age (Months): 3 <br> Listed: 4 days<br>\",\"Age (Months): 2 <br> Listed: 4 days<br>\",\"Age (Months): 3 <br> Listed: 2 days<br>\",\"Age (Months): 3 <br> Listed: 0 days<br>\",\"Age (Months): 2 <br> Listed: 5 days<br>\",\"Age (Months): 12 <br> Listed: 5 days<br>\",\"Age (Months): 144 <br> Listed: 10 days<br>\",\"Age (Months): 3 <br> Listed: 4 days<br>\",\"Age (Months): 3 <br> Listed: 1 days<br>\",\"Age (Months): 3 <br> Listed: 4 days<br>\",\"Age (Months): 3 <br> Listed: 2 days<br>\",\"Age (Months): 36 <br> Listed: 1 days<br>\",\"Age (Months): 36 <br> Listed: 1 days<br>\",\"Age (Months): 4 <br> Listed: 9 days<br>\",\"Age (Months): 3 <br> Listed: 7 days<br>\",\"Age (Months): 2 <br> Listed: 2 days<br>\",\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 2 <br> Listed: 2 days<br>\",\"Age (Months): 2 <br> Listed: 1 days<br>\",\"Age (Months): 36 <br> Listed: 5 days<br>\",\"Age (Months): 3 <br> Listed: 4 days<br>\",\"Age (Months): 3 <br> Listed: 7 days<br>\",\"Age (Months): 4 <br> Listed: 6 days<br>\",\"Age (Months): 4 <br> Listed: 6 days<br>\",\"Age (Months): 4 <br> Listed: 6 days<br>\",\"Age (Months): 4 <br> Listed: 6 days<br>\",\"Age (Months): 4 <br> Listed: 1 days<br>\",\"Age (Months): 3 <br> Listed: 10 days<br>\",\"Age (Months): 96 <br> Listed: 11 days<br>\",\"Age (Months): 1 <br> Listed: 0 days<br>\",\"Age (Months): 4 <br> Listed: 7 days<br>\",\"Age (Months): 4 <br> Listed: 5 days<br>\",\"Age (Months): 2 <br> Listed: 3 days<br>\",\"Age (Months): 96 <br> Listed: 2 days<br>\",\"Age (Months): 72 <br> Listed: 2 days<br>\",\"Age (Months): 24 <br> Listed: 37 days<br>\",\"Age (Months): 96 <br> Listed: 13 days<br>\",\"Age (Months): 3 <br> Listed: 14 days<br>\",\"Age (Months): 12 <br> Listed: 2 days<br>\",\"Age (Months): 2 <br> Listed: 5 days<br>\",\"Age (Months): 2 <br> Listed: 7 days<br>\",\"Age (Months): 2 <br> Listed: 7 days<br>\",\"Age (Months): 2 <br> Listed: 7 days<br>\",\"Age (Months): 2 <br> Listed: 2 days<br>\",\"Age (Months): 2 <br> Listed: 3 days<br>\",\"Age (Months): 48 <br> Listed: 1 days<br>\",\"Age (Months): 192 <br> Listed: 28 days<br>\",\"Age (Months): 192 <br> Listed: 5 days<br>\",\"Age (Months): 3 <br> Listed: 1 days<br>\",\"Age (Months): 3 <br> Listed: 1 days<br>\",\"Age (Months): 3 <br> Listed: 10 days<br>\",\"Age (Months): 3 <br> Listed: 17 days<br>\",\"Age (Months): 3 <br> Listed: 12 days<br>\",\"Age (Months): 3 <br> Listed: 12 days<br>\",\"Age (Months): 36 <br> Listed: 17 days<br>\",\"Age (Months): 2 <br> Listed: 3 days<br>\",\"Age (Months): 2 <br> Listed: 9 days<br>\",\"Age (Months): 2 <br> Listed: 7 days<br>\",\"Age (Months): 2 <br> Listed: 0 days<br>\",\"Age (Months): 2 <br> Listed: 0 days<br>\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":[\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(94,174,245,1)\",\"rgba(97,181,252,1)\",\"rgba(99,184,255,1)\",\"rgba(97,181,252,1)\",\"rgba(93,170,242,1)\",\"rgba(93,170,242,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(99,184,255,1)\",\"rgba(91,167,239,1)\",\"rgba(97,181,252,1)\",\"rgba(96,177,248,1)\",\"rgba(94,174,245,1)\",\"rgba(94,174,245,1)\",\"rgba(84,150,223,1)\",\"rgba(94,174,245,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(93,170,242,1)\",\"rgba(97,181,252,1)\",\"rgba(91,167,239,1)\",\"rgba(87,157,229,1)\",\"rgba(85,153,226,1)\",\"rgba(91,167,239,1)\",\"rgba(90,163,235,1)\",\"rgba(97,181,252,1)\",\"rgba(99,184,255,1)\",\"rgba(99,184,255,1)\",\"rgba(97,181,252,1)\",\"rgba(76,133,207,1)\",\"rgba(94,174,245,1)\",\"rgba(93,170,242,1)\",\"rgba(96,177,248,1)\",\"rgba(96,177,248,1)\",\"rgba(91,167,239,1)\",\"rgba(91,167,239,1)\",\"rgba(91,167,239,1)\",\"rgba(91,167,239,1)\",\"rgba(85,153,226,1)\",\"rgba(77,136,210,1)\",\"rgba(79,140,213,1)\",\"rgba(77,136,210,1)\",\"rgba(94,174,245,1)\",\"rgba(94,174,245,1)\",\"rgba(79,140,213,1)\",\"rgba(94,174,245,1)\",\"rgba(48,79,154,1)\",\"rgba(68,117,191,1)\",\"rgba(44,73,148,1)\",\"rgba(99,184,255,1)\",\"rgba(79,140,213,1)\",\"rgba(87,157,229,1)\",\"rgba(87,157,229,1)\",\"rgba(79,140,213,1)\",\"rgba(87,157,229,1)\",\"rgba(71,123,197,1)\",\"rgba(96,177,248,1)\",\"rgba(96,177,248,1)\",\"rgba(96,177,248,1)\",\"rgba(96,177,248,1)\",\"rgba(96,177,248,1)\",\"rgba(96,177,248,1)\",\"rgba(93,170,242,1)\",\"rgba(96,177,248,1)\",\"rgba(94,174,245,1)\",\"rgba(97,181,252,1)\",\"rgba(94,174,245,1)\",\"rgba(97,181,252,1)\",\"rgba(94,174,245,1)\",\"rgba(94,174,245,1)\",\"rgba(99,184,255,1)\",\"rgba(99,184,255,1)\",\"rgba(99,184,255,1)\",\"rgba(81,143,216,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(90,163,235,1)\",\"rgba(53,88,163,1)\",\"rgba(97,181,252,1)\",\"rgba(90,163,235,1)\",\"rgba(74,130,203,1)\",\"rgba(93,170,242,1)\",\"rgba(93,170,242,1)\",\"rgba(93,170,242,1)\",\"rgba(96,177,248,1)\",\"rgba(99,184,255,1)\",\"rgba(91,167,239,1)\",\"rgba(91,167,239,1)\",\"rgba(84,150,223,1)\",\"rgba(93,170,242,1)\",\"rgba(97,181,252,1)\",\"rgba(93,170,242,1)\",\"rgba(96,177,248,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(85,153,226,1)\",\"rgba(88,160,232,1)\",\"rgba(96,177,248,1)\",\"rgba(97,181,252,1)\",\"rgba(96,177,248,1)\",\"rgba(97,181,252,1)\",\"rgba(91,167,239,1)\",\"rgba(93,170,242,1)\",\"rgba(88,160,232,1)\",\"rgba(90,163,235,1)\",\"rgba(90,163,235,1)\",\"rgba(90,163,235,1)\",\"rgba(90,163,235,1)\",\"rgba(97,181,252,1)\",\"rgba(84,150,223,1)\",\"rgba(82,146,219,1)\",\"rgba(99,184,255,1)\",\"rgba(88,160,232,1)\",\"rgba(91,167,239,1)\",\"rgba(94,174,245,1)\",\"rgba(96,177,248,1)\",\"rgba(96,177,248,1)\",\"rgba(39,64,139,1)\",\"rgba(79,140,213,1)\",\"rgba(77,136,210,1)\",\"rgba(96,177,248,1)\",\"rgba(91,167,239,1)\",\"rgba(88,160,232,1)\",\"rgba(88,160,232,1)\",\"rgba(88,160,232,1)\",\"rgba(96,177,248,1)\",\"rgba(94,174,245,1)\",\"rgba(97,181,252,1)\",\"rgba(55,91,166,1)\",\"rgba(91,167,239,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(84,150,223,1)\",\"rgba(73,127,200,1)\",\"rgba(81,143,216,1)\",\"rgba(81,143,216,1)\",\"rgba(73,127,200,1)\",\"rgba(94,174,245,1)\",\"rgba(85,153,226,1)\",\"rgba(88,160,232,1)\",\"rgba(99,184,255,1)\",\"rgba(99,184,255,1)\"],\"opacity\":0.80000000000000004,\"size\":15.118110236220474,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":[\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(94,174,245,1)\",\"rgba(97,181,252,1)\",\"rgba(99,184,255,1)\",\"rgba(97,181,252,1)\",\"rgba(93,170,242,1)\",\"rgba(93,170,242,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(99,184,255,1)\",\"rgba(91,167,239,1)\",\"rgba(97,181,252,1)\",\"rgba(96,177,248,1)\",\"rgba(94,174,245,1)\",\"rgba(94,174,245,1)\",\"rgba(84,150,223,1)\",\"rgba(94,174,245,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(97,181,252,1)\",\"rgba(93,170,242,1)\",\"rgba(97,181,252,1)\",\"rgba(91,167,239,1)\",\"rgba(87,157,229,1)\",\"rgba(85,153,226,1)\",\"rgba(91,167,239,1)\",\"rgba(90,163,235,1)\",\"rgba(97,181,252,1)\",\"rgba(99,184,255,1)\",\"rgba(99,184,2"},{"title":"Rent","url":"325Analyses/Rent.html","content":"Code Show All Code Hide All Code Rent Week 1 Analysis # This gray area is called an \"R-chunk\". # These library commands install some powerful functions for your use later on. library(mosaic) library(pander) library(tidyverse) library(DT) # This read_csv command reads in the \"Rent\" data set into an object called \"Rent\" Rent <- read_csv(\"../Data/Rent.csv\") # To load this data set into your R-Console do the following: # 1. From your top file menu select \"Session -> Set Working Directory -> To Source File Location\" # 2. Press the green \"play button\" in the top right corner of this gray box (which is called an \"R-chunk\"). # 3. Then in your \"Console\" window of Background Here is a data table showing the available approved housing apartment options at BYU-Idaho for single students. There are 122 entries comprising 57 female and 65 male apartment options. # Code to get you started. # View(...) works great in the Console, but datatable(...) must be # used instead within an R-chunk. datatable(Rent, options=list(lengthMenu = c(3,10,30)), extensions=\"Responsive\") {\"x\":{\"filter\":\"none\",\"vertical\":false,\"extensions\":[\"Responsive\"],\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\"],[\"ABBY LANE MANOR\",\"ABRI APARTMENTS - MEN\",\"ABRI APARTMENTS - WOMEN\",\"ALLDREDGE HOUSE\",\"ALLEN'S SUNRISE VILLAGE - MEN\",\"ALLEN'S SUNRISE VILLAGE - WOMEN\",\"ALPINE CHALET\",\"AMERICAN AVENUE - MEN\",\"AMERICAN AVENUE - WOMEN\",\"ARCADIA APARTMENTS\",\"AT THE GROVE\",\"AUTUMN WINDS - MEN\",\"AUTUMN WINDS - WOMEN\",\"AVONLEA - MEN\",\"AVONLEA - WOMEN\",\"BAYSIDE MANOR\",\"BIRCH PLAZA - MEN\",\"BIRCH PLAZA - WOMEN\",\"BIRCH WOOD I\",\"BIRCH WOOD II\",\"BLUE DOOR, THE - MEN\",\"BLUE DOOR, THE - WOMEN\",\"BOUNTIFUL PLACE\",\"BRIARWOOD APARTMENTS\",\"BRIGHTON APARTMENTS - MEN\",\"BRIGHTON APARTMENTS - WOMEN\",\"BROOKLYN APARTMENTS\",\"BROOKSIDE VILLAGE - MEN\",\"BROOKSIDE VILLAGE - WOMEN\",\"BUENA VISTA\",\"BUNKHOUSE\",\"CAMDEN APARTMENTS - MEN\",\"CAMDEN APARTMENTS - WOMEN\",\"CARRIAGE HOUSE\",\"CARRIAGE HOUSE TOWNHOUSE\",\"CEDARS, THE - MEN\",\"CEDARS, THE - WOMEN\",\"CENTRE SQUARE - MEN\",\"CENTRE SQUARE - WOMEN\",\"CHAPMAN HOUSE\",\"CLARKE APARTMENTS\",\"COLLEGE AVENUE APARTMENTS\",\"COLONIAL HEIGHTS TOWNHOUSES\",\"COLONIAL HOUSE - MEN\",\"COLONIAL HOUSE - WOMEN\",\"COTTONWOOD - MEN\",\"COTTONWOOD - WOMEN\",\"COVE, THE - MEN\",\"COVE, THE - WOMEN\",\"CREEKSIDE COTTAGES - MEN\",\"CREEKSIDE COTTAGES - WOMEN\",\"CRESTWOOD APARTMENTS\",\"CRESTWOOD COTTAGE\",\"CRESTWOOD HOUSE\",\"DAVENPORT APARTMENTS\",\"DELTA PHI APARTMENTS\",\"GATES, THE - MEN\",\"GATES, THE - WOMEN\",\"GEORGETOWN APARTMENTS\",\"GREENBRIER - MEN\",\"GREENBRIER - WOMEN\",\"GREENBRIER HOUSE\",\"HERITAGE - MEN\",\"HERITAGE - WOMEN\",\"HILLCREST TOWNHOUSES - MEN\",\"HILLCREST TOWNHOUSES - WOMEN\",\"HILL'S COLLEGE AVE APTS\",\"JACOB'S HOUSE\",\"JORDAN RIDGE\",\"KENSINGTON MANOR - MEN\",\"KENSINGTON MANOR - WOMEN\",\"LA JOLLA - MEN\",\"LA JOLLA - WOMEN\",\"LANDING, THE - MEN\",\"LANDING, THE - WOMEN\",\"LEGACY RIDGE\",\"LODGE, THE - MEN\",\"LODGE, THE - WOMEN\",\"MILANO FLATS - MEN\",\"MILANO FLATS - WOMEN\",\"MOUNTAIN CREST\",\"NAUVOO HOUSE I\",\"NAUVOO HOUSE II\",\"NORTHPOINT - MEN\",\"NORTHPOINT - WOMEN\",\"PARK VIEW APTS - MEN\",\"PARK VIEW APTS - WOMEN\",\"PINCOCK HOUSE\",\"PINES, THE - MEN\",\"PINES, THE - WOMEN\",\"PINNACLE POINT\",\"RED DOOR, THE\",\"RIVIERA APARTMENTS\",\"ROCKLAND APARTMENTS\",\"ROYAL CREST\",\"SHELBOURNE APARTMENTS\",\"SNOWVIEW APARTMENTS\",\"SOMERSET APARTMENTS - MEN\",\"SOMERSET APARTMENTS - WOMEN\",\"SPORI VILLA\",\"SUNDANCE - MEN\",\"SUNDANCE - WOMEN\",\"SUNSET HALL\",\"TOWERS I\",\"TOWERS II\",\"UNIVERSITY VIEW - MEN\",\"UNIVERSITY VIEW - WOMEN\",\"WHITFIELD HOUSE\",\"WINDSOR MANOR - MEN\",\"WINDSOR MANOR - WOMEN\"],[\"F\",\"M\",\"F\",\"M\",\"M\",\"F\",\"M\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"F\",\"M\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"M\",\"M\",\"F\",\"F\",\"M\",\"F\",\"F\",\"M\",\"M\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"F\",\"F\",\"M\",\"F\",\"M\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"M\",\"F\",\"M\",\"M\",\"F\",\"M\",\"M\",\"F\",\"M\",\"F\",\"F\",\"M\",\"M\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"M\",\"F\",\"M\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"M\",\"F\",\"F\",\"M\",\"F\",\"M\",\"F\",\"F\",\"M\",\"F\"],[\"437 S 4TH W\",\"220 E 1ST S\",\"220 E 1ST S\",\"243 S 1ST E\",\"48 W 2ND S\",\"48 W 2ND S\",\"460 S 2ND W\",\"151 S 1ST E\",\"151 S 1ST E\",\"138 W 4th S\",\"349 HARVARD AVE\",\"160 W 5TH S\",\"160 W 5TH S\",\"175 W 3RD S\",\"175 W 3RD S\",\"248 1/2 CORNELL AVE\",\"236 S 1ST W\",\"236 S 1ST W\",\"253 S 2ND W\",\"253 S 2ND W\",\"123 PRINCETON CT\",\"123 PRINCETON CT\",\"345 W 5TH S\",\"163 E 2ND S\",\"242 W 6TH S\",\"225 W 6th S #9\",\"345 S 2ND W\",\"487 S 3RD W\",\"487 S 3RD W\",\"406 S 3RD E\",\"156 W 4TH S\",\"225 W 5TH S\",\"225 W 5TH S\",\"246 S 1ST W\",\"246 S 1 W APT 114\",\"120 W 2ND S\",\"155 W 2nd S\",\"650 S 1ST W\",\"650 S 1ST W\",\"179 E 2ND S\",\"156 S CENTER ST\",\"150 COLLEGE AVE\",\"251 E 4TH S\",\"151 W 4TH S\",\"151 W 4TH S\",\"42 S 1ST W\",\"42 S 1ST W\",\"220 S 2ND W\",\"220 S 2ND W\",\"276 STEINER AVE\",\"276 STEINER AVE\",\"235 S 1ST E\",\"221 S 1ST E\",\"235 S 1ST E\",\"145 W 3RD S\",\"139 VIKING DR\",\"370 W 7TH S\",\"350 W 6TH S\",\"236 W 3RD S\",\"129 PRINCETON CT\",\"129 PRINCETON CT\",\"129 PRINCETON CT\",\"129 W 4TH S\",\"129 W 4TH S\",\"465 S 2ND E\",\"465 S 2ND E\",\"136 COLLEGE AVE\",\"149 E 2ND S\",\"291 E 7TH S\",\"345 S 2ND E\",\"345 S 2ND E\",\"65 S 1ST W\",\"65 S 1ST W\",\"359 W 4TH S\",\"359 W 4TH S\",\"667 S 2ND E\",\"538 S 2ND W\",\"538 S 2ND W\",\"440 S 2ND W\",\"440 S 2ND W\",\"132 S 3RD W\",\"175 W 5TH S\",\"175 W 5TH S\",\"141 S 1ST W\",\"141 S 1ST W\",\"154 S 3RD W\",\"154 S 3RD W #1101\",\"163 E 2ND S\",\"140 W 2ND S\",\"140 W 2ND S\",\"51 S 1ST E\",\"135 S 1st E\",\"245 S 1ST E\",\"235 W 4TH S\",\"340 S 1ST W\",\"570 S 2ND W\",\"12 W 2ND S\",\"480 S 1ST W\",\"480 S 1ST W\",\"174 COLLEGE AVE\",\"266 W 3RD S\",\"266 W 3RD S\",\"175 W 3RD S\",\"333 W 6TH S\",\"335 W 5TH S\",\"649 S 2ND W\",\"649 S 2ND W\",\"175 E 2ND S\",\"125 E 2ND S\",\"125 E 2ND S\"],[\"(208) 643-3731\",\"(208) 417-7223\",\"(208) 417-7223\",\"986-231-8518\",\"208-356-3995\",\"208-356-3995\",\"(208) 356-9282\",\"208-356-5097\",\"208-356-5097\",\"208-881-2935\",\"512-363-2396\",\"(208) 356-0821\",\"(208) 356-0821\",\"208-359-0920\",\"(208) 359-0920\",\"208-794-0652\",\"(208) 359-8200\",\"(208) 359-8200\",\"(208) 359-8200\",\"(208) 359-8200\",\"(208) 356-3001\",\"(208) 356-3001\",\"(208) 351-7080\",\"208-356-3001\",\"208-356-9693\",\"208-356-9693\",\"208-356-9500\",\"(208) 356-0788\",\"(208) 356-0788\",\"208-359-9681\",\"208-356-7419\",\"208-656-9611\",\"208-656-9611\",\"208-356-4361\",\"208-356-4361\",\"208-356-0222\",\"208-346-7006\",\"208-496-9220\",\"208-496-9220\",\"208-356-3001\",\"(208) 356-6660\",\"208-313-8182\",\"208-356-6752\",\"208-359-2848\",\"208-359-2848\",\"208-356-9576\",\"(208) 356-9576\",\"208-701-4212\",\"208-701-4212\",\"(208) 270-3207\",\"(208) 270-3207\",\"208-274-3828\",\"208-274-3828\",\"208-274-3828\",\"208-356-3216\",\"208-359-8015\",\"208-356-6686\",\"208-356-6686\",\"208-359-3191\",\"208-356-3001\",\"208-356-3001\",\"208-356-3001\",\"208-356-7952\",\"208-356-7952\",\"208-356-7211\",\"208-356-7211\",\"3565786/3136745\",\"208-356-3001\",\"208-359-2221\",\"208-356-4473\",\"208-356-4473\",\"(208) 359-1985\",\"(208) 359-1985\",\"208-356-9560\",\"208-356-9560\",\"208-359-2211\",\"208-356-5638\",\"208-356-5638\",\"(208) 356-3480\",\"208-356-3480\",\"(208) 359-4193\",\"(208)356-7756\",\"208-356-7756\",\"208-681-9542\",\"208-681-9542\",\"208 356-3697\",\"208-356-3697\",\"208-356-3001\",\"208-356-5605\",\"208-356-5605\",\"208-360-2845\",\"208-356-3001\",\"208-360-6790\",\"(208) 356-0923\",\"208-356-7778\",\"208-656-0746\",\"(208) 359-5517\",\"(208) 356-7699\",\"(208) 356-7699\",\"(208) 351-8072\",\"208-881-4685\",\"208-360-0097\",\"208-359-0920\",\"208-359-5965\",\"208-390-3706\",\"208-356-6556\",\"(208) 356-6556\",\"208-356-3001\",\"208-356-6789\",\"208-356-6789\"],[1200,1660,1660,1045,1181.67,1225,1030,1505,1225,995,1250,1725,1725,1177,1398.33,985,1301.67,1352.5,1370,1425,1025,997.5,1099,1057.5,1325,1400,995,1650,1650,900,995,1575,1616.67,1256.67,1285,1595,1695,1449,1449,1435,1025,1200,900,1405,1415,995,995,1668,1599,1400,1366.67,995,995,895,895,1145,1752.5,1752.5,1145,1115,1216.67,1320,1300,1300,1425,1425,880,1220,1574,1175,1300,1179,1179,1640,1640,1574,1536.5,1675.67,1790,1790,1120,1450,1450,1642.33,1599,1300,1333.33,1625,892,894.33,1250,1045,975,1127.5,995,1295,1345,1200,1200,1045,1645,1645,1027.5,1550,1645,1375,1350,1432.5,1749,1749],[1350,1670,1670,1095,1225,1225,1085,1695,1225,995,1250,1725,1725,1290,1725,985,1505,1505,1505,1545,1025,1075,1099,1120,1375,1425,995,1650,1650,900,995,1575,1650,1260,1285,1595,1695,1499,1499,1625,1025,1350,900,1405,1415,995,995,1725,1649,1500,1500,995,995,895,895,1195,1820,1820,1145,1115,1625,1320,1300,1300,1625,1625,880,1220,1649,1175,1325,1179,1179,1640,1640,1649,1609,1809,1820,1820,1145,1450,1450,1774,1739,1600,1600,1625,899,899,1250,1045,975,1260,995,1375,1445,1200,1200,1045,1675,1675,1060,1550,1695,1375,1375,1625,1749,1749],[42,132,144,8,138,144,162,86,138,41,6,24,96,76,66,8,98,173,65,24,24,24,130,24,108,60,144,60,86,21,324,24,53,234,8,354,444,304,546,7,42,8,64,144,192,60,84,224,342,10,24,50,10,11,71,60,282,300,72,48,92,6,180,192,64,76,8,11,360,108,108,72,210,148,166,288,474,550,142,142,42,288,338,476,548,51,40,7,200,126,140,48,82,160,342,168,64,108,180,9,156,152,14,168,248,264,450,7,136,208],[35,204,204,8,133,133,111,110,110,30,6,150,150,31,54,8,60,100,20,20,25,25,85,50,165,165,45,98,96,7,195,65,70,85,85,300,230,100,200,50,36,8,40,199,199,100,100,450,450,10,28,30,10,10,31,44,206,224,51,40,40,40,145,145,60,60,0,50,350,95,95,200,200,82,82,288,575,575,240,240,42,310,310,389,389,61,0,50,200,173,84,25,38,150,135,118,31,80,105,13,100,100,10,168,248,50,100,50,87,87],[75,150,150,150,150,150,150,150,150,100,150,150,150,80,80,90,150,150,150,150,80,80,75,80,150,150,150,150,150,50,150,150,150,80,80,125,125,75,75,80,100,100,100,150,150,150,150,100,100,100,100,75,75,75,100,150,150,150,150,80,80,80,150,150,150,150,70,80,150,125,125,150,150,100,100,150,100,100,150,150,100,150,150,100,100,100,100,80,150,150,100,80,200,150,150,100,100,150,150,200,100,75,80,100,100,150,150,80,300,300],[3,3,3,3,6,3,6,7,3,6,1,1,1,15,6,1,6,3,3,3,2,3,5,3,10,3,3,1,5,6,6,1,6,5,1,1,1,6,12,3,6,3,1,1,1,1,1,10,10,3,6,3,3,3,3,3,3,3,1,1,9,1,1,1,3,3,1,1,3,1,3,1,1,2,2,3,10,21,3,3,3,1,1,36,34,10,6,1,3,6,10,1,3,21,3,3,3,1,1,1,6,6,3,1,3,2,5,3,11,11],[\"www.abbylaneapartments.com\",\"www.liveabri.com\",\"liveabri.com\",null,\"www.sunrisevillageapts.com\",\"www.sunrisevillageapts.com\",\"www.rexburghousing.com/alpine-chalet\",\"www.myamericanavenue.com\",\"myamericanavenue.com\",\"www.arcadiarexburg.com\",\"atthegroveapts.com\",\"www.rexburghousing.com/autumn-winds\",\"www.rexburghousing.com/autumn-winds\",\"www.rexburgstudenthousing.com/new/property/14\",\"www.rexburgstudenthousing.com/new/property/13\",\"baysidemanor.blogspot.com\",\"www.rexburghousing.com/birch-plaza\",\"www.rexburghousing.com/birch-plaza\",\"www.rexburghousing.com/birch-plaza\",\"www.rexburghousing.com/birch-plaza\",\"www.rexburgstudenthousing.com/new/property/1\",\"www.rexburgstudenthousing.com/new/property/1\",\"www.bountifulplace.com\",\"www.rexburgstudenthousing.com/new/property/3\",\"www.brightonhousing.net\",\"www.brightonhousing.net\",\"craiglandhousing.com/new/property/1\",\"www.rexburghousing.com/brookside-village\",\"www.rexburghousing.com/brookside-village\",\"buenavistaapartments.blogspot.com/p/home.html\",\"bunkhouseapts.com\",\"www.camdenrexburg.com/\",\"www.camdenapts.net\",\"www.rexburgstudenthousing.com/new/property/10\",\"www.rexburgstudenthousing.com/new/property/11\",\"Cedarshousing.com\",\"Cedarshousing.com\",\"www.byui.edu/housing/centre-square\",\"www.byui.edu/housing/centre-square\",\"www.rexburgstudenthousing.com/new/property/7\",\"www.clarkeapartments.com\",null,\"WWW.COLONIALHEIGHTSTOWNHOUSES.COM\",\"www.colonial-house.com\",\"https://www.the-colonial-house.com/\",\"www.cottonwoodstudentapartments.com\",\"www.cottonwoodstudentapartments.com\",\"rexburgcove.com\",\"rexburgcove.com\",\"creeksidemensbyui.blogspot.com\",\"creeksidewomenbyui.blogspot.com\",\"crestwoodrexburg.managebuilding.com/Resident/public/home\",\"crestwoodrexburg.managebuilding.com/Resident/public/home\",\"crestwoodrexburg.managebuilding.com/Resident/public/home\",\"davenportapartments.net\",\"www.deltaphiapts.com\",\"www.thegatesatrexburg.com\",\"www.thegatesatrexburg.com\",\"rexburghousing.com/georgetown\",\"www.rexburgstudenthousing.com/new/property/8\",\"www.rexburgstudenthousing.com/new/property/8\",\"www.rexburgstudenthousing.com/new/property/9\",\"HeritageRexburg.com\",\"HeritageRexburg.com\",\"www.hillcrest-townhouses.com\",\"www.hillcrest-townhouses.com\",null,\"www.rexburgstudenthousing.com/new/property/4\",\"www.meet-me-at-the-ridge.com\",\"www.kensingtonmanorrexburg.com\",\"www.kensingtonmanorrexburg.com\",\"www.LaJollaRexburg.com\",\"www.LaJollaRexburg.com\",\"thelandingrexburg.com\",\"thelandingrexburg.com\",\"www.meet-me-at-the-ridge.com\",\"www.rexburglodge.com\",\"www.rexburglodge.com\",\"www.milanoflats.com/\",\"www.milanoflats.com/\",\"www.mntcrestapts.com\",\"WWW.NAUVOOHOUSE.COM\",\"WWW.NAUVOOHOUSE.COM\",\"www.NorthPointRexburg.com\",\"www.NorthPointRexburg.com\",\"www.parkviewrexburg.com\",\"www.parkviewrexburg.com\",\"www.rexburgstudenthousing.com/new/property/5\",\"pineshousing.com\",\"pineshousing.com\",\"www.rexburgpinnacle.com\",\"www.rexburgstudenthousing.com/new/property/2\",\"WWW.RIVIERA-APTS.COM\",\"rocklandrexburg.com/\",\"WWW.ROYALCRESTAPARTMENTS.COM\",\"www.shelbourneapts.com\",\"www.snowviewapts.com\",\"www.rexburghousing.com/somerset\",\"www.rexburghousing.com/somerset\",null,\"www.sundancerexburg.com\",\"sundancerexburg.com\",\"www.rexburgstudenthousing.com/new/property/15\",\"thetowersrexburg.com\",\"thetowerstwo.com\",\"Universityviewrexburg.com\",\"Universityviewrexburg.com\",\"www.rexburgstudenthousing.com/new/property/6\",\"www.windsormanor.net\",\"www.windsormanor.net\"],[43.81710724,43.82340456,43.82340456,43.82074947,43.82275434,43.82275434,43.81603117,43.8227079,43.8227079,43.81813297,43.81865937,43.81599246,43.81599246,43.820138,43.81944123,43.82065271,43.82093525,43.82093525,43.8203779,43.8203779,43.82347422,43.82347422,43.81474992,43.82235957,43.8129693,43.8129693,43.81866711,43.81582215,43.81582215,43.8174285,43.8184852,43.81539249,43.81539249,43.8207456,43.8207848,43.82246407,43.82142293,43.81262865,43.81176154,43.822054,43.82252986,43.82300204,43.81789299,43.81713046,43.81713046,43.82469723,43.82469723,43.82126811,43.82126811,43.81998311,43.81998311,43.82107459,43.82124102,43.82093912,43.81930189,43.81708401,43.81406091,43.81406091,43.82025017,43.82296721,43.82296721,43.82296721,43.8170066,43.8170066,43.81597311,43.81597311,43.82271564,43.82206542,43.8120309,43.81843488,43.81843488,43.82439535,43.82439535,43.817429,43.817429,43.81212542,43.8146067,43.8146067,43.81698337,43.81698337,43.82296721,43.81437058,43.81437058,43.82217379,43.82217379,43.82278143,43.82278143,43.822094,43.82246794,43.82246794,43.82454242,43.82312977,43.82015341,43.81721949,43.81913933,43.81358865,43.82217379,43.81633696,43.81633696,43.82230538,43.82002955,43.82002955,43.8200102,43.81314349,43.81508282,43.81259768,43.81259768,43.822083,43.82215444,43.82215444],[-111.7949519,-111.7774263,-111.7774263,-111.7806074,-111.7857894,-111.7857894,-111.790199,-111.779674,-111.779674,-111.7880264,-111.7763239,-111.7884877,-111.7884877,-111.788876,-111.789228,-111.7798457,-111.78819,-111.78819,-111.7891556,-111.7891556,-111.7792019,-111.7792019,-111.793769,-111.778923,-111.7904967,-111.7904967,-111.7890135,-111.7916045,-111.7916045,-111.7754656,-111.7876616,-111.7901078,-111.7901078,-111.7874953,-111.7867195,-111.7871144,-111.7872673,-111.7871064,-111.787675,-111.778304,-111.7841747,-111.7827103,-111.7762595,-111.7891958,-111.7891958,-111.7871091,-111.7871091,-111.7907327,-111.7907327,-111.7942867,-111.7942867,-111.7804787,-111.7806664,-111.7805055,-111.7882195,-111.7880532,-111.7939299,-111.7939299,-111.7905852,-111.7790142,-111.7790142,-111.7790142,-111.7877421,-111.7877421,-111.7773619,-111.7773619,-111.7826513,-111.7792341,-111.7758519,-111.7776436,-111.7776436,-111.7863473,-111.7863473,-111.79445,-111.79445,-111.7772037,-111.7914328,-111.7914328,-111.7899147,-111.7899147,-111.7928276,-111.7887694,-111.7887694,-111.7862615,-111.7862615,-111.7928329,-111.7928329,-111.778896,-111.7876455,-111.7876455,-111.7803338,-111.7802373,-111.7805055,-111.7907837,-111.7877153,-111.7902285,-111.784341,-111.7875221,-111.7875221,-111.7826727,-111.7920283,-111.7920283,-111.7883,-111.7925915,-111.7934042,-111.788308,-111.788308,-111.778657,-111.780645,-111.780645],[\"Abby Lane Manor is a women's complex offering spacious private and shared rooms that is just a ten minute walk from campus. Our girls enjoy amenities such as free laundry, personal vanities in each bedroom, 3 bathrooms, 2 fridges, A/C, free off track storage, and parking passes for only $25! As you can imagine we fill up extremely fast, so come sign up today!\",\"We offer two beautiful and unique floor plans! Our Deluxe is all one floor and our Townhome is two floors. Upscale, modern finishes, state-of-the-art fitness center, fully equipped clubhouse, study rooms, courtyard with fire pit &amp; grills! 3 FULL Baths! 55\\\" Smart TVs in each apt! Contact us today!\",\"Deluxe &amp; Townhome Floorplans! Offering early bird special of $100 off! Upscale, modern finishes, state-of-the-art fitness center, fully equipped clubhouse, study rooms, courtyard with firepit &amp; grills! 3 FULL Baths! 55\\\" Smart TVs in each apt! Ask us about our amazing rates with our track leases &amp; 12 month lease! Price reflects early bird discount.\",\"Hurry quick! Come make ALLDREDGE HOUSE your home away from home! Located directly across the street from the Clarke Building and only a few blocks away from down town. Great Ward with the guys right next door!\",\"Sale on multi-semester contracts happening now. Save up to $175 + FREE Laundry! We are well-known for our friendliness, two refrigerators, low prices, great location, our free laundry, reliable OPTIX internet and amazing managers. You'll love having all Sunrise men and women in your ward! We would love to show you why everyone loves to live at Sunrise Village. www.SunriseVillageApts.com\",\"Sale on multi-semester contracts happening now. Save up to $175 + FREE Laundry! We are well-known for our friendliness, two refridgerators, low prices, great location, our free laundry, reliable OPTIX internet and amazing managers. You'll love having all Sunrise men and women in your ward! We would love to show you why everyone loves to live at Sunrise Village. www.SunriseVillageApts.com\",\"Alpine Chalet is a great place to live! It is close to campus and surrounded by many BYUI housing complexes. It has an amazing pool, hot tub, and weight room. Not to mention, we added 2nd fridges and smart TVs to all of our apartments! There is an outdoor gas fire pit and built-in barbecue area that is great for hanging out with friends or studying.\",\"At American Avenue we have several apartment layouts for you to make home. We are currently under renovation and things are amazing! Our apartment homes have shared room or private room options next to campus and steps from downtown Rexburg. We offer covered parking and great savings when you sign multi-semester contracts. Right now you can save up to $175!\",\"At American Avenue we have several apartment layouts for you to make home. We are currently under renovation &amp; things are amazing! Our apartment homes have shared room or private room options next to campus &amp; steps from downtown Rexburg. We offer great savings when you sign multi-semester contracts &amp; a friend referral bonus! Right now you can save up to $175!\",\"Arcadia Apartments is located just across from the BYU-Idaho Center, literally steps from campus. Arcadia is priced at just $400. Arcadia offers clean, comfortable apartments with a very social atmosphere. Awesome amenities include; sand Volleyball, Basketball Court, Apple TV, and so much more. Call/text us at 208-881-2935 for info or to schedule a tour!\",\"At The Grove is a spacious basement apartment, located just a stone's throw away from the east side of cam"},{"title":"Stephanie’s Housing Request","url":"325Analyses/Stephanie.html","content":"Code Show All Code Hide All Code Stephanie’s Housing Request Week 2 Analysis library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) # If you get an error stating: # Error in library(DT): there is no package called 'DT' # You will need to run: install.packages(\"DT\") # in your Console, then try \"Knit HTML\" again. install.packages(\"dplyr\") Rent <- read_csv(\"../Data/Rent.csv\") Response Dear Stephanie, I am more than happy to help you learn about the different housing options available to you here at BYU-Idaho for this upcoming semester! Based on your message, your criteria for housing seems to be something that is cost effective and in a location that is within close proximity to campus and other students. I caution that there are other factors not included in the calculations I present throughout, but these should at least be considered when making your final decision. Factors such as weather—which can affect travel time—or rent being paid by semester instead of monthly should all be taken into account. This way, when you choose an apartment, you can adjust accordingly. With that, I have narrowed down the available housing units and found a few options that meet your desired requirements! Rent_filtered <- Rent %>% mutate(`Walking Minutes to the MC`= round((CrowFlyMetersToMC/1609)*22,0)) %>% select(Gender,Name,Address,`Walking Minutes to the MC`,Residents,AvgFloorPlanCost) %>% mutate(`Monthly Floor Plan Cost`=round(AvgFloorPlanCost/3.5,2))%>% filter(Gender == \"F\")%>% filter(`Monthly Floor Plan Cost` < 300) %>% filter(Residents > 20) %>% rename(`Semester Floor Plan Cost` = AvgFloorPlanCost) %>% rename(`Apartment Complex`= Name)%>% arrange(`Monthly Floor Plan Cost`) %>% select(`Apartment Complex`,Address,`Walking Minutes to the MC`, Residents, `Monthly Floor Plan Cost`,`Semester Floor Plan Cost`) BYU-Idaho Approved Female Housing Options For your convenience, I filtered out all the Men’s housing along with any Female housing that was over your set budget of $300. The options I’ve compiled include both the monthly rent and the semester rent, allowing you to compare the overall amount you’ll be spending in a given semester versus month-to-month. Additionally, I’ve filtered out any student housings that have less than 20 female residents present as well as included the walking distance in minutes from each apartment complex to BYU-Idaho’s Hyrum Manwaring Student Center (MC), which is likely to be a hub for social activities and meeting new people. # Code to get you started, be sure to use a subset of Rent instead of Rent in this code though. datatable(Rent_filtered, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") {\"x\":{\"filter\":\"none\",\"vertical\":false,\"extensions\":[\"Responsive\"],\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"],[\"PINES, THE - WOMEN\",\"DAVENPORT APARTMENTS\",\"BUENA VISTA\",\"RIVIERA APARTMENTS\",\"BROOKLYN APARTMENTS\",\"COTTONWOOD - WOMEN\",\"ROYAL CREST\",\"BLUE DOOR, THE - WOMEN\"],[\"140 W 2ND S\",\"145 W 3RD S\",\"406 S 3RD E\",\"245 S 1ST E\",\"345 S 2ND W\",\"42 S 1ST W\",\"340 S 1ST W\",\"123 PRINCETON CT\"],[8,6,8,3,7,11,6,8],[126,71,21,82,144,84,342,24],[255.52,255.71,257.14,278.57,284.29,284.29,284.29,285],[894.33,895,900,975,995,995,995,997.5]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>Apartment Complex<\\/th>\\n <th>Address<\\/th>\\n <th>Walking Minutes to the MC<\\/th>\\n <th>Residents<\\/th>\\n <th>Monthly Floor Plan Cost<\\/th>\\n <th>Semester Floor Plan Cost<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"lengthMenu\":[3,10,30],\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[3,4,5,6]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Apartment Complex\",\"targets\":1},{\"name\":\"Address\",\"targets\":2},{\"name\":\"Walking Minutes to the MC\",\"targets\":3},{\"name\":\"Residents\",\"targets\":4},{\"name\":\"Monthly Floor Plan Cost\",\"targets\":5},{\"name\":\"Semester Floor Plan Cost\",\"targets\":6}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"responsive\":true}},\"evals\":[],\"jsHooks\":[]}To help you visualize these options, I’ve created a map of Rexburg that includes all the listed apartment complexes, marked with blue indicators. Each marker shows the complex’s name, the monthly rent, and the walking distance from the Manwaring Center, which is highlighted with a light blue marker for reference. mc_icon <- makeAwesomeIcon(icon= \"university\",iconColor = \"white\",markerColor = \"lightblue\", library = \"fa\") leaflet(data = Rent_filtered)%>% addTiles() %>% setView(lng=-111.7833595717528, lat=43.82217013118815,zoom =14.511)%>% addAwesomeMarkers(lng=-111.7827756599601, lat=43.81821783971177, icon= mc_icon, popup= \"Manwarding Center(MC)\")%>% addMarkers(lng=-111.7876455, lat= 43.82246794, popup= \"PINES, WOMEN<br>Rent: $255.52<br>8 Min. Walk\")%>% addMarkers(lng=-111.7882195, lat=43.81930189, popup= \"DAVENPORT APARTMENTS<br>Rent: $255.71<br>6 Min. Walk\")%>% addMarkers(lng= -111.7754656, lat=43.8174285, popup= \"BUENA VISTA<br>Rent: $257.14<br>8 Min. Walk\")%>% addMarkers(lng=-111.7805055, lat=43.82015341, popup= \"RIVIERA APARTMENTS<br>Rent: $278.57<br>3 Min. Walk\")%>% addMarkers(lng=-111.7890135, lat=43.81866711, popup= \"BROOKLYN APARTMENTS<br>Rent: $284.29<br>7 Min. Walk\")%>% addMarkers(lng=-111.7871091, lat=43.82469723, popup= \"COTTONWOOD - WOMEN<br>Rent: $284.29<br>11 Min. Walk\")%>% addMarkers(lng=-111.7877153, lat=43.81913933, popup= \"ROYAL CREST<br>Rent: $284.29<br>6 Min. Walk\")%>% addMarkers(lng=-111.7792019, lat=43.82347422, popup= \"BLUE DOOR, WOMEN<br>Rent: $285<br>8 Min. Walk\")%>% addLegend(position=\"topright\", colors= c(\"lightblue\",\"cornflowerblue\"), labels = c(\"Manwarding Center (MC)\",\"Apartment Complexes\"), title= \"Building Locations\")%>% addControl(\"<strong>Click the marker to see additional information<\/strong>\",position = \"topright\", className = \"map-caption\") {\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"&copy; <a href=\\\"https://openstreetmap.org/copyright/\\\">OpenStreetMap<\\/a>, <a href=\\\"https://opendatacommons.org/licenses/odbl/\\\">ODbL<\\/a>\"}]},{\"method\":\"addAwesomeMarkers\",\"args\":[43.81821783971177,-111.7827756599601,{\"icon\":\"university\",\"markerColor\":\"lightblue\",\"iconColor\":\"white\",\"spin\":false,\"squareMarker\":false,\"iconRotate\":0,\"font\":\"monospace\",\"prefix\":\"fa\"},null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"Manwarding Center(MC)\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addMarkers\",\"args\":[43.82246794,-111.7876455,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"PINES, WOMEN<br>Rent: $255.52<br>8 Min. Walk\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addMarkers\",\"args\":[43.81930189,-111.7882195,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"DAVENPORT APARTMENTS<br>Rent: $255.71<br>6 Min. Walk\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addMarkers\",\"args\":[43.8174285,-111.7754656,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"BUENA VISTA<br>Rent: $257.14<br>8 Min. Walk\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addMarkers\",\"args\":[43.82015341,-111.7805055,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"RIVIERA APARTMENTS<br>Rent: $278.57<br>3 Min. Walk\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addMarkers\",\"args\":[43.81866711,-111.7890135,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"BROOKLYN APARTMENTS<br>Rent: $284.29<br>7 Min. Walk\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addMarkers\",\"args\":[43.82469723,-111.7871091,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"COTTONWOOD - WOMEN<br>Rent: $284.29<br>11 Min. Walk\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addMarkers\",\"args\":[43.81913933,-111.7877153,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"ROYAL CREST<br>Rent: $284.29<br>6 Min. Walk\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addMarkers\",\"args\":[43.82347422,-111.7792019,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"BLUE DOOR, WOMEN<br>Rent: $285<br>8 Min. Walk\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addLegend\",\"args\":[{\"colors\":[\"lightblue\",\"cornflowerblue\"],\"labels\":[\"Manwarding Center (MC)\",\"Apartment Complexes\"],\"na_color\":null,\"na_label\":\"NA\",\"opacity\":0.5,\"position\":\"topright\",\"type\":\"unknown\",\"title\":\"Building Locations\",\"extra\":null,\"layerId\":null,\"className\":\"info legend\",\"group\":null}]},{\"method\":\"addControl\",\"args\":[\"<strong>Click the marker to see additional information<\\/strong>\",\"topright\",null,\"map-caption\"]}],\"setView\":[[43.82217013118815,-111.7833595717528],14.511,[]],\"limits\":{\"lat\":[43.8174285,43.82469723],\"lng\":[-111.7890135,-111.7754656]}},\"evals\":[],\"jsHooks\":[]} Is it within budget? Understanding the broader context of housing costs can be helpful in making your decision. I’ve prepared a five-number summary of the cost per month and per semester for BYU-Idaho Female student housing. It’s important to note that while your budget is $300 a month, some apartment complexes require payment for the whole semester upfront. With this in mind, since a semester is around 3 months you could be paying somewhere around $890 - $990. This comparison between monthly and semester costs can help you prepare for different payment structures and assess the value of each option. outputTable <- rbind(`Cost Per Month` = favstats(Rent_filtered$`Monthly Floor Plan Cost`), `Cost Per Semester`= favstats(Rent_filtered$`Semester Floor Plan Cost`)) pander(outputTable[c(\"min\",\"Q1\",\"median\",\"mean\",\"Q3\",\"max\")], caption=\"<strong>BYU-Idaho Female Housing Rent Summary<\/strong>\") BYU-Idaho Female Housing Rent Summary   min Q1 median mean Q3 max Cost Per Month 255.5 256.8 281.4 273.1 284.3 285 Cost Per Semester 894.3 898.8 985 955.9 995 997.5 Is it hospitable? Since you expressed a preference for a social environment, I’ve also included a graph showing the housing choices of other female students in relation to the complex’s proximity to campus. The color intensity indicates the walking minutes from the apartment complex to the Manwarding Center, with blue shades representing closer options, yellow shades indicating farther ones, and green shades indicating them as the in between. The dot size specifies rent, bigger dots being on the expensive side and the smaller dots on the cheaper side. Hovering over each dot will tell you which apartment it is as well as the rent cost and resident count. This visualization can help you gauge the balance between social activity and cost, as it shows where most female students choose to live and how much they’re willing to spend for a socially active apartment. custom_colorscale<- list( list(0, \"blue\"), list(0.13,\"blue\"), list(0.14,\"yellow\"), list(0.153,\"red\")) plot_ly( Rent_filtered, x= ~`Walking Minutes to the MC`, y= ~Residents, size= ~`Monthly Floor Plan Cost`, color = ~`Walking Minutes to the MC`, colorscale= custom_colorscale, text = ~paste(`Apartment Complex`,\"\\n\",\"Residents :\",Residents,\"\\n\",\"$\",`Monthly Floor Plan Cost`), hoverinfo= 'text')%>% plotly::layout ( title = \"Popular BYU-Idaho Female Student Housing\", xaxis = list(title = \"Walking Minutes to the MC\"), yaxis = list(title = \"Residents\")) {\"x\":{\"visdat\":{\"16d0be76ec1\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"16d0be76ec1\",\"attrs\":{\"16d0be76ec1\":{\"x\":{},\"y\":{},\"colorscale\":[[0,\"blue\"],[0.13,\"blue\"],[0.14000000000000001,\"yellow\"],[0.153,\"red\"]],\"text\":{},\"hoverinfo\":\"text\",\"color\":{},\"size\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20]}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"title\":\"Popular BYU-Idaho Female Student Housing\",\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Walking Minutes to the MC\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Residents\"},\"hovermode\":\"closest\",\"showlegend\":false,\"legend\":{\"yanchor\":\"top\",\"y\":0.5}},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[8,6,8,3,7,11,6,8],\"y\":[126,71,21,82,144,84,342,24],\"colorscale\":[[0,\"blue\"],[0.13,\"blue\"],[0.14000000000000001,\"yellow\"],[0.153,\"red\"]],\"text\":[\"PINES, THE - WOMEN <br /> Residents : 126 <br /> $ 255.52\",\"DAVENPORT APARTMENTS <br /> Residents : 71 <br /> $ 255.71\",\"BUENA VISTA <br /> Residents : 21 <br /> $ 257.14\",\"RIVIERA APARTMENTS <br /> Residents : 82 <br /> $ 278.57\",\"BROOKLYN APARTMENTS <br /> Residents : 144 <br /> $ 284.29\",\"COTTONWOOD - WOMEN <br /> Residents : 84 <br /> $ 284.29\",\"ROYAL CREST <br /> Residents : 342 <br /> $ 284.29\",\"BLUE DOOR, THE - WOMEN <br /> Residents : 24 <br /> $ 285\"],\"hoverinfo\":[\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"colorbar\":{\"title\":\"`Walking Minutes to the MC`\",\"ticklen\":2},\"cmin\":3,\"cmax\":11,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666667\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":false,\"color\":[8,6,8,3,7,11,6,8],\"size\":[10,10.580054274084118,14.945725915875098,80.369742198100383,97.832428765264652,97.832428765264652,97.832428765264652,100],\"sizemode\":\"area\",\"line\":{\"colorbar\":{\"title\":\"\",\"ticklen\":2},\"cmin\":3,\"cmax\":11,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666667\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":false,\"color\":[8,6,8,3,7,11,6,8]}},\"textfont\":{\"size\":[10,10.580054274084118,14.945725915875098,80.369742198100383,97.832428765264652,97.832428765264652,97.832428765264652,100]},\"error_y\":{\"width\":[]},\"error_x\":{\"width\":[]},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[3,11],\"y\":[21,342],\"type\":\"scatter\",\"mode\":\"markers\",\"opacity\":0,\"hoverinfo\":\"none\",\"showlegend\":false,\"marker\":{\"colorbar\":{\"title\":\"`Walking Minutes to the MC`\",\"ticklen\":2,\"len\":0.5,\"lenmode\":\"fraction\",\"y\":1,\"yanchor\":\"top\"},\"cmin\":3,\"cmax\":11,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666667\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":true,\"color\":[3,11],\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} Conclusion In summary, it appears that the most populated and campus-proximate apartments tend to be more expensive, while the more affordable options are typically further from campus and less densely populated. Ultimately, your choice of housing will depend on which factor you prioritize more. The Cheapest Option: PINES, THE - WOMEN Rent: $255.52 Residents : 126 Walk to the MC : 8 Mins. The Closest Option: RIVERA APARTMENTS Rent : $278.57 Residents : 82 Walk to the MC : 3 Mins. The Sociable Option: ROYAL CREST Rent : $284.29 Residents : 342 Walk to the"},{"title":"Student Housing Costs","url":"325Analyses/StudentHousing.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } button.code-folding-btn:focus { outline: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Student Housing Costs library(mosaic) library(DT) library(pander) library(plotly) Rent <- read.csv(\"../Data/Rent.csv\", header=TRUE) Background With the start of a new semester here at BYU-Idaho most college students have recently made decisions about where to live. Many factors go into housing decisions. But certainly some important considerations involve the cost of rent and distance from campus. So just how much does student housing in Rexburg cost? And are apartments close to campus more expensive? These are just a few possible questions one could ask as they look for housing in Rexburg. Data was collected from the BYU-Idaho Housing Website to answer these questions. The first three observations are shown in the following table. There are a total of 122 observations in the dataset. Clicking the green + signs opens more information about each apartment. {\"x\":{\"filter\":\"none\",\"extensions\":[\"Responsive\"],\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\"],[\"ABBY LANE MANOR\",\"ABODE DUPLEX\",\"ABRI APARTMENTS - MEN\",\"ABRI APARTMENTS - WOMEN\",\"ALBION APARTMENT\",\"ALEXANDER APARTMENT\",\"ALLDREDGE HOUSE \",\"ALLEN RIDGE APARTMENTS\",\"ALLEN'S SUNRISE VILLAGE - MEN\",\"ALLEN'S SUNRISE VILLAGE - WOMEN\",\"ALPINE CHALET\",\"ALTA VIEW APARTMENTS\",\"AMERICAN AVENUE - MEN\",\"AMERICAN AVENUE - WOMEN\",\"ARBOR COVE\",\"ARCADIA APARTMENTS\",\"ARPAD\",\"ASPEN VILLAGE - MEN \",\"ASPEN VILLAGE - WOMEN\",\"AT THE GROVE\",\"AUTUMN WINDS\",\"AVONLEA APARTMENTS\",\"BAYSIDE MANOR\",\"BIRCH PLAZA\",\"BIRCH WOOD I\",\"BIRCH WOOD II\",\"BLUE DOOR, THE\",\"BOUNTIFUL PLACE\",\"BRIARWOOD APARTMENTS\",\"BRIGHAM'S MILL\",\"BRIGHTON APARTMENTS-MEN\",\"BRIGHTON APARTMENTS-WOMEN\",\"BROOKLYN APARTMENTS\",\"BROOKSIDE VILLAGE - MEN\",\"BROOKSIDE VILLAGE - WOMEN\",\"BUENA VISTA\",\"BUNKHOUSE\",\"CAMDEN APARTMENTS-MEN\",\"CAMDEN APARTMENTS-WOMEN\",\"CAMPUS VIEW APARTMENTS\",\"CARRIAGE HOUSE\",\"CEDARS, THE-MEN\",\"CEDARS, THE-WOMEN\",\"CENTRE SQUARE-MEN\",\"CENTRE SQUARE-WOMEN\",\"CHAPMAN HOUSE\",\"CLARKE APARTMENTS\",\"COLLEGE AVENUE APARTMENTS\",\"COLONIAL HEIGHTS TOWNHOUSE\",\"COLONIAL HOUSE\",\"CONDIE COTTAGE\",\"COTTONWOOD-MEN\",\"COTTONWOOD-WOMEN\",\"CREEKSIDE COTTAGES - WOMEN\",\"CREEKSIDE COTTAGES-MEN\",\"CRESTWOOD APARTMENTS\",\"CRESTWOOD COTTAGE\",\"CRESTWOOD HOUSE\",\"DANBURY MANOR\",\"DAVENPORT APARTMENTS\",\"DELTA PHI APARTMENTS\",\"GATES, THE - MEN\",\"GATES, THE - WOMEN\",\"GEORGETOWN APARTMENTS\",\"GREENBRIER NORTH\",\"GREENBRIER SOUTH\",\"HARRIS HALL\",\"HEMMING HOUSE I\",\"HEMMING HOUSE III\",\"HEMMING HOUSE IV\",\"HERITAGE MEN\",\"HERITAGE WOMEN\",\"HILLCREST TOWNHOUSES\",\"HILL'S COLLEGE AVE APTS\",\"IVY, THE - Men\",\"IVY, THE - Women\",\"JACOB'S HOUSE\",\"JORDAN RIDGE\",\"KENSINGTON MANOR - MEN\",\"KENSINGTON MANOR - WOMEN\",\"LA JOLLA - MEN\",\"LA JOLLA - WOMEN\",\"LEGACY RIDGE\",\"MOUNTAIN CREST\",\"MOUNTAIN LOFTS - MEN\",\"MOUNTAIN LOFTS - WOMEN\",\"NAUVOO HOUSE I\",\"NAUVOO HOUSE II\",\"NORMANDY APARTMENTS\",\"NORTHPOINT - MEN\",\"NORTHPOINT - WOMEN\",\"PARK VIEW APTS - WOMEN\",\"PARK VIEW APTS-MEN\",\"PINCOCK HOUSE\",\"PINES NORTH, THE\",\"PINES SOUTH, THE\",\"PINNACLE POINT\",\"QUINCY HOUSE\",\"RED BRICK HOUSE\",\"RED DOOR, THE\",\"RIVIERA APARTMENTS\",\"ROCKLAND APARTMENTS\",\"ROOST, THE - MEN\",\"ROOST, THE - WOMEN\",\"ROYAL CREST\",\"SHADETREE APARTMENT\",\"SHELBOURNE APARTMENTS\",\"SNOWED INN, THE\",\"SNOWVIEW APARTMENTS\",\"SOMERSET APARTMENTS - MEN\",\"SOMERSET APARTMENTS - WOMEN\",\"SPORI VILLA\",\"SUNSET HALL\",\"TOWERS I\",\"TOWERS II\",\"TUSCANY PLACE - MEN\",\"TUSCANY PLACE - WOMEN\",\"WEBSTER HOUSE\",\"WEST WINDS\",\"WHITFIELD HOUSE\",\"WINDSOR MANOR-MEN\",\"WINDSOR MANOR-WOMEN\"],[\"F\",\"M\",\"M\",\"F\",\"F\",\"F\",\"F\",\"M\",\"M\",\"F\",\"M\",\"M\",\"M\",\"F\",\"M\",\"M\",\"M\",\"M\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"F\",\"M\",\"M\",\"M\",\"M\",\"M\",\"F\",\"F\",\"M\",\"F\",\"F\",\"M\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"M\",\"F\",\"M\",\"F\",\"F\",\"M\",\"F\",\"F\",\"M\",\"M\",\"F\",\"M\",\"M\",\"F\",\"M\",\"M\",\"F\",\"M\",\"M\",\"F\",\"M\",\"F\",\"M\",\"M\",\"M\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"M\",\"M\",\"F\",\"M\",\"F\",\"F\",\"M\",\"M\",\"F\",\"M\",\"F\",\"F\",\"M\",\"F\",\"F\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"F\",\"M\",\"F\",\"M\",\"M\",\"F\",\"F\",\"F\",\"M\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"F\",\"M\",\"M\",\"M\",\"F\"],[\"437 S 4TH W\",\"224 E 2ND S\",\"220 E 1ST S\",\"220 E 1ST S\",\"231 W 2ND S\",\"276 W 3RD S\",\"243 S 1ST E \",\"57 S CENTER ST\",\"48 W 2ND S\",\"48 W 2ND S \",\"460 S 2ND W\",\"346 W 2ND S\",\"151 S 1ST E\",\"151 S 1ST E\",\"220 S 2ND W\",\"138 VIKING DR\",\"271 S 2ND W\",\"545 S 2ND E \",\"545 S 2ND E\",\"349 HARVARD AVE\",\"160 W 5TH S\",\"175 W 3RD S\",\"248 1/2 CORNELL AVE\",\"236 S 1ST W\",\"253 S 2ND W\",\"253 S 2ND W\",\"123 PRINCETON CT\",\"345 W 5TH S\",\"163 E 2ND S\",\"431 S 3RD W\",\"242 W 6TH S \",\"242 W 6TH S \",\"345 S 2ND W\",\"487 S 3RD W\",\"487 S 3RD W\",\"406 S 3RD E\",\"156 W 4th S\",\"225 W 5TH S\",\"225 W 5TH S\",\"136 W 3RD S\",\"246 S 1ST W\",\"120 W 2ND S\",\"226 S 1st W\",\"650 S 1ST W\",\"650 S 1ST W\",\"179 E 2ND S\",\"156 S CENTER ST\",\"150 COLLEGE AVE\",\"251 E 4TH S\",\"151 W 4TH S\",\"267 W 3RD S\",\"42 S 1ST W\",\"42 S 1ST W\",\"276 STEINER AVE\",\"336 W 3RD S\",\"235 S 1ST E \",\"221 S 1ST E\",\"235 S 1ST E\",\"156 E 2ND S\",\"145 W 3RD S\",\"139 VIKING DR\",\"370 W 7TH S\",\"350 W 7TH S\",\"236 W 3RD S\",\"129 PRINCETON CT\",\"129 PRINCETON CT\",\"364 S 1ST W \",\"133 W 1ST S\",\"124 W 1ST S\",\"113 W 1ST S\",\"129 W 4TH S\",\"129 W 4TH S\",\"465 S 2ND E\",\"136 COLLEGE AVE\",\"649 S 2ND W\",\"649 S 2ND W\",\"149 E 2ND S\",\"291 E 7TH S \",\"345 S 2ND E\",\"345 S 2ND E\",\"65 S 1ST W\",\"65 S 1ST W\",\"667 S 2ND E\",\"132 S 3rd W\",\"538 S 2ND W\",\"538 S 2ND W\",\"175 W 5TH S\",\"175 W 5TH S\",\"114 W 3RD S\",\"141 S 1ST W\",\"141 S 1ST W\",\"154 S 3RD W\",\"154 S 3RD W\",\"163 E 2ND S\",\"140 W 2ND S\",\"140 W 2ND S\",\"51 S 1ST E\",\"264 S 2ND W\",\"128 W 3RD S\",\"135 S 1ST E\",\"277 S 1ST E\",\"235 W 4TH S\",\"486 S 3RD W\",\"486 S 3RD W\",\"340 S 1ST W\",\"60 W 1ST S\",\"570 S 2ND W\",\"271 W 2ND S\",\"12 W. 2nd S.\",\"480 S 1ST W\",\"480 S 1ST W\",\"174 COLLEGE AVE\",\"150 W 3RD S\",\"333 W 6TH S\",\"335 W 5TH S\",\"440 S 2ND W\",\"440 S 2ND W\",\"268 S 1ST W\",\"160 W 5TH S\",\"175 E 2ND S\",\"125 E 2ND S\",\"125 E 2ND S\"],[\"(208) 359-3687\",\"(912) 399-1107\",\"2084177223\",\"(208) 417-7223\",\"(801) 856-0732\",\"208-995-8543\",\"208-206-4697\",\"(801) 864-1708\",\"356-3995\",\"208-356-3995\",\"(208) 356-9282\",\"208-360-0097\",\"2083565097\",\"(208) 356-5097\",\"(208) 356-8988\",\"208-881-2935\",\"208-881-5026\",\"208-356-7701\",\"(208) 356-7701\",\"801-319-4334\",\"(208) 356-0821\",\"(208) 359-0920\",\"7202574493\",\"(208) 359-8200\",\"(208) 359-8200\",\"(208) 359-8200\",\"(208) 240-2261\",\"(208) 351-7080\",\"356-3001\",\"208-538-9058\",\"208-356-9693\",\"208-356-9693\",\"208-356-9500\",\"(208) 356-0788\",\"(208) 356-0788\",\"208-359-9681\",\"208-356-7419\",\"208-656-9611\",\"208-656-9611\",\"359-0920\",\"208-356-4361\",\"208-356-0222\",\"208-346-7006\",\"208-496-9220\",\"208-496-9220\",\"3563001\",\"(208) 356-6660\",\"208-313-8182\",\"356-6752\",\"208-359-2848\",\"(208) 557-1520\",\"208-356-9576\",\"(208) 356-9576\",\"(503) 515-8377\",\"503-515-8377\",\"208-356-5149\",\"208-356-5149\",\"208-356-5149\",\"208-356-8921\",\"208-356-3216\",\"208-359-8015\",\"208-356-6686\",\"208-356-6686\",\"359-3191\",\"356-3001\",\"356-3001\",\"208-359-5141\",\"208-356-6142\",\"356-6142\",\"208-356-6142\",\"208-356-7952\",\"208-356-7952\",\"356-7211\",\"208-356-5786 or\",\"208-356-6556\",\"(208) 356-6556\",\"208-356-3001\",\"208-359-2221\",\"208.356.4473\",\"208.356.4473\",\"(208) 359-1985\",\"(208) 359-1985\",\"359-2211\",\"(208) 359-4193\",\"208-356-5638\",\"208-356-5638\",\"356-7756\",\"356-7756\",\"208-356-7234\",\"208-681-9542\",\"208-681-9542\",\"356-3697\",\"208 356-3697\",\"2083563001\",\"208-356-5605\",\"208-356-5605\",\"208-360-2845\",\"208-356-7568\",\"356-4361\",\"208-201-3466\",\"3604043\",\"(208) 356-0923\",\"208-881-6249\",\"208-881-6249\",\"208-356-7778\",\"208-709-1608\",\"208-656-0746\",\"562-572-2943\",\"(208) 359-5517\",\"(208) 356-7699\",\"(208) 356-7699\",\"(208) 351-8072\",\"356-4361\",\"208-359-5965\",\"208-390-3706\",\"(208) 356-3480\",\"(208) 356-3480\",\"356-4361\",\"(208) 356-0821\",\"2083563001\",\"208-356-6789\",\"208-356-6789\"],[1035,980,1420,1445,1062.5,972.5,1045,890,1000,1000,999,895,1202.5,1050,1018.333333,995,1150,1100,900,null,1345,1222,960,980,1019,1275,925,1200,998,1325,1225,1300,870,1310,1310,925,961.666666,1250,1216.666666,988,1075,1345,1495,1349,1349,1198,995,995,850,1245,1120,955,950,1261.666666,1245,940,940,925,null,895,995,1375,1375,995,981,981,1020,1120,975,1010,1189,1259,1125,880,1267.5,1267.5,1120,1374,1095,1224,1189,1129,1374,950,1323,1388,1279,1279,995,1537.333333,1537.333333,1050,1133.333333,1276,null,975,1046.25,900,1126,972.5,null,896.666666,1175,1175,995,915,1155,1150,1125,1110,1110,895,967,1300,1400,null,1302.5,1125,1410,1198,1560,1585],[1060,980,1445,1495,1150,1115,1095,890,1075,1075,999,895,1380,1075,1045,995,1225,1200,1000,null,1345,1369,960,980,1019,1390,925,1250,998,1325,1275,1325,895,1310,1310,925,995,1250,1250,988,1075,1345,1495,1399,1399,1276,995,995,850,1245,1120,960,950,1395,1295,940,940,925,null,895,1045,1425,1425,995,981,981,1045,1150,1000,1010,1189,1259,1245,880,1330,1330,1120,1399,1095,1249,1189,1129,1399,950,1356,1557,1279,1279,995,1669,1669,1100,1200,1276,null,995,1135,900,1227,995,null,960,1200,1200,995,950,1240,1150,1200,1110,1110,895,967,1300,1450,null,1315,1215,1410,1276,1585,1585],[42,6,138,150,5,22,8,17,138,144,252,26,59,165,97,41,67,144,210,6,96,60,8,343,65,24,48,148,24,180,108,60,144,60,86,21,324,24,53,32,234,360,444,304,546,7,42,9,64,342,6,60,84,24,11,50,10,11,19,71,60,288,324,72,48,92,44,9,7,8,180,192,140,11,264,636,11,360,108,108,72,234,288,42,474,590,288,338,40,476,548,32,29,7,144,200,140,29,5,53,100,238,59,114,342,6,168,8,64,108,180,10,14,168,248,142,142,9,24,7,136,208],[38,3,216,216,4,18,8,9,133,133,111,30,0,0,60,30,27,80,84,5,96,54,8,160,160,160,25,84,50,0,102,60,0,96,96,7,100,65,65,19,85,300,230,100,200,50,42,10,54,241,5,100,100,28,10,10,10,10,0,30,44,0,0,51,40,70,28,6,7,0,0,145,60,3,50,100,50,350,103,103,200,200,288,42,0,0,300,300,20,508,508,0,0,50,0,300,84,20,5,25,0,0,35,35,0,0,128,0,35,70,80,0,0,168,248,0,0,14,96,50,66,100],[100,100,150,150,100,100,150,100,100,100,75,80,100,100,75,100,150,75,75,0,75,150,90,75,75,75,100,75,125,100,150,150,75,75,75,75,75,75,75,125,125,100,100,75,75,125,100,100,100,85,100,75,75,100,100,75,75,75,0,0,75,100,100,75,125,125,75,100,100,100,150,150,100,70,100,100,125,150,100,100,150,150,150,110,100,100,0,0,75,100,100,0,150,125,0,100,75,50,125,100,0,75,75,75,75,0,100,200,100,75,75,150,125,75,75,125,125,125,75,125,75,75],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,50,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,175,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[3,1,3,3,3,10,3,1,3,3,1,19,3,6,6,6,6,20,10,1,1,3,1,1,1,3,1,3,1,1,10,3,3,1,3,6,6,2,6,1,1,1,1,6,12,3,6,1,1,1,1,3,1,6,3,3,3,3,10,3,3,3,3,1,1,1,3,3,3,1,1,1,3,1,3,3,1,3,1,3,1,1,3,1,10,21,1,1,3,36,34,10,10,1,6,3,10,10,3,3,3,12,10,10,3,3,10,1,3,1,1,1,3,3,3,3,3,6,1,3,6,8],[\"www.abbylaneapartments.com\",\"sites.google.com/site/robertspencerbrown/abodehouse\",\"www.liveabri.com\",\"www.liveabri.com\",\"\",\"www.alexanderapartments.wordpress.com\",\"\",\"allenridge.blogspot.com\",\"www.sunrisevillageapts.com\",\"www.sunrisevillageapts.com\",\"www.rexburghousing.com\",\"www.altaviewapts.com\",\"myamericanavenue.com\",\"myamericanavenue.com\",\"\",\"www.arcadiarexburg.com\",\"www.arpadhousing.com\",\"www.aspenvillageapartments.com\",\"www.aspenvillageapartments.com\",\"https://www.facebook.com/atthegroveapts/\",\"www.rexburghousing.com\",\"www.bestnesthousing.com/avonlea.php\",\"baysidemanor.blogspot.com\",\"www.rexburghousing.com\",\"www.rexburghousing.com\",\"www.rexburghousing.com\",\"bluedoorapts.com\",\"www.bountifulplace.com\",\"www.bestnesthousing.com\",\"www.brighamsmill.com\",\"www.brighton-apartments.net\",\"www.brighton-apartments.net\",\"WWW.BROOKLYNAPTS.NET\",\"www.rexburghousing.com\",\"www.rexburghousing.com\",\"buenavistarexburg.blogspot.com\",\"www.bunkhouseapts.com\",\"www.camdenapts.net\",\"www.camdenapts.net\",\"www.bestnesthousing.com/campus-view.php\",\"www.bestnesthousing.com/carriage-house.php\",\"Cedarshousing.com\",\"Cedarshousing.com\",\"www.byui.edu/housing/centre-square\",\"www.byui.edu/housing/centre-square\",\"www.bestnesthousing.com/chapman-house.php\",\"clarkeapartments.com\",\"\",\"WWW.COLONIALHEIGHTSTOWNHOUSES.COM\",\"www.colonial-house.com\",\"condiecottage.weebly.com\",\"www.cottonwoodstudentapartments.com\",\"www.cottonwoodstudentapartments.com\",\"creeksidewomenbyui.blogspot.com\",\"creeksidemensbyui.blogspot.com\",\"www.crestwoodapt.com\",\"www.crestwoodapt.com\",\"www.crestwoodapt.com\",\"www.danburymanor.com\",\"davenportapartments.net\",\"www.deltaphiapts.com\",\"www.thegatesatrexburg.com\",\"www.thegatesatrexburg.com\",\"www.rexburghousing.com\",\"www.bestnesthousing.com\",\"www.bestnesthousing.com\",\"harrishallapartments.com\",\"www.hemminghousing.com\",\"www.hemminghousing.com\",\"www.hemminghousing.com\",\"HeritageRexburg.com\",\"HeritageRexburg.com\",\"hillcrestinrexburg.com\",\"\",\"theivyapartments.com\",\"theivyapartments.com\",\"www.bestnesthousing.com\",\"www.meet-me-at-the-ridge.com\",\"www.kensingtonmanorrexburg.com\",\"www.kensingtonmanorrexburg.com\",\"www.LaJollaRexburg.com\",\"www.LaJollaRexburg.com\",\"www.meet-me-at-the-ridge.com\",\"www.mntcrestapts.com\",\"www.mountainloftsatrexburg.com\",\"www.mountainloftsatrexburg.com\",\"WWW.NAUVOOHOUSE.COM\",\"WWW.NAUVOOHOUSE.COM\",\"www.normandystudentapartments.com\",\"www.NorthPointRexburg.com\",\"www.NorthPointRexburg.com\",\"www.parkviewrexburg.com\",\"www.parkviewrexburg.com\",\"www.bestnesthousing.com/pincock-house.php\",\"pineshousing.com\",\"pineshousing.com\",\"rexburgpinnacle.com\",\"www.quincyhouserexburgmenshousing.blogspot.com\",\"www.bestnesthousing.com/red-brick-house.php\",\"reddoorapts.com\",\"WWW.RIVIERA-APTS.COM\",\"www.rexburgHousing.com\",\"rexburgroost.com\",\"rexburgroost.com\",\"WWW.ROYALCRESTAPARTMENTS.COM\",\"\",\"www.shelbourneapts.com\",\"www.thesnowedinn.blogspot.com\",\"www.snowviewapts.com\",\"www.rexburghousing.com\",\"www.rexburghousing.com\",\"\",\"www.bestnesthousing.com/sunset-hall.php\",\"thetowersrexburg.com\",\"thetowerstwo.com\",\"Tuscanyplace.net\",\"Tuscanyplace.net\",\"www.bestnesthousing.com/webster-house.php\",\"www.rexburghousing.com\",\"www.bestnesthousing.com/whitfield-house.php\",\"www.windsormanor.net\",\"www.windsormanor.net\"],[43.8169418,43.8218153,43.8239342,43.8239342,43.8218385,43.8197811,43.82073,43.824663,43.821866,43.82186,43.8165761,43.8218751,43.822203,43.8228628,43.8214093,43.8177393,43.820138,43.8146367,43.814649,43.8185731,43.8156171,43.8197584,43.8203851,43.8209717,43.8209717,43.820673,43.823664,43.815652,43.82186,43.817458,43.8133921,43.8133921,43.8188718,43.8161353,43.8161353,43.8175075,43.8187344,43.8156024,43.8156024,43.820143,43.8207848,43.8218672,43.8211586,43.812247,43.811831,43.822054,43.8225813,43.8226754,43.8176752,43.8176902,43.819536,43.8250778,43.8250778,43.8197851,43.8197851,43.8209405,43.8213116,43.8209405,43.8218191,43.819765,43.8177035,43.8116909,43.81355,43.819781,43.823056,43.823056,43.8185058,43.823583,43.8239553,43.8239263,43.817232,43.8177064,43.8163212,43.822994,43.812377,43.812377,43.822228,43.8120309,43.8187924,43.8187924,43.8246842,43.8246842,43.812168,43.8230786,43.815,43.815,43.8155931,43.8155931,43.8197996,43.822758,43.822758,43.8222826,43.8222826,43.822094,43.8218723,43.8218723,43.824505,43.8204947,43.8197965,43.8285259,43.8200969,43.8176975,43.815739,43.815739,43.8191657,43.823957,43.814119,43.8218453,43.821957,43.8163422,43.8163422,43.822269,43.820059,43.813151,null,43.8170624,43.8170624,43.820268,43.8156171,43.822083,43.81351,43.822092],[-111.7952062,-111.7771495,-111.7773062,-111.7773062,-111.7906463,-111.7918951,-111.780478,-111.7835934,-111.7852097,-111.785197,-111.7895659,-111.7938578,-111.780098,-111.7807632,-111.7895566,-111.7886609,-111.788876,-111.7780764,-111.778083,-111.7766137,-111.7884885,-111.788829,-111.7795297,-111.7867163,-111.7867163,-111.7895151,-111.779473,-111.792396,-111.779097,-111.792466,-111.7945301,-111.7945301,-111.7895166,-111.7924099,-111.7924099,-111.7752093,-111.7867224,-111.7917058,-111.7917058,-111.78777,-111.7867195,-111.7872458,-111.7867132,-111.787101,-111.787004,-111.778304,-111.7838412,-111.7824172,-111.7764603,-111.788803,-111.791806,-111.7867042,-111.7867042,-111.7943981,-111.7943981,-111.7809226,-111.7809272,-111.7809226,-111.7791712,-111.7879773,-111.7886615,-111.794504,-111.791355,-111.7907129,-111.779371,-111.779371,-111.7867268,-111.787926,-111.7875196,-111.7871752,-111.787433,-111.7881413,-111.7780659,-111.782747,-111.789553,-111.789553,-111.779221,-111.7758519,-111.7780591,-111.7780591,-111.7866669,-111.7866669,-111.778051,-111.7924581,-111.791299,-111.791299,-111.7888627,-111.7888627,-111.7871315,-111.78617,-111.78617,-111.7924522,-111.7924522,-111.778896,-111.7878656,-111.7878656,-111.780339,-111.7895635,-111.7875345,-111.779619,-111.7809297,-111.7906892,-111.792608,-111.792608,-111.7869267,-111.7855467,-111.789569,-111.7917922,-111.784145,-111.7869059,-111.7869059,-111.782776,-111.788248,-111.792518,null,-111.7895741,-111.7895741,-111.7868774,-111.7884885,-111.778657,-111.786668,-111.78069],[0.745647284,0.186411821,0.310686368,0.310686368,0.186411821,0.434960916,0.186411821,0.310686368,0.062137274,0.062137274,0.434960916,0.372823642,0.248549095,0.372823642,0.186411821,0.372823642,0.248549095,0.372823642,0.372823642,0.186411821,0.434960916,0.248549095,0.124274547,0.062137274,0.062137274,0.248549095,0.372823642,0.497098189,0.186411821,0.621372737,0.434960916,0.434960916,0.372823642,0.559235463,0.559235463,0.310686368,0.248549095,0.497098189,0.497098189,0.186411821,0.062137274,0.35418246,0.553021736,0.497098189,0.366609915,0.186411821,0.186411821,0.248549095,0.248549095,0.372823642,0.434960916,0.248549095,0.248549095,0.559235463,0.559235463,0.248549095,0.248549095,0.248549095,0.186411821,0.186411821,0.372823642,0.310686368,0.310686368,0.372823642,0.434960916,0.434960916,0.248549095,0.186411821,0.186411821,0.186411821,0.310686368,0.372823642,0.248549095,0.310686368,0.124274547,0.124274547,0.186411821,0.434960916,0.062137274,0.062137274,0.186411821,0.186411821,0.248549095,0.372823642,0.434960916,0.434960916,0.372823642,0.372823642,0.186411821,0.062137274,0.062137274,0.310686368,0.310686368,0.186411821,0.062137274,0.062137274,0.434960916,0.248549095,0.186411821,0.68351001,0.186411821,0.497098189,0.497098189,0.497098189,0.186411821,0.186411821,0.248549095,0.248549095,0.124274547,0.372823642,0.372823642,0.248549095,0.248549095,0.372823642,null,0.497098189,0.497098189,0.124274547,0.434960916,0.186411821,0.186411821,0.310686368],[16,4,8,8,4,8,5,6,1,1,10,7,6,7,4,8,5,6,6,3,9,5,3,1,1,5,9,12,5,12,10,10,7,12,12,6,4,11,11,4,2,1,1,10,1,4,4,5,4,8,8,5,5,11,11,5,6,5,5,4,8,8,7,7,9,9,5,4,4,3,6,7,4,6,3,3,5,8,1,1,4,4,6,8,9,9,9,9,3,1,1,7,7,4,1,1,9,5,4,15,4,10,12,12,4,4,6,5,2,7,7,5,4,8,null,9,9,2,9,4,4,6],[\"We are a small women's complex offering private rooms and quaint living. Many students enjoy our advantages enough that they stay with us until graduation. We are sure you would like staying with us! We offer free parking, off-track storage and wireless internet- NOW with cheaper rent prices!\",\"This cozy little apartment is located only one block from campus. There are a lot of freebies for the price. All utilities are included, there is a free-to-use, in-apartment washer and dryer, free WiFi, and free covered parking. When you sign for two semesters at a time, you receive a $100 discount. Join us now for a great \\\"small complex\\\" feel!\",\"24 hour special $100 off! Look no further than the Abri, Rexburg's newest student housing c"},{"title":"Student Housing Costs","url":"325Analyses/StudentHousingPOOR.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); Student Housing Costs Background I got data from the BYU-Idaho Housing Website. I wanted to see how much rent costs for BYUI students. Analysis Interpretation Houssing seems to cost around a 1000 dollers. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Analysis Scores – One Sample t Test","url":"325Analyses/t Tests/Examples/Math325AnalysisResubmits.html","content":"Code Show All Code Hide All Code Analysis Scores – One Sample t Test library(mosaic) library(tidyverse) library(car) library(pander) library(DT) resub <- read_csv(\"../../../Data/analysisResubmitTimes.csv\") Background In the Math 325 Intermediate Statistics course at BYU-Idaho, the weekly analyses that students are required to complete make up a large portion of their grade in the class, 40% to be exact. One of the unique features of the course is that students are allowed to improve and resubmit their work, even after the deadline has passed. In fact, resubmissions are encouraged until students are awarded full credit. Ideally, students would resubmit within one day of receiving feedback on their analysis, but most teachers allow a week or two for resubmissions. The following looks to see how long students take on average to resubmit, and if it is significantly different from 7 days on average. \\[ H_0: \\mu_\\text{resubmission days} = 7 \\] \\[ H_a: \\mu_\\text{resubmission days} \\neq 7 \\] The significance level for this study will be set at \\(\\alpha = 0.05\\). Hide Show Data Data was selected by convenience sample from a teacher who did not have a deadline on when students could resubmit their analyses. Some students have missing values for DaysToResubmit and FinalScore because they never resubmit, or never submitted an analysis, respectively. So only the 18 students who both submit an analysis and resubmit their work will be included in this study. datatable(arrange(resub, desc(DaysToResubmit))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\"],[28,29,26,27,24,25,23,22,21,20,19,18,17,16,14,15,12,13,1,2,3,4,5,6,7,8,9,10,11],[62,62,59,59,51,51,49,48,42,41,27,24,20,9,8,8,7,7,null,null,null,null,null,null,null,null,null,null,null],[15,11,14.75,15,15,14.2,12,15,15,15,15,15,15,15,15,15,15,11,0,0,0,0,11,11.5,0,10,9,13,9]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>Student<\\/th>\\n <th>DaysToResubmit<\\/th>\\n <th>FinalScore<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Analysis Somewhat surprisingly, the average days to resubmit the t Test analysis is a little over a month at 35.2 days (of those who choose to resubmit). The fastest resubmission was 7 days after the original feedback was given. Note that only the last resubmission time was recorded. A few students resubmit several times before their final resubmission, and those intermediate resubmissions were not recorded. stripchart(resub$DaysToResubmit, pch=16, method=\"stack\", cex=2, col=\"orange\", main=\"Math 325 Student Resubmissions, t Test Analysis\", xlab=\"Days Til Final Resubmission\") lines(c(7,7), c(0.5,1.5), lty=1, col=\"skyblue\", lwd=4) points(mean(resub$DaysToResubmit, na.rm=TRUE), 1, pch=3, cex=2, col=\"skyblue\") text(mean(resub$DaysToResubmit, na.rm=TRUE), 1, \"Sample Mean\", pos=1, cex=0.8, col=\"skyblue\") text(7,0.5, \"Null Hypothesis\", pos=4, col=\"skyblue\", cex=0.8) abline(h=1, col=rgb(.1,.1,.1,.1)) The QQ-Plot below (click tab to view) demonstrates that the population data for all student resubmission times can be assumed to be normally distributed as all points remain within the dotted lines. This shows that the results of the t Test should be valid, so long as we can assume that this convenience sample of one class of students accurately represents the full population of students who take Math 325. Hide Show Q-Q Plot qqPlot(resub$DaysToResubmit, id=FALSE) t.test(resub$DaysToResubmit, mu = 7, alternative = \"two.sided\", conf.level = 0.95) %>% pander(caption=\"One Sample t-Test: Hours of Extra Sleep\", split.table=Inf) One Sample t-Test: Hours of Extra Sleep Test statistic df P value Alternative hypothesis mean of x 5.632 17 2.986e-05 * * * two.sided 35.22 There is sufficient evidence to reject the null hypothesis \\((p = 2.986e-05 < \\alpha = 0.05)\\) and conclude that the average time of 35.2 days is significantly longer than the 7 days that was expected. Interpretation favstats(resub$DaysToResubmit) %>% pander(caption=\"Summary Statistics for Days til Final Resubmission\") Summary Statistics for Days til Final Resubmission min Q1 median Q3 max mean sd n missing 7 11.75 41.5 51 62 35.22 21.26 18 11 It looks like students find it difficult to resubmit their work quickly after receiving feedback. Again, the average time it takes for those who choose to resubmit is a little over a month at 35.2 days. There is a cluster of five students on the left of the distribution sitting at 7 days, or just over, causing the first quartile to land at 11.75 days. But more than half of the students are taking over 40 days (median 41.5) to resubmit. It is worth noting that 11/29 or 39.9% of students chose not to resubmit (or submit) this analysis. Fortunately, steps are being taken to try to remedy all of these issues within the Math 325 course as teachers are modifying the way they deliver this particular analysis. Only time will tell if the changes are effective or not. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Normal Probability Plot","url":"325Analyses/t Tests/Examples/NormalProbabilityPlots.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } button.code-folding-btn:focus { outline: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Normal Probability Plot There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot. Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution. par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) set.seed(123) tmp <- rnorm(100) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n') qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Normal\") tmp <- Davis$weight qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n') qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Right-skewed\", breaks=15) par(mfrow=c(2,2), mai=c(.5,.5,.25,.25), mgp=c(1,.75,0)) tmp <- rbeta(100, 5,1) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n') qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Left-skewed\", breaks=seq(min(tmp),max(tmp), length.out=13)) tmp <- rbeta(100,2,2) qqnorm(tmp, pch=20, ylab=\"Observed\", xaxt='n', yaxt='n') qqline(tmp) hist(tmp, xlab=\"\", xaxt='n', yaxt='n', main=\"Heavy-tailed\") // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Sleep – Independent Samples t Test","url":"325Analyses/t Tests/Examples/SleepIndependentt.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Sleep – Independent Samples t Test Background An experiment was conducted to determine which of two soporific drugs was better at increasing the hours of sleep individuals received, on average. There were two groups of 10 patients each. One group received the first drug, and the other group received the second drug. The amount of extra sleep that individuals received when drugged was measured. The data is contained in the sleep data set in R. Hide Data Show Data datatable(sleep) {\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\"],[0.7,-1.6,-0.2,-1.2,-0.1,3.4,3.7,0.8,0,2,1.9,0.8,1.1,0.1,-0.1,4.4,5.5,1.6,4.6,3.4],[\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\"],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>extra<\\/th>\\n <th>group<\\/th>\\n <th>ID<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":1},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} It is assumed that the amount of extra sleep individuals will receive when using each drug is normally distributed. Thus, the interest is in knowing if the difference in average hours of extra sleep for each drug, symbolically \\(\\mu_\\text{Drug 1} \\mu_\\text{Drug 2}\\), is different from zero. In other words, is one drug better than the other at increasing the average hours of extra sleep? Formally, the null and alternative hypotheses are written as \\[ H_0: \\mu_\\text{Extra Hours of Sleep with Drug 1} - \\mu_\\text{Extra Hours of Sleep with Drug 2} = 0 \\] \\[ H_a: \\mu_\\text{Extra Hours of Sleep with Drug 1} - \\mu_\\text{Extra Hours of Sleep with Drug 2} \\neq 0 \\] The significance level for this study will be set at \\[ \\alpha = 0.05 \\] Note that the \\(\\neq\\) alternative hypothesis allows for either possibility, \\(\\mu_1 > \\mu_2\\) or \\(\\mu_1 < \\mu_2\\). If we selected a one-sided hypothesis, then only the stated alternative is considered possible, or of interest. Analysis The side-by-side dotplots of extra sleep demonstrate that the individuals in the study who took the second drug received more extra sleep on average than those taking the first drug. Also, notice that 9 out of 10 individuals taking the second drug experienced an increase in extra sleep. Of those taking the first drug, 5 out of 10 individuals experienced an increase in extra sleep. While these results are true for the individuals in the study, it is of interest to know if these results can be considered to hold generally in the population. stripchart(extra ~ group, data=sleep, pch=16, col=c(\"skyblue\",\"firebrick\"), vertical = TRUE, xlim=c(0.5,2.5), xlab=\"Soporific Drug No.\", ylab=\"Hours of Extra Sleep\", main=\"Effectiveness of two Different Soporific Drugs \\n at Increasing Sleep Times\") abline(h=0, lty=2, col=\"gray\") sleepmeans <- mean(extra ~ group, data=sleep) lines(sleepmeans ~ c(1,2), lty=2, lwd=2, col=\"darkgray\") points(sleepmeans ~ c(1,2), pch=3, cex=2, col=c(\"skyblue\",\"firebrick\")) legend(\"topleft\", bty=\"n\", pch=3, col=c(\"skyblue\",\"firebrick\"), title=\"Mean\", legend=c(\"Drug 1\", \"Drug 2\")) An independent samples t test could be used to test the previously stated null hypothesis. This will allow us to decide if the pattern in the sample data can be assumed to hold for the full population. Before we can use an independent samples t test, the assumptions of the test must be shown to be satisfied. It is difficult to verify if the sampling distribution of \\(\\bar{x}_1 - \\bar{x}_2\\) is normal. However, it is true that if the separate sampling distributions of \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are normally distributed, then it follows that the sampling distribution of \\(\\bar{x}_1 - \\bar{x}_2\\) will be normally distributed. As long as the population data is normal, it follows that the sampling distribution of the sample mean is normal. Hide Q-Q Plots Show Q-Q Plots qqPlot(extra ~ group, data=sleep, ylab=\"extra sleep\") Based on the Q-Q Plots above, it appears that the extra sleep data can be considered normal for each drug group, which implies it is okay to assume that \\(\\bar{x}_1 - \\bar{x}_2\\) is normally distributed, even though the sample size for each group is small (less than 30). The independent samples t test is appropriate for these data. pander(t.test(extra ~ group, data = sleep, mu = 0, alternative = \"two.sided\", conf.level = 0.95), caption=\"Independent Samples t Test of Extra Sleep for Drug 1 and 2\", split.table=Inf) Independent Samples t Test of Extra Sleep for Drug 1 and 2 Test statistic df P value Alternative hypothesis mean in group 1 mean in group 2 -1.861 17.78 0.07939 two.sided 0.75 2.33 MyResults <- sleep %>% group_by(group) %>% summarise(min=min(extra), median=median(extra), mean=mean(extra), max=max(extra), sd=sd(extra), n=n()) %>% rename(Drug = group) There is insufficient evidence to reject the null hypothesis (\\(p = 0.07939 > \\alpha\\)). pander(MyResults, caption=\"Summary Statistics of Extra Hours of Sleep by Drug\") Summary Statistics of Extra Hours of Sleep by Drug Drug min median mean max sd n 1 -1.6 0.35 0.75 3.7 1.789 10 2 -0.1 1.75 2.33 5.5 2.002 10 Interpretation The data from the experiment showed a higher average number of hours of extra sleep for drug 2 (2.33 hours) than drug 1 (0.75 hours). However, as demonstrated by the p-value from the Independent Samples t Test, there is insufficient evidence to claim that this pattern will remain true for the general population, or even for repeated versions of this study (p = 0.07939). It appears that drugs 1 and 2 generally do the same thing at increasing sleep. Thus, while this particular study happened to show more favorable results for Drug 2, the lack of significance leads us to believe that these results are simply due to random chance. We recommend further studies be performed before making any definitive conclusions about the advantage of drug 2 over drug 1. Potentially a paired analysis study could look more carefully at how the drugs effect individuals differently. This may reveal further insights. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Sleep – Paired Samples t Test","url":"325Analyses/t Tests/Examples/SleepPairedt.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Sleep – Paired Samples t Test Background An experiment was conducted to determine which of two soporific drugs was better at increasing the hours of sleep individuals received, on average. Ten patients took each of the two drugs at different times. The amount of extra sleep that each individual received when using each drug was measured. The data is contained in the sleep data set. Note that the variable group would be better labeled as drug because the 10 individuals in each group are the same individuals as shown in the ID column. library(mosaic) library(car) library(pander) The only point of interest in this study is the difference in hours of extra sleep each individual received under the two drugs. Hence, this is a paired study as two measurements were obtained for each individual, one measurement under each condition. Formally, the null and alternative hypotheses are written as \\[ H_0: \\mu_\\text{difference in hours of sleep (drug 2 - drug 1)} = 0 \\] \\[ H_a: \\mu_\\text{difference in hours of sleep (drug 2 - drug 1)} \\neq 0 \\] The significance level for this study will be set at \\[ \\alpha = 0.05 \\] Note that the \\(\\neq\\) alternative hypothesis is the best choice for this study because we want to know which of the two drugs gives more sleep over the other. Hide Data Show Data pander(sleep) extra group ID 0.7 1 1 -1.6 1 2 -0.2 1 3 -1.2 1 4 -0.1 1 5 3.4 1 6 3.7 1 7 0.8 1 8 0 1 9 2 1 10 1.9 2 1 0.8 2 2 1.1 2 3 0.1 2 4 -0.1 2 5 4.4 2 6 5.5 2 7 1.6 2 8 4.6 2 9 3.4 2 10 Analysis The dotplot below shows the differences (drug 2 extra sleep \\(-\\) drug 1 extra sleep) in extra sleep for each individual. Since 9 out of 10 differences are positive, it shows that most individuals are getting more extra sleep while using drug 2 than when using drug 1. differences <- sleep$extra[sleep$group==2] - sleep$extra[sleep$group==1] boxplot(differences, main=\"Drug 2 Increase In Extra Sleep Over Drug 1\", horizontal = TRUE, border = \"gray\", xlab=\"Hours of Extra Sleep\") stripchart(differences, pch=16, add=TRUE) The paired samples t test is only appropriate if the sampling distribution of the sample mean of the differences, \\(\\bar{d}\\), is normally distributed. Hide Q-Q Plot Show Q-Q Plot qqPlot(differences, ylab=\"difference\", main=\"Group 2 - Group 1 extra sleep differences\", id=FALSE) Based on the Q-Q Plot (click the botton above to show the plot), it appears that the normality assumption of the differences is questionable. This is because three points go out of the “bounds of normality”. As seen in the combinded dot plot boxplot graphic above, the distribution seems to be right-skewed, not normal. The paired samples t test will be performed, but the validity of the test is highly questionable because the sample size is small (under 30) and the data appears to not be normal. pander(t.test(differences, mu = 0, alternative = \"two.sided\", conf.level = 0.95), caption=\"Paired t Test: Hours of Extra Sleep (Drug 2 - Drug 1)\", split.table=Inf) Paired t Test: Hours of Extra Sleep (Drug 2 - Drug 1) Test statistic df P value Alternative hypothesis mean of x 4.062 9 0.002833 * * two.sided 1.58 There is sufficient evidence to reject the null hypothesis (\\(p = 0.002833 < \\alpha\\)), but again, the validity of this result is questionable as discussed above. Interpretation The assumptions of the t test were not shown to be satisfied, so the results of this test are questionable. The significance of the p-value does suggest that Drug 2 would provide, on average, 1.58 more hours of extra sleep than Drug 2. Also, as all individuals in the study experienced an equal, or greater amount of sleep under Drug 2 than they did with Drug 1, our recommendation would be for Drug 2. However, for validity purposes we recommend performing a nonparametric paired samples test (Wilcoxon Signed-Rank Test) instead of this t test. Fortunately, when this the nonparametric version of the test is performed, a similar result is obtained. So we are able to safely conclude that drug 2 out performs drug 1 at increasing the extra hours of sleep individuals will obtain when using the drug. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Rail Trail t Test","url":"325Analyses/t Tests/Examples/Student1Independent.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } button.code-folding-btn:focus { outline: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Rail Trail t Test TrailSS <- subset(RailTrail, spring== \"1\" | summer== \"1\") TrailSpring <- subset(RailTrail, spring== \"1\") TrailSummer <- subset(RailTrail, summer== \"1\") Background The Pioneer Valley Planning Commission (PVPC) collected data north of Chesnut Street in FLorence, MA for ninety days from April 5, 2005 to November 15, 2005. The data is contained in the RailTrail dataset. The PVPC wants to know if there are more rail trail users in the Spring than in the Summer so they can know if they need to do any extra advertisemnet for the Summer trail users. Therefore we want to know if \\(\\mu_1 - \\mu_2\\) (the population averages for spring and summer), or the difference between the volume of users in Spring and Summer is different than zero. Formally, the null and alternative hypotheses are written as \\[ H_0: \\mu_1 - \\mu_2 = 0 \\] \\[ H_a: \\mu_1 - \\mu_2 \\neq 0 \\] The level of significance will be set at \\[ a = 0.05 \\] datatable(TrailSS, options=list(lengthMenu = c(10,30))) {\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"16\",\"17\",\"18\",\"19\",\"21\",\"22\",\"23\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"39\",\"40\",\"41\",\"42\",\"43\",\"45\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"78\",\"79\",\"81\",\"82\",\"83\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\"],[83,73,74,95,44,69,66,66,80,79,78,65,41,54,97,75,63,70,57,71,61,81,64,57,62,83,76,74,59,77,52,67,90,70,81,72,70,59,72,70,76,56,66,84,58,56,79,67,75,65,70,64,51,77,60,74,84,61,49,70,83,90,92,88,62,86,67,89,63,74,80,55,74,95,65,55,89,80],[50,49,52,61,52,54,39,38,55,45,55,48,49,32,71,43,35,49,48,39,35,65,58,36,36,54,44,50,40,52,43,43,65,30,39,53,63,34,47,47,72,38,38,69,51,41,47,39,47,30,41,41,39,43,34,55,70,42,43,62,60,66,66,54,39,50,43,69,46,55,33,45,33,66,20,43,57,53],[66.5,61,63,78,48,61.5,52.5,52,67.5,62,66.5,56.5,45,43,84,59,49,59.5,52.5,55,48,73,61,46.5,49,68.5,60,62,49.5,64.5,47.5,55,77.5,50,60,62.5,66.5,46.5,59.5,58.5,74,47,52,76.5,54.5,48.5,63,53,61,47.5,55.5,52.5,45,60,47,64.5,77,51.5,46,66,71.5,78,79,71,50.5,68,55,79,54.5,64.5,56.5,50,53.5,80.5,42.5,49,73,66.5],[0,0,1,0,1,1,1,1,0,0,0,1,1,1,0,1,1,1,1,1,1,0,0,1,1,0,1,1,1,1,1,1,0,1,1,0,0,1,1,1,0,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,0,0,0,1,1,1,0,1,1,1,1,1,0,1,1,0,0],[1,1,0,1,0,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,1,0,0,1,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,1,1,0,0,0,1,0,0,0,0,0,1,0,0,1,1],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[7.59999990463257,6.30000019073486,7.5,2.59999990463257,10,6.59999990463257,2.40000009536743,0,3.79999995231628,4.09999990463257,8.5,7.19999980926514,10,3.59999990463257,6.09999990463257,6.30000019073486,8.60000038146973,7.90000009536743,6.80000019073486,5.19999980926514,4.40000009536743,10,10,6.5,0,1.20000004768372,1.79999995231628,8,9.39999961853027,5,5.80000019073486,6.69999980926514,8.39999961853027,0,0,7.90000009536743,9.89999961853027,2.40000009536743,4.09999990463257,9.89999961853027,9.69999980926514,0,3.90000009536743,8.89999961853027,10,7,7,6.80000019073486,6.80000019073486,4.30000019073486,5.09999990463257,3.40000009536743,10,0.400000005960464,10,4.59999990463257,9.89999961853027,6.40000009536743,9.69999980926514,3,8.19999980926514,6.59999990463257,7.90000009536743,6.19999980926514,0.800000011920929,6.19999980926514,0.400000005960464,9,6.90000009536743,7.80000019073486,0.699999988079071,9.60000038146973,2.5,4.09999990463257,9.39999961853027,9,1.89999997615814,3.79999995231628],[0,0.28999999165535,0.319999992847443,0,0.140000000596046,0.0199999995529652,0,0,0,0,0,0,0.0299999993294477,0,0.680000007152557,0,0,0,0,0,0,1.44000005722046,1.14999997615814,0.00999999977648258,0,0,0,0.119999997317791,0.340000003576279,0,0.00999999977648258,0,0,0,0,0,0,0,0,0,0.349999994039536,0,0,0.00999999977648258,0.150000005960464,0,0.00999999977648258,0.0199999995529652,0,0,0,0,0.159999996423721,0,0,0,1.49000000953674,0.00999999977648258,0.119999997317791,0,0.140000000596046,0,0,0,0,0,0,0.360000014305115,0.170000001788139,0,0,0.200000002980232,0,0,0.219999998807907,0,0,0],[501,419,397,385,200,375,417,629,533,547,432,418,193,304,352,156,365,514,259,452,362,256,155,174,259,535,650,409,311,736,186,371,411,461,540,376,554,344,517,328,298,335,186,505,156,362,507,407,421,393,422,349,150,429,293,461,388,322,129,460,405,453,395,531,236,484,445,286,400,339,617,316,635,449,314,264,356,564],[\"1\",\"1\",\"1\",\"0\",\"1\",\"1\",\"1\",\"0\",\"0\",\"1\",\"1\",\"1\",\"1\",\"1\",\"0\",\"1\",\"1\",\"0\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"0\",\"0\",\"1\",\"1\",\"0\",\"1\",\"1\",\"1\",\"0\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"0\",\"0\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"0\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\"],[\"no\",\"yes\",\"yes\",\"no\",\"yes\",\"yes\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"yes\",\"no\",\"yes\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"yes\",\"yes\",\"yes\",\"no\",\"no\",\"no\",\"yes\",\"yes\",\"no\",\"yes\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"yes\",\"no\",\"no\",\"yes\",\"yes\",\"no\",\"yes\",\"yes\",\"no\",\"no\",\"no\",\"no\",\"yes\",\"no\",\"no\",\"no\",\"yes\",\"yes\",\"yes\",\"no\",\"yes\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"yes\",\"yes\",\"no\",\"no\",\"yes\",\"no\",\"no\",\"yes\",\"no\",\"no\",\"no\"]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>hightemp<\\/th>\\n <th>lowtemp<\\/th>\\n <th>avgtemp<\\/th>\\n <th>spring<\\/th>\\n <th>summer<\\/th>\\n <th>fall<\\/th>\\n <th>cloudcover<\\/th>\\n <th>precip<\\/th>\\n <th>volume<\\/th>\\n <th>weekday<\\/th>\\n <th>rain<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"lengthMenu\":[10,30],\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Analysis This boxplot shows the volume of trail users with Spring data on the left, and Summer on the right. It looks like that the amount of people on the trail could be very similar between the spring and summer, with a slightly higher volume of users in the summer. boxplot(TrailSpring$volume, TrailSummer$volume, xlab= \"Spring and Summer\", ylab= \"Volume of Trail Users\", main= \"Volume of Trail Users in \\n the Spring vs. Summer\", col= \"steelblue2\") To test the previously stated null and alternative hypotheses, we will be using an independent samples t-test to find out if there is a difference in the amount of people using the trail in the spring versus the summer. Before we do this, we must verify a few things about the data. We need to verify if the sampling data for the spring and summer are normally distributed. To verify this we will use QQ-Plots. qqPlot(TrailSpring$volume, main= \"Spring Trail Users\") qqPlot(TrailSummer$volume, main= \"Summer Trail Users\") We can see that the data is approximately normal. There are a few outliers in the spring data, but the majority of the points are between the dotted lines. We will continue with the independent samples t-test. Test Statistic df P-value Alternative Hypothesis -1.7878 61.935 0.07871 two.sided There is insufficient evidence to reject the null hypothesis \\((p= 0.079 > a)\\). Interpretation The data from the line graph looked like there might be approximately the same amount of people using the trail in the summer and the summer, and we showed that that is the case with our independent samples t-test. We failed to reject the null hypothesis, therefore there is not a big enough differnce in the number of people using the trail in the spring versus the summer to worry about extra advertisement from the PVPC. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Kids Feet t Test","url":"325Analyses/t Tests/Examples/Student1Paired.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } button.code-folding-btn:focus { outline: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Kids Feet t Test Background A study was done to determine whether or not the idea that shoes for boys tend to be wider than shoes for girls. A statistician collected data in a fourth grade classroom where the length and width of 39 children’s feet were measured Below is a table showing the collected data. The width3 column is the 3x the width. datatable(KidsFeet[,c(\"name\", \"length\",\"width\", \"width3\")], options = list(lengthMenu = c(3,10,30))) {\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\"],[\"David\",\"Lars\",\"Zach\",\"Josh\",\"Lang\",\"Scotty\",\"Edward\",\"Caitlin\",\"Eleanor\",\"Damon\",\"Mark\",\"Ray\",\"Cal\",\"Cam\",\"Julie\",\"Kate\",\"Caroline\",\"Maggie\",\"Lee\",\"Heather\",\"Andy\",\"Josh\",\"Laura\",\"Erica\",\"Peggy\",\"Glen\",\"Abby\",\"David\",\"Mike\",\"Dwayne\",\"Danielle\",\"Caitlin\",\"Leigh\",\"Dylan\",\"Peter\",\"Hannah\",\"Teshanna\",\"Hayley\",\"Alisha\"],[24.4,25.4,24.5,25.2,25.1,25.7,26.1,23,23.6,22.9,27.5,24.8,26.1,27,26,23.7,24,24.7,26.7,25.5,24,24.4,24,24.5,24.2,27.1,26.1,25.5,24.2,23.9,24,22.5,24.5,23.6,24.7,22.9,26,21.6,24.6],[8.4,8.8,9.7,9.8,8.9,9.7,9.6,8.8,9.3,8.8,9.8,8.9,9.1,9.8,9.3,7.9,8.7,8.8,9,9.5,9.2,8.6,8.3,9,8.1,9.4,9.5,9.5,8.9,9.3,9.3,8.6,8.6,9,8.6,8.5,9,7.9,8.8],[25.2,26.4,29.1,29.4,26.7,29.1,28.8,26.4,27.9,26.4,29.4,26.7,27.3,29.4,27.9,23.7,26.1,26.4,27,28.5,27.6,25.8,24.9,27,24.3,28.2,28.5,28.5,26.7,27.9,27.9,25.8,25.8,27,25.8,25.5,27,23.7,26.4]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>name<\\/th>\\n <th>length<\\/th>\\n <th>width<\\/th>\\n <th>width3<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"lengthMenu\":[3,10,30],\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3,4]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}With this data I am hypothesizing that the length of a child’s foot is the same as 3x the width of their foot. Because I am going to compare the length and the width of the foot, this is a paired study as two measurements were obtained from each individual. The null and alternative hypotheses are written as \\[ H_0: \\mu_{\\text{d}} = 0 \\] \\[ H_a: \\mu_{\\text{d}} \\neq 0 \\] where the differences \\(d\\) are defined as \\[ d = \\text{length} - 3\\times\\text{width} \\] In other words, I am testing to see if the length of a foot is equal to three times the width. The significance level for this study will be set at \\[ \\alpha = 0.05 \\] Analysis The dotplot below shoes the differences between the length and 3 times the width for each individual. Since 28 out of the 39 differences are negative, it shows that most individuals length is shorter than 3 times the width of their foot with(KidsFeet, stripchart(length - width3, main = \"Length vs Width\", xlab = \"Length - 3x the Width\"), pch = 20) The paired samples t test is only appropriate if the sampling distribution of the sample mean of the differences, \\(\\bar{d}\\), is normally distributed with(KidsFeet, qqPlot(length - width3, pch = 1, xlab = \"Foot Length\", ylab = \"Difference\", main = \"Difference of Width \\nGiven Length\")) Based on the QQ-Plot above, it appears that the data is normally distributed making it possible to continue on with the paired samples t test. pander(with(KidsFeet, t.test(KidsFeet$length, KidsFeet$width3, paired = TRUE, mu = 0, alternative = \"two.sided\", conf.level = 0.95)), caption = \"Paired Samples t Test\") Paired Samples t Test (continued below) Test statistic df P value Alternative hypothesis -11.53 38 5.668e-14 * * * two.sided mean of the differences -2.254 \\[ p = 0.0000 < \\alpha \\] The p-value is less than our significance level, \\(\\alpha\\), so we reject the null hypothesis Interpretation Due to the p-value being lower than the significance level, there is sufficient evidence to conclude that the length of the foot is not equivalent to 3 times the width of the foot. Because this sampling of data was a convinience sample we can’t not make any solid conclusions. Because of this test we see that most of the data is actually close to 2 maybe 2.5.I would probably run this test one more time and see if 2.5 would work better than 3. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Rail Trail t Test","url":"325Analyses/t Tests/Examples/Student2Independent.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } button.code-folding-btn:focus { outline: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Rail Trail t Test RailTrail$rain <- ifelse(RailTrail$precip > 0, \"yes\", \"no\") norain <- (subset(RailTrail, rain ==\"no\")) rain <- (subset(RailTrail, rain ==\"yes\")) Background In Florence of Massachusetts a rail trail was surveyed for the purpose finding the amount of daily users. The questioned posed is: Does the daily precipitation affect the traffic of the rail trails? This could prove useful because if the city knows that they should expect less traffic from cars on the streets in the summer/spring and less in the fall/winter, then they can plan accordingly. Surveying was done from April 5, 2005 to November 15, 2005. 90 days were recorded. Other observations made besides volume of users included: temperature (Fahrenheit), season, cloud cover, and precipitation. A sample of the first six in data set of precipitation and volume only are shown below. Precipitation Volume 0.00 501 0.29 419 0.32 397 0.00 385 0.14 200 0.02 375 Hypothesis We are interested in knowing if there are more users on the trail during days with no precipitation. Our Null and alternative hypothesis is as follows: \\[ H_0 : \\mu_1 - \\mu_2 = 0 \\] \\[ H_a : \\mu_1 - \\mu_2 > 0 \\] where, \\(\\mu_1\\) = {mean of volume during days without precipitation} \\(\\mu_2\\) = {mean of volume during days with precipitation} And our level of significance will be \\[ \\alpha = 0.05 \\] Analysis The number of days where there was a measurable amount of precipitation is 29, and the number of days with no amount of precipitation is 61. Since the amount of observations aren’t very large, particularly for days with precipitation, a check for normality through QQ-plots would prove helpful. par(mfrow=c(1,2)) qqPlot(rain$volume, main = \"QQ-plot of days with Precip.\", ylab = \"Number of Rail Trail Users\", pch=19) qqPlot(norain$volume, main = \"QQ-plot of days with no Precip.\", ylab = \"Number of Rail Trail Users\", pch=19) We can see that the QQ-plot shows that the data for days with and without precipitation is normally distributed. With this we can assume that the parent population is normally distributed as well, proving that it would be appropriate for us to continue with our independent sample t-test. Below are the box plots of the two types of data sets along with a table summarizing the sample statistics. boxplot(volume ~ rain, data = RailTrail, col=\"brown1\", main = \"Boxplot of Trail Users\", ylab = \"Number of Trail Users\", names = c(\"No Precip\", \"Precip\")) Precip. Min Q1 Median Q3 Max Mean \\(\\sigma\\) Sample Size No 156 335 411 484 736 410.48 120.16 61 Yes 129 189 314 397 507 301.62 111.29 29 There appears to be a noticeable difference between the two groups. To further confirm the observations the independent t-Test results are shown below. Test Statistic df \\(p\\)-value Alternative Hypothesis t = 4.225 59.156 4.185e-05 Greater Comparing our \\(p\\)-value to \\(\\alpha\\) we get: \\[ (p = 0.000042 < \\alpha) \\] We see that test statistic is quite high, suggesting that there is sufficient evidence to reject the null hypothesis. This is further confirmed when comparing the p-value to the chosen alpha. Interpretation The evidence from the graphical summary and t-test support the alternative hypothesis, in that the rail trail receives more traffic from users when there is no precipitation. Suggesting that the city might see an influx of traffic on the streets, because there are less people walking or biking. However conclusive this study was it lacks detail. To provide more affirmative information to this question, the analysis could be expanded to include temperature, cloud cover, varying amounts of precipitation, and weekend vs. weekday. Perhaps it is known that the trails are not safe during heavy rain, and maybe something could be done to help with water flow. There may be more significant factors at play here than just precipitation. Source: Pioneer Valley Planning Commission References: (http://www.northamptonma.gov/DocumentCenter/View/5244) // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"High School Seniors t Test","url":"325Analyses/t Tests/HighSchoolSeniors.html","content":"Code Show All Code Hide All Code High School Seniors t Test Background A school questionnaire titled, U.S Census at School Questionnaire, was created by the organization Census at School and dispersed to high school seniors throughout the United States. The questionnaire contained a series of 40 questions which ranged from their physical attributes to their personal judgment. The data is contained in the HSS data set. Given the vast amount of data to work with, we were able to reorganize and zone in a particular question: Which students have a fastest reaction time on average, those who play outdoor activities more or those who play video games more? Hypothesis HSS.GJ <- select(HSS, c(Reaction_time,Outdoor_Activities_Hours,Video_Games_Hours)) %>% filter_all(all_vars(!is.na(.))) %>% filter(Outdoor_Activities_Hours != Video_Games_Hours) %>% filter(abs(Outdoor_Activities_Hours - Video_Games_Hours) >= 10) %>% filter(Reaction_time > 0.1 & Reaction_time < 0.4) %>% mutate(More_Activity = if_else(Video_Games_Hours > Outdoor_Activities_Hours, \"Gamer\", \"Jock\")) %>% rename(`Reaction Time (Seconds)` = Reaction_time) %>% rename(`Time Spent Outside (Hours)` = Outdoor_Activities_Hours) %>% rename(`Time Spent Playing Video Games (Hours)` = Video_Games_Hours) %>% rename(`Gamer or Jock?` = More_Activity) HSS.G <- select(HSS.GJ, c(`Reaction Time (Seconds)`,`Gamer or Jock?`)) %>% filter(`Gamer or Jock?` == \"Gamer\") HSS.J <- select(HSS.GJ, c(`Reaction Time (Seconds)`,`Gamer or Jock?`)) %>% filter(`Gamer or Jock?` == \"Jock\") The data provided from the Census at School questionnaire contained a variety of data. For this reason, the data has been filtered down to show the student’s reaction time (given in seconds) as well as the time they spend on outdoor activities and video game activities per week (given in hours). In order to find out what is considered a fast reaction time, we conducted some preliminary research. According to the platform PubNub, the fastest reaction time is around 100 milliseconds (0.1 seconds) while most average more than 250 milliseconds (0.25 seconds). While we’re focusing on the fastest average reaction times, analyzing only students who achieved reaction times between 100-200 milliseconds would create too small a sample size. To address this, we expanded our data set to include reaction times between 100 and 400 milliseconds, capturing both fast and moderate responses. This approach allows us to maintain a focus on the quickest reactions while ensuring a sufficiently large data set for our study. It also effectively excludes extremely slow responses from the original data set. Those who spend more hours a week on outdoor activities are classified as “Jocks”, and those who spend more on video games are considered “Gamers”. While many of the students in the data set have spent hours doing both, those who spend 10 hours more in one or the other determines their classification. Thus, the student must have at least 10 hours a week in either activity to receive a category. An additional column was then made to display their classification. datatable(HSS.GJ, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") {\"x\":{\"filter\":\"none\",\"vertical\":false,\"extensions\":[\"Responsive\"],\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\"],[0.358,0.29,0.23,0.391,0.384,0.383,0.317,0.297,0.393,0.305,0.303,0.357,0.294,0.329,0.392,0.34,0.31,0.276,0.289,0.317,0.282,0.35,0.389,0.269,0.361,0.363,0.332,0.268,0.356,0.329,0.366,0.315,0.37,0.27,0.29,0.357,0.34,0.38,0.32,0.2,0.375,0.338,0.324,0.32,0.394,0.38,0.38,0.3,0.31,0.322,0.141,0.334,0.341,0.357,0.375,0.283,0.366,0.353,0.384,0.388,0.357,0.36,0.223,0.368,0.34,0.374,0.298,0.351,0.292,0.3,0.352,0.37,0.382,0.279,0.3,0.37,0.338,0.38,0.362,0.3],[20,0,1,1,14,20,12,21,11,13,3,18,2,0,0,1,2,10,1,14,18,1,0,25,0,14,20,35,2,10,4,12,15,5,30,20,0,4,20,58,1,8,10,20,0,13,13,50,15,10,16,15,20,30,4,10,15,15,5,0,30,10,1,10,0,25,15,10,21,20,21,1,0,20,17,7,15,8,15,168],[10,105,12,12,3,3,0.5,1,45,1,40,0,20,25,106,15,15,20,60,0,8,20,40,0,24,0,1,3,60,0,78,0,0,15,2,2,10,24,80,0,80,50,20,80,10,2,2,30,4,70,5,5,3,10,18,20,1,0,20,10,10,0,42,0,10,0,2,0,3,0,0,11,14,70,1,43,0,20,2,80],[\"Jock\",\"Gamer\",\"Gamer\",\"Gamer\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Gamer\",\"Jock\",\"Gamer\",\"Jock\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Jock\",\"Jock\",\"Gamer\",\"Gamer\",\"Jock\",\"Gamer\",\"Jock\",\"Jock\",\"Jock\",\"Gamer\",\"Jock\",\"Gamer\",\"Jock\",\"Jock\",\"Gamer\",\"Jock\",\"Jock\",\"Gamer\",\"Gamer\",\"Gamer\",\"Jock\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Gamer\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Gamer\",\"Gamer\",\"Jock\",\"Jock\",\"Gamer\",\"Gamer\",\"Jock\",\"Jock\",\"Gamer\",\"Jock\",\"Gamer\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Gamer\",\"Gamer\",\"Gamer\",\"Jock\",\"Gamer\",\"Jock\",\"Gamer\",\"Jock\",\"Jock\"]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>Reaction Time (Seconds)<\\/th>\\n <th>Time Spent Outside (Hours)<\\/th>\\n <th>Time Spent Playing Video Games (Hours)<\\/th>\\n <th>Gamer or Jock?<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"lengthMenu\":[3,10,30],\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Reaction Time (Seconds)\",\"targets\":1},{\"name\":\"Time Spent Outside (Hours)\",\"targets\":2},{\"name\":\"Time Spent Playing Video Games (Hours)\",\"targets\":3},{\"name\":\"Gamer or Jock?\",\"targets\":4}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"responsive\":true}},\"evals\":[],\"jsHooks\":[]} Using this data set, we will implement an Independent Samples t-Test to address this study’s research question. The null and alternative hypothesis are written as: \\[ H_0: \\text{There is no significant difference in average fastest reaction times between jocks and gamers} \\] \\[ H_a:\\text{There is a significant difference in average fastest reaction times between jocks and gamers} \\] Additionally, our significance level for this study will be : \\[ \\alpha = 0.05 \\] Analysis The following side by side box plots summarize the sample statistics of each group. The statistics shown include the Minimum, 1st Quartile, Median, 3rd Quartile, and Maximum. ** Hover over each box plot to see each group’s summary statistics. plot_ly(HSS.GJ, y=~`Reaction Time (Seconds)`, x=~as.factor(`Gamer or Jock?`),type=\"box\",color=~`Gamer or Jock?`, colors=c(\"lightblue\",\"lightgreen\")) %>% layout( title=\"High School Students' Reaction Times\",yaxis=list(title=\"Reaction Time (sec)\"),xaxis=list(title=\"\")) {\"x\":{\"visdat\":{\"5d9818db5668\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"5d9818db5668\",\"attrs\":{\"5d9818db5668\":{\"y\":{},\"x\":{},\"color\":{},\"colors\":[\"lightblue\",\"lightgreen\"],\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"box\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"title\":\"High School Students' Reaction Times\",\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Reaction Time (sec)\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"\",\"type\":\"category\",\"categoryorder\":\"array\",\"categoryarray\":[\"Gamer\",\"Jock\"]},\"hovermode\":\"closest\",\"showlegend\":true},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"fillcolor\":\"rgba(173,216,230,0.5)\",\"y\":[0.28999999999999998,0.23000000000000001,0.39100000000000001,0.39300000000000002,0.30299999999999999,0.29399999999999998,0.32900000000000001,0.39200000000000002,0.34000000000000002,0.31,0.27600000000000002,0.28899999999999998,0.34999999999999998,0.38900000000000001,0.36099999999999999,0.35599999999999998,0.36599999999999999,0.27000000000000002,0.34000000000000002,0.38,0.32000000000000001,0.375,0.33800000000000002,0.32400000000000001,0.32000000000000001,0.39400000000000002,0.32200000000000001,0.375,0.28299999999999997,0.38400000000000001,0.38800000000000001,0.223,0.34000000000000002,0.37,0.38200000000000001,0.27900000000000003,0.37,0.38],\"x\":[\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\",\"Gamer\"],\"type\":\"box\",\"name\":\"Gamer\",\"marker\":{\"color\":\"rgba(173,216,230,1)\",\"line\":{\"color\":\"rgba(173,216,230,1)\"}},\"line\":{\"color\":\"rgba(173,216,230,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"fillcolor\":\"rgba(144,238,144,0.5)\",\"y\":[0.35799999999999998,0.38400000000000001,0.38300000000000001,0.317,0.29699999999999999,0.30499999999999999,0.35699999999999998,0.317,0.28199999999999997,0.26900000000000002,0.36299999999999999,0.33200000000000002,0.26800000000000002,0.32900000000000001,0.315,0.37,0.28999999999999998,0.35699999999999998,0.20000000000000001,0.38,0.38,0.29999999999999999,0.31,0.14099999999999999,0.33400000000000002,0.34100000000000003,0.35699999999999998,0.36599999999999999,0.35299999999999998,0.35699999999999998,0.35999999999999999,0.36799999999999999,0.374,0.29799999999999999,0.35099999999999998,0.29199999999999998,0.29999999999999999,0.35199999999999998,0.29999999999999999,0.33800000000000002,0.36199999999999999,0.29999999999999999],\"x\":[\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\",\"Jock\"],\"type\":\"box\",\"name\":\"Jock\",\"marker\":{\"color\":\"rgba(144,238,144,1)\",\"line\":{\"color\":\"rgba(144,238,144,1)\"}},\"line\":{\"color\":\"rgba(144,238,144,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}As we compare the two groups, there appears to be a noticeable difference in reaction times. To further confirm these observations, we will now use an Independent Samples t-Test to further confirm these findings. However, the Independent Samples t-Test has two requirements before proceeding: Both samples are representative of the population (via Simple Random Sampling) The sampling distribution of the difference sample means can be assumed to be normal The questionnaire used was given out to a variety of students across the United States, as a result fulfilling that first requirement through simple random sampling. As for the second requirement, we must determine the normality of each group through the use of a QQ Plot. ** Each QQ Plot is shown below qqPlot(HSS.G$`Reaction Time (Seconds)`, main =\"Gamer Reaction Times\",ylab= \"Reaction Time (sec)\",col=\"green3\",col.lines = \"lightgreen\", pch= 19, id= FALSE) qqPlot(HSS.J$`Reaction Time (Seconds)`, main =\"Jock Reaction Times\", ,ylab= \"Reaction Time (sec)\", col=\"lightblue3\",col.lines = \"lightblue\", pch= 19, id= FALSE) According to each QQ plot, the data set fails the test of normality. While the majority of the data falls within the boundaries of normality, both groups have a few outliers that result in slight skewness. Despite this, the large sample size is sufficient to compensate for the data’s skewness. Consequently, the results of the test can still be considered valid. With our two requirements fulfilled, we can now conduct our Independent Samples t-Test on the data set. ** The results of the test are shown below. pander(t.test(`Reaction Time (Seconds)`~`Gamer or Jock?`, data=HSS.GJ, mu=0, alternative=\"two.sided\", conf.level=0.95),caption=\"Independent Samples t Test of Gamers vs. Jocks Reaction Times\", split.table=Inf) Independent Samples t Test of Gamers vs. Jocks Reaction Times Test statistic df P value Alternative hypothesis mean in group Gamer mean in group Jock 1.021 77.74 0.3106 two.sided 0.3373 0.3264 Comparing our given p-value to our level of significance, we are shown that: (\\(p = 0.3106 > \\alpha\\)) Given the high p-value, we are given insufficient evidence to prove that the alternative hypothesis is true and reasonable grounds to fail to reject the null hypothesis. Therefore, there is no significant difference in average fastest reaction times between gamers and jocks. Interpretation With the data shown from the plots and the t-Test, there appears to be a similar average quick reaction time between those who spend more time on outdoor activities and those who spend more time on video games. While the average fastest reaction time for Jocks (0.3264 seconds) was lower than Gamers (0.3373 seconds), it is only a difference of around 0.0109 of a second. This difference could be effected by the outliers in each group since the Gamer group has outliers in the high end, while the Jock group has outliers in the lower end. At face value, this difference is very minuscule to be considered something significant. A recommendation for further studies is to adjust the sampling method when collecting data. While the current data was gathered from a questionnaire that was dispersed to schools throughout the United States, the chosen schools were participating in the Census at School program. This suggests that the data may be biased towards high-performing students. Widening the criteria to include schools outside this program would provide a more representative sample of the population, thus reducing the bias inherent in the current data set. Sources: U.S. Census at School Questionnaire https://ww2.amstat.org/censusatschool/pdfs/C@SQuestionnaire.pdf How fast is Real-Time? Perception & Technology. (n.d.). PubNub. https://www.pubnub.com/blog/how-fast-is-realtime-human-perception-and-technology/ Formatting inspiration Rail Trail t Test Example // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":" Brigham Young University of Idaho Testing Center ","url":"325Analyses/TestingCenter.html","content":"Code Show All Code Hide All Code Brigham Young University of Idaho Testing Center library(GSODR) library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(sf) library(ggspatial) library(leaflet.extras) library(bslib) library(shiny) library(broom) library(MASS) testingcenter <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/Testingcenterscores.csv\") Background In Fall Semester 2024, a new protocol required 100 level Math students to take their exams in Brigham Young University of Idaho’s Testing Center. This change came after faculty suspected cheating during exams, since in the previous semester (Spring 2024) students could take tests in quiet areas like dorm rooms or isolated campus spaces without proctoring. While preventing cheating is a valid reason for requiring Testing Center exams, is this new requirement severely affecting average test scores? Click the tabs below to explore the data collection Hide Data Show Data To investigate how Testing Center exams affect scores, I requested exam data from teachers of 100-level classes during Spring and Fall 2024. The test scores come from classes 100B (Beginning Algebra) and 101 (Intermediate Algebra). To ensure a fair comparison between semesters, I collected scores from the midpoint of both terms. For each exam, I recorded the teacher (Baird, Ballou, Oldyroyd, or Ashcraft), whether it was taken in the Testing Center (In or Out), and the student’s score. datatable(testingcenter, options=list(lengthMenu=c(3,10,30))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"223\",\"224\",\"225\",\"226\",\"227\",\"228\",\"229\",\"230\",\"231\",\"232\",\"233\",\"234\",\"235\",\"236\",\"237\",\"238\",\"239\",\"240\",\"241\",\"242\",\"243\",\"244\",\"245\",\"246\",\"247\",\"248\",\"249\",\"250\",\"251\",\"252\",\"253\",\"254\",\"255\",\"256\",\"257\",\"258\",\"259\",\"260\",\"261\",\"262\",\"263\",\"264\",\"265\",\"266\",\"267\",\"268\",\"269\",\"270\",\"271\",\"272\",\"273\",\"274\",\"275\",\"276\",\"277\",\"278\",\"279\",\"280\",\"281\",\"282\",\"283\",\"284\",\"285\",\"286\",\"287\",\"288\",\"289\",\"290\",\"291\",\"292\",\"293\",\"294\",\"295\",\"296\",\"297\",\"298\",\"299\",\"300\",\"301\",\"302\",\"303\",\"304\",\"305\",\"306\",\"307\",\"308\",\"309\",\"310\",\"311\",\"312\",\"313\",\"314\",\"315\",\"316\",\"317\",\"318\",\"319\",\"320\",\"321\",\"322\",\"323\",\"324\",\"325\",\"326\",\"327\",\"328\",\"329\",\"330\",\"331\",\"332\",\"333\",\"334\",\"335\",\"336\",\"337\",\"338\",\"339\",\"340\",\"341\",\"342\",\"343\",\"344\",\"345\",\"346\",\"347\",\"348\",\"349\",\"350\",\"351\",\"352\",\"353\",\"354\",\"355\",\"356\",\"357\",\"358\",\"359\",\"360\",\"361\",\"362\",\"363\",\"364\",\"365\",\"366\",\"367\",\"368\",\"369\",\"370\",\"371\",\"372\",\"373\",\"374\",\"375\",\"376\",\"377\",\"378\",\"379\",\"380\",\"381\",\"382\",\"383\",\"384\",\"385\",\"386\",\"387\",\"388\",\"389\",\"390\",\"391\",\"392\",\"393\",\"394\",\"395\",\"396\",\"397\",\"398\",\"399\",\"400\",\"401\",\"402\",\"403\",\"404\",\"405\",\"406\",\"407\",\"408\",\"409\",\"410\",\"411\",\"412\",\"413\",\"414\",\"415\",\"416\",\"417\",\"418\",\"419\",\"420\",\"421\",\"422\",\"423\",\"424\",\"425\",\"426\",\"427\",\"428\",\"429\",\"430\",\"431\",\"432\",\"433\",\"434\",\"435\",\"436\",\"437\",\"438\",\"439\",\"440\",\"441\",\"442\",\"443\",\"444\",\"445\",\"446\",\"447\",\"448\",\"449\",\"450\",\"451\",\"452\",\"453\",\"454\",\"455\",\"456\",\"457\",\"458\",\"459\",\"460\",\"461\",\"462\",\"463\",\"464\",\"465\",\"466\",\"467\",\"468\",\"469\",\"470\",\"471\",\"472\",\"473\",\"474\",\"475\",\"476\",\"477\",\"478\",\"479\",\"480\",\"481\",\"482\",\"483\",\"484\",\"485\",\"486\",\"487\",\"488\",\"489\",\"490\",\"491\",\"492\",\"493\",\"494\",\"495\",\"496\",\"497\",\"498\",\"499\",\"500\",\"501\",\"502\",\"503\",\"504\",\"505\",\"506\",\"507\",\"508\",\"509\",\"510\",\"511\",\"512\",\"513\",\"514\",\"515\",\"516\",\"517\",\"518\",\"519\",\"520\",\"521\",\"522\",\"523\",\"524\",\"525\",\"526\",\"527\",\"528\",\"529\",\"530\",\"531\",\"532\",\"533\",\"534\",\"535\",\"536\",\"537\",\"538\",\"539\",\"540\",\"541\",\"542\",\"543\",\"544\",\"545\",\"546\",\"547\",\"548\",\"549\",\"550\",\"551\",\"552\",\"553\",\"554\",\"555\",\"556\",\"557\",\"558\",\"559\",\"560\",\"561\",\"562\",\"563\",\"564\",\"565\",\"566\",\"567\",\"568\",\"569\",\"570\",\"571\",\"572\",\"573\",\"574\",\"575\",\"576\",\"577\",\"578\",\"579\",\"580\",\"581\",\"582\",\"583\",\"584\",\"585\",\"586\",\"587\",\"588\",\"589\",\"590\",\"591\",\"592\",\"593\",\"594\",\"595\",\"596\",\"597\",\"598\",\"599\",\"600\",\"601\",\"602\",\"603\",\"604\",\"605\",\"606\",\"607\",\"608\",\"609\",\"610\",\"611\",\"612\",\"613\",\"614\",\"615\",\"616\",\"617\",\"618\",\"619\",\"620\",\"621\",\"622\",\"623\",\"624\",\"625\",\"626\",\"627\",\"628\",\"629\",\"630\",\"631\",\"632\",\"633\",\"634\",\"635\",\"636\",\"637\",\"638\",\"639\",\"640\",\"641\",\"642\",\"643\",\"644\",\"645\",\"646\",\"647\",\"648\",\"649\",\"650\",\"651\",\"652\",\"653\",\"654\",\"655\",\"656\",\"657\",\"658\",\"659\",\"660\",\"661\",\"662\",\"663\",\"664\",\"665\",\"666\",\"667\",\"668\",\"669\",\"670\",\"671\",\"672\",\"673\",\"674\",\"675\",\"676\",\"677\",\"678\",\"679\",\"680\",\"681\",\"682\",\"683\",\"684\",\"685\",\"686\",\"687\",\"688\",\"689\",\"690\",\"691\",\"692\",\"693\",\"694\",\"695\",\"696\",\"697\",\"698\",\"699\",\"700\",\"701\",\"702\",\"703\",\"704\",\"705\",\"706\",\"707\",\"708\",\"709\",\"710\",\"711\",\"712\",\"713\",\"714\",\"715\",\"716\",\"717\",\"718\",\"719\",\"720\",\"721\",\"722\",\"723\",\"724\",\"725\",\"726\",\"727\",\"728\",\"729\",\"730\",\"731\",\"732\",\"733\",\"734\",\"735\",\"736\",\"737\",\"738\",\"739\",\"740\",\"741\",\"742\",\"743\",\"744\",\"745\",\"746\",\"747\",\"748\",\"749\",\"750\",\"751\",\"752\",\"753\",\"754\",\"755\",\"756\",\"757\",\"758\",\"759\",\"760\",\"761\",\"762\",\"763\",\"764\",\"765\",\"766\",\"767\",\"768\",\"769\",\"770\",\"771\",\"772\",\"773\",\"774\",\"775\",\"776\",\"777\",\"778\",\"779\",\"780\",\"781\",\"782\",\"783\",\"784\",\"785\",\"786\",\"787\",\"788\",\"789\",\"790\",\"791\",\"792\",\"793\",\"794\",\"795\",\"796\",\"797\",\"798\",\"799\",\"800\",\"801\",\"802\",\"803\",\"804\",\"805\",\"806\",\"807\",\"808\",\"809\",\"810\",\"811\",\"812\",\"813\",\"814\",\"815\",\"816\",\"817\",\"818\",\"819\",\"820\",\"821\",\"822\",\"823\",\"824\",\"825\",\"826\",\"827\",\"828\",\"829\",\"830\",\"831\",\"832\",\"833\",\"834\",\"835\",\"836\",\"837\",\"838\",\"839\",\"840\",\"841\",\"842\",\"843\",\"844\",\"845\",\"846\",\"847\",\"848\",\"849\",\"850\",\"851\",\"852\",\"853\",\"854\",\"855\",\"856\",\"857\",\"858\",\"859\",\"860\",\"861\",\"862\",\"863\",\"864\",\"865\",\"866\",\"867\",\"868\",\"869\",\"870\",\"871\",\"872\",\"873\",\"874\",\"875\",\"876\",\"877\",\"878\",\"879\",\"880\",\"881\",\"882\",\"883\",\"884\",\"885\",\"886\",\"887\",\"888\",\"889\",\"890\",\"891\",\"892\",\"893\",\"894\",\"895\",\"896\",\"897\",\"898\",\"899\",\"900\",\"901\",\"902\",\"903\",\"904\",\"905\",\"906\",\"907\",\"908\",\"909\",\"910\",\"911\",\"912\",\"913\",\"914\",\"915\",\"916\",\"917\",\"918\",\"919\",\"920\",\"921\",\"922\",\"923\",\"924\",\"925\",\"926\",\"927\",\"928\",\"929\",\"930\",\"931\",\"932\",\"933\",\"934\",\"935\",\"936\",\"937\",\"938\",\"939\",\"940\",\"941\",\"942\",\"943\",\"944\",\"945\",\"946\",\"947\",\"948\",\"949\",\"950\",\"951\",\"952\",\"953\",\"954\",\"955\",\"956\",\"957\",\"958\",\"959\",\"960\",\"961\",\"962\",\"963\",\"964\",\"965\",\"966\",\"967\",\"968\",\"969\",\"970\",\"971\",\"972\",\"973\",\"974\",\"975\",\"976\",\"977\",\"978\",\"979\",\"980\",\"981\",\"982\",\"983\",\"984\",\"985\",\"986\",\"987\",\"988\",\"989\",\"990\",\"991\",\"992\",\"993\",\"994\",\"995\",\"996\",\"997\",\"998\",\"999\",\"1000\",\"1001\",\"1002\",\"1003\",\"1004\",\"1005\",\"1006\",\"1007\",\"1008\",\"1009\",\"1010\",\"1011\",\"1012\",\"1013\",\"1014\",\"1015\",\"1016\",\"1017\",\"1018\",\"1019\",\"1020\",\"1021\",\"1022\",\"1023\",\"1024\",\"1025\",\"1026\",\"1027\",\"1028\",\"1029\",\"1030\",\"1031\",\"1032\",\"1033\",\"1034\",\"1035\",\"1036\",\"1037\",\"1038\",\"1039\",\"1040\",\"1041\",\"1042\",\"1043\",\"1044\",\"1045\",\"1046\",\"1047\",\"1048\",\"1049\",\"1050\",\"1051\",\"1052\",\"1053\",\"1054\",\"1055\",\"1056\",\"1057\",\"1058\",\"1059\",\"1060\",\"1061\",\"1062\",\"1063\",\"1064\",\"1065\",\"1066\",\"1067\",\"1068\",\"1069\",\"1070\",\"1071\",\"1072\",\"1073\",\"1074\",\"1075\",\"1076\",\"1077\",\"1078\",\"1079\",\"1080\",\"1081\",\"1082\",\"1083\",\"1084\",\"1085\",\"1086\",\"1087\",\"1088\",\"1089\",\"1090\",\"1091\",\"1092\",\"1093\",\"1094\",\"1095\",\"1096\",\"1097\",\"1098\",\"1099\",\"1100\",\"1101\",\"1102\",\"1103\",\"1104\",\"1105\",\"1106\",\"1107\",\"1108\",\"1109\",\"1110\",\"1111\",\"1112\",\"1113\",\"1114\",\"1115\",\"1116\",\"1117\",\"1118\",\"1119\",\"1120\",\"1121\",\"1122\",\"1123\",\"1124\",\"1125\",\"1126\",\"1127\",\"1128\",\"1129\",\"1130\",\"1131\",\"1132\",\"1133\",\"1134\",\"1135\",\"1136\",\"1137\",\"1138\",\"1139\",\"1140\",\"1141\",\"1142\",\"1143\",\"1144\",\"1145\",\"1146\",\"1147\",\"1148\",\"1149\",\"1150\",\"1151\",\"1152\",\"1153\",\"1154\",\"1155\",\"1156\",\"1157\",\"1158\",\"1159\",\"1160\",\"1161\",\"1162\",\"1163\",\"1164\",\"1165\",\"1166\",\"1167\",\"1168\",\"1169\",\"1170\",\"1171\",\"1172\",\"1173\",\"1174\",\"1175\",\"1176\",\"1177\",\"1178\",\"1179\",\"1180\",\"1181\",\"1182\",\"1183\",\"1184\",\"1185\",\"1186\",\"1187\",\"1188\",\"1189\",\"1190\",\"1191\",\"1192\",\"1193\",\"1194\",\"1195\",\"1196\",\"1197\",\"1198\",\"1199\",\"1200\",\"1201\",\"1202\",\"1203\",\"1204\",\"1205\",\"1206\",\"1207\",\"1208\",\"1209\",\"1210\",\"1211\",\"1212\",\"1213\",\"1214\",\"1215\",\"1216\",\"1217\",\"1218\",\"1219\",\"1220\",\"1221\",\"1222\",\"1223\",\"1224\",\"1225\",\"1226\",\"1227\",\"1228\",\"1229\",\"1230\",\"1231\",\"1232\",\"1233\",\"1234\",\"1235\",\"1236\",\"1237\",\"1238\",\"1239\",\"1240\",\"1241\",\"1242\",\"1243\",\"1244\",\"1245\",\"1246\",\"1247\",\"1248\",\"1249\",\"1250\",\"1251\",\"1252\",\"1253\",\"1254\",\"1255\",\"1256\",\"1257\",\"1258\",\"1259\",\"1260\",\"1261\",\"1262\",\"1263\",\"1264\",\"1265\",\"1266\",\"1267\",\"1268\",\"1269\",\"1270\",\"1271\",\"1272\",\"1273\",\"1274\",\"1275\",\"1276\",\"1277\",\"1278\",\"1279\",\"1280\",\"1281\",\"1282\",\"1283\",\"1284\",\"1285\",\"1286\",\"1287\",\"1288\",\"1289\",\"1290\",\"1291\",\"1292\",\"1293\",\"1294\",\"1295\",\"1296\",\"1297\",\"1298\",\"1299\",\"1300\",\"1301\",\"1302\",\"1303\",\"1304\",\"1305\",\"1306\",\"1307\",\"1308\",\"1309\",\"1310\",\"1311\",\"1312\",\"1313\",\"1314\",\"1315\",\"1316\",\"1317\",\"1318\",\"1319\",\"1320\",\"1321\",\"1322\",\"1323\",\"1324\",\"1325\",\"1326\",\"1327\",\"1328\",\"1329\",\"1330\",\"1331\",\"1332\",\"1333\",\"1334\",\"1335\",\"1336\",\"1337\",\"1338\",\"1339\",\"1340\",\"1341\",\"1342\",\"1343\",\"1344\",\"1345\",\"1346\",\"1347\",\"1348\",\"1349\",\"1350\",\"1351\",\"1352\",\"1353\",\"1354\",\"1355\",\"1356\",\"1357\",\"1358\",\"1359\",\"1360\",\"1361\",\"1362\",\"1363\",\"1364\",\"1365\",\"1366\",\"1367\",\"1368\",\"1369\",\"1370\",\"1371\",\"1372\",\"1373\",\"1374\",\"1375\",\"1376\",\"1377\",\"1378\",\"1379\",\"1380\",\"1381\",\"1382\",\"1383\",\"1384\",\"1385\",\"1386\",\"1387\",\"1388\",\"1389\",\"1390\",\"1391\",\"1392\",\"1393\",\"1394\",\"1395\",\"1396\",\"1397\",\"1398\",\"1399\",\"1400\",\"1401\",\"1402\",\"1403\",\"1404\",\"1405\",\"1406\",\"1407\",\"1408\",\"1409\",\"1410\",\"1411\",\"1412\",\"1413\",\"1414\",\"1415\",\"1416\",\"1417\",\"1418\",\"1419\",\"1420\",\"1421\",\"1422\",\"1423\",\"1424\",\"1425\",\"1426\",\"1427\",\"1428\",\"1429\",\"1430\",\"1431\",\"1432\",\"1433\",\"1434\",\"1435\",\"1436\",\"1437\",\"1438\",\"1439\",\"1440\",\"1441\",\"1442\",\"1443\",\"1444\",\"1445\",\"1446\",\"1447\",\"1448\",\"1449\",\"1450\",\"1451\",\"1452\",\"1453\",\"1454\",\"1455\",\"1456\",\"1457\",\"1458\",\"1459\",\"1460\",\"1461\",\"1462\",\"1463\",\"1464\",\"1465\",\"1466\",\"1467\",\"1468\",\"1469\",\"1470\",\"1471\",\"1472\",\"1473\",\"1474\",\"1475\",\"1476\",\"1477\",\"1478\",\"1479\",\"1480\",\"1481\",\"1482\",\"1483\",\"1484\",\"1485\",\"1486\",\"1487\",\"1488\",\"1489\",\"1490\",\"1491\",\"1492\",\"1493\",\"1494\",\"1495\",\"1496\",\"1497\",\"1498\",\"1499\",\"1500\",\"1501\",\"1502\",\"1503\",\"1504\",\"1505\",\"1506\",\"1507\",\"1508\",\"1509\",\"1510\",\"1511\",\"1512\",\"1513\",\"1514\",\"1515\",\"1516\",\"1517\",\"1518\",\"1519\",\"1520\",\"1521\",\"1522\",\"1523\",\"1524\",\"1525\",\"1526\",\"1527\",\"1528\",\"1529\",\"1530\",\"1531\",\"1532\",\"1533\",\"1534\",\"1535\",\"1536\",\"1537\",\"1538\",\"1539\",\"1540\",\"1541\",\"1542\",\"1543\",\"1544\",\"1545\",\"1546\",\"1547\",\"1548\",\"1549\",\"1550\",\"1551\",\"1552\",\"1553\",\"1554\",\"1555\",\"1556\",\"1557\",\"1558\",\"1559\",\"1560\",\"1561\",\"1562\",\"1563\",\"1564\",\"1565\",\"1566\",\"1567\",\"1568\",\"1569\",\"1570\",\"1571\",\"1572\",\"1573\",\"1574\",\"1575\",\"1576\",\"1577\",\"1578\",\"1579\",\"1580\",\"1581\",\"1582\",\"1583\",\"1584\",\"1585\",\"1586\",\"1587\",\"1588\",\"1589\",\"1590\",\"1591\",\"1592\",\"1593\",\"1594\",\"1595\",\"1596\",\"1597\",\"1598\",\"1599\",\"1600\",\"1601\",\"1602\",\"1603\",\"1604\",\"1605\",\"1606\",\"1607\",\"1608\",\"1609\",\"1610\",\"1611\",\"1612\",\"1613\",\"1614\",\"1615\",\"1616\",\"1617\",\"1618\",\"1619\",\"1620\",\"1621\",\"1622\",\"1623\",\"1624\",\"1625\",\"1626\",\"1627\",\"1628\",\"1629\",\"1630\",\"1631\",\"1632\",\"1633\",\"1634\",\"1635\",\"1636\",\"1637\",\"1638\",\"1639\",\"1640\",\"1641\",\"1642\",\"1643\",\"1644\",\"1645\",\"1646\",\"1647\",\"1648\",\"1649\",\"1650\",\"1651\",\"1652\",\"1653\",\"1654\",\"1655\",\"1656\",\"1657\",\"1658\",\"1659\",\"1660\",\"1661\",\"1662\",\"1663\",\"1664\",\"1665\",\"1666\",\"1667\",\"1668\",\"1669\",\"1670\",\"1671\",\"1672\",\"1673\",\"1674\",\"1675\",\"1676\",\"1677\",\"1678\",\"1679\",\"1680\",\"1681\",\"1682\",\"1683\",\"1684\",\"1685\",\"1686\",\"1687\",\"1688\",\"1689\",\"1690\",\"1691\",\"1692\",\"1693\",\"1694\",\"1695\",\"1696\",\"1697\",\"1698\",\"1699\",\"1700\",\"1701\",\"1702\",\"1703\",\"1704\",\"1705\",\"1706\",\"1707\",\"1708\",\"1709\",\"1710\",\"1711\",\"1712\",\"1713\",\"1714\",\"1715\",\"1716\",\"1717\",\"1718\",\"1719\",\"1720\",\"1721\",\"1722\",\"1723\",\"1724\",\"1725\",\"1726\",\"1727\",\"1728\",\"1729\",\"1730\",\"1731\",\"1732\",\"1733\",\"1734\",\"1735\",\"1736\",\"1737\",\"1738\",\"1739\",\"1740\",\"1741\",\"1742\",\"1743\",\"1744\",\"1745\",\"1746\",\"1747\",\"1748\",\"1749\",\"1750\",\"1751\",\"1752\",\"1753\",\"1754\",\"1755\",\"1756\",\"1757\",\"1758\",\"1759\",\"1760\",\"1761\",\"1762\",\"1763\",\"1764\",\"1765\",\"1766\",\"1767\",\"1768\",\"1769\",\"1770\",\"1771\",\"1772\",\"1773\",\"1774\",\"1775\",\"1776\",\"1777\",\"1778\",\"1779\",\"1780\",\"1781\",\"1782\",\"1783\",\"1784\",\"1785\",\"1786\",\"1787\",\"1788\",\"1789\",\"1790\",\"1791\",\"1792\",\"1793\",\"1794\",\"1795\",\"1796\",\"1797\",\"1798\",\"1799\",\"1800\",\"1801\",\"1802\",\"1803\",\"1804\",\"1805\",\"1806\",\"1807\",\"1808\",\"1809\",\"1810\",\"1811\",\"1812\",\"1813\",\"1814\",\"1815\",\"1816\",\"1817\",\"1818\",\"1819\",\"1820\",\"1821\",\"1822\",\"1823\",\"1824\",\"1825\",\"1826\",\"1827\",\"1828\",\"1829\",\"1830\",\"1831\",\"1832\",\"1833\",\"1834\",\"1835\",\"1836\",\"1837\",\"1838\",\"1839\",\"1840\",\"1841\",\"1842\",\"1843\",\"1844\",\"1845\",\"1846\",\"1847\",\"1848\",\"1849\",\"1850\",\"1851\",\"1852\",\"1853\",\"1854\",\"1855\",\"1856\",\"1857\",\"1858\",\"1859\",\"1860\",\"1861\",\"1862\",\"1863\",\"1864\",\"1865\",\"1866\",\"1867\",\"1868\",\"1869\",\"1870\",\"1871\",\"1872\",\"1873\",\"1874\",\"1875\",\"1876\",\"1877\",\"1878\",\"1879\",\"1880\",\"1881\",\"1882\",\"1883\",\"1884\",\"1885\",\"1886\",\"1887\",\"1888\",\"1889\",\"1890\",\"1891\",\"1892\",\"1893\",\"1894\",\"1895\",\"1896\",\"1897\",\"1898\",\"1899\",\"1900\",\"1901\",\"1902\",\"1903\",\"1904\",\"1905\",\"1906\",\"1907\",\"1908\",\"1909\",\"1910\",\"1911\",\"1912\",\"1913\",\"1914\",\"1915\",\"1916\",\"1917\",\"1918\",\"1919\",\"1920\",\"1921\",\"1922\",\"1923\",\"1924\",\"1925\",\"1926\",\"1927\",\"1928\",\"1929\",\"1930\",\"1931\",\"1932\",\"1933\",\"1934\",\"1935\",\"1936\",\"1937\",\"1938\",\"1939\",\"1940\",\"1941\",\"1942\",\"1943\",\"1944\",\"1945\",\"1946\",\"1947\",\"1948\",\"1949\",\"1950\",\"1951\",\"1952\",\"1953\",\"1954\",\"1955\",\"1956\",\"1957\",\"1958\",\"1959\",\"1960\",\"1961\",\"1962\",\"1963\",\"1964\",\"1965\",\"1966\",\"1967\",\"1968\",\"1969\",\"1970\",\"1971\",\"1972\",\"1973\",\"1974\",\"1975\",\"1976\",\"1977\",\"1978\",\"1979\",\"1980\",\"1981\",\"1982\",\"1983\",\"1984\",\"1985\",\"1986\",\"1987\",\"1988\",\"1989\",\"1990\",\"1991\",\"1992\",\"1993\",\"1994\",\"1995\",\"1996\",\"1997\",\"1998\",\"1999\",\"2000\",\"2001\",\"2002\",\"2003\",\"2004\",\"2005\",\"2006\",\"2007\",\"2008\",\"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\",\"2021\",\"2022\",\"2023\",\"2024\",\"2025\",\"2026\",\"2027\",\"2028\",\"2029\",\"2030\",\"2031\",\"2032\",\"2033\",\"2034\",\"2035\",\"2036\",\"2037\",\"2038\",\"2039\",\"2040\",\"2041\",\"2042\",\"2043\",\"2044\",\"2045\",\"2046\",\"2047\",\"2048\",\"2049\",\"2050\",\"2051\",\"2052\",\"2053\",\"2054\",\"2055\",\"2056\",\"2057\",\"2058\",\"2059\",\"2060\",\"2061\",\"2062\",\"2063\",\"2064\",\"2065\",\"2066\",\"2067\",\"2068\",\"2069\",\"2070\",\"2071\",\"2072\",\"2073\",\"2074\",\"2075\",\"2076\",\"2077\",\"2078\",\"2079\",\"2080\",\"2081\",\"2082\",\"2083\",\"2084\",\"2085\",\"2086\",\"2087\",\"2088\",\"2089\",\"2090\",\"2091\",\"2092\",\"2093\",\"2094\",\"2095\",\"2096\",\"2097\",\"2098\",\"2099\",\"2100\",\"2101\",\"2102\",\"2103\",\"2104\",\"2105\",\"2106\",\"2107\",\"2108\",\"2109\",\"2110\",\"2111\",\"2112\",\"2113\",\"2114\",\"2115\",\"2116\",\"2117\",\"2118\",\"2119\",\"2120\",\"2121\",\"2122\",\"2123\",\"2124\",\"2125\",\"2126\",\"2127\",\"2128\",\"2129\",\"2130\",\"2131\",\"2132\",\"2133\",\"2134\",\"2135\",\"2136\",\"2137\",\"2138\",\"2139\",\"2140\",\"2141\",\"2142\",\"2143\",\"2144\",\"2145\",\"2146\",\"2147\",\"2148\",\"2149\",\"2150\",\"2151\",\"2152\",\"2153\",\"2154\",\"2155\",\"2156\",\"2157\",\"2158\",\"2159\",\"2160\",\"2161\",\"2162\",\"2163\",\"2164\",\"2165\",\"2166\",\"2167\",\"2168\",\"2169\",\"2170\",\"2171\",\"2172\",\"2173\",\"2174\",\"2175\",\"2176\",\"2177\",\"2178\",\"2179\",\"2180\",\"2181\",\"2182\",\"2183\",\"2184\",\"2185\",\"2186\",\"2187\",\"2188\",\"2189\",\"2190\",\"2191\",\"2192\",\"2193\",\"2194\",\"2195\",\"2196\",\"2197\",\"2198\",\"2199\",\"2200\",\"2201\",\"2202\",\"2203\",\"2204\",\"2205\",\"2206\",\"2207\",\"2208\",\"2209\",\"2210\",\"2211\",\"2212\",\"2213\",\"2214\",\"2215\",\"2216\",\"2217\",\"2218\",\"2219\",\"2220\",\"2221\",\"2222\",\"2223\",\"2224\",\"2225\",\"2226\",\"2227\",\"2228\",\"2229\",\"2230\",\"2231\",\"2232\",\"2233\",\"2234\",\"2235\",\"2236\",\"2237\",\"2238\",\"2239\",\"2240\",\"2241\",\"2242\",\"2243\",\"2244\",\"2245\",\"2246\",\"2247\",\"2248\",\"2249\",\"2250\",\"2251\",\"2252\",\"2253\",\"2254\",\"2255\",\"2256\",\"2257\",\"2258\",\"2259\",\"2260\",\"2261\",\"2262\",\"2263\",\"2264\",\"2265\",\"2266\",\"2267\",\"2268\",\"2269\",\"2270\",\"2271\",\"2272\",\"2273\",\"2274\",\"2275\",\"2276\",\"2277\",\"2278\",\"2279\",\"2280\",\"2281\",\"2282\",\"2283\",\"2284\",\"2285\",\"2286\",\"2287\",\"2288\",\"2289\",\"2290\",\"2291\",\"2292\",\"2293\",\"2294\",\"2295\",\"2296\",\"2297\",\"2298\",\"2299\",\"2300\",\"2301\",\"2302\",\"2303\",\"2304\",\"2305\",\"2306\",\"2307\",\"2308\",\"2309\",\"2310\",\"2311\",\"2312\",\"2313\",\"2314\",\"2315\",\"2316\",\"2317\",\"2318\",\"2319\",\"2320\",\"2321\",\"2322\",\"2323\",\"2324\",\"2325\",\"2326\",\"2327\",\"2328\",\"2329\",\"2330\",\"2331\",\"2332\",\"2333\",\"2334\",\"2335\",\"2336\",\"2337\",\"2338\",\"2339\",\"2340\",\"2341\",\"2342\",\"2343\",\"2344\",\"2345\",\"2346\",\"2347\",\"2348\",\"2349\",\"2350\",\"2351\",\"2352\",\"2353\",\"2354\",\"2355\",\"2356\",\"2357\",\"2358\",\"2359\",\"2360\",\"2361\",\"2362\",\"2363\",\"2364\",\"2365\",\"2366\",\"2367\",\"2368\",\"2369\",\"2370\",\"2371\",\"2372\",\"2373\",\"2374\",\"2375\",\"2376\",\"2377\",\"2378\",\"2379\",\"2380\",\"2381\",\"2382\",\"2383\",\"2384\",\"2385\",\"2386\",\"2387\",\"2388\",\"2389\",\"2390\",\"2391\",\"2392\",\"2393\",\"2394\",\"2395\",\"2396\",\"2397\",\"2398\",\"2399\",\"2400\",\"2401\",\"2402\",\"2403\",\"2404\",\"2405\",\"2406\",\"2407\",\"2408\",\"2409\",\"2410\",\"2411\",\"2412\",\"2413\",\"2414\",\"2415\",\"2416\",\"2417\",\"2418\",\"2419\",\"2420\",\"2421\",\"2422\",\"2423\",\"2424\",\"2425\",\"2426\"],[\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Baird\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ballou\",\"Ball"},{"title":"Tutoring for Beginning Algebra","url":"325Analyses/TutoringBA.html","content":"Code Show All Code Hide All Code Tutoring for Beginning Algebra library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Bug Spray – Wilcoxon Rank Sum Test","url":"325Analyses/Wilcoxon Tests/Examples/BugSprayWilcoxonRankSum.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } summary { display: list-item; } .tabset-dropdown > .nav-tabs { display: inline-table; max-height: 500px; min-height: 44px; overflow-y: auto; background: white; border: 1px solid #ddd; border-radius: 4px; } .tabset-dropdown > .nav-tabs > li.active:before { content: \"\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before { content: \"\"; border: none; } .tabset-dropdown > .nav-tabs.nav-tabs-open:before { content: \"\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown > .nav-tabs > li.active { display: block; } .tabset-dropdown > .nav-tabs > li > a, .tabset-dropdown > .nav-tabs > li > a:focus, .tabset-dropdown > .nav-tabs > li > a:hover { border: none; display: inline-block; border-radius: 4px; } .tabset-dropdown > .nav-tabs.nav-tabs-open > li { display: block; float: none; } .tabset-dropdown > .nav-tabs > li { display: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open') }); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Bug Spray – Wilcoxon Rank Sum Test library(pander) Background Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon. The percent of flies (bugs) killed from two different concentrations of a certain spray were recorded from 16 different trials, 8 trials per treatment concentration. The experiment hypothesized that the center of the distributions of the percent killed by either concentration were the same. In other words, that both treatments were equally effective. The alternative hypothesis was that the treatments differed in their effectiveness. In short, we wish to know which bug spray is more effective at killing flies (bugs). Spray Concentration Percent Killed A 68, 68, 59, 72, 64, 67, 70, 74 B 60, 67, 61, 62, 67, 63, 56, 58 Formally, the null and alternative hypotheses are written as \\[ H_0: \\text{difference in medians} = 0 \\] \\[ H_a: \\text{difference in medians} \\neq 0 \\] The significance level for this study will be set at \\[ \\alpha = 0.05 \\] Analysis The side-by-side dotplots below (with boxplots overlaid) suggest that Spray A is more effective than Spray B at killing bugs. The actual summary values depicted in the plot above are shown in the following table. bugspray %>% group_by(Spray) %>% summarise(min = min(Killed), median = median(Killed), mean = mean(Killed), max = max(Killed), sd = sd(Killed), `Number of Trials` = n()) %>% pander(caption=\"Summary of Bug Spray Effectiveness\") Summary of Bug Spray Effectiveness Spray min median mean max sd Number of Trials A 59 68 67.75 74 4.683 8 B 56 61.5 61.75 67 3.919 8 To determine if it is reasonable to infer that this conclusion is valid in general (for the full population) a Wilcoxon Rank Sum Test will be used. While the two samples are not identically shaped (shown in the boxplots above) they are not different enough to violate the assumption of identical shape and spread that is required by the Rank Sum Test. Note that since there are ties present in the data, an exact \\(p\\)-value cannot be computed. An approximation will be used instead. Because of the small sample size, a continuity correction will also be applied. The results of the test are still valid and show sufficient evidence to reject the null hypothesis \\((p=0.01771 < \\alpha)\\). Wilcoxon rank sum test with continuity correction data: Killed by Spray W = 55, p-value = 0.01771 alternative hypothesis: true location shift is not equal to 0 Interpretation Spray A is the more effective spray. Sadly, it still only kills roughly 68% of bugs that make contact with the spray, on average. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Corn Heights – Wilcoxon Signed-Rank Test","url":"325Analyses/Wilcoxon Tests/Examples/CornHeightsPairedWilcoxon.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } summary { display: list-item; } .tabset-dropdown > .nav-tabs { display: inline-table; max-height: 500px; min-height: 44px; overflow-y: auto; background: white; border: 1px solid #ddd; border-radius: 4px; } .tabset-dropdown > .nav-tabs > li.active:before { content: \"\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before { content: \"\"; border: none; } .tabset-dropdown > .nav-tabs.nav-tabs-open:before { content: \"\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown > .nav-tabs > li.active { display: block; } .tabset-dropdown > .nav-tabs > li > a, .tabset-dropdown > .nav-tabs > li > a:focus, .tabset-dropdown > .nav-tabs > li > a:hover { border: none; display: inline-block; border-radius: 4px; } .tabset-dropdown > .nav-tabs.nav-tabs-open > li { display: block; float: none; } .tabset-dropdown > .nav-tabs > li { display: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open') }); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Corn Heights – Wilcoxon Signed-Rank Test library(pander) Background Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon. Height differences “between cross- and self- fertilized corn plants of the same pair” were collected. The experiment hypothesized that the center of the distribution of the height differences would be zero, with the alternative being that the center was not zero. The result of the data collection was 15 height differences: Differences: 14, 56, 60, 16, 6, 8, -48, 49, 24, 28, 29, 41, -67, 23, 75 Formally, the null and alternative hypotheses are written as \\[ H_0: \\text{median of the differences} = 0 \\] \\[ H_a: \\text{median of the differences} \\neq 0 \\] The significance level for this study will be set at \\[ \\alpha = 0.05 \\] Analysis The dotplot below (with boxplot overlaid) shows the differences between the corn plants that were self-fertilzed versus those that were cross-fertilized. There seems to be strong evidence that the differences are generally positive. This implies that the cross-fertilized plants are generally taller than when self-fertilization was implemented. boxplot(corn, horizontal=TRUE, col=\"cornsilk\", main=\"Differences in Corn Plant Heights\", xlab=\"Cross-Fertilized Height minus Self-Fertilized Height\") stripchart(corn, pch=16, method=\"stack\", col=\"darkgray\", add=TRUE) pander(summary(corn)) Min. 1st Qu. Median Mean 3rd Qu. Max. -67 11 24 20.93 45 75 To determine if it is reasonable to infer that this conclusion is valid in general (for the full population) a Wilcoxon Signed-Rank Test will be used. The Wilcoxon Signed-Rank test is appropriate for any distribution of data, and especially for small sample sizes. pander(wilcox.test(corn, mu = 0, alternative = \"two.sided\", conf.level = 0.95)) Wilcoxon signed rank test: corn Test statistic P value Alternative hypothesis 96 0.04126 * two.sided There is sufficient evidence to reject the null hypothesis (\\(p = 0.04126 < \\alpha\\)). It is safe to conclude that the median of the differences (for the full population) is greater than zero. Interpretation Every now and again a self-fertilized corn plant will be taller than its cross-fertilized counterpart. However, the evidence shows that it is overwhelming more common for cross-fertilized plants to be taller. This study shows that cross-fertilized plants are on median, 24 units taller (95% confindence interval: 4.0-41.5) than self-fertilized plants. # Code used to compute confidence interval: wilcox.test(corn, mu = 0, alternative = \"two.sided\", conf.level = 0.95, conf.int = TRUE) // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Moral Integration of American Cities","url":"325Analyses/Wilcoxon Tests/Examples/MoralIntegration.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } summary { display: list-item; } .tabset-dropdown > .nav-tabs { display: inline-table; max-height: 500px; min-height: 44px; overflow-y: auto; background: white; border: 1px solid #ddd; border-radius: 4px; } .tabset-dropdown > .nav-tabs > li.active:before { content: \"\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before { content: \"\"; border: none; } .tabset-dropdown > .nav-tabs.nav-tabs-open:before { content: \"\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown > .nav-tabs > li.active { display: block; } .tabset-dropdown > .nav-tabs > li > a, .tabset-dropdown > .nav-tabs > li > a:focus, .tabset-dropdown > .nav-tabs > li > a:hover { border: none; display: inline-block; border-radius: 4px; } .tabset-dropdown > .nav-tabs.nav-tabs-open > li { display: block; float: none; } .tabset-dropdown > .nav-tabs > li { display: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open') }); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Moral Integration of American Cities This document uses the Angell dataset from library(car) to determine if there is greater mobility between the East and the West among the cities in the U.S. (around 1950). First, because this file is being used to demonstrate the Wilcoxon Rank Sum Test, we need to isolate the data to two groups, East and West. We will do this by combining S and NE to be E and combining MW and W to be W. We will use the library(tidyverse) and the function recode to do this. Notice how the dataset is modifed by the recode command in the code below. Angell2 <- Angell %>% mutate(area = recode(region, S=\"E\", NE=\"E\", MW=\"W\")) # alternatively we could have used: # Angell2 <- Angell %>% # mutate(area = mapvalues(region, # from = c(\"S\", \"MW\"), # to = c(\"E\", \"W\"))) rownames(Angell2) <- rownames(Angell) Hide Data Show Data Original Angell Data pander(Angell)   moral hetero mobility region Rochester 19 20.6 15 E Syracuse 17 15.6 20.2 E Worcester 16.4 22.1 13.6 E Erie 16.2 14 14.8 E Milwaukee 15.8 17.4 17.6 MW Bridgeport 15.3 27.9 17.5 E Buffalo 15.2 22.3 14.7 E Dayton 14.3 23.7 23.8 MW Reading 14.2 10.6 19.4 E Des_Moines 14.1 12.7 31.9 MW Cleveland 14 39.7 18.6 MW Denver 13.9 13 34.5 W Peoria 13.8 10.7 35.1 MW Wichita 13.6 11.9 42.7 MW Trenton 13 32.5 15.8 E Grand_Rapids 12.8 15.7 24.2 MW Toledo 12.7 19.2 21.6 MW San_Diego 12.5 15.9 49.8 W Baltimore 12 45.8 12.1 E South_Bend 11.8 17.9 27.4 MW Akron 11.3 20.4 22.1 MW Detroit 11.1 38.3 19.5 MW Tacoma 10.9 17.8 31.2 W Flint 9.8 19.3 32.2 MW Spokane 9.6 12.3 38.9 W Seattle 9 23.9 34.2 W Indianapolis 8.8 29.2 23.1 MW Columbus 8 27.4 25 MW Portland_Oregon 7.2 16.4 35.8 W Richmond 10.4 65.3 24.9 S Houston 10.2 49 36.1 S Fort_Worth 10.2 30.5 36.8 S Oklahoma_City 9.7 20.7 47.2 S Chattanooga 9.3 57.7 27.2 S Nashville 8.6 57.4 25.4 S Birmingham 8.2 83.1 25.9 S Dallas 8 36.8 37.8 S Louisville 7.7 31.5 19.4 S Jacksonville 6 73.7 27.7 S Memphis 5.4 84.5 26.7 S Tulsa 5.3 23.8 44.9 S Miami 5.1 50.2 41.8 S Atlanta 4.2 70.6 32.6 S Modified Angell Data pander(Angell2)   moral hetero mobility region area Rochester 19 20.6 15 E E Syracuse 17 15.6 20.2 E E Worcester 16.4 22.1 13.6 E E Erie 16.2 14 14.8 E E Milwaukee 15.8 17.4 17.6 MW W Bridgeport 15.3 27.9 17.5 E E Buffalo 15.2 22.3 14.7 E E Dayton 14.3 23.7 23.8 MW W Reading 14.2 10.6 19.4 E E Des_Moines 14.1 12.7 31.9 MW W Cleveland 14 39.7 18.6 MW W Denver 13.9 13 34.5 W W Peoria 13.8 10.7 35.1 MW W Wichita 13.6 11.9 42.7 MW W Trenton 13 32.5 15.8 E E Grand_Rapids 12.8 15.7 24.2 MW W Toledo 12.7 19.2 21.6 MW W San_Diego 12.5 15.9 49.8 W W Baltimore 12 45.8 12.1 E E South_Bend 11.8 17.9 27.4 MW W Akron 11.3 20.4 22.1 MW W Detroit 11.1 38.3 19.5 MW W Tacoma 10.9 17.8 31.2 W W Flint 9.8 19.3 32.2 MW W Spokane 9.6 12.3 38.9 W W Seattle 9 23.9 34.2 W W Indianapolis 8.8 29.2 23.1 MW W Columbus 8 27.4 25 MW W Portland_Oregon 7.2 16.4 35.8 W W Richmond 10.4 65.3 24.9 S E Houston 10.2 49 36.1 S E Fort_Worth 10.2 30.5 36.8 S E Oklahoma_City 9.7 20.7 47.2 S E Chattanooga 9.3 57.7 27.2 S E Nashville 8.6 57.4 25.4 S E Birmingham 8.2 83.1 25.9 S E Dallas 8 36.8 37.8 S E Louisville 7.7 31.5 19.4 S E Jacksonville 6 73.7 27.7 S E Memphis 5.4 84.5 26.7 S E Tulsa 5.3 23.8 44.9 S E Miami 5.1 50.2 41.8 S E Atlanta 4.2 70.6 32.6 S E Now we can compare the East and West with respect to their mobility scores. boxplot(mobility ~ area, data=Angell2, names=c(\"Eastern Cities\",\"Western Cities\"), ylab=\"Mobility Score\", col='gray', boxwex=.25, main = \"Geographic Mobility of U.S. Cities, 1950\", xlab=\"Cities in the Western U.S. Show Higher Mobility\") It appears there may be a slight shift in medians with the West being higher. Since the distibutions are similarly shaped (slightly right skewed), an official test of the hypotheses \\[ H_0: \\text{difference in medians} = 0 \\] \\[ H_a: \\text{difference in medians} \\neq 0 \\] can be performed. Using a Wilcoxon Rank Sum Test (using the normal approximation with continuity correction due to ties in the data), we obtain a test statistic of \\(W = 181\\) and a p-value of \\(0.2376\\). There is insufficient evidence to reject the null. We conclude that any differences in medians demonstrated by the above boxplot is simply due to random sampling. The mobility scores for the entire U.S. appear to be the same on average (median) between the East and West. Appendix Hide Show To see the R Code that produced the Wilcoxon Test results reported above, click the code button to the right. wilcox.test(mobility ~ area, data=Angell2) ## Warning in wilcox.test.default(x = c(15, 20.2, 13.6, 14.8, 17.5, 14.7, ## 19.4, : cannot compute exact p-value with ties ## ## Wilcoxon rank sum test with continuity correction ## ## data: mobility by area ## W = 181, p-value = 0.2376 ## alternative hypothesis: true location shift is not equal to 0 // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Sleep – Wilcoxon Signed-Rank Test","url":"325Analyses/Wilcoxon Tests/Examples/SleepPairedWilcoxon.html","content":".main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; height: auto; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } summary { display: list-item; } .tabset-dropdown > .nav-tabs { display: inline-table; max-height: 500px; min-height: 44px; overflow-y: auto; background: white; border: 1px solid #ddd; border-radius: 4px; } .tabset-dropdown > .nav-tabs > li.active:before { content: \"\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before { content: \"\"; border: none; } .tabset-dropdown > .nav-tabs.nav-tabs-open:before { content: \"\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown > .nav-tabs > li.active { display: block; } .tabset-dropdown > .nav-tabs > li > a, .tabset-dropdown > .nav-tabs > li > a:focus, .tabset-dropdown > .nav-tabs > li > a:hover { border: none; display: inline-block; border-radius: 4px; } .tabset-dropdown > .nav-tabs.nav-tabs-open > li { display: block; float: none; } .tabset-dropdown > .nav-tabs > li { display: none; } $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open') }); }); .code-folding-btn { margin-bottom: 4px; } $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); Code Show All Code Hide All Code Sleep – Wilcoxon Signed-Rank Test Background An experiment was conducted to determine which of two soporific drugs was better at increasing the hours of sleep individuals received, on average. Ten patients took each of the two drugs at different times. The amount of extra sleep that each individual received when using each drug was measured. The data is contained in the sleep data set. Note that the variable group would be better labeled as drug because the 10 individuals in each group are the same individuals as shown in the ID column. Hide Data Show Data Long Version pander(sleep) extra group ID 0.7 1 1 -1.6 1 2 -0.2 1 3 -1.2 1 4 -0.1 1 5 3.4 1 6 3.7 1 7 0.8 1 8 0 1 9 2 1 10 1.9 2 1 0.8 2 2 1.1 2 3 0.1 2 4 -0.1 2 5 4.4 2 6 5.5 2 7 1.6 2 8 4.6 2 9 3.4 2 10 Wide Version sleepWide <- sleep %>% spread(key=group, value=extra, sep=\"\") %>% mutate(g1_minus_g2 = group1 - group2) pander(sleepWide) ID group1 group2 g1_minus_g2 1 0.7 1.9 -1.2 2 -1.6 0.8 -2.4 3 -0.2 1.1 -1.3 4 -1.2 0.1 -1.3 5 -0.1 -0.1 0 6 3.4 4.4 -1 7 3.7 5.5 -1.8 8 0.8 1.6 -0.8 9 0 4.6 -4.6 10 2 3.4 -1.4 The only point of interest in this study is the difference in hours of extra sleep each individual received under the two drugs. Hence, this is a paired study as two measurements were obtained for each individual, one measurement under each condition. Formally, the null and alternative hypotheses are written as \\[ H_0: \\text{median of the differences} = 0 \\] \\[ H_a: \\text{median of the differences} \\neq 0 \\] The significance level for this study will be set at \\[ \\alpha = 0.05 \\] Analysis The dotplot below shows the differences (drug 2 extra sleep \\(-\\) drug 1 extra sleep) in extra sleep for each individual. Since 9 out of 10 differences are positive, it shows that most individuals are getting more extra sleep while using drug 2 than when using drug 1. ggplot(sleepWide, aes(x=g1_minus_g2)) + geom_dotplot(binwidth = 0.1) + theme_bw() The Wilcoxon Signed-Rank test is appropriate for any distribution of data, and especially for small sample sizes. with(sleep, wilcox.test(extra[group==1], extra[group==2], mu = 0, alternative = \"two.sided\", paired = TRUE, conf.level = 0.95) ) ## Warning in wilcox.test.default(extra[group == 1], extra[group == 2], mu = ## 0, : cannot compute exact p-value with ties ## Warning in wilcox.test.default(extra[group == 1], extra[group == 2], mu = ## 0, : cannot compute exact p-value with zeroes ## ## Wilcoxon signed rank test with continuity correction ## ## data: extra[group == 1] and extra[group == 2] ## V = 0, p-value = 0.009091 ## alternative hypothesis: true location shift is not equal to 0 Note: the Warnings above remind us of some problems that sometimes arise when using the Wilcoxon Signed-Rank Test. These problems do not occur in the Rank Sum Test. The problems are that the normal approximation to the \\(p\\)-value has to be used when there are ties present in the data and when a value of 0 is present in the data. Since two differences (see dotplot above) have the same value, there is a tie present in the data. Also, since one individual experienced the same amount of extra sleep under both scenarios, there is a zero present in the data. When the sample size gets larger, the normal approximation is pretty good. However, when the sample size is small the normal approximation is not the best. Thus, when using a normal approximation with small sample sizes, it is important to also use the “continuity correction.” Note that the output states that it is using the continuity correction, which accounts for the difficulties encountered by using the normal approximation with a small sample size. The results are still valid when this happens. It just needs to be stated in the report somewhere that the normal approximation was used, along with the continuity correction because ties (or zeros) were present in the data. There is sufficient evidence to reject the null hypothesis (\\(p = 0.009091 < \\alpha\\)). Interpretation The results of the statistical test claim that the patterns in the data stated above can be considered to apply to the general population. Drug 2 will provide more extra sleep for most individuals in the full population than will Drug 1. Most people should expect about one hour more of extra sleep while using Drug 2 than they would get while using Drug 1. Further descriptive statistics could now be performed describing how many extra hours of sleep Drug 2 should give individuals so that a marketing campaign could be put together. But that is for a different analysis. The goals of this analysis have been accomplished. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"The Benefits of Word Recall Strategies","url":"325Analyses/Wilcoxon Tests/RecallingWords.html","content":"Code Show All Code Hide All Code The Benefits of Word Recall Strategies Background Many teachers and other educators are interested in understanding how to best deliver new content to students. In general, they have two choices of how to do this. The Meshed Approach Deliver new content while simultaneously reviewing previously understood content. The Before Approach Deliver new content after fully reviewing previously understood content. A study was performed to answer the question : Does the Meshed or Before approach have any positive benefits on memory recall when it comes to remembering content? Let’s examine the experiment in more detail to determine how we can answer this question. Experiment Details People sat at computers to take a memory test, and the process would go as follows: Step 1) 40 words were shown on the screen one at a time - Each word stayed for 2 seconds Step 2) After seeing all the words, they did some simple math problems for 15 seconds - This was to stop them from remembering the words right away Step 3) They then tried to write down as many words as they could remember - They had up to 5.3 minutes to do this The process of showing words and recalling words was repeated four times with the same list of words each time (four chances to get it right). - The presentation of the first trial was the same for all treatment conditions. - EXCEPT trials 2, 3, and 4 were slightly different for each treatment condition Recall Groups They tested 30 students, split into three groups of 10 each. The groups were called SFR, Before, and Meshed: The SFR group (the control group) stands for Standard Free Recall In all four trials the same list of 40 words was presented, in a random order each time The Before group also used the same 40 words during each trial But there’s a twist: words that people remembered correctly in earlier tests were shown first. These came before the words they didn’t remember last time. After showing all the words people remembered correctly in a random order, they then showed the words people didn’t remember, also in a random order The Meshed group also used the same 40 words during each trial But there’s a twist: They mixed up the words people remembered with the ones they forgot. So, a word someone got right before would come first, then a word they missed, then another word they got right, and so on. Data Results The researchers counted how many words each student remembered correctly on their fourth try. datatable(Friendly, options=list(lengthMenu = c(3,10,30))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\"],[\"SFR\",\"SFR\",\"SFR\",\"SFR\",\"SFR\",\"SFR\",\"SFR\",\"SFR\",\"SFR\",\"SFR\",\"Before\",\"Before\",\"Before\",\"Before\",\"Before\",\"Before\",\"Before\",\"Before\",\"Before\",\"Before\",\"Meshed\",\"Meshed\",\"Meshed\",\"Meshed\",\"Meshed\",\"Meshed\",\"Meshed\",\"Meshed\",\"Meshed\",\"Meshed\"],[39,25,37,25,29,39,21,39,24,25,40,38,39,37,39,24,30,39,40,40,40,39,34,37,40,36,36,38,36,30]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>condition<\\/th>\\n <th>correct<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"lengthMenu\":[3,10,30],\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":2},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"condition\",\"targets\":1},{\"name\":\"correct\",\"targets\":2}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}There are a total of 30 participants, however each group consists of 10 people each. Therefore, there the small sample size will effect the calculations going forward. In order to test whether or not there are positive benefits that come from either recall method, we will conduct a Wilcoxon Rank Sum (Mann-Whitney) Test to test the medians of each approach’s distribution against the control group. Hypothesis In order to see if positive benefits come from either recall method, we will conduct two Wilcoxon Rank Sum Tests comparing the center distributions of each recall method to the control group (SFR). The level of significance for both Wilcoxon Tests will be the same: \\[ \\alpha = 0.05 \\] The null hypothesis and alternative hypothesis for both tests will be: \\[ H_0:\\text{the distibutions are stochastically equal} \\] \\[ H_a:\\text{one distribution is stochastically different than the other} \\] Note: When we describe our distribution as stochastically equal or different, we are just pointing out if the distributions are significantly equal or different. The Before Hypothesis The hypothesis that the Before group’s distribution will be different than the SFR group’s can be explained by the concept of information overload. According to the National Library of Medicine, a consistent feature found in information overload describes that “the amount of information is initially related to better performance or better decisions but that, above a certain amount of information, the effect changes, and the amount of information leads to worse outcomes” (Arnold et al., 2023). When individuals are presented with an excessive amount of new information at once, they may struggle to process each section effectively. The brain enters a challenging state as it attempts to retain previously learned information while simultaneously processing and storing new data. Given the time constraints for analyzing the information, participants are more likely to experience cognitive overload. This may result in a greater tendency to forget initially correct responses as the mind makes room for new information. Thus, the distribution of people’s scores could be farther apart. The Meshed Hypothesis The assumption that the Meshed group’s distribution will differ from the SFR group’s can be supported by the concept of memory updating. According to the National Library of Medicine, “successful updating of memory involves strengthening the more current memory trace, weakening the older information, and/or differentiating old and new memories, in order to ensure that the old memory is less interfering”(Ye et al., 2020). The constant switching between new and old information in the Meshed group allows the brain to adapt to this rhythm, potentially avoiding confusion that might occur when transitioning between processing new information and recalling old information. Consequently, this approach may lead to a more consistent distribution of people’s scores, with results clustered more closely together. Analysis The following graph presents a side-by-side dot plot (with box plots overlaid) of each test group. The box plots are able to help us visually compare the distribution of the control group to each recall method. The dot plot shows the differences in correct words for each individual. The very left off- white box plot is the control group, the middle light-yellow box plot is the before group, and the very right orange box plot is the meshed group. The bright red line in each box plot represents the median of each group. StanBeef <- filter(Friendly, condition == \"Before, SFR\") StanMeesh <- filter(Friendly, condition == \"Meshed, SFR\") Friendly$condition <- factor(Friendly$condition, levels=c(\"SFR\",\"Before\",\"Meshed\")) ggplot(data = Friendly, aes(x = as.factor(condition), y = correct, fill=as.factor(condition))) + geom_boxplot() + stat_summary(fun.y = median, geom = \"errorbar\", aes(ymax = ..y.., ymin = ..y..), width = .75, linetype = \"solid\", color=\"orangered1\") + theme_light() + theme(panel.grid.major=element_blank()) + scale_fill_manual(values = c(\"seashell1\", \"lightgoldenrod1\",\"darkgoldenrod1\")) + geom_jitter(width=0.1, height=0) + labs(title = \"The Effects of Memory Techniques\", x=\"Recall Methods\", fill=\"condition\", y=\"Number of words correctly recalled\") Based on the graph, we are able to see that both recall methods produce positive benefits when comparing each method to the SFR control group. The medians of the Before and Meshed groups are notably higher, yet while each graph looks significantly different in size, the distribution of data looks slightly similar. Also, we are able to see that there were a number of people who had shared the same scores within each recall method. Additionally, none of the box plots are normally distributed. We can tell this based off the placement of the median. If the median is more to the bottom of the box plot (such as the SFR and Meshed groups), it will be right skewed. If the median is more to the top of the box plot (such as the Before group), it will be left skewed. The dot plot also shows us that the scores of people in the SFR and the Before group have more variety as they are depicted spread apart, while the scores of people in the Meshed group are seen closer together. However, we cannot base off our assumptions on the graph alone. To support the graphs findings, the numerical summaries of each group are depicted below. Friendly %>% group_by(condition) %>% summarise(Min = min(correct), Median = median(correct), Mean = mean(correct), Max = max(correct), SD = sd(correct), `Sample Size` = n()) %>% pander(caption=\"Summary of Recall Methods\") Summary of Recall Methods condition Min Median Mean Max SD Sample Size SFR 21 27 30.3 39 7.334 10 Before 24 39 36.6 40 5.337 10 Meshed 30 36.5 36.6 40 3.026 10 To further validate the findings from our graphical and numerical summaries, we will conduct a Wilcoxon Rank Sum Test. While each summary reveals distinct differences in the data, several factors must be considered to ensure the validity of these findings. One challenge we face is the presence of ties—instances where two or more individuals have the same score—which will affect the calculation of the p-value. Additionally, each group has a small sample size. To address these issues, we’ll approximate the p-value and apply a continuity correction when conducting our tests. The following shows the two tests for each recall group: Wilcoxon Rank Sum Test of the Before Group Test statistic P value Alternative hypothesis 76.5 0.04555 * two.sided Wilcoxon Rank Sum Test of the Meshed Group Test statistic P value Alternative hypothesis 72 0.1015 two.sided Comparing the p-value to the level of significance for each test: \\[ \\text{Before Group p-value} = 0.04555 < \\alpha\\] There is sufficient evidence to reject the null hypothesis. \\[ \\text{Meshed Group p-value} = 0.1015 > \\alpha\\] There is insufficient evidence to reject the null hypothesis. Interpretation Based on our findings from each Wilcoxon Test, we can conclude two things: Between the Control Group (SFR) and the Before Group, one distribution is stochastically different than the other The side-by-side box plot and numerical summary help us visualize the distribution of each group. The distributions in the box plots differ, with the Control Group being right-skewed (median line closer to the bottom) and the Before Group being left-skewed (median line closer to the top). There’s also a significant difference of 12 words between the medians. Therefore, there is sufficient evidence suggesting positive benefits from using the Before Approach. Between the Control Group (SFR) and the Meshed Group, the distributions are stochastically equal Although the medians differ by 9.5 words, both groups’ distributions are right-skewed (with median lines near the bottom). This similarity suggests that the difference in medians may be due to simple random sampling rather than a meaningful effect. Consequently, there is insufficient evidence to support that there are positive benefits that come from the Meshed Approach. After analyzing all factors, the Before Approach shows more significant improvements in memory recall compared to the Control group than the Meshed Approach. Sources: Friendly Data set Friendly, M. and Franklin, P. (1980) Interactive presentation in multitrial free recall. Memory and Cognition 8 265–270 [Personal communication from M. Friendly]. National Library of Medicine Website Arnold, M., Goldschmitt, M., & Rigotti, T. (2023, June 21). Dealing with information overload: A comprehensive review. Frontiers in psychology. https://pmc.ncbi.nlm.nih.gov/articles/PMC10322198/ Ye, Z., Shi, L., Li, A., Chen, C., & Xue, G. (2020, May 18). Retrieval practice facilitates memory updating by enhancing and differentiating medial prefrontal cortex representations. eLife. https://pmc.ncbi.nlm.nih.gov/articles/PMC7272192/ Inspiration from Wilcoxon Rank Sum Test Examples Bug Spray Example Moral Integration of American Cities Chat GPT for answering my questions and error codes // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Experimental Design Notes","url":"326PaigesNotes.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Experimental Design Notes (MATH 326) function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } function openTab(evt, tabName) { var i, tabcontent, tablinks; tabcontent = document.getElementsByClassName(\"tabcontent\"); for (i = 0; i < tabcontent.length; i++) { tabcontent[i].style.display = \"none\"; } tablinks = document.getElementsByClassName(\"tablinks\"); for (i = 0; i < tablinks.length; i++) { tablinks[i].className = tablinks[i].className.replace(\" active\", \"\"); } document.getElementById(tabName).style.display = \"block\"; evt.currentTarget.className += \" active\"; } Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts Don’t forget to always load your libraries and to Knit often!! library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) This is the templet for the Hover Box Thingy Hover over words box! [insert description here]       …  Click to View Output. There are three parts to the help box: The starter code < a href=“javascript:showhide(‘[Insert classifying name]’)”> < div class=“hoverchunk” > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions! < span class=“tooltipr” > [What shows up in the box] < span class=“tooltiprtext” > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box! & nbsp;& nbsp;& nbsp;& nbsp; < span class=“tooltiprtext” >Press Enter to run the code. < /span> & nbsp;…& nbsp; < span class=“tooltiprtext” >Click to View Output.< /span > < /span > < /div > < /a > < div id=“[The classifying name that you put in the starter code!]” style=“display:none;” > *When using, make sure you fix all the spaces when using and use “< /div >” to show where the drop down is meant to end! Personal Notes OoOoooOOOoo let’s go assignments!! Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester. As well as some personal notes on those assignments! Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys! Skill Quiz - Introduction to R // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); $(document).ready(function () { // temporarily add toc-ignore selector to headers for the consistency with Pandoc $('.unlisted.unnumbered').addClass('toc-ignore') // move toc-ignore selectors from section div to header $('div.section.toc-ignore') .removeClass('toc-ignore') .children('h1,h2,h3,h4,h5').addClass('toc-ignore'); // establish options var options = { selectors: \"h1,h2,h3,h4,h5\", theme: \"bootstrap3\", context: '.toc-content', hashGenerator: function (text) { return text.replace(/[.\\\\/?&!#<>]/g, '').replace(/\\s/g, '_'); }, ignoreSelector: \".toc-ignore\", scrollTo: 0 }; options.showAndHide = true; options.smoothScroll = true; // tocify var toc = $(\"#TOC\").tocify(options).data(\"toc-tocify\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"425 Midterm Decision","url":"425Analyses/425MidtermDecision.html","content":"Code Show All Code Hide All Code 425 Midterm Decision 2025-02-25 library(GSODR) library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(sf) library(ggspatial) library(leaflet.extras) thefallen <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/Math425PastGrades.csv\") Midterm Decision Drop the Midterm score. Data thefallen <- thefallen %>% select(MagicTwoGroups, Midterm, FinalExam) datatable(thefallen, options = list(pageLength = 5)) %>% formatStyle(columns = names(thefallen),color= \"white\") %>% htmltools::tagList(tags$style(HTML(\"thead th {color: red !important; font-weight: bold !important; }\"))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\"],[2,2,2,1,2,2,2,2,1,1,2,2,2,1,2,2,2,2,2,2,2,2,1,null,2,2,1,2,2,1,2,2,2,2,2,1,2,1,2,2,2,2,2,2,1,2,1,1,1,2,2,1,2,2,2,2,1,2,2,1,1,1,2,2,2,1,2,2,2,1,2,1,1,2,1,1,2,1,2,1,1,1,2,2,1,2,1,2,2,1,2,1,1,1,2,2,2,1,1,1,2,1,1,2,1,1,2,1,1,1,2,2,1,1,1,2,2,1,2,1,2,1,1,1,2,1,1,1,2,2,1,1,1,1,2,1,1,1,2,2,1,1,2,2,1,1,2,2,2,1,1,1,2],[88,92,88,76,84,32,56,60,88,72,84,48,32,36,68,72,72,60,96,84,56,88,80,80,76,68,88,88,80,80,64,68,68,92,56,84,84,92,56,50,68,84,60,76,80,76,72,80,64,84,84,72,76,72,84,72,88,44,76,64,68,64,60,92,76,48,72,100,88,88,84,56,92,84,92,52,92,60,84,56,68,72,96,84,88,52,64,81,100,76,76,60,84,92,92,92,96,92,72,94,84,52,60,72,100,60,84,56,72,52,84,88,80,64,56,84,76,null,88,87,92,32,null,32,72,32,60,44,90,84,52,64,60,76,84,60,80,48,80,91,68,56,64,88,64,28,90,72,64,88,66,72,96],[80,88,96,44,72,36,60,48,60,48,76,44,56,16,56,72,72,60,88,72,48,88,36,null,64,68,48,80,84,52,56,56,64,92,68,52,76,64,60,60,52,76,64,76,64,72,40,28,28,84,88,44,68,64,100,68,48,52,72,80,80,56,52,96,88,60,92,92,92,48,76,72,88,76,92,0,76,56,88,32,0,80,80,68,60,92,68,68,88,76,64,68,0,76,84,100,80,72,72,94,78,40,68,88,32,43.73,88,36,52,52,84,100,64,52,40,88,92,56,84,52,80,0,68,0,72,24,72,0,80,68,0,0,0,76,96,68,64,76,64,92,68,64,92,92,0,0,72,60,64,84,80,80,96]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>MagicTwoGroups<\\/th>\\n <th>Midterm<\\/th>\\n <th>FinalExam<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":5,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"MagicTwoGroups\",\"targets\":1},{\"name\":\"Midterm\",\"targets\":2},{\"name\":\"FinalExam\",\"targets\":3}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100],\"rowCallback\":\"function(row, data, displayNum, displayIndex, dataIndex) {\\nvar value=data[1]; $(this.api().cell(row, 1).node()).css({'color':'white'});\\nvar value=data[2]; $(this.api().cell(row, 2).node()).css({'color':'white'});\\nvar value=data[3]; $(this.api().cell(row, 3).node()).css({'color':'white'});\\n}\"}},\"evals\":[\"options.rowCallback\"],\"jsHooks\":[]}thead th {color: red !important; font-weight: bold !important; } Two Lines Model thewarriorsbeforeme <- lm(FinalExam ~ Midterm + as.factor(MagicTwoGroups) + Midterm:as.factor(MagicTwoGroups), data = thefallen) summary(thewarriorsbeforeme) %>% pander() Table continues below   Estimate Std. Error t value (Intercept) -3.692 9.125 -0.4046 Midterm 0.7731 0.1297 5.961 as.factor(MagicTwoGroups)2 23.44 13.95 1.681 Midterm:as.factor(MagicTwoGroups)2 -0.05078 0.1868 -0.2718   Pr(>|t|) (Intercept) 0.6863 Midterm 1.804e-08 as.factor(MagicTwoGroups)2 0.09493 Midterm:as.factor(MagicTwoGroups)2 0.7862 Fitting linear model: FinalExam ~ Midterm + as.factor(MagicTwoGroups) + Midterm:as.factor(MagicTwoGroups) Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 150 17.98 0.496 0.4857 \\[\\underbrace{Y_i}_\\text{Final Exam} = \\overbrace{\\beta_0}^\\text{y - int} + \\overbrace{\\beta_1}^\\text{slope}\\underbrace{X_{1i}}_\\text{Midterm} + \\overbrace{\\beta_2}^\\text{change in y-int}\\underbrace{X_{2i}}_\\text{MagicTwoGroups} + \\overbrace{\\beta_3}^\\text{change in slope}\\underbrace{X_{1i}X_{2i}}_\\text{Midterm:MagicTwoGroups} + \\epsilon_i\\] \\[Y_i = 9.125 + 0.1297X_i + 13.95X_{2i} -0.05078X_{1i}X_{2i} + \\epsilon_i\\] Graph Hot pink point is my midterm score (yikes i know) prediction <- data.frame( Midterm=24, FinalExam= 37, label = \"Prediction Final Exam Score (Group 2): 37\" ) predi.p <- predict(thewarriorsbeforeme, data.frame(Midterm=24, MagicTwoGroups = \"2\"), interval= \"prediction\") b <- coef(thewarriorsbeforeme) thefallen$MagicTwoGroups <- as.factor(thefallen$MagicTwoGroups) warriors.plot <- ggplot(thefallen, aes(y = FinalExam, x = Midterm, color = MagicTwoGroups)) + geom_point(aes( text = paste( \"Group:\", MagicTwoGroups, \"<br>\", \"Midterm Score:\", Midterm, \"<br>\", \"Final Exam Score:\", FinalExam ) ), size = 1) + stat_function(fun = function(x) b[1] + b[2]*x, color = \"red\") + stat_function(fun = function(x) (b[1] + b[3]) + (b[2] + b[4])*x, color = \"darkred\") + scale_color_manual(name = \"MagicTwoGroups\", values = c(\"red\", \"darkred\")) + labs(title = \"Test Performances of MATH 425 BYU-Idaho Students\", x = \"Midterm Score\", y=\"Final Exam Score\" ) + geom_segment(aes(x=24, xend=24, y=predi.p[2], yend=predi.p[3]), alpha = 0.1, color= \"pink\", lwd = 3) + geom_point(data=prediction, aes(x=Midterm, y=FinalExam), size = 3, color= \"hotpink\")+ theme_classic() ggplotly(warriors.plot, tooltip=\"text\") {\"x\":{\"data\":[{\"x\":[76,88,72,36,80,88,80,84,92,80,72,80,64,72,88,64,68,64,48,88,56,92,92,52,60,56,68,72,88,64,76,60,84,92,92,72,94,52,60,100,60,56,72,52,80,64,56,null,87,32,null,32,32,60,44,52,64,60,76,60,80,48,68,56,64,28,88,66,72],\"y\":[44,60,48,16,36,48,52,52,64,64,40,28,28,44,48,80,80,56,60,48,72,88,92,0,56,32,0,80,60,68,76,68,0,76,72,72,94,40,68,32,43.729999999999997,36,52,52,64,52,40,56,52,0,68,0,24,72,0,0,0,0,76,68,64,76,68,64,0,0,84,80,80],\"text\":[\"Group: 1 <br> Midterm Score: 76 <br> Final Exam Score: 44\",\"Group: 1 <br> Midterm Score: 88 <br> Final Exam Score: 60\",\"Group: 1 <br> Midterm Score: 72 <br> Final Exam Score: 48\",\"Group: 1 <br> Midterm Score: 36 <br> Final Exam Score: 16\",\"Group: 1 <br> Midterm Score: 80 <br> Final Exam Score: 36\",\"Group: 1 <br> Midterm Score: 88 <br> Final Exam Score: 48\",\"Group: 1 <br> Midterm Score: 80 <br> Final Exam Score: 52\",\"Group: 1 <br> Midterm Score: 84 <br> Final Exam Score: 52\",\"Group: 1 <br> Midterm Score: 92 <br> Final Exam Score: 64\",\"Group: 1 <br> Midterm Score: 80 <br> Final Exam Score: 64\",\"Group: 1 <br> Midterm Score: 72 <br> Final Exam Score: 40\",\"Group: 1 <br> Midterm Score: 80 <br> Final Exam Score: 28\",\"Group: 1 <br> Midterm Score: 64 <br> Final Exam Score: 28\",\"Group: 1 <br> Midterm Score: 72 <br> Final Exam Score: 44\",\"Group: 1 <br> Midterm Score: 88 <br> Final Exam Score: 48\",\"Group: 1 <br> Midterm Score: 64 <br> Final Exam Score: 80\",\"Group: 1 <br> Midterm Score: 68 <br> Final Exam Score: 80\",\"Group: 1 <br> Midterm Score: 64 <br> Final Exam Score: 56\",\"Group: 1 <br> Midterm Score: 48 <br> Final Exam Score: 60\",\"Group: 1 <br> Midterm Score: 88 <br> Final Exam Score: 48\",\"Group: 1 <br> Midterm Score: 56 <br> Final Exam Score: 72\",\"Group: 1 <br> Midterm Score: 92 <br> Final Exam Score: 88\",\"Group: 1 <br> Midterm Score: 92 <br> Final Exam Score: 92\",\"Group: 1 <br> Midterm Score: 52 <br> Final Exam Score: 0\",\"Group: 1 <br> Midterm Score: 60 <br> Final Exam Score: 56\",\"Group: 1 <br> Midterm Score: 56 <br> Final Exam Score: 32\",\"Group: 1 <br> Midterm Score: 68 <br> Final Exam Score: 0\",\"Group: 1 <br> Midterm Score: 72 <br> Final Exam Score: 80\",\"Group: 1 <br> Midterm Score: 88 <br> Final Exam Score: 60\",\"Group: 1 <br> Midterm Score: 64 <br> Final Exam Score: 68\",\"Group: 1 <br> Midterm Score: 76 <br> Final Exam Score: 76\",\"Group: 1 <br> Midterm Score: 60 <br> Final Exam Score: 68\",\"Group: 1 <br> Midterm Score: 84 <br> Final Exam Score: 0\",\"Group: 1 <br> Midterm Score: 92 <br> Final Exam Score: 76\",\"Group: 1 <br> Midterm Score: 92 <br> Final Exam Score: 72\",\"Group: 1 <br> Midterm Score: 72 <br> Final Exam Score: 72\",\"Group: 1 <br> Midterm Score: 94 <br> Final Exam Score: 94\",\"Group: 1 <br> Midterm Score: 52 <br> Final Exam Score: 40\",\"Group: 1 <br> Midterm Score: 60 <br> Final Exam Score: 68\",\"Group: 1 <br> Midterm Score: 100 <br> Final Exam Score: 32\",\"Group: 1 <br> Midterm Score: 60 <br> Final Exam Score: 43.73\",\"Group: 1 <br> Midterm Score: 56 <br> Final Exam Score: 36\",\"Group: 1 <br> Midterm Score: 72 <br> Final Exam Score: 52\",\"Group: 1 <br> Midterm Score: 52 <br> Final Exam Score: 52\",\"Group: 1 <br> Midterm Score: 80 <br> Final Exam Score: 64\",\"Group: 1 <br> Midterm Score: 64 <br> Final Exam Score: 52\",\"Group: 1 <br> Midterm Score: 56 <br> Final Exam Score: 40\",\"Group: 1 <br> Midterm Score: NA <br> Final Exam Score: 56\",\"Group: 1 <br> Midterm Score: 87 <br> Final Exam Score: 52\",\"Group: 1 <br> Midterm Score: 32 <br> Final Exam Score: 0\",\"Group: 1 <br> Midterm Score: NA <br> Final Exam Score: 68\",\"Group: 1 <br> Midterm Score: 32 <br> Final Exam Score: 0\",\"Group: 1 <br> Midterm Score: 32 <br> Final Exam Score: 24\",\"Group: 1 <br> Midterm Score: 60 <br> Final Exam Score: 72\",\"Group: 1 <br> Midterm Score: 44 <br> Final Exam Score: 0\",\"Group: 1 <br> Midterm Score: 52 <br> Final Exam Score: 0\",\"Group: 1 <br> Midterm Score: 64 <br> Final Exam Score: 0\",\"Group: 1 <br> Midterm Score: 60 <br> Final Exam Score: 0\",\"Group: 1 <br> Midterm Score: 76 <br> Final Exam Score: 76\",\"Group: 1 <br> Midterm Score: 60 <br> Final Exam Score: 68\",\"Group: 1 <br> Midterm Score: 80 <br> Final Exam Score: 64\",\"Group: 1 <br> Midterm Score: 48 <br> Final Exam Score: 76\",\"Group: 1 <br> Midterm Score: 68 <br> Final Exam Score: 68\",\"Group: 1 <br> Midterm Score: 56 <br> Final Exam Score: 64\",\"Group: 1 <br> Midterm Score: 64 <br> Final Exam Score: 0\",\"Group: 1 <br> Midterm Score: 28 <br> Final Exam Score: 0\",\"Group: 1 <br> Midterm Score: 88 <br> Final Exam Score: 84\",\"Group: 1 <br> Midterm Score: 66 <br> Final Exam Score: 80\",\"Group: 1 <br> Midterm Score: 72 <br> Final Exam Score: 80\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(255,0,0,1)\",\"opacity\":1,\"size\":3.7795275590551185,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(255,0,0,1)\"}},\"hoveron\":\"points\",\"name\":\"1\",\"legendgroup\":\"1\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[88,92,88,84,32,56,60,84,48,32,68,72,72,60,96,84,56,88,76,68,88,80,64,68,68,92,56,84,56,50,68,84,60,76,76,84,84,76,72,84,72,44,76,60,92,76,72,100,88,84,84,92,84,96,84,52,81,100,76,92,92,96,84,72,84,84,88,84,76,88,92,72,90,84,84,80,91,64,88,90,72,64,96],\"y\":[80,88,96,72,36,60,48,76,44,56,56,72,72,60,88,72,48,88,64,68,80,84,56,56,64,92,68,76,60,60,52,76,64,76,72,84,88,68,64,100,68,52,72,52,96,88,92,92,92,76,76,76,88,80,68,92,68,88,64,84,100,80,78,88,88,84,100,88,92,84,80,72,80,68,96,64,92,92,92,72,60,64,96],\"text\":[\"Group: 2 <br> Midterm Score: 88 <br> Final Exam Score: 80\",\"Group: 2 <br> Midterm Score: 92 <br> Final Exam Score: 88\",\"Group: 2 <br> Midterm Score: 88 <br> Final Exam Score: 96\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 72\",\"Group: 2 <br> Midterm Score: 32 <br> Final Exam Score: 36\",\"Group: 2 <br> Midterm Score: 56 <br> Final Exam Score: 60\",\"Group: 2 <br> Midterm Score: 60 <br> Final Exam Score: 48\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 76\",\"Group: 2 <br> Midterm Score: 48 <br> Final Exam Score: 44\",\"Group: 2 <br> Midterm Score: 32 <br> Final Exam Score: 56\",\"Group: 2 <br> Midterm Score: 68 <br> Final Exam Score: 56\",\"Group: 2 <br> Midterm Score: 72 <br> Final Exam Score: 72\",\"Group: 2 <br> Midterm Score: 72 <br> Final Exam Score: 72\",\"Group: 2 <br> Midterm Score: 60 <br> Final Exam Score: 60\",\"Group: 2 <br> Midterm Score: 96 <br> Final Exam Score: 88\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 72\",\"Group: 2 <br> Midterm Score: 56 <br> Final Exam Score: 48\",\"Group: 2 <br> Midterm Score: 88 <br> Final Exam Score: 88\",\"Group: 2 <br> Midterm Score: 76 <br> Final Exam Score: 64\",\"Group: 2 <br> Midterm Score: 68 <br> Final Exam Score: 68\",\"Group: 2 <br> Midterm Score: 88 <br> Final Exam Score: 80\",\"Group: 2 <br> Midterm Score: 80 <br> Final Exam Score: 84\",\"Group: 2 <br> Midterm Score: 64 <br> Final Exam Score: 56\",\"Group: 2 <br> Midterm Score: 68 <br> Final Exam Score: 56\",\"Group: 2 <br> Midterm Score: 68 <br> Final Exam Score: 64\",\"Group: 2 <br> Midterm Score: 92 <br> Final Exam Score: 92\",\"Group: 2 <br> Midterm Score: 56 <br> Final Exam Score: 68\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 76\",\"Group: 2 <br> Midterm Score: 56 <br> Final Exam Score: 60\",\"Group: 2 <br> Midterm Score: 50 <br> Final Exam Score: 60\",\"Group: 2 <br> Midterm Score: 68 <br> Final Exam Score: 52\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 76\",\"Group: 2 <br> Midterm Score: 60 <br> Final Exam Score: 64\",\"Group: 2 <br> Midterm Score: 76 <br> Final Exam Score: 76\",\"Group: 2 <br> Midterm Score: 76 <br> Final Exam Score: 72\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 84\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 88\",\"Group: 2 <br> Midterm Score: 76 <br> Final Exam Score: 68\",\"Group: 2 <br> Midterm Score: 72 <br> Final Exam Score: 64\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 100\",\"Group: 2 <br> Midterm Score: 72 <br> Final Exam Score: 68\",\"Group: 2 <br> Midterm Score: 44 <br> Final Exam Score: 52\",\"Group: 2 <br> Midterm Score: 76 <br> Final Exam Score: 72\",\"Group: 2 <br> Midterm Score: 60 <br> Final Exam Score: 52\",\"Group: 2 <br> Midterm Score: 92 <br> Final Exam Score: 96\",\"Group: 2 <br> Midterm Score: 76 <br> Final Exam Score: 88\",\"Group: 2 <br> Midterm Score: 72 <br> Final Exam Score: 92\",\"Group: 2 <br> Midterm Score: 100 <br> Final Exam Score: 92\",\"Group: 2 <br> Midterm Score: 88 <br> Final Exam Score: 92\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 76\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 76\",\"Group: 2 <br> Midterm Score: 92 <br> Final Exam Score: 76\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 88\",\"Group: 2 <br> Midterm Score: 96 <br> Final Exam Score: 80\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 68\",\"Group: 2 <br> Midterm Score: 52 <br> Final Exam Score: 92\",\"Group: 2 <br> Midterm Score: 81 <br> Final Exam Score: 68\",\"Group: 2 <br> Midterm Score: 100 <br> Final Exam Score: 88\",\"Group: 2 <br> Midterm Score: 76 <br> Final Exam Score: 64\",\"Group: 2 <br> Midterm Score: 92 <br> Final Exam Score: 84\",\"Group: 2 <br> Midterm Score: 92 <br> Final Exam Score: 100\",\"Group: 2 <br> Midterm Score: 96 <br> Final Exam Score: 80\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 78\",\"Group: 2 <br> Midterm Score: 72 <br> Final Exam Score: 88\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 88\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 84\",\"Group: 2 <br> Midterm Score: 88 <br> Final Exam Score: 100\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 88\",\"Group: 2 <br> Midterm Score: 76 <br> Final Exam Score: 92\",\"Group: 2 <br> Midterm Score: 88 <br> Final Exam Score: 84\",\"Group: 2 <br> Midterm Score: 92 <br> Final Exam Score: 80\",\"Group: 2 <br> Midterm Score: 72 <br> Final Exam Score: 72\",\"Group: 2 <br> Midterm Score: 90 <br> Final Exam Score: 80\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 68\",\"Group: 2 <br> Midterm Score: 84 <br> Final Exam Score: 96\",\"Group: 2 <br> Midterm Score: 80 <br> Final Exam Score: 64\",\"Group: 2 <br> Midterm Score: 91 <br> Final Exam Score: 92\",\"Group: 2 <br> Midterm Score: 64 <br> Final Exam Score: 92\",\"Group: 2 <br> Midterm Score: 88 <br> Final Exam Score: 92\",\"Group: 2 <br> Midterm Score: 90 <br> Final Exam Score: 72\",\"Group: 2 <br> Midterm Score: 72 <br> Final Exam Score: 60\",\"Group: 2 <br> Midterm Score: 64 <br> Final Exam Score: 64\",\"Group: 2 <br> Midterm Score: 96 <br> Final Exam Score: 96\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(139,0,0,1)\",\"opacity\":1,\"size\":3.7795275590551185,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(139,0,0,1)\"}},\"hoveron\":\"points\",\"name\":\"2\",\"legendgroup\":\"2\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[80],\"y\":[null],\"text\":\"Group: NA <br> Midterm Score: 80 <br> Final Exam Score: NA\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(127,127,127,1)\",\"opacity\":1,\"size\":3.7795275590551185,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(127,127,127,1)\"}},\"hoveron\":\"points\",\"name\":\"NA\",\"legendgroup\":\"NA\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[24,24.760000000000002,25.52,26.280000000000001,27.039999999999999,27.800000000000001,28.560000000000002,29.32,30.079999999999998,30.84,31.600000000000001,32.359999999999999,33.120000000000005,33.880000000000003,34.640000000000001,35.399999999999999,36.159999999999997,36.920000000000002,37.68,38.439999999999998,39.200000000000003,39.960000000000001,40.719999999999999,41.480000000000004,42.240000000000002,43,43.760000000000005,44.519999999999996,45.280000000000001,46.039999999999999,46.799999999999997,47.560000000000002,48.32,49.079999999999998,49.840000000000003,50.600000000000001,51.359999999999999,52.120000000000005,52.879999999999995,53.640000000000001,54.399999999999999,55.159999999999997,55.920000000000002,56.68,57.439999999999998,58.200000000000003,58.960000000000001,59.719999999999999,60.480000000000004,61.240000000000002,62,62.759999999999998,63.520000000000003,64.280000000000001,65.039999999999992,65.799999999999997,66.560000000000002,67.319999999999993,68.079999999999998,68.840000000000003,69.599999999999994,70.359999999999999,71.120000000000005,71.879999999999995,72.640000000000001,73.400000000000006,74.159999999999997,74.920000000000002,75.680000000000007,76.439999999999998,77.200000000000003,77.960000000000008,78.719999999999999,79.480000000000004,80.240000000000009,81,81.759999999999991,82.52000000000001,83.280000000000001,84.039999999999992,84.799999999999997,85.560000000000002,86.319999999999993,87.079999999999998,87.840000000000003,88.599999999999994,89.359999999999999,90.120000000000005,90.879999999999995,91.640000000000001,92.400000000000006,93.159999999999997,93.920000000000002,94.680000000000007,95.439999999999998,96.200000000000003,96.960000000000008,97.719999999999999,98.480000000000004,99.239999999999995,100],\"y\":[14.863071750760099,15.450657590480791,16.038243430201483,16.625829269922175,17.213415109642867,17.801000949363559,18.38858678908425,18.976172628804942,19.563758468525631,20.151344308246323,20.738930147967018,21.326515987687706,21.914101827408402,22.50168766712909,23.089273506849782,23.67685934657047,24.264445186291162,24.852031026011858,25.439616865732546,26.027202705453238,26.614788545173933,27.202374384894622,27.789960224615314,28.377546064336006,28.965131904056701,29.552717743777389,30.140303583498085,30.727889423218766,31.315475262939461,31.903061102660157,32.490646942380842,33.078232782101537,33.665818621822226,34.253404461542914,34.8409903012636"},{"title":" Acura TL Selling Price ","url":"425Analyses/CarSelling.html","content":"Code Show All Code Hide All Code Acura TL Selling Price library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) acuraprices <- read_csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/acuraprices.csv\") Overview Our Simple Linear Regression analysis and accompanying graphics revealed that there is a correlation between car price and mileage that we can take advantage of to predict how much we can sell my current vehicle based on mileage. Our model indicated that for every 1 unit increase in mileage, a Acura TL’s selling price would decrease by \\(5.359 e^{-6}\\) units as well. In other words, lower price correlated with higher mileage and vice versa. For my car specifically, the slope ended up coming out to -0.058, showing a greater depreciation in price as my car’s mileage increases. Therefore, the best time for me to sell my car for the best price according to the transformed data is at 10000 for around $10536.84. Focusing on the slope, we obtained a p-value of 0.001082, indicating a statistically significant relationship between the two variables. However, it’s crucial to note that since the data set violated one of the Simple Linear Regression assumptions, these interpretations and recommendations should be viewed with caution. While these findings may not be conclusive, they head us in the right direction of when and how much to sell for my current vehicle. To see the background, click the section below Hide Background Show Background The car that I own is an Black 2014 Acura TL. I technically don’t own it since it is my husband’s car, but I drive it all the same. In this study, *we will be demonstrating my vehicle’s “purchase cost per mile” by comparing the price my husband paid to the price and mileage for when he plans to sell as well as when would be the best time to sell to get the best dollar-per-mile value with regards to the purchasing costs of our vehicle. When collecting the data, I chose the listings of other Acura TL’s from three different car selling websites ranging from different mileages and prices. The only thing that I could think about that could possibly effect the pricing of the cars I’ve chosen is that the car that we bought was from Phoenix, Arizona and the cars listed all come from with a 20 mile radius of Rexburg, Idaho. But otherwise, there should be no other tampering with the data. In the table, I list the year of the car, the mileage, and the Price of said car. Each row listed is another Acura TL. datatable(acuraprices, options = list(pageLength = 5)) %>% formatStyle(columns = names(acuraprices),color= \"white\") %>% htmltools::tagList(tags$style(HTML(\"thead th {color: mediumseagreen !important; font-weight: bold !important; }\"))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\"],[2011,2014,2010,2012,2008,2012,2014,2013,2005,2014,2009,2009,2013,2013,2012,2011,2014,2008,2000,2004,2006,2007,2012,2000,2013],[25457,39165,39189,62253,71622,69048,78326,53748,41409,58515,54008,62263,116170,180000,143332,185479,115926,187131,221333,59364,102792,153834,109702,25244,15737],[15999,19496,16977,13880,1700,15999,16998,17863,16799,18995,12988,13432,12580,8500,9997,7750,11495,5995,4495,11999,6000,9990,8200,12977,18995]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>Year<\\/th>\\n <th>Mileage<\\/th>\\n <th>Price<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":5,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Year\",\"targets\":1},{\"name\":\"Mileage\",\"targets\":2},{\"name\":\"Price\",\"targets\":3}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100],\"rowCallback\":\"function(row, data, displayNum, displayIndex, dataIndex) {\\nvar value=data[1]; $(this.api().cell(row, 1).node()).css({'color':'white'});\\nvar value=data[2]; $(this.api().cell(row, 2).node()).css({'color':'white'});\\nvar value=data[3]; $(this.api().cell(row, 3).node()).css({'color':'white'});\\n}\"}},\"evals\":[\"options.rowCallback\"],\"jsHooks\":[]}thead th {color: mediumseagreen !important; font-weight: bold !important; } Analysis The graphic below shows us the side by side of regression of the original data as well as the transformed data with the box cox suggestion. The transformed graph shows my car’s position and its predicted selling price. The slope calculation below reveals the model’s estimate of my car’s price depreciation as mileage increases. \\[ m = \\frac{14000 - 10536.84}{40000 - 100000} = -0.0577193333333 \\] Additionally, we can see in the transformation graph the confidence interval that tells us the estimate of where the average selling price for a Acura TL is and the prediction interval that estimates where an individual car’s selling price might land. Examining my car’s data point specifically, while it falls within the prediction interval, it lies outside the confidence interval. This indicates that I paid more than average for a car with this mileage. Hover over each dot to see specific scores acuralm <- lm(Price ~ Mileage, data= acuraprices) lm.kachow <- lm(log(Price) ~ Mileage, data=acuraprices) b.kachow <- coef(lm.kachow) #prediction point mycar <- data.frame(Mileage = 40000, Price = 14000, Year = 2014) mysellingprice <- round(exp(b.kachow[1] +b.kachow[2] * 100000), 2) mycar$predictionprice <- mysellingprice acuraplot <- ggplot(acuraprices, aes(x=Mileage, y= Price, color = Year)) + geom_point(size=1.5, alpha =2) + geom_smooth(method=\"lm\", formula= y~x, se=FALSE, size= 0.5, color=\"mediumseagreen\")+ labs(title=\"Acura TL Dealership Listings \\n (Original Model)\", x = \"Mileage\", y = \"Prices ($)\")+ scale_color_gradient(low = \"aquamarine4\", high=\"aquamarine1\") + theme_minimal()+ theme(legend.position = \"none\") ggplotly(acuraplot) {\"x\":{\"data\":[{\"x\":[25457,39165,39189,62253,71622,69048,78326,53748,41409,58515,54008,62263,116170,180000,143332,185479,115926,187131,221333,59364,102792,153834,109702,25244,15737],\"y\":[15999,19496,16977,13880,1700,15999,16998,17863,16799,18995,12988,13432,12580,8500,9997,7750,11495,5995,4495,11999,6000,9990,8200,12977,18995],\"text\":[\"Mileage: 25457<br />Price: 15999<br />Year: 2011\",\"Mileage: 39165<br />Price: 19496<br />Year: 2014\",\"Mileage: 39189<br />Price: 16977<br />Year: 2010\",\"Mileage: 62253<br />Price: 13880<br />Year: 2012\",\"Mileage: 71622<br />Price: 1700<br />Year: 2008\",\"Mileage: 69048<br />Price: 15999<br />Year: 2012\",\"Mileage: 78326<br />Price: 16998<br />Year: 2014\",\"Mileage: 53748<br />Price: 17863<br />Year: 2013\",\"Mileage: 41409<br />Price: 16799<br />Year: 2005\",\"Mileage: 58515<br />Price: 18995<br />Year: 2014\",\"Mileage: 54008<br />Price: 12988<br />Year: 2009\",\"Mileage: 62263<br />Price: 13432<br />Year: 2009\",\"Mileage: 116170<br />Price: 12580<br />Year: 2013\",\"Mileage: 180000<br />Price: 8500<br />Year: 2013\",\"Mileage: 143332<br />Price: 9997<br />Year: 2012\",\"Mileage: 185479<br />Price: 7750<br />Year: 2011\",\"Mileage: 115926<br />Price: 11495<br />Year: 2014\",\"Mileage: 187131<br />Price: 5995<br />Year: 2008\",\"Mileage: 221333<br />Price: 4495<br />Year: 2000\",\"Mileage: 59364<br />Price: 11999<br />Year: 2004\",\"Mileage: 102792<br />Price: 6000<br />Year: 2006\",\"Mileage: 153834<br />Price: 9990<br />Year: 2007\",\"Mileage: 109702<br />Price: 8200<br />Year: 2012\",\"Mileage: 25244<br />Price: 12977<br />Year: 2000\",\"Mileage: 15737<br />Price: 18995<br />Year: 2013\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":[\"rgba(114,229,191,1)\",\"rgba(127,255,212,1)\",\"rgba(110,221,184,1)\",\"rgba(118,238,198,1)\",\"rgba(101,204,170,1)\",\"rgba(118,238,198,1)\",\"rgba(127,255,212,1)\",\"rgba(123,246,205,1)\",\"rgba(89,179,149,1)\",\"rgba(127,255,212,1)\",\"rgba(106,212,177,1)\",\"rgba(106,212,177,1)\",\"rgba(123,246,205,1)\",\"rgba(123,246,205,1)\",\"rgba(118,238,198,1)\",\"rgba(114,229,191,1)\",\"rgba(127,255,212,1)\",\"rgba(101,204,170,1)\",\"rgba(69,139,116,1)\",\"rgba(85,171,142,1)\",\"rgba(93,187,156,1)\",\"rgba(97,195,163,1)\",\"rgba(118,238,198,1)\",\"rgba(69,139,116,1)\",\"rgba(123,246,205,1)\"],\"opacity\":2,\"size\":5.6692913385826778,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":[\"rgba(114,229,191,1)\",\"rgba(127,255,212,1)\",\"rgba(110,221,184,1)\",\"rgba(118,238,198,1)\",\"rgba(101,204,170,1)\",\"rgba(118,238,198,1)\",\"rgba(127,255,212,1)\",\"rgba(123,246,205,1)\",\"rgba(89,179,149,1)\",\"rgba(127,255,212,1)\",\"rgba(106,212,177,1)\",\"rgba(106,212,177,1)\",\"rgba(123,246,205,1)\",\"rgba(123,246,205,1)\",\"rgba(118,238,198,1)\",\"rgba(114,229,191,1)\",\"rgba(127,255,212,1)\",\"rgba(101,204,170,1)\",\"rgba(69,139,116,1)\",\"rgba(85,171,142,1)\",\"rgba(93,187,156,1)\",\"rgba(97,195,163,1)\",\"rgba(118,238,198,1)\",\"rgba(69,139,116,1)\",\"rgba(123,246,205,1)\"]}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[15737,18339.481012658227,20941.962025316454,23544.443037974685,26146.924050632912,28749.405063291139,31351.886075949369,33954.3670886076,36556.848101265823,39159.329113924046,41761.810126582277,44364.291139240508,46966.772151898738,49569.253164556962,52171.734177215192,54774.215189873416,57376.696202531646,59979.177215189877,62581.6582278481,65184.139240506331,67786.620253164554,70389.101265822785,72991.582278481015,75594.063291139231,78196.544303797476,80799.025316455693,83401.506329113923,86003.987341772154,88606.468354430384,91208.949367088615,93811.430379746831,96413.911392405062,99016.392405063292,101618.87341772152,104221.35443037975,106823.83544303797,109426.3164556962,112028.79746835443,114631.27848101266,117233.75949367089,119836.24050632911,122438.72151898734,125041.20253164557,127643.6835443038,130246.16455696203,132848.64556962025,135451.12658227846,138053.60759493671,140656.08860759495,143258.56962025317,145861.05063291139,148463.53164556963,151066.01265822785,153668.49367088609,156270.97468354431,158873.45569620252,161475.93670886077,164078.41772151898,166680.89873417723,169283.37974683545,171885.86075949366,174488.34177215191,177090.82278481012,179693.30379746837,182295.78481012658,184898.2658227848,187500.74683544305,190103.22784810126,192705.70886075951,195308.18987341772,197910.67088607594,200513.15189873418,203115.6329113924,205718.11392405065,208320.59493670886,210923.07594936708,213525.55696202532,216128.03797468354,218730.51898734178,221333],\"y\":[16945.223042678143,16787.86241788177,16630.501793085401,16473.141168289032,16315.780543492661,16158.419918696291,16001.059293899922,15843.698669103551,15686.338044307182,15528.977419510811,15371.616794714442,15214.256169918071,15056.895545121701,14899.534920325332,14742.174295528963,14584.813670732592,14427.453045936221,14270.092421139852,14112.731796343482,13955.371171547113,13798.010546750742,13640.649921954373,13483.289297158002,13325.928672361633,13168.568047565263,13011.207422768894,12853.846797972523,12696.486173176152,12539.125548379783,12381.764923583412,12224.404298787043,12067.043673990673,11909.683049194304,11752.322424397933,11594.961799601562,11437.601174805193,11280.240550008824,11122.879925212454,10965.519300416083,10808.158675619714,10650.798050823345,10493.437426026972,10336.076801230603,10178.716176434235,10021.355551637864,9863.994926841493,9706.6343020451241,9549.2736772487533,9391.9130524523825,9234.5524276560136,9077.1918028596447,8919.8311780632739,8762.470553266905,8605.1099284705324,8447.7493036741635,8290.3886788777945,8133.0280540814238,7975.6674292850548,7818.3068044886841,7660.9461796923151,7503.5855548959444,7346.2249300995736,7188.8643053032047,7031.5036805068339,6874.143055710465,6716.782430914096,6559.4218061177235,6402.0611813213545,6244.7005565249838,6087.3399317286148,5929.9793069322459,5772.6186821358751,5615.2580573395062,5457.8974325431336,5300.5368077467647,5143.1761829503957,4985.815558154025,4828.454933357656,4671.0943085612853,4513.7336837649163],\"text\":[\"Mileage: 15737.00<br />Price: 16945.223<br />Year: mediumseagreen\",\"Mileage: 18339.48<br />Price: 16787.862<br />Year: mediumseagreen\",\"Mileage: 20941.96<br />Price: 16630.502<br />Year: mediumseagreen\",\"Mileage: 23544.44<br />Price: 16473.141<br />Year: mediumseagreen\",\"Mileage: 26146.92<br />Price: 16315.781<br />Year: mediumseagreen\",\"Mileage: 28749.41<br />Price: 16158.420<br />Year: mediumseagreen\",\"Mileage: 31351.89<br />Price: 16001.059<br />Year: mediumseagreen\",\"Mileage: 33954.37<br />Price: 15843.699<br />Year: mediumseagreen\",\"Mileage: 36556.85<br />Price: 15686.338<br />Year: mediumseagreen\",\"Mileage: 39159.33<br />Price: 15528.977<br />Year: mediumseagreen\",\"Mileage: 41761.81<br />Price: 15371.617<br />Year: mediumseagreen\",\"Mileage: 44364.29<br />Price: 15214.256<br />Year: mediumseagreen\",\"Mileage: 46966.77<br />Price: 15056.896<br />Year: mediumseagreen\",\"Mileage: 49569.25<br />Price: 14899.535<br />Year: mediumseagreen\",\"Mileage: 52171.73<br />Price: 14742.174<br />Year: mediumseagreen\",\"Mileage: 54774.22<br />Price: 14584.814<br />Year: mediumseagreen\",\"Mileage: 57376.70<br />Price: 14427.453<br />Year: mediumseagreen\",\"Mileage: 59979.18<br />Price: 14270.092<br />Year: mediumseagreen\",\"Mileage: 62581.66<br />Price: 14112.732<br />Year: mediumseagreen\",\"Mileage: 65184.14<br />Price: 13955.371<br />Year: mediumseagreen\",\"Mileage: 67786.62<br />Price: 13798.011<br />Year: mediumseagreen\",\"Mileage: 70389.10<br />Price: 13640.650<br />Year: mediumseagreen\",\"Mileage: 72991.58<br />Price: 13483.289<br />Year: mediumseagreen\",\"Mileage: 75594.06<br />Price: 13325.929<br />Year: mediumseagreen\",\"Mileage: 78196.54<br />Price: 13168.568<br />Year: mediumseagreen\",\"Mileage: 80799.03<br />Price: 13011.207<br />Year: mediumseagreen\",\"Mileage: 83401.51<br />Price: 12853.847<br />Year: mediumseagreen\",\"Mileage: 86003.99<br />Price: 12696.486<br />Year: mediumseagreen\",\"Mileage: 88606.47<br />Price: 12539.126<br />Year: mediumseagreen\",\"Mileage: 91208.95<br />Price: 12381.765<br />Year: mediumseagreen\",\"Mileage: 93811.43<br />Price: 12224.404<br />Year: mediumseagreen\",\"Mileage: 96413.91<br />Price: 12067.044<br />Year: mediumseagreen\",\"Mileage: 99016.39<br />Price: 11909.683<br />Year: mediumseagreen\",\"Mileage: 101618.87<br />Price: 11752.322<br />Year: mediumseagreen\",\"Mileage: 104221.35<br />Price: 11594.962<br />Year: mediumseagreen\",\"Mileage: 106823.84<br />Price: 11437.601<br />Year: mediumseagreen\",\"Mileage: 109426.32<br />Price: 11280.241<br />Year: mediumseagreen\",\"Mileage: 112028.80<br />Price: 11122.880<br />Year: mediumseagreen\",\"Mileage: 114631.28<br />Price: 10965.519<br />Year: mediumseagreen\",\"Mileage: 117233.76<br />Price: 10808.159<br />Year: mediumseagreen\",\"Mileage: 119836.24<br />Price: 10650.798<br />Year: mediumseagreen\",\"Mileage: 122438.72<br />Price: 10493.437<br />Year: mediumseagreen\",\"Mileage: 125041.20<br />Price: 10336.077<br />Year: mediumseagreen\",\"Mileage: 127643.68<br />Price: 10178.716<br />Year: mediumseagreen\",\"Mileage: 130246.16<br />Price: 10021.356<br />Year: mediumseagreen\",\"Mileage: 132848.65<br />Price: 9863.995<br />Year: mediumseagreen\",\"Mileage: 135451.13<br />Price: 9706.634<br />Year: mediumseagreen\",\"Mileage: 138053.61<br />Price: 9549.274<br />Year: mediumseagreen\",\"Mileage: 140656.09<br />Price: 9391.913<br />Year: mediumseagreen\",\"Mileage: 143258.57<br />Price: 9234.552<br />Year: mediumseagreen\",\"Mileage: 145861.05<br />Price: 9077.192<br />Year: mediumseagreen\",\"Mileage: 148463.53<br />Price: 8919.831<br />Year: mediumseagreen\",\"Mileage: 151066.01<br />Price: 8762.471<br />Year: mediumseagreen\",\"Mileage: 153668.49<br />Price: 8605.110<br />Year: mediumseagreen\",\"Mileage: 156270.97<br />Price: 8447.749<br />Year: mediumseagreen\",\"Mileage: 158873.46<br />Price: 8290.389<br />Year: mediumseagreen\",\"Mileage: 161475.94<br />Price: 8133.028<br />Year: mediumseagreen\",\"Mileage: 164078.42<br />Price: 7975.667<br />Year: mediumseagreen\",\"Mileage: 166680.90<br />Price: 7818.307<br />Year: mediumseagreen\",\"Mileage: 169283.38<br />Price: 7660.946<br />Year: mediumseagreen\",\"Mileage: 171885.86<br />Price: 7503.586<br />Year: mediumseagreen\",\"Mileage: 174488.34<br />Price: 7346.225<br />Year: mediumseagreen\",\"Mileage: 177090.82<br />Price: 7188.864<br />Year: mediumseagreen\",\"Mileage: 179693.30<br />Price: 7031.504<br />Year: mediumseagreen\",\"Mileage: 182295.78<br />Price: 6874.143<br />Year: mediumseagreen\",\"Mileage: 184898.27<br />Price: 6716.782<br />Year: mediumseagreen\",\"Mileage: 187500.75<br />Price: 6559.422<br />Year: mediumseagreen\",\"Mileage: 190103.23<br />Price: 6402.061<br />Year: mediumseagreen\",\"Mileage: 192705.71<br />Price: 6244.701<br />Year: mediumseagreen\",\"Mileage: 195308.19<br />Price: 6087.340<br />Year: mediumseagreen\",\"Mileage: 197910.67<br />Price: 5929.979<br />Year: mediumseagreen\",\"Mileage: 200513.15<br />Price: 5772.619<br />Year: mediumseagreen\",\"Mileage: 203115.63<br />Price: 5615.258<br />Year: mediumseagreen\",\"Mileage: 205718.11<br />Price: 5457.897<br />Year: mediumseagreen\",\"Mileage: 208320.59<br />Price: 5300.537<br />Year: mediumseagreen\",\"Mileage: 210923.08<br />Price: 5143.176<br />Year: mediumseagreen\",\"Mileage: 213525.56<br />Price: 4985.816<br />Year: mediumseagreen\",\"Mileage: 216128.04<br />Price: 4828.455<br />Year: mediumseagreen\",\"Mileage: 218730.52<br />Price: 4671.094<br />Year: mediumseagreen\",\"Mileage: 221333.00<br />Price: 4513.734<br />Year: mediumseagreen\"],\"type\":\"scatter\",\"mode\":\"lines\",\"name\":\"fitted values\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(60,179,113,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":39.622526636225267,\"r\":7.3059360730593621,\"b\":36.042617960426185,\"l\":54.794520547945211},\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724},\"title\":{\"text\":\"Acura TL Dealership Listings <br /> (Original Model)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.534246575342465},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[5457.1999999999989,231612.79999999999],\"tickmode\":\"array\",\"ticktext\":[\"50000\",\"100000\",\"150000\",\"200000\"],\"tickvals\":[50000,100000,150000,200000],\"categoryorder\":\"array\",\"categoryarray\":[\"50000\",\"100000\",\"150000\",\"200000\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.6529680365296811,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.68949771689498},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176002,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"Mileage\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[810.19999999999993,20385.799999999999],\"tickmode\":\"array\",\"ticktext\":[\"5000\",\"10000\",\"15000\",\"20000\"],\"tickvals\":[5000,10000,15000.000000000002,20000],\"categoryorder\":\"array\",\"categoryarray\":[\"5000\",\"10000\",\"15000\",\"20000\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.6529680365296811,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.68949771689498},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176002,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"Prices ($)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":null,\"bordercolor\":null,"},{"title":"Final Exam Review","url":"425Analyses/FinalExamReview.html","content":"Code Show All Code Hide All Code Final Exam Review library(mosaic) library(mosaicData) library(tidyverse) library(car) library(ggplot2) library(plotly) library(datasets) library(DT) library(knitr) library(pander) library(alr4) library(dplyr) library(MASS) Midterm Questions Question 1 Q: Perform a regression using the airquality data set in R where the response variable is Ozone and the explanatory variable is Wind. What is the value of the MSE of this regression rounded to the nearest whole number? A: 701 Squaring the residual standard error of 26.47 gives 700.6609 for the MSE. Or, computing the MSE directly gives sum(air.lm$res^2)/114 In either case the result is MSE = 701 air.lm <- lm(Ozone ~ Wind, data=airquality) summary(air.lm) ## ## Call: ## lm(formula = Ozone ~ Wind, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -51.572 -18.854 -4.868 15.234 90.000 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 96.8729 7.2387 13.38 < 2e-16 *** ## Wind -5.5509 0.6904 -8.04 9.27e-13 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 26.47 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.3619, Adjusted R-squared: 0.3563 ## F-statistic: 64.64 on 1 and 114 DF, p-value: 9.272e-13 sum(air.lm$res^2)/114 ## [1] 700.5177 Question 2 Q: Use the RailTrail data set in R from library(mosaic) to fit a regression to the following scatterplot. Compute the value of the residual for the solid dot labeled in the plot. A: 0.17157 Run these codes: View(RailTrail) locate row 25 notice a hightemp = 61 and lowtemp = 35 for observation #25 rail.lm <- lm(hightemp ~ lowtemp, data=RailTrail) rail.lm$residuals[25] gives the residual for the 25th dot directly or… predict(rail.lm, data.frame(lowtemp=c(35))) 60.82843 That gives the Y-hat (or predicted) high temperature in degrees Fahrenheit for a day with a low temperature of 35 degrees Fahrenheit. To find the residual, use: \\[r_i = Y_i - \\hat{Y_i} = 61 - 60.82... = 0.17157\\] rt.lm <- lm(hightemp ~ lowtemp, data = RailTrail) plot(hightemp ~ lowtemp, data = RailTrail) abline(rt.lm) predict(rt.lm, data.frame(lowtemp = 35)) ## 1 ## 60.82843 61 - 60.82843 ## [1] 0.17157 Question 3 Q: Each of the following equations come from student submissions of the Predicting the Weather Analysis. Most of them have mistakes in them. Which equation does not have any mistakes in it? A: \\(E{\\underbrace{Y_i}_\\text{High Temp}} = \\beta_0 + \\beta_i \\underbrace{X_i}_\\text{Complicated X}\\) In our course, the three symbols of \\(Y_i, \\ \\hat{Y}_i, \\ \\text{and} \\ E\\{Y_i\\}\\) all represent different things. So when an equation uses the \\(Y_i\\) it should be of the form: \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0,\\sigma^2)\\) When an equation uses the predicted value of it should be of the form: \\(\\hat{Y}_i = b_0 + b_1 X_{i}\\) and usually we replace the \\(b_0\\) and \\(b_1\\) values with their numeric values from the “Estimate” column of the regression summary output in RStudio. Finally, when an equation uses the \\(E\\{Y_i\\}\\) it is denoting the true regression relation and it should be of the form: \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_{i}\\) Question 4 Q: What is the value of the MSE for the simple linear regression depicted in the plot? - You shouldn’t be able to tell by just looking at it. Do some calculations. The residual for each point has been labeled for you. A: 118.4 As the residuals are all depicted in the graph, we can calculate the SSE using: sum( c(-6.6,13.8,-9.8,4.6,-2)^2 ) which equals 355.2. Since the line is a “simple linear regression” line, it must use two parameters, - By visually counting, we see there are 5 dots in the plot, so n=5. MSE = SSE/(n-p) = 355.2 / (5 - 2) = 118.4 res <- c(-6.6, 13.8, -9.8, 4.6, -2) SSE <- sum(res^2) n <- length(res) MSE <- SSE / n-2 print(MSE) ## [1] 69.04 Question 5 Q: For the Residuals vs Fitted plot shown, what is the value of \\(Y_i\\) for the solid orange dot? A: 45 The plot shows the solid orange dot in the plot has a fitted-value of 60 by looking at the x-axis of the plot. The vertical axis shows the residual axis which gives the point a -15 value by looking at the y-axis of the plot. Using the equation of a residual \\[ r_i = Y_i - \\hat{Y}_i \\\\ Y_i = r_i + \\hat{Y}_i \\ \\text{(solve for Y)} \\] shows that the y-value can be found by adding the value of the residual to the fitted value. That gives a guess of 60 - 15 = 45 for the value of for the solid orange dot. Question 6 Q: Suppose a researcher developed the theory that each one degree increase in daily average temperatures reduced average daily wind speed by 1/6 a mile per hour. Use the airquality data to test the following hypothesis. \\[\\underbrace{Y_i}_\\text{Wind} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Temp} + \\epsilon_i \\text{ where } \\epsilon_i \\sim N(0,\\sigma^2)\\] \\[H_0: \\beta_1 = -0.167 \\\\ H_a: \\beta_1 \\neq -0.167\\] Report the value of the p-value of the test. A: p-value = 0.898 wind.lm <- lm(Wind ~ Temp, data = airquality) summary(wind.lm) ## ## Call: ## lm(formula = Wind ~ Temp, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.5784 -2.4489 -0.2261 1.9853 9.7398 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 23.23369 2.11239 10.999 < 2e-16 *** ## Temp -0.17046 0.02693 -6.331 2.64e-09 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 3.142 on 151 degrees of freedom ## Multiple R-squared: 0.2098, Adjusted R-squared: 0.2045 ## F-statistic: 40.08 on 1 and 151 DF, p-value: 2.642e-09 predict(wind.lm, data.frame(Temp=-0.167)) ## 1 ## 23.26216 twind = (-0.17046 - -0.167) / 0.02693 2*pt(-abs(twind), 151) ## [1] 0.8979391 Question 7 Q: Continue using the regression model from the previous question on the airquality data set: \\[\\underbrace{Y_i}_\\text{Wind} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Temp} + \\epsilon_i \\text{ where } \\epsilon_i \\sim N(0,\\sigma^2)\\] Suppose the impossible happened and May 1st of a certain year ended up being 0 degrees Fahrenheit in New York City. What daily average wind speed (in miles per hour) does your regression model expect for such a day? A: 23 mph wind.lm <- lm(Wind ~ Temp, data=airquality) coef(wind.lm) ## (Intercept) Temp ## 23.2336881 -0.1704644 Question 8 Q: A regression is performed, and the following output obtained. Use this output to create a 95% confidence interval for the true y-intercept (\\(\\beta_0\\)) corresponding to this regression. A: (1.2711, 2.6979) (1.9845 + 5.636)*0.3521 ## [1] 2.683178 (1.9845 - 5.636)*0.3521 ## [1] -1.285693 1.9845 + qt(c(0.025, 0.975), 37)*0.3521 ## [1] 1.271078 2.697922 Question 9 Q: Use the mtcars data set in R to perform a log(Y) transformation regression of mpg predicted by wt. Use your transformation regression to predict the gas mileage (mpg) of a vehicle that has a weight of 2,000 lbs. Be sure to take note of the ?mtcars help file and see the wt variable for details on how this variable is recorded. A: 26.79846 lm1 <- lm(mpg ~ wt, data=mtcars) boxCox(lm1) #Suggests a 0 value is best, or log(Y) transformation. lm2 <- lm(log(mpg) ~ wt, data=mtcars) exp(predict(lm2, data.frame(wt=2))) #prediction transformed back ## 1 ## 26.79846 Question 10 Q: Run these codes in R: library(mosaic) #contains the Weather data plot(high_temp ~ high_dewpt, data=Weather) Fit the following regression model on the data. \\[\\underbrace{Y_i}_\\text{High Temp} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{High Dew Point} + \\epsilon_i \\text{ where } \\epsilon_i \\sim N(0,\\sigma^2)\\] Select the plot below that correctly depicts (for the above model) the 95% prediction interval for the high temperature when the high dew point is 80. A: # actual plot(high_temp ~ high_dewpt, data=Weather, ylim=c(0,120)) temp_lm <- lm(high_temp ~ high_dewpt, data=Weather) abline(temp_lm) predict(temp_lm, data.frame(high_dewpt=80), interval=\"prediction\") ## fit lwr upr ## 1 91.52634 74.49373 108.559 lines(c(80,80), c(74.49373, 108.559), lwd=6, col=rgb(0.2,.8,.5,.5)) lines(c(-21,80,80,-21), c(74.49373, 74.49373, 108.559, 108.559), lty=2, col=\"gray\") Question 11 Q: Which of the following is a “best” statement to make when interpreting a regression slope? A: The change in the average y-value per unit change in x. Incorrect answers: The average change in y as x varies. The average y-value when x is zero. The average change in the y-value per unit change in x. Further Explanation: The regression line models the average y-value for any given x-value. Since the slope is the change in the line as x changes by 1 unit, then the slope should be interpreted as the change in the average y-value (the line) for each unit change in x. Question 12 Q: Suppose you are asked to help decide how wide (in centimeters) kid’s shoes should be for fourth grade girls with foot lengths of 22 centimeters using the KidsFeet dataset, which contains data about fourth grade girls and boys. Perform a meaningful regression on the girls data provided in the KidsFeet data set. Select the interval below that, according to your regression, we are 95% confident that it contains 95% of the actual widths of fourth-grade girl’s feet that are 22 centimeters long. A: We predict girls feet to be anywhere from 7.32 cm to 9.19 cm wide, when their foot length is 22 cm. Incorrect answers: We predict girls feet to be anywhere from 8.13 cm to 9.94 cm wide, when their foot length is 22 cm. We predict girls feet to be anywhere from 8.07 cm to 9.81 cm wide, when their foot length is 22 cm. We predict girls feet to be anywhere from 7.86 cm to 8.65 cm wide, when their foot length is 22 cm. girlsFeet <- filter(KidsFeet, sex==\"G\") plot(width ~ length, data=girlsFeet) girls.lm <- lm(width ~ length, data=girlsFeet) predict(girls.lm, data.frame(length=22), interval=\"prediction\") ## fit lwr upr ## 1 8.253979 7.320637 9.187321 Question 13 Q: The solid orange dot shown in the scatterplot below corresponds with which point on the residuals vs. fitted plot? A: The point labeled “Chrysler Imperial” plot(mpg ~ wt, data=mtcars, main=\"mtcars data set\") points(mpg ~ wt, data=mtcars[17,], pch=16, col=\"orange\", cex=1.2) mt.lm <- lm(mpg ~ wt, data=mtcars) abline(mt.lm) plot(mt.lm, which=1) points(mt.lm$res[17] ~ mt.lm$fit[17], pch=16, cex=1.2, col=\"orange\") Question 14 Q: Consider the following scatterplot. Apply an appropriate Y transformation to the data shown in the plot. State the R-squared value of the regression on the transformed data. plot(Ozone ~ Temp, data=airquality) Or, if you prefer, here is the plot in ggplot2: ggplot(airquality, aes(x=Temp, y=Ozone)) + geom_point() A: 0.5632 ggplot(airquality, aes(x=Temp, y=Ozone)) + geom_point() ## Warning: Removed 37 rows containing missing values or values outside the scale range ## (`geom_point()`). air3.lm <- lm(Ozone ~ Temp, data=airquality) boxCox(air3.lm) air3ss.lm <- lm(sqrt(sqrt(Ozone)) ~ Temp, data=airquality) summary(air3ss.lm) ## ## Call: ## lm(formula = sqrt(sqrt(Ozone)) ~ Temp, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.86085 -0.23007 0.00676 0.21087 1.07342 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -0.661530 0.254646 -2.598 0.0106 * ## Temp 0.039362 0.003246 12.125 <2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.3302 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.5632, Adjusted R-squared: 0.5594 ## F-statistic: 147 on 1 and 114 DF, p-value: < 2.2e-16 air3log.lm <- lm(log(Ozone) ~ Temp, data=airquality) summary(air3log.lm) ## ## Call: ## lm(formula = log(Ozone) ~ Temp, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.14469 -0.33095 0.02961 0.36507 1.49421 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -1.83797 0.45100 -4.075 8.53e-05 *** ## Temp 0.06750 0.00575 11.741 < 2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.5848 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.5473, Adjusted R-squared: 0.5434 ## F-statistic: 137.8 on 1 and 114 DF, p-value: < 2.2e-16 # saunders code lm1 <- lm(Ozone ~ Temp, data=airquality) boxCox(lm1) #the results of Box-Cox show a transformation should be applied, which is lambde = 0.25 air.lm.t <- lm(sqrt(sqrt(Ozone)) ~ Temp, data=airquality) summary(air.lm.t) ## ## Call: ## lm(formula = sqrt(sqrt(Ozone)) ~ Temp, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.86085 -0.23007 0.00676 0.21087 1.07342 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -0.661530 0.254646 -2.598 0.0106 * ## Temp 0.039362 0.003246 12.125 <2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.3302 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.5632, Adjusted R-squared: 0.5594 ## F-statistic: 147 on 1 and 114 DF, p-value: < 2.2e-16 #Gives an R-squared of 0.5632 Question 15 Q: Open the ChickWeight data set in R. Perform a linear regression that predicts the weight of chickens according to how old the chick is in days. Which of the following correctly diagnoses the appropriateness of this regression? A: Constant Variance and normal errors are both violated, but the linear relation isn’t too bad, relatively speaking. Incorrect answers: Linear Relation is the only assumption being violated. The data looks to have constant variance and normal errors. Everything looks good except for the non-normal errors. The constant variance is violated, but linearity and normality look fine. Further explanation: Then, looking at the plot it should be clear that the variance of the residuals (in the vertical direction) is increasing from left to right. Also, from the Q-Q Plot, it is obvious that normality fails dramatically. Though there is a slight bend in the red smoothing line on the left of the graph, the red line is flat for most of the regression, witnessing that linearity has some problems, but nothing close to the problems with the variance and normality. chick.lm <- lm(weight ~ Time, data = ChickWeight) summary(chick.lm) ## ## Call: ## lm(formula = weight ~ Time, data = ChickWeight) ## ## Residuals: ## Min 1Q Median 3Q Max ## -138.331 -14.536 0.926 13.533 160.669 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 27.4674 3.0365 9.046 <2e-16 *** ## Time 8.8030 0.2397 36.725 <2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 38.91 on 576 degrees of freedom ## Multiple R-squared: 0.7007, Adjusted R-squared: 0.7002 ## F-statistic: 1349 on 1 and 576 DF, p-value: < 2.2e-16 par(mfrow=c(1,3)) plot(chick.lm, which=1) qqPlot(chick.lm$residuals, main=\"Q-Q Plot\", col=\"steelblue3\", col.lines=\"steelblue4\", pch= 19, id=FALSE) plot(chick.lm$residuals, main = \"Residuals vs. Order\") Question 16 Q: Consider the linear model \\[Y_i = \\beta_0 + \\beta_1X_i+ \\epsilon_i \\text{ where } \\epsilon_i \\sim N(0,\\sigma^2)\\] Which of the following is a true statement about this model? A: \\(\\beta_0 + \\beta_1 X_i\\) describes the functional relationship of the data to the true model. incorrect answers: A small value for results in a very low value of \\(R^2\\) The model is purely statistical and does not contain any functional relations. \\(\\epsilon_i\\) describes the average value of \\(Y_i\\) for given values of \\(X_i\\) Question 17 Q: What is the value of SSE according to the output shown below? A: Value of SSE: 25.2 Using the residual standard error: 1.875^2*22 - Squaring the residual standard error gives MSE. Multiplying MSE by (n-p) or 22 degrees of freedom gives back the SSE because MSE = SSE/(n-p) and sqrt(MSE) = residual standard error. Question 18 Q: A researcher develops a regression model to predict the highway miles per gallon of a vehicle based on the city miles per gallon of the vehicle. They run the following codes to do this. ?mpg View(mpg) plot(hwy ~ cty, data = mpg) mpg.lm <- lm(hwy ~ cty, data=mpg) The researcher is wondering how to best interpret the slope from their model. Select the statement below that best interprets the slope of their model. mpg.lm <- lm(hwy ~ cty, data=mpg) summary(mpg.lm) ## ## Call: ## lm(formula = hwy ~ cty, data = mpg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.3408 -1.2790 0.0214 1.0338 4.0461 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 0.89204 0.46895 1.902 0.0584 . ## cty 1.33746 0.02697 49.585 <2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 1.752 on 232 degrees of freedom ## Multiple R-squared: 0.9138, Adjusted R-squared: 0.9134 ## F-statistic: 2459 on 1 and 232 DF, p-value: < 2.2e-16 plot(hwy ~ cty, data = mpg) abline(mpg.lm) A: The average highway gas mileage of vehicles increases by 1.337 miles per gallon for every 1 mpg increase in city gas mileage of the vehicle. Incorrect interpretations: On average, highway gas mileage of vehicles is 1.337 miles per gallon greater than the city gas mileage of the vehicle. Every one mpg increase in city gas mileage of a vehicle results in 1.337 miles per gallon increase in the highway gas mileage of the vehicle. The change in average highway gas mileage of vehicles increases by 1 mile per gallon for every 1.337 mpg increase in city gas mileage of the vehicle. Further Explanation: The slope is the change in the average Y-value for a 1 unit change in the x-value. Every one mpg increase in city gas mileage of a vehicle results in 1.337 miles per gallon increase in the highway gas mileage of the vehicle. (Missing the word “average” when talking about Y.) On average, highway gas mileage of vehicles is 1.337 miles per gallon greater than the city gas mileage of the vehicle. (The word “average” is incorrectly placed. Also, the slope is multiplied to the X-value, not added to it, so “1.337 miles per gallon greater than the city gas mileage” is an incorrect statement. It is 1.337 times as great in the gas mileage as the city gas mileage, plus the intercept of 0.892 miles per gallon.) The change in average highway gas mileage of vehicles increases by 1 mile per gallon for every 1.337 mpg increase in city gas mileage of the vehicle. (The numbers for “1 unit” and the slope of “1.337” are misplaced.) Question 19 Q: Select the plot below that correctly depicts the least squares regression line for the data shown. - You should be able to visually tell which one is the least squares line, just by looking at the plots. A: The least squares line should go through the average of the Y-values as you move across the x-values. Only one plot does this visually. The correct answer! NOT the correct answers!! Question 20 Q: This question goes a little beyond your current knowledge, but uses ideas you should be familiar with in the same way you have been using them previously. What is the p-value for the missing entry in the output below? Note: the sample size for this regression was n=30. A: where pt(estimate / std. error, n - p)*2 where p is the number of parameters being estimated by the model, which is 4. (Recognizing the 4 is the hardest part.) tm <- -109.736 / 374.404 print(tm) ## [1] -0.2930952 pt(-abs(tm), 28)*2 ## [1] 0.7716107 # correct answer: pt(-109.736/374.404, 30-4)*2 ## [1] 0.771776 Question 21 Q: Use the Orange dataset in R to perform a linear regression of the circumference (y) of an orange tree according to the age of the tree (x). Which regression assumption is not satisfied for this regression? A: The Q-Q Plot looks great. Linearity is okay, even though the red line is a little jagged, the points don’t seem to hold too closely to the line. Clearly the x-values are all fixed as they are “replicated” in the study. The variance shows a strong increa"},{"title":" Homework Completeness or Homework Correctness? ","url":"425Analyses/homeworkfocus.html","content":"Code Show All Code Hide All Code Homework Completeness or Homework Correctness? library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) gradesmath100b <- read.csv(\"~/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/math100bgrades.csv\", stringsAsFactors=TRUE) Overview How do homework scores and homework grading style—completeness (COM) vs. correctness (COR)—predict final exam performance in BYU-Idaho’s Math 100B? More specifically, does the effect of homework grades on final exam scores vary by grading type? To address these questions, we used a multiple linear regression model to analyze the data. For students in the completeness (COM) group, our model shows that for every 1-point increase in homework score, their final exam score increases by approximately 0.78 points (\\(\\beta_1\\) = 0.7812, p < 2.28e−08). This strong positive relationship suggests that when homework is graded for completion, students see greater benefits from increased assignment effort. In contrast, students in correctness-focused (COR) sections begin with significantly higher baseline scores, though the effect of each homework point is less pronounced. The model estimates that with a homework grade of 0, a COR student’s final exam score would be 54.6% (46.83 + 7.232 = 54.06, \\(\\beta_2\\) = 46.83, p = 0.0023). For every 1-point increase in homework score, their final exam score increases by approximately 0.35 points (0.7812 + (-0.4312) = 0.35, \\(\\beta_3\\) = −0.4312, p = 0.0107). This indicates that while COR students’ final exam scores are less sensitive to homework grade changes, they consistently outperform COM students regardless of homework performance. As a concrete example, for students in the correctness group (COR), our model predicts that a homework score of 90 corresponds to a final exam score of approximately 85.56, while in the completeness group (COM), the predicted score is about 77.54. This difference suggests that grading based on correctness may lead to stronger performance on cumulative assessments like the final exam. These trends are evident in our scatter plot. The COM line (light blue) shows a steep rise from a lower starting point, while the COR line (dark blue) begins higher but climbs more gradually. In practical terms, students in COR classes—who must meet standards of correctness rather than just completion—tend to be better prepared for the cumulative final exam. While the data largely meets the assumptions of linear regression, we should note any minor deviations when generalizing these results to future classes. Nevertheless, our findings suggest that instructors should consider correctness-based grading as a strategy to promote deeper learning and long-term retention. Background As a Teacher’s Assistant at Brigham Young University-Idaho, I have observed various teaching styles, engagement tactics, and grading standards while working with professors who teach Beginning Algebra (MATH 100B). MATH 100B serves as a foundational course at BYU-Idaho, preparing students with essential skills for advanced mathematics courses. After grading countless homework assignments, I began to wonder: To what extent do homework completion and correctness influence students’ homework grades and final exam performance in MATH 100B? Click the tab to explore the data used for the study. Hide Data Show Data This data is taken from Math 100B students, noting these three things: homeworkfocus: How the homework was graded in that student’s class COM : Completeness COR : Correctness homeworkscore : A student’s overall homework score finalexamscore: A student’s final exam score The data presented was collected during the year and semester track, but that is undocumented to keep these students and their professors anonymous. datatable(gradesmath100b) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\"],[\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COR\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\",\"COM\"],[101.81,94.84999999999999,101.02,101.05,98.39,97.43000000000001,100.37,67.75,89.83,91.91,99.23999999999999,102.01,73.98,99.56999999999999,100.32,98.23999999999999,87.81999999999999,92.78,15.16,102.81,97.94,59.62,99.76000000000001,81.89,84.31999999999999,81.86,100,99.15000000000001,79.58,97.2,102.88,88.64,79.41,99.15000000000001,106.69,101.53,59.14,11.23,91.09999999999999,88.31,106.36,55.59,81.16,65.76000000000001,97.03,107.71,82.54000000000001,115.17,86.2,83.3,97.97,54.49,101.95,80.17,104.07,103.73,113.39,37.4,107.29,92.88,42.37,86.86,94.98999999999999,48.73,94.98999999999999,48.73,94.58,75.84999999999999,110.61,58.56,106.61,104.66,96.58,88.14,91.40000000000001,84.56999999999999,96.17,99.45,93.38,102.6,97.79000000000001,97.93000000000001,78.76000000000001,54.5,101.15,102.35,102.42,103.05,93.14,58.62,100.67,74.44,101.04,99.52,31.93,88.54000000000001,100.29],[89,94,88,94,85,94,82,67,58,85,88,97,33,97,100,91,75,85,0,100,71,71,94,42,89,82,100,94,62,91,85,100,82,91,84,91,91,0,64,82,97,91,86,70,94,94,82,100,73,70,96,88,84,88,97,97,76,80,68,79,85,42,82,75,97,94,82,91,100,97,100,91,85,60,77,90,79,73,85,93,42,88,76,0,94,52,99,97,79,74,78,69,91,94,82,71,79]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>homeworkfocus<\\/th>\\n <th>homeworkscore<\\/th>\\n <th>finalexamscore<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"homeworkfocus\",\"targets\":1},{\"name\":\"homeworkscore\",\"targets\":2},{\"name\":\"finalexamscore\",\"targets\":3}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Visually Modeling the Data Based on what we can see from the graphic below, the students who have their homework assignments graded based on correctness seem to outperform the students with homework assignments graded by completeness. While the steepness of the COR group is not as steep as the COM group, showing steady improvement in the COR group and drastic improvement in the COM group, the COR group still seems to have an extremely better scores even when their students homework scores are low. Even at a homework score of 60%, the COR group of students still seem to score around 75% on their final exam scores while the COM group just barely grazes over a 50% final exam score. Hover over each dot (representing a student) to see their specifics. gradelm <- lm(finalexamscore ~ homeworkscore + homeworkfocus + homeworkscore:homeworkfocus, data= gradesmath100b) b <- coef(gradelm) gradeplot <- ggplot(gradesmath100b, aes(x=homeworkscore, y= finalexamscore, color = homeworkfocus)) + geom_point(size=1.5, alpha =2) + stat_function(fun=function(x) b[1] + b[2]*x, color=\"dodgerblue\")+ stat_function(fun=function(x) (b[1]+b[3]) + (b[2]+b[4])*x, color=\"darkblue\")+ scale_color_manual(name = \"Homework Focus\", values = c(\"dodgerblue\", \"darkblue\")) + labs(title=\"Homework Completeness vs. Correctness: Final Exam Scores of BYU-Idaho Math 100B Students\", x = \"Homework Grade\", y = \"Final Exam Grade\")+ theme_minimal() ggplotly(gradeplot) {\"x\":{\"data\":[{\"x\":[101.81,94.849999999999994,101.02,101.05,98.390000000000001,97.430000000000007,100.37,67.75,89.829999999999998,91.909999999999997,99.239999999999995,102.01000000000001,73.980000000000004,99.569999999999993,100.31999999999999,98.239999999999995,87.819999999999993,92.780000000000001,15.16,102.81,97.939999999999998,59.619999999999997,99.760000000000005,81.890000000000001,96.579999999999998,88.140000000000001,91.400000000000006,84.569999999999993,96.170000000000002,99.450000000000003,93.379999999999995,102.59999999999999,97.790000000000006,97.930000000000007,78.760000000000005,54.5,101.15000000000001,102.34999999999999,102.42,103.05,93.140000000000001,58.619999999999997,100.67,74.439999999999998,101.04000000000001,99.519999999999996,31.93,88.540000000000006,100.29000000000001],\"y\":[89,94,88,94,85,94,82,67,58,85,88,97,33,97,100,91,75,85,0,100,71,71,94,42,85,60,77,90,79,73,85,93,42,88,76,0,94,52,99,97,79,74,78,69,91,94,82,71,79],\"text\":[\"homeworkscore: 101.81<br />finalexamscore: 89<br />homeworkfocus: COM\",\"homeworkscore: 94.85<br />finalexamscore: 94<br />homeworkfocus: COM\",\"homeworkscore: 101.02<br />finalexamscore: 88<br />homeworkfocus: COM\",\"homeworkscore: 101.05<br />finalexamscore: 94<br />homeworkfocus: COM\",\"homeworkscore: 98.39<br />finalexamscore: 85<br />homeworkfocus: COM\",\"homeworkscore: 97.43<br />finalexamscore: 94<br />homeworkfocus: COM\",\"homeworkscore: 100.37<br />finalexamscore: 82<br />homeworkfocus: COM\",\"homeworkscore: 67.75<br />finalexamscore: 67<br />homeworkfocus: COM\",\"homeworkscore: 89.83<br />finalexamscore: 58<br />homeworkfocus: COM\",\"homeworkscore: 91.91<br />finalexamscore: 85<br />homeworkfocus: COM\",\"homeworkscore: 99.24<br />finalexamscore: 88<br />homeworkfocus: COM\",\"homeworkscore: 102.01<br />finalexamscore: 97<br />homeworkfocus: COM\",\"homeworkscore: 73.98<br />finalexamscore: 33<br />homeworkfocus: COM\",\"homeworkscore: 99.57<br />finalexamscore: 97<br />homeworkfocus: COM\",\"homeworkscore: 100.32<br />finalexamscore: 100<br />homeworkfocus: COM\",\"homeworkscore: 98.24<br />finalexamscore: 91<br />homeworkfocus: COM\",\"homeworkscore: 87.82<br />finalexamscore: 75<br />homeworkfocus: COM\",\"homeworkscore: 92.78<br />finalexamscore: 85<br />homeworkfocus: COM\",\"homeworkscore: 15.16<br />finalexamscore: 0<br />homeworkfocus: COM\",\"homeworkscore: 102.81<br />finalexamscore: 100<br />homeworkfocus: COM\",\"homeworkscore: 97.94<br />finalexamscore: 71<br />homeworkfocus: COM\",\"homeworkscore: 59.62<br />finalexamscore: 71<br />homeworkfocus: COM\",\"homeworkscore: 99.76<br />finalexamscore: 94<br />homeworkfocus: COM\",\"homeworkscore: 81.89<br />finalexamscore: 42<br />homeworkfocus: COM\",\"homeworkscore: 96.58<br />finalexamscore: 85<br />homeworkfocus: COM\",\"homeworkscore: 88.14<br />finalexamscore: 60<br />homeworkfocus: COM\",\"homeworkscore: 91.40<br />finalexamscore: 77<br />homeworkfocus: COM\",\"homeworkscore: 84.57<br />finalexamscore: 90<br />homeworkfocus: COM\",\"homeworkscore: 96.17<br />finalexamscore: 79<br />homeworkfocus: COM\",\"homeworkscore: 99.45<br />finalexamscore: 73<br />homeworkfocus: COM\",\"homeworkscore: 93.38<br />finalexamscore: 85<br />homeworkfocus: COM\",\"homeworkscore: 102.60<br />finalexamscore: 93<br />homeworkfocus: COM\",\"homeworkscore: 97.79<br />finalexamscore: 42<br />homeworkfocus: COM\",\"homeworkscore: 97.93<br />finalexamscore: 88<br />homeworkfocus: COM\",\"homeworkscore: 78.76<br />finalexamscore: 76<br />homeworkfocus: COM\",\"homeworkscore: 54.50<br />finalexamscore: 0<br />homeworkfocus: COM\",\"homeworkscore: 101.15<br />finalexamscore: 94<br />homeworkfocus: COM\",\"homeworkscore: 102.35<br />finalexamscore: 52<br />homeworkfocus: COM\",\"homeworkscore: 102.42<br />finalexamscore: 99<br />homeworkfocus: COM\",\"homeworkscore: 103.05<br />finalexamscore: 97<br />homeworkfocus: COM\",\"homeworkscore: 93.14<br />finalexamscore: 79<br />homeworkfocus: COM\",\"homeworkscore: 58.62<br />finalexamscore: 74<br />homeworkfocus: COM\",\"homeworkscore: 100.67<br />finalexamscore: 78<br />homeworkfocus: COM\",\"homeworkscore: 74.44<br />finalexamscore: 69<br />homeworkfocus: COM\",\"homeworkscore: 101.04<br />finalexamscore: 91<br />homeworkfocus: COM\",\"homeworkscore: 99.52<br />finalexamscore: 94<br />homeworkfocus: COM\",\"homeworkscore: 31.93<br />finalexamscore: 82<br />homeworkfocus: COM\",\"homeworkscore: 88.54<br />finalexamscore: 71<br />homeworkfocus: COM\",\"homeworkscore: 100.29<br />finalexamscore: 79<br />homeworkfocus: COM\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(30,144,255,1)\",\"opacity\":2,\"size\":5.6692913385826778,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(30,144,255,1)\"}},\"hoveron\":\"points\",\"name\":\"COM\",\"legendgroup\":\"COM\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[84.319999999999993,81.859999999999999,100,99.150000000000006,79.579999999999998,97.200000000000003,102.88,88.640000000000001,79.409999999999997,99.150000000000006,106.69,101.53,59.140000000000001,11.23,91.099999999999994,88.310000000000002,106.36,55.590000000000003,81.159999999999997,65.760000000000005,97.030000000000001,107.70999999999999,82.540000000000006,115.17,86.200000000000003,83.299999999999997,97.969999999999999,54.490000000000002,101.95,80.170000000000002,104.06999999999999,103.73,113.39,37.399999999999999,107.29000000000001,92.879999999999995,42.369999999999997,86.859999999999999,94.989999999999995,48.729999999999997,94.989999999999995,48.729999999999997,94.579999999999998,75.849999999999994,110.61,58.560000000000002,106.61,104.66],\"y\":[89,82,100,94,62,91,85,100,82,91,84,91,91,0,64,82,97,91,86,70,94,94,82,100,73,70,96,88,84,88,97,97,76,80,68,79,85,42,82,75,97,94,82,91,100,97,100,91],\"text\":[\"homeworkscore: 84.32<br />finalexamscore: 89<br />homeworkfocus: COR\",\"homeworkscore: 81.86<br />finalexamscore: 82<br />homeworkfocus: COR\",\"homeworkscore: 100.00<br />finalexamscore: 100<br />homeworkfocus: COR\",\"homeworkscore: 99.15<br />finalexamscore: 94<br />homeworkfocus: COR\",\"homeworkscore: 79.58<br />finalexamscore: 62<br />homeworkfocus: COR\",\"homeworkscore: 97.20<br />finalexamscore: 91<br />homeworkfocus: COR\",\"homeworkscore: 102.88<br />finalexamscore: 85<br />homeworkfocus: COR\",\"homeworkscore: 88.64<br />finalexamscore: 100<br />homeworkfocus: COR\",\"homeworkscore: 79.41<br />finalexamscore: 82<br />homeworkfocus: COR\",\"homeworkscore: 99.15<br />finalexamscore: 91<br />homeworkfocus: COR\",\"homeworkscore: 106.69<br />finalexamscore: 84<br />homeworkfocus: COR\",\"homeworkscore: 101.53<br />finalexamscore: 91<br />homeworkfocus: COR\",\"homeworkscore: 59.14<br />finalexamscore: 91<br />homeworkfocus: COR\",\"homeworkscore: 11.23<br />finalexamscore: 0<br />homeworkfocus: COR\",\"homeworkscore: 91.10<br />finalexamscore: 64<br />homeworkfocus: COR\",\"homeworkscore: 88.31<br />finalexamscore: 82<br />homeworkfocus: COR\",\"homeworkscore: 106.36<br />finalexamscore: 97<br />homeworkfocus: COR\",\"homeworkscore: 55.59<br />finalexamscore: 91<br />homeworkfocus: COR\",\"homeworkscore: 81.16<br />finalexamscore: 86<br />homeworkfocus: COR\",\"homeworkscore: 65.76<br />finalexamscore: 70<br />homeworkfocus: COR\",\"homeworkscore: 97.03<br />finalexamscore: 94<br />homeworkfocus: COR\",\"homeworkscore: 107.71<br />finalexamscore: 94<br />homeworkfocus: COR\",\"homeworkscore: 82.54<br />finalexamscore: 82<br />homeworkfocus: COR\",\"homeworkscore: 115.17<br />finalexamscore: 100<br />homeworkfocus: COR\",\"homeworkscore: 86.20<br />finalexamscore: 73<br />homeworkfocus: COR\",\"homeworkscore: 83.30<br />finalexamscore: 70<br />homeworkfocus: COR\",\"homeworkscore: 97.97<br />finalexamscore: 96<br />homeworkfocus: COR\",\"homeworkscore: 54.49<br />finalexamscore: 88<br />homeworkfocus: COR\",\"homeworkscore: 101.95<br />finalexamscore: 84<br />homeworkfocus: COR\",\"homeworkscore: 80.17<br />finalexamscore: 88<br />homeworkfocus: COR\",\"homeworkscore: 104.07<br />finalexamscore: 97<br />homeworkfocus: COR\",\"homeworkscore: 103.73<br />finalexamscore: 97<br />homeworkfocus: COR\",\"homeworkscore: 113.39<br />finalexamscore: 76<br />homeworkfocus: COR\",\"homeworkscore: 37.40<br />finalexamscore: 80<br />homeworkfocus: COR\",\"homeworkscore: 107.29<br />finalexamscore: 68<br />homeworkfocus: COR\",\"homeworkscore: 92.88<br />finalexamscore: 79<br />homeworkfocus: COR\",\"homeworkscore: 42.37<br />finalexamscore: 85<br />homeworkfocus: COR\",\"homeworkscore: 86.86<br />finalexamscore: 42<br />homeworkfocus: COR\",\"homeworkscore: 94.99<br />finalexamscore: 82<br />homeworkfocus: COR\",\"homeworkscore: 48.73<br />finalexamscore: 75<br />homeworkfocus: COR\",\"homeworkscore: 94.99<br />finalexamscore: 97<br />homeworkfocus: COR\",\"homeworkscore: 48.73<br />finalexamscore: 94<br />homeworkfocus: COR\",\"homeworkscore: 94.58<br />finalexamscore: 82<br />homeworkfocus: COR\",\"homeworkscore: 75.85<br />finalexamscore: 91<br />homeworkfocus: COR\",\"homeworkscore: 110.61<br />finalexamscore: 100<br />homeworkfocus: COR\",\"homeworkscore: 58.56<br />finalexamscore: 97<br />homeworkfocus: COR\",\"homeworkscore: 106.61<br />finalexamscore: 100<br />homeworkfocus: COR\",\"homeworkscore: 104.66<br />finalexamscore: 91<br />homeworkfocus: COR\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,0,139,1)\",\"opacity\":2,\"size\":5.6692913385826778,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(0,0,139,1)\"}},\"hoveron\":\"points\",\"name\":\"COR\",\"legendgroup\":\"COR\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[11.23,12.269400000000001,13.3088,14.3482,15.387599999999999,16.427,17.4664,18.505800000000001,19.545200000000001,20.584600000000002,21.623999999999999,22.663399999999999,23.7028,24.742199999999997,25.781599999999997,26.820999999999998,27.860399999999998,28.899799999999999,29.9392,30.978599999999997,32.018000000000001,33.057400000000001,34.096800000000002,35.136200000000002,36.175600000000003,37.214999999999996,38.254399999999997,39.293799999999997,40.333199999999998,41.372599999999998,42.411999999999992,43.451399999999992,44.490799999999993,45.530199999999994,46.569599999999994,47.608999999999995,48.648399999999995,49.687799999999996,50.727199999999996,51.766599999999997,52.805999999999997,53.845399999999998,54.884799999999998,55.924199999999999,56.9636,58.003,59.042400000000001,60.081800000000001,61.121200000000002,62.160599999999988,63.199999999999989,64.239399999999989,65.27879999999999,66.31819999999999,67.357599999999991,68.396999999999991,69.436399999999992,70.475799999999992,71.515199999999993,72.554599999999994,73.593999999999994,74.633399999999995,75.672799999999995,76.712199999999996,77.751599999999996,78.790999999999997,79.830399999999997,80.869799999999998,81.909199999999998,82.948599999999999,83.988,85.0274,86.066800000000001,87.106200000000001,88.145600000000002,89.184999999999988,90.224399999999989,91.263799999999989,92.30319999999999,93.34259999999999,94.381999999999991,95.421399999999991,96.460799999999992,97.500199999999992,98.539599999999993,99.578999999999994,100.61839999999999,101.65779999999999,102.6972,103.7366,104.776,105.8154,106.8548,107.8942,108.9336,109.973,111.0124,112.05179999999999,113.09119999999999,114.13059999999999,115.17],\"y\":[16.004987886734355,16.816980509470174,17.628973132205992,18.440965754941811,19.252958377677629,20.064951000413448,20.876943623149266,21.688936245885088,22.500928868620903,23.312921491356725,24.12491411409254,24.936906736828362,25.748899359564177,26.560891982299999,27.372884605035814,28.184877227771636,28.996869850507451,29.808862473243273,30.620855095979095,31.43284771871491,32.244840341450733,33.056832964186547,33.86882558692237,34.68"},{"title":" House Selling Prices ","url":"425Analyses/HouseSellingPrices.html","content":"Code Show All Code Hide All Code House Selling Prices library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) library(reshape2) library(GGally) Background In this study, we will be exploring every aspect of residential homes in Ames, Iowa in order to predict the final price of each home. Below, I start by mutating and determining which variables to utilize. Click the tab below to see that exploration. Hide Data Exploration Show Data Exploration First, we’ll take a look at the pairs plot. Alright, I’ll say it first, this thing is HUGE!! So, lets first consider some main criteria when it comes to picking a home and choose those specific variables to look at in the pairs plot. When choosing variables, I had 3 main criteria I wanted to hit: Location Utilities/Space Appearance With that, I chose these variables to look at the fit my criteria. train <- read.csv(\"train.csv\", stringsAsFactors = TRUE) pairs(train [c(\"SalePrice\", \"GrLivArea\",\"OverallQual\", \"TotalBsmtSF\", \"GarageCars\", \"GarageArea\", \"X1stFlrSF\", \"X2ndFlrSF\", \"Neighborhood\", \"YearBuilt\",\"YearRemodAdd\")]) In order to fit the data with those criteria in mind, I mutated the data to fit more columns into our model. I created these new columns: TotalSF : The total surface area of the house including the all floors of the house (first, second, and basement) and the garage LocationScore : captures the location quality based on two factors Neighborhood (Most popular to least popular neighborhood) Condition (Near a positive or negative feature from the house) UtilityScore : based on the home’s usability different features contributes a different score based on its importance/ how essential it is TimeRemodel: The Year it was Sold - The Year it was Remodeled = Shows how many years have passed since it was last remodeled up to the year it sold lower values = recently remodeled hight values = older/outdated remodel OverallScore: the average rating of the overall condition and overall quality/finish of the home Overall, here are the variables I chose to use and what they can tell us in terms of this study. # Load necessary library library(knitr) # Create a data frame with Variable, Description, and How it Helps Us table_data <- data.frame( Variable = c(\"SalePrice\", \"TotalSF\", \"LocationScore\", \"UtilityScore\", \"TimeRemodel\", \"Neighborhood\", \"OverallScore\", \"Neighborhood:TotalSF\"), Description = c(\"The final price at which the house was sold.\", \"Total square footage of the house, including basement and garage.\", \"A score that evaluates the desirability of the neighborhood and location conditions.\", \"A score representing the house’s overall utility, considering space, features, and livability.\", \"Number of years since the last remodeling or addition was completed.\", \"The specific neighborhood in which the house is located.\", \"An average of Overall Quality and Overall Condition ratings.\", \"An interaction term that accounts for how the effect of total square footage varies across neighborhoods.\"), How_it_Helps_Us = c(\"Target variable we are trying to predict.\", \"Bigger houses generally sell for more, making this a key predictor.\", \"Homes in desirable locations tend to have higher sale prices.\", \"Higher utility scores indicate more livable homes, increasing value.\", \"More recently remodeled homes tend to sell for higher prices.\", \"Neighborhood greatly influences home values due to amenities and demand.\", \"Houses with better quality and condition typically sell for more.\", \"Captures how the impact of house size varies depending on the neighborhood.\") ) # Print the table in a markdown-friendly format kable(table_data, format = \"markdown\", col.names = c(\"Variable\", \"What it Looks at\", \"How it Helps Us\")) Variable What it Looks at How it Helps Us SalePrice The final price at which the house was sold. Target variable we are trying to predict. TotalSF Total square footage of the house, including basement and garage. Bigger houses generally sell for more, making this a key predictor. LocationScore A score that evaluates the desirability of the neighborhood and location conditions. Homes in desirable locations tend to have higher sale prices. UtilityScore A score representing the house’s overall utility, considering space, features, and livability. Higher utility scores indicate more livable homes, increasing value. TimeRemodel Number of years since the last remodeling or addition was completed. More recently remodeled homes tend to sell for higher prices. Neighborhood The specific neighborhood in which the house is located. Neighborhood greatly influences home values due to amenities and demand. OverallScore An average of Overall Quality and Overall Condition ratings. Houses with better quality and condition typically sell for more. Neighborhood:TotalSF An interaction term that accounts for how the effect of total square footage varies across neighborhoods. Captures how the impact of house size varies depending on the neighborhood. train <- train %>% mutate(TotalSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF + GarageArea) %>% mutate(TotalRoom = FullBath + (HalfBath * 0.5) + BsmtFullBath + (BsmtHalfBath * 0.5) + KitchenAbvGr + BedroomAbvGr ) %>% # total amount of rooms in the house (bedrooms, bathrooms, etc.) mutate( Utilities_score = case_when( Utilities == \"AllPub\" ~ 4, Utilities == \"NoSewr\" ~ 3, Utilities == \"NoSeWa\" ~ 2, Utilities == \"ELO\" ~ 1, TRUE ~ 0 ), Street_score = case_when( Street == \"Pave\" ~ 1, Street == \"Grvl\" ~ 0, TRUE ~ 0 ), Alley_score = case_when( Alley == \"Pave\" ~ 2, Alley == \"Grvl\" ~ 1, Alley == \"NA\" ~ 0, TRUE ~ 0 ), LandSlope_score = case_when( LandSlope == \"Gtl\" ~ 2, LandSlope == \"Mod\" ~ 1, LandSlope == \"Sev\" ~ 0, TRUE ~ 0 ), CentralAir_score = ifelse(CentralAir == \"Y\", 1, 0), PavedDrive_score = case_when( PavedDrive == \"Y\" ~ 2, PavedDrive == \"P\" ~ 1, PavedDrive == \"N\" ~ 0, TRUE ~ 0 ), OverallQual_norm = OverallQual / 10, # Scale from 1-10 OverallCond_norm = OverallCond / 10, HeatingQC_score = case_when( HeatingQC == \"Ex\" ~ 5, HeatingQC == \"Gd\" ~ 4, HeatingQC == \"TA\" ~ 3, HeatingQC == \"Fa\" ~ 2, HeatingQC == \"Po\" ~ 1, TRUE ~ 0 ), KitchenQual_score = case_when( KitchenQual == \"Ex\" ~ 5, KitchenQual == \"Gd\" ~ 4, KitchenQual == \"TA\" ~ 3, KitchenQual == \"Fa\" ~ 2, KitchenQual == \"Po\" ~ 1, TRUE ~ 0 ), Functional_score = case_when( Functional == \"Typ\" ~ 5, Functional == \"Min1\" ~ 4, Functional == \"Min2\" ~ 3, Functional == \"Mod\" ~ 2, Functional == \"Maj1\" ~ 1, Functional == \"Maj2\" ~ 0, TRUE ~ 0 ) ) %>% mutate( UtilityScore = (0.15 * Utilities_score) + (0.10 * GrLivArea) + (0.07 * TotalBsmtSF) + (0.06 * GarageArea) + (0.05 * KitchenQual_score) + (0.05 * HeatingQC_score) + (0.05 * Functional_score) + (0.04 * PavedDrive_score) + (0.03 * Alley_score) + (0.02 * Street_score) + (0.02 * LandSlope_score) + (0.02 * CentralAir_score) + (0.05 * WoodDeckSF) + (0.05 * OpenPorchSF) ) %>% mutate( # scores based on popularity fo outside look! HouseStyle = as.character(HouseStyle), HouseStyle = replace_na(HouseStyle, \"None\"), HouseStyle = as.factor(HouseStyle), HouseStyle_Score = case_when( # scored on popularity HouseStyle == \"2.5Fin\" ~ 8, HouseStyle == \"2Story\" ~ 7, HouseStyle == \"1Story\" ~ 6, HouseStyle == \"SLvl\" ~ 5, HouseStyle == \"2.5Unf\" ~ 4, HouseStyle == \"1.5Fin\" ~ 3, HouseStyle == \"SFoyer\" ~ 2, HouseStyle == \"1.5Unf\" ~ 1 ), LotShape = as.character(LotShape), LotShape = replace_na(LotShape, \"None\"), LotShape = as.factor(LotShape), LotShape_Score = case_when( LotShape == \"Reg\" ~ 4, LotShape == \"IR1\" ~ 3, LotShape == \"IR2\" ~ 2, LotShape == \"IR3\" ~ 1 ), ExterQual = as.character(ExterQual), ExterQual = as.factor(ExterQual), ExterQual_Score = case_when( ExterQual == \"Ex\" ~ 5, ExterQual == \"Gd\" ~ 4, ExterQual == \"TA\" ~ 3, ExterQual == \"Fa\" ~ 2, ExterQual == \"Po\" ~ 1 ), ExterCond = as.character(ExterCond), ExterCond = as.factor(ExterCond), ExterCond_Score = case_when( ExterCond == \"Ex\" ~ 5, ExterCond == \"Gd\" ~ 4, ExterCond == \"TA\" ~ 3, ExterCond == \"Fa\" ~ 2, ExterCond == \"Po\" ~ 1) ) %>% mutate(OverallScore = (OverallQual + OverallCond)/2) %>% mutate( LocationScore = case_when( Neighborhood %in% c(\"NoRidge\", \"NridgHt\", \"StoneBr\", \"Veenker\") ~ 5, Neighborhood %in% c(\"NWAmes\", \"Somerst\", \"Timber\", \"ClearCr\") ~ 4, Neighborhood %in% c(\"Sawyer\", \"SawyerW\", \"Edwards\", \"BrkSide\") ~ 2, TRUE ~ 3 ) + case_when( Condition1 %in% c(\"PosN\", \"PosA\") | Condition2 %in% c(\"PosN\", \"PosA\") ~ 2, Condition1 %in% c(\"Artery\", \"Feedr\", \"RRAn\", \"RRNe\") | Condition2 %in% c(\"Artery\", \"Feedr\", \"RRAn\", \"RRNe\") ~ -1, TRUE ~ 0 ) ) %>% mutate(PopularNbrHd = case_when(Neighborhood %in% c(\"NAmes\", \"CollgCr\", \"OldTown\", \"Edwards\", \"Somerst\", \"Gilbert\", \"NridgHt\", \"Sawyer\", \"NWAmes\", \"SawyerW\" )~ 1, TRUE ~ 0)) %>% mutate(TimeRemodel = YrSold - YearRemodAdd) %>% mutate(OutdoorScore = HouseStyle_Score + LotShape_Score + ExterQual_Score + ExterCond_Score) house.lm <- lm(SalePrice ~ TotalSF + LocationScore + UtilityScore + TimeRemodel + Neighborhood + Neighborhood:TotalSF + OverallScore, data=train) pairs(train [c(\"SalePrice\", \"TotalSF\", \"TimeRemodel\", \"UtilityScore\", \"Neighborhood\", \"LocationScore\", \"OverallScore\")],panel=panel.smooth) Visuals The visuals below will look at how each variable effect and interact each other when it comes to predicting SalePrice. Some of them at a glance will be difficult to read, thus a subset of each graph will be given to look at each factor individually. Click through the tabs to see each visual. Total Surface Area Neighborhood Key Findings: Steeper slopes show a stronger impact TotalSF has on SalesPrices More expensive neighborhoods show to have higher SalePrices at any given TotalSF(ex. StoneBr) TSA.N <- ggplot(train, aes(y = SalePrice, x = TotalSF, color = factor(Neighborhood))) + geom_point() + geom_line(aes(y = house.lm$fit, group = interaction(LocationScore)), cex = 0.5) + theme_minimal() ggplotly(TSA.N) {\"x\":{\"data\":[{\"x\":[2934,3358,2955,3164,3187,3351,3308,3735,2687,3129,2920,3307,3308,3287,3588,3504,3615],\"y\":[167240,192500,192000,172500,178740,234000,194201,264561,160200,215000,159895,181000,213490,191000,174000,246578,175900],\"text\":[\"TotalSF: 2934<br />SalePrice: 167240<br />factor(Neighborhood): Blmngtn\",\"TotalSF: 3358<br />SalePrice: 192500<br />factor(Neighborhood): Blmngtn\",\"TotalSF: 2955<br />SalePrice: 192000<br />factor(Neighborhood): Blmngtn\",\"TotalSF: 3164<br />SalePrice: 172500<br />factor(Neighborhood): Blmngtn\",\"TotalSF: 3187<br />SalePrice: 178740<br />factor(Neighborhood): Blmngtn\",\"TotalSF: 3351<br />SalePrice: 234000<br />factor(Neighborhood): Blmngtn\",\"TotalSF: 3308<br />SalePrice: 194201<br />factor(Neighborhood): Blmngtn\",\"TotalSF: 3735<br />SalePrice: 264561<br />factor(Neighborhood): Blmngtn\",\"TotalSF: 2687<br />SalePrice: 160200<br />factor(Neighborhood): Blmngtn\",\"TotalSF: 3129<br />SalePrice: 215000<br />factor(Neighborhood): Blmngtn\",\"TotalSF: 2920<br />SalePrice: 159895<br />factor(Neighborhood): Blmngtn\",\"TotalSF: 3307<br />SalePrice: 181000<br />factor(Neighborhood): Blmngtn\",\"TotalSF: 3308<br />SalePrice: 213490<br />factor(Neighborhood): Blmngtn\",\"TotalSF: 3287<br />SalePrice: 191000<br />factor(Neighborhood): Blmngtn\",\"TotalSF: 3588<br />SalePrice: 174000<br />factor(Neighborhood): Blmngtn\",\"TotalSF: 3504<br />SalePrice: 246578<br />factor(Neighborhood): Blmngtn\",\"TotalSF: 3615<br />SalePrice: 175900<br />factor(Neighborhood): Blmngtn\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(248,118,109,1)\",\"opacity\":1,\"size\":5.6692913385826778,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(248,118,109,1)\"}},\"hoveron\":\"points\",\"name\":\"Blmngtn\",\"legendgroup\":\"Blmngtn\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[2724,2252],\"y\":[151000,124000],\"text\":[\"TotalSF: 2724<br />SalePrice: 151000<br />factor(Neighborhood): Blueste\",\"TotalSF: 2252<br />SalePrice: 124000<br />factor(Neighborhood): Blueste\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(237,129,65,1)\",\"opacity\":1,\"size\":5.6692913385826778,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(237,129,65,1)\"}},\"hoveron\":\"points\",\"name\":\"Blueste\",\"legendgroup\":\"Blueste\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[2212,1750,1758,1734,1734,1734,2570,2154,1881,1734,2154,2372,1890,2196,2570,1734],\"y\":[112000,106000,94500,89500,118000,85400,122500,113000,88000,100000,118000,106000,91500,119500,125000,83000],\"text\":[\"TotalSF: 2212<br />SalePrice: 112000<br />factor(Neighborhood): BrDale\",\"TotalSF: 1750<br />SalePrice: 106000<br />factor(Neighborhood): BrDale\",\"TotalSF: 1758<br />SalePrice: 94500<br />factor(Neighborhood): BrDale\",\"TotalSF: 1734<br />SalePrice: 89500<br />factor(Neighborhood): BrDale\",\"TotalSF: 1734<br />SalePrice: 118000<br />factor(Neighborhood): BrDale\",\"TotalSF: 1734<br />SalePrice: 85400<br />factor(Neighborhood): BrDale\",\"TotalSF: 2570<br />SalePrice: 122500<br />factor(Neighborhood): BrDale\",\"TotalSF: 2154<br />SalePrice: 113000<br />factor(Neighborhood): BrDale\",\"TotalSF: 1881<br />SalePrice: 88000<br />factor(Neighborhood): BrDale\",\"TotalSF: 1734<br />SalePrice: 100000<br />factor(Neighborhood): BrDale\",\"TotalSF: 2154<br />SalePrice: 118000<br />factor(Neighborhood): BrDale\",\"TotalSF: 2372<br />SalePrice: 106000<br />factor(Neighborhood): BrDale\",\"TotalSF: 1890<br />SalePrice: 91500<br />factor(Neighborhood): BrDale\",\"TotalSF: 2196<br />SalePrice: 119500<br />factor(Neighborhood): BrDale\",\"TotalSF: 2570<br />SalePrice: 125000<br />factor(Neighborhood): BrDale\",\"TotalSF: 1734<br />SalePrice: 83000<br />factor(Neighborhood): BrDale\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(224,139,0,1)\",\"opacity\":1,\"size\":5.6692913385826778,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(224,139,0,1)\"}},\"hoveron\":\"points\",\"name\":\"BrDale\",\"legendgroup\":\"BrDale\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[2273,2262,1280,2160,2197,1770,2480,2332,2034,3172,2478,3419,2861,2628,1576,1968,3011,2264,3086,3088,2475,334,3140,2321,2697,1929,1829,1064,3042,1437,999,2496,2886,3428,1568,2233,1875,1716,3279,1309,2282,2236,2126,2577,2843,2448,912,2811,3108,2472,1007,1128,2498,2393,2632,2228,3051,2096],\"y\":[118000,132000,68500,114500,127000,105000,115000,127000,76500,205000,153575,214500,140200,145000,100000,119000,162900,110000,184000,159000,128000,39300,210000,113000,133000,130000,79500,60000,141500,89000,52000,129000,100000,223500,109500,88000,79900,116500,167500,85500,121600,128000,102000,144000,149000,117000,80500,127500,180500,132500,72500,82500,134900,131000,137500,120000,207000,121000],\"text\":[\"TotalSF: 2273<br />SalePrice: 118000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2262<br />SalePrice: 132000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 1280<br />SalePrice: 68500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2160<br />SalePrice: 114500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2197<br />SalePrice: 127000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 1770<br />SalePrice: 105000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2480<br />SalePrice: 115000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2332<br />SalePrice: 127000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2034<br />SalePrice: 76500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 3172<br />SalePrice: 205000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2478<br />SalePrice: 153575<br />factor(Neighborhood): BrkSide\",\"TotalSF: 3419<br />SalePrice: 214500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2861<br />SalePrice: 140200<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2628<br />SalePrice: 145000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 1576<br />SalePrice: 100000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 1968<br />SalePrice: 119000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 3011<br />SalePrice: 162900<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2264<br />SalePrice: 110000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 3086<br />SalePrice: 184000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 3088<br />SalePrice: 159000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2475<br />SalePrice: 128000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 334<br />SalePrice: 39300<br />factor(Neighborhood): BrkSide\",\"TotalSF: 3140<br />SalePrice: 210000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2321<br />SalePrice: 113000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2697<br />SalePrice: 133000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 1929<br />SalePrice: 130000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 1829<br />SalePrice: 79500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 1064<br />SalePrice: 60000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 3042<br />SalePrice: 141500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 1437<br />SalePrice: 89000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 999<br />SalePrice: 52000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2496<br />SalePrice: 129000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2886<br />SalePrice: 100000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 3428<br />SalePrice: 223500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 1568<br />SalePrice: 109500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2233<br />SalePrice: 88000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 1875<br />SalePrice: 79900<br />factor(Neighborhood): BrkSide\",\"TotalSF: 1716<br />SalePrice: 116500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 3279<br />SalePrice: 167500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 1309<br />SalePrice: 85500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2282<br />SalePrice: 121600<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2236<br />SalePrice: 128000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2126<br />SalePrice: 102000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2577<br />SalePrice: 144000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2843<br />SalePrice: 149000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2448<br />SalePrice: 117000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 912<br />SalePrice: 80500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2811<br />SalePrice: 127500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 3108<br />SalePrice: 180500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2472<br />SalePrice: 132500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 1007<br />SalePrice: 72500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 1128<br />SalePrice: 82500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2498<br />SalePrice: 134900<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2393<br />SalePrice: 131000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2632<br />SalePrice: 137500<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2228<br />SalePrice: 120000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 3051<br />SalePrice: 207000<br />factor(Neighborhood): BrkSide\",\"TotalSF: 2096<br />SalePrice: 121000<br />factor(Neighborhood): BrkSide\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(207,148,0,1)\",\"opacity\":1,\"size\":5.6692913385826778,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(207,148,0,1)\"}},\"hoveron\":\"points\",\"name\":\"BrkSide\",\"legendgroup\":\"BrkSide\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[4258,2466,3418,3787,3732,3977,3187,3687,3351,2702,2268,4034,3642,3556,3362,3013,4206,3363,2907,2502,3677,3625,3784,4099,4205,3196,3442,3596],\"y\":[225000,180000,235000,190000,211000,277000,241500,192000,190000,130000,134432,240000,280000,256000,143000,173000,302000,187500,200000,155000,185000,240000,244400,328000,200500,161500,260000,190000],\"text\":[\"TotalSF: 4258<br />Sale"},{"title":"Logistic Thingys","url":"425Analyses/logisitcmodels.html","content":"Logistic Thingys 2025-03-23 library(tidyverse) library(mosaic) Question 1 Graph graph set.seed(31) n <- 50 x <- runif(n, 0, 10) beta0 <- -8.5 beta1 <- 1.7 p <- exp(beta0 + beta1*x)/(1+exp(beta0 + beta1*x)) y <- rbinom(n, 1, p) mydat <- data.frame(y, x) myglm <- glm(y ~ x, data=mydat, family=binomial) b <- coef(myglm) plot(y ~ x, mydat, pch=16, cex=1.2, col=rgb(0,.8,.0,.2)) curve(exp(beta0 + beta1*x)/(1+exp(beta0 + beta1*x)), lty=2, add=TRUE, col=\"green\") curve(exp(b[1] + b[2]*x)/(1+exp(b[1] + b[2]*x)), lty=1, add=TRUE, col=\"green\") Question 2 Desmos Graph Graph Graph set.seed(458) n <- 200 x <- runif(n, 0, 10) beta0 <- 5.5 beta1 <- -3 beta2 <- 0.3 p <- exp(beta0 + beta1*x + beta2*x^2)/(1+exp(beta0 + beta1*x + beta2*x^2)) y <- rbinom(n, 1, p) mydat <- data.frame(y, x) yourglm <- glm(y ~ x + I(x^2), data=mydat, family=binomial) g <- coef(yourglm) plot(y ~ x, mydat, pch=16, cex=1.2, col=rgb(0,.8,.0,.2)) curve(exp(beta0 + beta1*x + beta2*x^2)/(1+exp(beta0 + beta1*x + beta2*x^2)), lty=2, add=TRUE, col=\"green3\") curve(exp(g[1] + g[2]*x + g[3]*x^2)/(1+exp(g[1] + g[2]*x+ g[3]*x^2)), lty=1, add=TRUE, col=\"green3\") Graph 3 Desmos Graph Graph Graph set.seed(2782) n <- 200 x <- runif(n, 0, 10) x2 <- sample(c(0,1), n, replace=TRUE) beta0 <- 9.5 beta1 <- -1.3 beta2 <- 0.8 beta3 <- -3.2 p <- exp(beta0 + beta1*x + beta2*x2 + beta3*x*x2)/(1+exp(beta0 + beta1*x + beta2*x2 + beta3*x*x2)) y <- rbinom(n, 1, p) mydat <- data.frame(y, x) ourglm <- glm(y ~ x + x2 + x:x2, data=mydat, family=binomial) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred b <- coef(ourglm) b0 <- b[1] b1 <- b[2] b2 <- b[3] b3 <- b[4] ggplot(mydat, aes(x=x, y=y, color = as.factor(x2)))+ geom_point(alpha=0.5, size = 2) + geom_smooth(method= \"glm\", method.args = list(family=\"binomial\")) + stat_function(fun=function(x) exp(beta0 + beta1*x)/(1+exp(beta0 + beta1*x)), color = \"coral\", linetype = \"dashed\", lwd = 1) + stat_function(fun=function(x) exp(b0 + b1*x)/(1+exp(b0 + b1*x)), color = \"coral\", linetype = \"solid\", lwd = 1) + stat_function(fun=function(x) exp((beta0 + beta2) + (beta1 +beta3)*x)/(1+exp((beta0 + beta2) + (beta1 +beta3)*x)), color = \"cyan3\", linetype = \"dashed\", lwd = 1) + stat_function(fun=function(x) exp((b0+b2) + (b1+ b3)*x)/(1+exp((b0 + b2)+ (b1+b3)*x)), color = \"cyan3\", linetype = \"solid\", lwd = 1) + coord_cartesian(ylim = c(0,1), xlim = c(0,10)) ## `geom_smooth()` using formula = 'y ~ x' ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred library(tidyverse) m425 <- read.csv(\"~/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/Math425PastGrades.csv\", stringsAsFactors=TRUE) m425 <- m425 %>% mutate(f70 = ifelse(FinalExam > 70, 1, 0)) pairs(m425, panel=panel.smooth, col=rgb(.2,.2,.2,.2)) glm1 <- glm(f70 ~ Midterm, data=m425, family=binomial) summary(glm1) ## ## Call: ## glm(formula = f70 ~ Midterm, family = binomial, data = m425) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -7.08691 1.26692 -5.594 2.22e-08 *** ## Midterm 0.09298 0.01641 5.667 1.45e-08 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 207.28 on 149 degrees of freedom ## Residual deviance: 157.40 on 148 degrees of freedom ## (3 observations deleted due to missingness) ## AIC: 161.4 ## ## Number of Fisher Scoring iterations: 5 AIC(glm1) ## [1] 161.3968 glm2 <- glm(f70 ~ Midterm + MagicTwoGroups, data=m425, family=binomial) summary(glm2) ## ## Call: ## glm(formula = f70 ~ Midterm + MagicTwoGroups, family = binomial, ## data = m425) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -8.98420 1.50671 -5.963 2.48e-09 *** ## Midterm 0.08801 0.01692 5.201 1.98e-07 *** ## MagicTwoGroups 1.43432 0.41845 3.428 0.000609 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 207.28 on 149 degrees of freedom ## Residual deviance: 145.01 on 147 degrees of freedom ## (3 observations deleted due to missingness) ## AIC: 151.01 ## ## Number of Fisher Scoring iterations: 5 AIC(glm2) ## [1] 151.0149 glm3 <- glm(f70 ~ Midterm + MagicTwoGroups + Midterm:MagicTwoGroups, data=m425, family=binomial) summary(glm3) ## ## Call: ## glm(formula = f70 ~ Midterm + MagicTwoGroups + Midterm:MagicTwoGroups, ## family = binomial, data = m425) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 5.19736 4.02860 1.290 0.19701 ## Midterm -0.09959 0.05343 -1.864 0.06232 . ## MagicTwoGroups -8.87080 3.18558 -2.785 0.00536 ** ## Midterm:MagicTwoGroups 0.13624 0.04200 3.244 0.00118 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 207.28 on 149 degrees of freedom ## Residual deviance: 131.59 on 146 degrees of freedom ## (3 observations deleted due to missingness) ## AIC: 139.59 ## ## Number of Fisher Scoring iterations: 5 AIC(glm3) ## [1] 139.5884 glm4 <- glm(f70 ~ Midterm + MagicTwoGroups + Midterm:MagicTwoGroups, data=m425, family=binomial) summary(glm4) ## ## Call: ## glm(formula = f70 ~ Midterm + MagicTwoGroups + Midterm:MagicTwoGroups, ## family = binomial, data = m425) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 5.19736 4.02860 1.290 0.19701 ## Midterm -0.09959 0.05343 -1.864 0.06232 . ## MagicTwoGroups -8.87080 3.18558 -2.785 0.00536 ** ## Midterm:MagicTwoGroups 0.13624 0.04200 3.244 0.00118 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 207.28 on 149 degrees of freedom ## Residual deviance: 131.59 on 146 degrees of freedom ## (3 observations deleted due to missingness) ## AIC: 139.59 ## ## Number of Fisher Scoring iterations: 5 m425$ClassActivitiesCompletedPerfectly ## [1] Y Y Y N N N N N Y N N N Y Y Y N Y N Y Y Y Y N Y Y N N N N N Y Y Y Y Y N Y ## [38] N N Y Y N N Y N N N N N Y Y N N N Y N Y N N Y Y Y Y Y Y Y Y Y Y N Y Y Y Y ## [75] Y N Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y N Y Y Y Y Y Y N Y Y Y Y N Y Y N Y Y N ## [112] Y Y Y Y Y Y N Y Y Y N Y N Y Y Y N Y Y N N N Y Y Y Y N Y Y Y N Y Y N N Y Y ## [149] N N Y Y Y ## Levels: N Y glm5 <- glm(f70 ~ Midterm*AssessmentQuizzes*MagicTwoGroups*ClassActivitiesCompletedPerfectly, data=m425, family=binomial) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred summary(glm5) ## ## Call: ## glm(formula = f70 ~ Midterm * AssessmentQuizzes * MagicTwoGroups * ## ClassActivitiesCompletedPerfectly, family = binomial, data = m425) ## ## Coefficients: ## Estimate ## (Intercept) 2.105e+01 ## Midterm -6.156e-01 ## AssessmentQuizzes 4.024e-01 ## MagicTwoGroups -1.858e+01 ## ClassActivitiesCompletedPerfectlyY -2.763e+01 ## Midterm:AssessmentQuizzes 4.016e-04 ## Midterm:MagicTwoGroups 4.237e-01 ## AssessmentQuizzes:MagicTwoGroups -5.474e-01 ## Midterm:ClassActivitiesCompletedPerfectlyY 7.020e-01 ## AssessmentQuizzes:ClassActivitiesCompletedPerfectlyY -2.324e-01 ## MagicTwoGroups:ClassActivitiesCompletedPerfectlyY 1.869e+01 ## Midterm:AssessmentQuizzes:MagicTwoGroups 4.265e-03 ## Midterm:AssessmentQuizzes:ClassActivitiesCompletedPerfectlyY -2.709e-03 ## Midterm:MagicTwoGroups:ClassActivitiesCompletedPerfectlyY -4.460e-01 ## AssessmentQuizzes:MagicTwoGroups:ClassActivitiesCompletedPerfectlyY 4.519e-01 ## Midterm:AssessmentQuizzes:MagicTwoGroups:ClassActivitiesCompletedPerfectlyY -2.531e-03 ## Std. Error ## (Intercept) 3.694e+01 ## Midterm 5.677e-01 ## AssessmentQuizzes 1.174e+00 ## MagicTwoGroups 3.456e+01 ## ClassActivitiesCompletedPerfectlyY 4.035e+01 ## Midterm:AssessmentQuizzes 1.613e-02 ## Midterm:MagicTwoGroups 4.961e-01 ## AssessmentQuizzes:MagicTwoGroups 1.138e+00 ## Midterm:ClassActivitiesCompletedPerfectlyY 6.070e-01 ## AssessmentQuizzes:ClassActivitiesCompletedPerfectlyY 1.214e+00 ## MagicTwoGroups:ClassActivitiesCompletedPerfectlyY 3.695e+01 ## Midterm:AssessmentQuizzes:MagicTwoGroups 1.508e-02 ## Midterm:AssessmentQuizzes:ClassActivitiesCompletedPerfectlyY 1.661e-02 ## Midterm:MagicTwoGroups:ClassActivitiesCompletedPerfectlyY 5.249e-01 ## AssessmentQuizzes:MagicTwoGroups:ClassActivitiesCompletedPerfectlyY 1.162e+00 ## Midterm:AssessmentQuizzes:MagicTwoGroups:ClassActivitiesCompletedPerfectlyY 1.537e-02 ## z value ## (Intercept) 0.570 ## Midterm -1.084 ## AssessmentQuizzes 0.343 ## MagicTwoGroups -0.538 ## ClassActivitiesCompletedPerfectlyY -0.685 ## Midterm:AssessmentQuizzes 0.025 ## Midterm:MagicTwoGroups 0.854 ## AssessmentQuizzes:MagicTwoGroups -0.481 ## Midterm:ClassActivitiesCompletedPerfectlyY 1.156 ## AssessmentQuizzes:ClassActivitiesCompletedPerfectlyY -0.192 ## MagicTwoGroups:ClassActivitiesCompletedPerfectlyY 0.506 ## Midterm:AssessmentQuizzes:MagicTwoGroups 0.283 ## Midterm:AssessmentQuizzes:ClassActivitiesCompletedPerfectlyY -0.163 ## Midterm:MagicTwoGroups:ClassActivitiesCompletedPerfectlyY -0.850 ## AssessmentQuizzes:MagicTwoGroups:ClassActivitiesCompletedPerfectlyY 0.389 ## Midterm:AssessmentQuizzes:MagicTwoGroups:ClassActivitiesCompletedPerfectlyY -0.165 ## Pr(>|z|) ## (Intercept) 0.569 ## Midterm 0.278 ## AssessmentQuizzes 0.732 ## MagicTwoGroups 0.591 ## ClassActivitiesCompletedPerfectlyY 0.494 ## Midterm:AssessmentQuizzes 0.980 ## Midterm:MagicTwoGroups 0.393 ## AssessmentQuizzes:MagicTwoGroups 0.630 ## Midterm:ClassActivitiesCompletedPerfectlyY 0.247 ## AssessmentQuizzes:ClassActivitiesCompletedPerfectlyY 0.848 ## MagicTwoGroups:ClassActivitiesCompletedPerfectlyY 0.613 ## Midterm:AssessmentQuizzes:MagicTwoGroups 0.777 ## Midterm:AssessmentQuizzes:ClassActivitiesCompletedPerfectlyY 0.870 ## Midterm:MagicTwoGroups:ClassActivitiesCompletedPerfectlyY 0.395 ## AssessmentQuizzes:MagicTwoGroups:ClassActivitiesCompletedPerfectlyY 0.697 ## Midterm:AssessmentQuizzes:MagicTwoGroups:ClassActivitiesCompletedPerfectlyY 0.869 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 207.277 on 149 degrees of freedom ## Residual deviance: 99.165 on 134 degrees of freedom ## (3 observations deleted due to missingness) ## AIC: 131.17 ## ## Number of Fisher Scoring iterations: 9 glm5 <- glm(f70 ~ Midterm*MagicTwoGroups + AssessmentQuizzes + ClassActivitiesCompletedPerfectly, data=m425, family=binomial) summary(glm5) ## ## Call: ## glm(formula = f70 ~ Midterm * MagicTwoGroups + AssessmentQuizzes + ## ClassActivitiesCompletedPerfectly, family = binomial, data = m425) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.37063 4.48194 0.529 0.596854 ## Midterm -0.08967 0.05991 -1.497 0.134480 ## MagicTwoGroups -7.93086 3.34521 -2.371 0.017749 * ## AssessmentQuizzes 0.04817 0.01452 3.317 0.000911 *** ## ClassActivitiesCompletedPerfectlyY 0.78083 0.50661 1.541 0.123246 ## Midterm:MagicTwoGroups 0.11636 0.04459 2.610 0.009067 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 207.28 on 149 degrees of freedom ## Residual deviance: 113.25 on 144 degrees of freedom ## (3 observations deleted due to missingness) ## AIC: 125.25 ## ## Number of Fisher Scoring iterations: 6 predict(glm5, data.frame(Midterm=70, AssessmentQuizzes=33, ClassActivitiesCompletedPerfectly=\"Y\",MagicTwoGroups=1), type=\"response\") ## 1 ## 0.2104901 summary(m425[,c(\"Midterm\",\"AssessmentQuizzes\",\"MagicTwoGroups\",\"ClassActivitiesCompletedPerfectly\")]) ## Midterm AssessmentQuizzes MagicTwoGroups ## Min. : 28.00 Min. : 0.00 Min. :1.000 ## 1st Qu.: 62.00 1st Qu.:33.33 1st Qu.:1.000 ## Median : 76.00 Median :53.33 Median :2.000 ## Mean : 73.17 Mean :50.81 Mean :1.546 ## 3rd Qu.: 85.50 3rd Qu.:66.67 3rd Qu.:2.000 ## Max. :100.00 Max. :93.33 Max. :2.000 ## NA's :2 NA's :1 ## ClassActivitiesCompletedPerfectly ## N: 53 ## Y:100 ## ## ## ## ## library(ResourceSelection) ## ResourceSelection 0.3-6 2023-06-27 hoslem.test(glm5$y, glm5$fit, g=10) ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: glm5$y, glm5$fit ## X-squared = 6.9876, df = 8, p-value = 0.538 plot(f70 ~ Midterm, data=m425) b <- coef(glm5) b ## (Intercept) Midterm ## 2.37063354 -0.08967274 ## MagicTwoGroups AssessmentQuizzes ## -7.93085833 0.04816615 ## ClassActivitiesCompletedPerfectlyY Midterm:MagicTwoGroups ## 0.78082787 0.11635760 drawit <- function(Magic=1, AQ=33, CA=1){ curve(1/(1+exp(-(b[1] + b[2]*Midterm + b[3]*Magic + b[4]*AQ + b[5]*CA + b[6]*Magic*Midterm))), add=TRUE, xname=\"Midterm\") } drawit(Magic=1, AQ=33, CA=1) points(70, 0.21, col=\"green\", cex=4, pch=16) keep <- sample(1:nrow(m425), 90) mytrain <- m425[keep,] mytest <- m425[-keep,] glm.test <- glm(f70 ~ Midterm*MagicTwoGroups + AssessmentQuizzes + ClassActivitiesCompletedPerfectly, data=mytrain, family=binomial) mypreds <- predict(glm.test, newdata=mytest, type=\"response\") mydecs <- ifelse(mypreds > 0.5, 1, 0) cm <- table(mydecs, mytest$f70) pcc <- (cm[1] + cm[4]) / sum(cm) pcc ## [1] 0.8196721 // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":" Assessing Influential Points in Regression ","url":"425Analyses/OutlierAnalysis.html","content":"Code Show All Code Hide All Code Assessing Influential Points in Regression library(tidyverse) library(pander) library(mosaic) library(olsrr) library(car) library(DT) library(robustbase) library(sjPlot) library(ggplot2) library(broom) outie <- tibble( Point = c( 1, 2, 3, 4, 5), Value = c(0.8, 2.3, 2.8, 0, 5.3) ) mydata <- data.frame( x = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 23, 4.5, 22), y = c(2, 1, 4, 1.3, 3, 5, 4.2, 16.1, 18.2, 20.3,21, 23,40,56)) Objectives In this document we will discuss… Leverage and points that should be classified as overly influential on the regression parameter estimates and removed from a regression. Cook’s Distance measurement for points in a regression. Apply robust regression to mitigate the effects of outliers. ggplot(mydata, aes(x=x, y=y)) + geom_point(color = \"dodgerblue\", pch = 19, size = 4) + theme_bw() Outliers Linear regressions can be SUPER sensitive to unusual data: outliers, high leverage values, or a combination of the two. Outliers are the points that fall far from the other data points and or the visual pattern seen in a scatter plot. Through their deviation from the overall pattern, they can pull and effect the regression line to give us both an inaccurate visual representation and statistical results of the regression. In this Residuals vs. Leverage plot, it compares a point’s leverage (x-axis) and studentized residuals (y-axis, standardizes the residuals by dividing them by an estimate of its standard deviation) to identify the different types of unusual points. library(grid) model <- lm(y ~ x, data = mydata) ols_plot_resid_lev(model) Identifying these outliers can be visually easy within smaller data sets and simpler regression. Yet, as the regressions get more complex and the data sets become more extensive, then locating these negatively effecting points can get tedious and difficult. Fortunately, there are two sources of measurement, Cook’s Distance and Leverage Values, that can help us detect and quantify the presence of these outliers. Additionally, once we can confirm the existence of outliers and their influence in a data set, we can use Robust Regression to lessen their effect and give us accurate and interpretative results. Leverage Values Leverage/Influential points are measures how extreme a data point’s predictor values (its x-values) are compared to the rest of the data set using values between 1 and 0. 1 : lots of leverage / pulling the regression towards itself (falls outside the overall pattern, being highly influential) 0 : just “one of many”, not unduly influencing the regression line Click through the tabs to explore this concept more in depth Hide Exploration Mathematical Perspective Mathematically, there are a lot of layers when it comes to understanding leverage values. But follow along and I will explain every step of the way! 1) Starting Point: Simple Linear Regression The foundation is the basic linear regression equation that relates a response variable to a predictor: \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] 2) Vector Representation In our model, each variable (\\(Y_i\\), \\(X_i\\), and \\(\\epsilon_i\\)) represents an individual observation at position i. The coefficients \\(\\beta_0\\) and \\(\\beta_1\\) measure the relationships between variables. To account for all n observations in our dataset, we use vectors instead of individual values. A column of 1’s is included alongside \\(\\beta_0\\) to represent the intercept term. \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] 3) Matrix Organization Once we have our variables in vector form, we can arrange them into a matrix. This matrix combines our predictor variables (both intercept and slope) with our parameter vector (\\(\\beta_0\\) and \\(\\beta_1\\)), creating an organized structure for our calculations. \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] 4) Simplified Vector Notation We can then express our matrix equation more concisely using arrow notation: \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] 5) Sum of Squared Errors (SSE) While our model can accommodate various combinations of \\(\\beta_0\\) and \\(\\beta_1\\), our goal is to find the specific values that minimize the Sum of Squared Errors (SSE). The SSE quantifies how well our model fits the data by measuring the total deviation of our predictions from the actual values. A smaller SSE indicates more accurate predictions. Thus, our SSE can be written as the following notation: \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] 6) Vector Expression of SSE We can express the Sum of Squared Errors more elegantly using vector notation. By multiplying the error vector (\\(\\vec{\\epsilon}\\)) by its transpose (\\(\\vec{\\epsilon}^t\\)), we compute the sum of squared errors. This multiplication effectively squares each error term, giving us an equivalent expression to \\(\\epsilon^2\\). \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] 7) Rearranging for Error Vector Starting with our original equation from step 4, we can isolate the error vector \\(\\vec{\\epsilon}\\). Since this equation contains \\(\\vec{\\epsilon}\\), we can substitute it into our SSE formula to create a comprehensive expression for the error term. \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] 8) Taking the Derivate To find the optimal values for our coefficients (\\(\\vec{\\beta}\\)) that minimize SSE, we use calculus. By taking the derivative of the SSE function, we can determine where the rate of change equals zero, indicating the point of minimum error. This process involves matrix calculus and leads us to the values that best fit our observed data. \\[ \\frac{d}{d\\mathbf{\\beta}} (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta})^T (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta}) \\] \\[ (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta})^T (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta}) = (\\mathbf{Y}^T - \\mathbf{\\beta}^T\\mathbf{X}^T) (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta}) \\] \\[ = \\mathbf{Y}^T\\mathbf{Y} - \\mathbf{Y}^T\\mathbf{X}\\mathbf{\\beta} - \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{Y} + \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} \\] \\[ \\frac{d}{d\\mathbf{\\beta}} (\\mathbf{Y}^T\\mathbf{Y} - \\mathbf{Y}^T\\mathbf{X}\\mathbf{\\beta} - \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{Y} + \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta}) \\] \\[ \\frac{d}{d\\mathbf{\\beta}} (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta})^T (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta}) = 0 - \\mathbf{X}^T\\mathbf{Y} - \\mathbf{X}^T\\mathbf{Y} + 2\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X} \\] \\[ = -2\\mathbf{X}^T\\mathbf{Y} + 2\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} \\] \\[ = -2\\mathbf{X}^T(\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta}) \\] Overall, our process has led us to this result that we can then rewrite in the simlar notation of vectors: \\[ \\frac{d}{d\\mathbf{\\beta}} (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta})^T (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta}) = -2\\mathbf{X}^T(\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta}) \\] \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] 9) Setting Derivative to Zero To minimize the SSE, we set our derived function equal to a zero vector and solve for the beta vector. This mathematical step helps us find the coefficients that will produce the smallest possible error in our model. \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\\\ \\frac{-2\\mathbf{X}^t\\vec{Y}}{-2} = \\frac{-2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}}{-2} \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\\\ (\\mathbf{X}^t\\mathbf{X})^{-1} * (\\mathbf{X}^t\\vec{Y}) = (\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) * (\\mathbf{X}^t\\mathbf{X})^{-1} \\\\ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] 10) Estimating Beta Coefficients We replace the beta vector (\\(\\beta\\)) with a lowercase b vector to indicate these are estimates rather than true population parameters. These estimated coefficients represent how much our dependent variable changes for each one-unit increase in an independent variable, holding all other variables constant. \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] 11) Predicted Values To complete our model, we replace Y with Ŷ (Y-hat) to indicate these are predicted values rather than actual observations. This notation clarifies that our model generates estimates based on our data rather than exact values. \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] 12) Combining Equations By substituting our b vector into the original equation, we can express our predicted values (Ŷ) in terms of what’s known as the Hat Matrix. This matrix transformation connects our observed values to their predictions. \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\leftarrow \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\\\ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] 13) The Hat Matrix The Hat Matrix transforms observed values into predicted values. Its diagonal elements (\\(h_{ii}\\)) are called leverage values, which measure how much influence each data point has on the model’s predictions. These values quantify how strongly each observation “pulls” the regression line toward itself. \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] Demonstrated in R Thankfully, we don’t have to go through that entire calculation process when we instead have the hatvalues(lmname) function in R Studio to calculate these for us! Additionally, we can graphically depict them as well with `plot(lmname, which=5) out.lm <- lm(Value ~ Point, data=outie) hatvalues(out.lm) %>% pander() 1 2 3 4 5 0.6 0.3 0.2 0.3 0.6 plot(out.lm, which=5) With this graph, another thing that it visually shows is the Cook’s Distance of each points, and its with points that have high leverage and large Cook’s Distance that we should reconsider for accuracy or possibly removing from the regression. But what is Cook’s Distance? Real Life Analogy Think of a group photo where everyone is standing together. Most people (leverage value close to 0) are just part of the group, blending in with others. However, imagine one person standing far off to the side by themselves (leverage value close to 1). When you try to frame the photo, you have to adjust the WHOLE composition just to include that one person - they’re “pulling” the frame towards them, just like a high-leverage point pulls the regression line towards itself. Cook’s Distances Cook’s Distance is a statistical diagnostic tool used in regression analysis to identify influential data points that might disproportionately affect the results of a regression model. It combines two key concepts when measuring the impact each individual point has on the regression estimates: Outlyingness : how unusual a data point is Leverage: how much influence a data point has on the regression line due to its position (as looked at previously) Click through the tabs to explore this concept more in depth Hide Exploration Mathematical Perspective Calculating Cook’s Distance mathematically can be visualized with the following steps: Given a data set Cars predicting car price based on different x/predictor values. mtcarz <- mtcars %>% select(mpg, cyl, disp, hp, drat, wt) set.seed(123) mtcarz$salesprice <- sample(15000:30000, nrow(mtcarz), replace = TRUE) datatable(mtcarz, options = list( pageLength = 3, lengthMenu = c(3,10,30))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"Mazda RX4\",\"Mazda RX4 Wag\",\"Datsun 710\",\"Hornet 4 Drive\",\"Hornet Sportabout\",\"Valiant\",\"Duster 360\",\"Merc 240D\",\"Merc 230\",\"Merc 280\",\"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\"Merc 450SLC\",\"Cadillac Fleetwood\",\"Lincoln Continental\",\"Chrysler Imperial\",\"Fiat 128\",\"Honda Civic\",\"Toyota Corolla\",\"Toyota Corona\",\"Dodge Challenger\",\"AMC Javelin\",\"Camaro Z28\",\"Pontiac Firebird\",\"Fiat X1-9\",\"Porsche 914-2\",\"Lotus Europa\",\"Ford Pantera L\",\"Ferrari Dino\",\"Maserati Bora\",\"Volvo 142E\"],[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],[6,6,4,6,8,6,8,4,4,6,6,8,8,8,8,8,8,4,4,4,4,8,8,8,8,4,4,4,8,6,8,4],[160,160,108,258,360,225,360,146.7,140.8,167.6,167.6,275.8,275.8,275.8,472,460,440,78.7,75.7,71.09999999999999,120.1,318,304,350,400,79,120.3,95.09999999999999,351,145,301,121],[110,110,93,110,175,105,245,62,95,123,123,180,180,180,205,215,230,66,52,65,97,150,150,245,175,66,91,113,264,175,335,109],[3.9,3.9,3.85,3.08,3.15,2.76,3.21,3.69,3.92,3.92,3.92,3.07,3.07,3.07,2.93,3,3.23,4.08,4.93,4.22,3.7,2.76,3.15,3.73,3.08,4.08,4.43,3.77,4.22,3.62,3.54,4.11],[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],[17462,17510,25418,23717,27482,17985,16841,24333,18370,28540,28555,28325,26637,19760,21745,24818,17756,20106,28402,24144,27635,24208,25204,28666,17887,21169,17566,24641,28960,24981,27498,17979]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>mpg<\\/th>\\n <th>cyl<\\/th>\\n <th>disp<\\/th>\\n <th>hp<\\/th>\\n <th>drat<\\/th>\\n <th>wt<\\/th>\\n <th>salesprice<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":3,\"lengthMenu\":[3,10,30],\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"mpg\",\"targets\":1},{\"name\":\"cyl\",\"targets\":2},{\"name\":\"disp\",\"targets\":3},{\"name\":\"hp\",\"targets\":4},{\"name\":\"drat\",\"targets\":5},{\"name\":\"wt\",\"targets\":6},{\"name\":\"salesprice\",\"targets\":7}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} **Computing the Model The model is then trained and creates a new row of predicted \\(\\hat{Y}\\) values with all the cars in the data set set.seed(373) mtcarz$salesprice_yhat <- sample(15000:30000, nrow(mtcarz), replace = TRUE) datatable(mtcarz, options = list( pageLength = 3, lengthMenu = c(3,10,30))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"Mazda RX4\",\"Mazda RX4 Wag\",\"Datsun 710\",\"Hornet 4 Drive\",\"Hornet Sportabout\",\"Valiant\",\"Duster 360\",\"Merc 240D\",\"Merc 230\",\"Merc 280\",\"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\"Merc 450SLC\",\"Cadillac Fleetwood\",\"Lincoln Continental\",\"Chrysler Imperial\",\"Fiat 128\",\"Honda Civic\",\"Toyota Corolla\",\"Toyota Corona\",\"Dodge Challenger\",\"AMC Javelin\",\"Camaro Z28\",\"Pontiac Firebird\",\"Fiat X1-9\",\"Porsche 914-2\",\"Lotus Europa\",\"Ford Pantera L\",\"Ferrari Dino\",\"Maserati Bora\",\"Volvo 142E\"],[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],[6,6,4,6,8,6,8,4,4,6,6,8,8,8,8,8,8,4,4,4,4,8,8,8,8,4,4,4,8,6,8,4],[160,160,108,258,360,225,360,146.7,140.8,167.6,167.6,275.8,275.8,275.8,472,460,440,78.7,75.7,71.09999999999999,120.1,318,304,350,400,79,120.3,95.09999999999999,351,145,301,121],[110,110,93,110,175,105,245,62,95,123,123,180,180,180,205,215,230,66,52,65,97,150,150,245,175,66,91,113,264,175,335,109],[3.9,3.9,3.85,3.08,3.15,2.76,3.21,3.69,3.92,3.92,3.92,3.07,3.07,3.07,2.93,3,3.23,4.08,4.93,4.22,3.7,2.76,3.15,3.73,3.08,4.08,4.43,3.77,4.22,3.62,3.54,4.11],[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],[17462,17510,25418,23717,27482,17985,16841,24333,18370,28540,28555,28325,26637,19760,21745,24818,17756,20106,28402,24144,27635,24208,25204,28666,17887,21169,17566,24641,28960,24981,27498,17979],[25461,15471,18337,29084,17468,19143,25668,29647,18987,29355,17776,15214,20299,22301,25535,19371,27034,26042,17403,17763,27521,21838,28917,15812,19691,22430,26959,17165,19788,16219,28616,29821]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>mpg<\\/th>\\n <th>cyl<\\/th>\\n <th>disp<\\/th>\\n <th>hp<\\/th>\\n <th>drat<\\/th>\\n <th>wt<\\/th>\\n <th>salesprice<\\/th>\\n <th>salesprice_yhat<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":3,\"lengthMenu\":[3,10,30],\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"mpg\",\"targets\":1},{\"name\":\"cyl\",\"targets\":2},{\"name\":\"disp\",\"targets\":3},{\"name\":\"hp\",\"targets\":4},{\"name\":\"drat\",\"targets\":5},{\"name\":\"wt\",\"targets\":6},{\"name\":\"salesprice\",\"targets\":7},{\"name\":\"salesprice_yhat\",\"targets\":8}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Now we will see if one specific car had an impact on the data set by excluding it from this new calculation Produces another y hat, but it is now y hat “i”, with “i” being that specific car/ data point that we took out (the row we excluded) - This process will repeat as many time as there are rows in the data set set.seed(287) mtcarz$salesprice_yhat_withoutspecificcar <- sample(15000:30000, nrow(mtcarz), replace = TRUE) datatable(mtcarz, options = list( pageLength = 3, lengthMenu = c(3,10,30))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"Mazda RX4\",\"Mazda RX4 Wag\",\"Datsun 710\",\"Hornet 4 Drive\",\"Hornet Sportabout\",\"Valiant\",\"Duster 360\",\"Merc 240D\",\"Merc 230\",\"Merc 280\",\"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\"Merc 450SLC\",\"Cadillac Fleetwood\",\"Lincoln Continental\",\"Chrysler Imperial\",\"Fiat 128\",\"Honda Civic\",\"Toyota Corolla\",\"Toyota Corona\",\"Dodge Challenger\",\"AMC Javelin\",\"Camaro Z28\",\"Pontiac Firebird\",\"Fiat X1-9\",\"Porsche 914-2\",\"Lotus Europa\",\"Ford Pantera L\",\"Ferrari Dino\",\"Maserati Bora\",\"Volvo 142E\"],[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],[6,6,4,6,8,6,8,4,4,6,6,8,8,8,8,8,8,4,4,4,4,8,8,8,8,4,4,4,8,6,8,4],[160,160,108,258,360,225,360,146.7,140.8,167.6,167.6,275.8,275.8,275.8,472,460,440,78.7,75.7,71.09999999999999,120.1,318,304,350,400,79,120.3,95.09999999999999,351,145,301,121],[110,110,93,110,175,105,245,62,95,123,123,180,180,180,205,215,230,66,52,65,97,150,150,245,175,66,91,113,264,175,335,109],[3.9,3.9,3.85,3.08,3.15,2.76,3.21,3.69,3.92,3.92,3.92,3.07,3.07,3.07,2.93,3,3.23,4.08,4.93,4.22,3.7,2.76,3.15,3.73,3.08,4.08,4.43,3.77,4.22,3.62,3.54,4.11],[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],[17462,17510,25418,23717,27482,17985,16841,24333,18370,28540,28555,28325,26637,19760,21745,24818,17756,20106,28402,24144,27635,24208,25204,28666,17887,21169,17566,24641,28960,24981,27498,17979],[25461,15471,18337,29084,17468,19143,25668,29647,18987,29355,17776,15214,20299,22301,25535,19371,27034,26042,17403,17763,27521,21838,28917,15812,19691,22430,26959,17165,19788,16219,28616,29821],[26556,18817,18975,20388,15578,21001,21370,16149,27032,27226,26736,16268,25129,23467,25371,16915,19120,29720,19537,20876,20796,19925,20529,20406,22023,20335,21062,24068,29670,17696,28905,23104]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>mpg<\\/th>\\n <th>cyl<\\/th>\\n <th>disp<\\/th>\\n <th>hp<\\/th>\\n <th>drat<\\/th>\\n <th>wt<\\/th>\\n <th>salesprice<\\/th>\\n <th>salesprice_yhat<\\/th>\\n <th>salesprice_yhat_withoutspecificcar<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":3,\"lengthMenu\":[3,10,30],\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9]},{\"orderable\":false,\"t"},{"title":" Assessing Influential Points in Regression ","url":"425Analyses/OutlierTheoryAssignment.html","content":"Code Show All Code Hide All Code Assessing Influential Points in Regression library(tidyverse) library(pander) library(mosaic) library(olsrr) library(car) library(DT) library(robustbase) library(sjPlot) library(ggplot2) library(broom) outie <- tibble( Point = c( 1, 2, 3, 4, 5), Value = c(0.8, 2.3, 2.8, 0, 5.3) ) mydata <- data.frame( x = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 23, 4.5, 22), y = c(2, 1, 4, 1.3, 3, 5, 4.2, 16.1, 18.2, 20.3,21, 23,40,56)) Objectives In this document we will discuss… Leverage and points that should be classified as overly influential on the regression parameter estimates and removed from a regression. Cook’s Distance measurement for points in a regression. Apply robust regression to mitigate the effects of outliers. ggplot(mydata, aes(x=x, y=y)) + geom_point(color = \"dodgerblue\", pch = 19, size = 4) + theme_bw() Outliers Linear regressions can be SUPER sensitive to unusual data: outliers, high leverage values, or a combination of the two. Outliers are the points that fall far from the other data points and or the visual pattern seen in a scatter plot. Through their deviation from the overall pattern, they can pull and effect the regression line to give us both an inaccurate visual representation and statistical results of the regression. In this Residuals vs. Leverage plot, it compares a point’s leverage (x-axis) and studentized residuals (y-axis, standardizes the residuals by dividing them by an estimate of its standard deviation) to identify the different types of unusual points. library(grid) model <- lm(y ~ x, data = mydata) ols_plot_resid_lev(model) Identifying these outliers can be visually easy within smaller data sets and simpler regression. Yet, as the regressions get more complex and the data sets become more extensive, then locating these negatively effecting points can get tedious and difficult. Fortunately, there are two sources of measurement, Cook’s Distance and Leverage Values, that can help us detect and quantify the presence of these outliers. Additionally, once we can confirm the existence of outliers and their influence in a data set, we can use Robust Regression to lessen their effect and give us accurate and interpretative results. Leverage Values Leverage/Influential points are measures how extreme a data point’s predictor values (its x-values) are compared to the rest of the data set using values between 1 and 0. 1 : lots of leverage / pulling the regression towards itself (falls outside the overall pattern, being highly influential) 0 : just “one of many”, not unduly influencing the regression line Click through the tabs to explore this concept more in depth Hide Exploration Mathematical Perspective Mathematically, there are a lot of layers when it comes to understanding leverage values. But follow along and I will explain every step of the way! 1) Starting Point: Simple Linear Regression The foundation is the basic linear regression equation that relates a response variable to a predictor: \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] 2) Vector Representation In our model, each variable (\\(Y_i\\), \\(X_i\\), and \\(\\epsilon_i\\)) represents an individual observation at position i. The coefficients \\(\\beta_0\\) and \\(\\beta_1\\) measure the relationships between variables. To account for all n observations in our dataset, we use vectors instead of individual values. A column of 1’s is included alongside \\(\\beta_0\\) to represent the intercept term. \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\beta_0 \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] + \\beta_1 \\left[\\begin{array}{c} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] 3) Matrix Organization Once we have our variables in vector form, we can arrange them into a matrix. This matrix combines our predictor variables (both intercept and slope) with our parameter vector (\\(\\beta_0\\) and \\(\\beta_1\\)), creating an organized structure for our calculations. \\[ \\left[ \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{array}\\right] = \\left[ \\begin{array}{cc} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{array}\\right] \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right] + \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right] \\] 4) Simplified Vector Notation We can then express our matrix equation more concisely using arrow notation: \\[ \\vec{Y} = \\mathbf{X}\\vec{\\beta} + \\vec{\\epsilon} \\] 5) Sum of Squared Errors (SSE) While our model can accommodate various combinations of \\(\\beta_0\\) and \\(\\beta_1\\), our goal is to find the specific values that minimize the Sum of Squared Errors (SSE). The SSE quantifies how well our model fits the data by measuring the total deviation of our predictions from the actual values. A smaller SSE indicates more accurate predictions. Thus, our SSE can be written as the following notation: \\[ \\sum_{i=1}^n \\epsilon_i ^2 \\] 6) Vector Expression of SSE We can express the Sum of Squared Errors more elegantly using vector notation. By multiplying the error vector (\\(\\vec{\\epsilon}\\)) by its transpose (\\(\\vec{\\epsilon}^t\\)), we compute the sum of squared errors. This multiplication effectively squares each error term, giving us an equivalent expression to \\(\\epsilon^2\\). \\[ \\vec{\\epsilon}^t \\vec{\\epsilon} = \\sum_{i=1}^n \\epsilon_i ^2 \\] 7) Rearranging for Error Vector Starting with our original equation from step 4, we can isolate the error vector \\(\\vec{\\epsilon}\\). Since this equation contains \\(\\vec{\\epsilon}\\), we can substitute it into our SSE formula to create a comprehensive expression for the error term. \\[ \\vec{\\epsilon} = \\vec{Y} - \\mathbf{X}\\vec{\\beta} \\] \\[ \\sum_{i=1}^n \\epsilon_i^2 = \\vec{\\epsilon}^t \\vec{\\epsilon} = (\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] 8) Taking the Derivate To find the optimal values for our coefficients (\\(\\vec{\\beta}\\)) that minimize SSE, we use calculus. By taking the derivative of the SSE function, we can determine where the rate of change equals zero, indicating the point of minimum error. This process involves matrix calculus and leads us to the values that best fit our observed data. \\[ \\frac{d}{d\\mathbf{\\beta}} (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta})^T (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta}) \\] \\[ (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta})^T (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta}) = (\\mathbf{Y}^T - \\mathbf{\\beta}^T\\mathbf{X}^T) (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta}) \\] \\[ = \\mathbf{Y}^T\\mathbf{Y} - \\mathbf{Y}^T\\mathbf{X}\\mathbf{\\beta} - \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{Y} + \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} \\] \\[ \\frac{d}{d\\mathbf{\\beta}} (\\mathbf{Y}^T\\mathbf{Y} - \\mathbf{Y}^T\\mathbf{X}\\mathbf{\\beta} - \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{Y} + \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta}) \\] \\[ \\frac{d}{d\\mathbf{\\beta}} (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta})^T (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta}) = 0 - \\mathbf{X}^T\\mathbf{Y} - \\mathbf{X}^T\\mathbf{Y} + 2\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X} \\] \\[ = -2\\mathbf{X}^T\\mathbf{Y} + 2\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} \\] \\[ = -2\\mathbf{X}^T(\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta}) \\] Overall, our process has led us to this result that we can then rewrite in the simlar notation of vectors: \\[ \\frac{d}{d\\mathbf{\\beta}} (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta})^T (\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta}) = -2\\mathbf{X}^T(\\mathbf{Y} - \\mathbf{X}\\mathbf{\\beta}) \\] \\[ \\frac{d}{d\\vec{\\beta}}(\\vec{Y} - \\mathbf{X}\\vec{\\beta})^t (\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) \\] 9) Setting Derivative to Zero To minimize the SSE, we set our derived function equal to a zero vector and solve for the beta vector. This mathematical step helps us find the coefficients that will produce the smallest possible error in our model. \\[ -2\\mathbf{X}^t(\\vec{Y} - \\mathbf{X}\\vec{\\beta}) = \\vec{0} \\\\ -2\\mathbf{X}^t\\vec{Y} = -2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\\\ \\frac{-2\\mathbf{X}^t\\vec{Y}}{-2} = \\frac{-2\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}}{-2} \\\\ \\mathbf{X}^t\\vec{Y} = \\mathbf{X}^t\\mathbf{X}\\vec{\\beta} \\\\ (\\mathbf{X}^t\\mathbf{X})^{-1} * (\\mathbf{X}^t\\vec{Y}) = (\\mathbf{X}^t\\mathbf{X}\\vec{\\beta}) * (\\mathbf{X}^t\\mathbf{X})^{-1} \\\\ (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} = \\vec{\\beta} \\] 10) Estimating Beta Coefficients We replace the beta vector (\\(\\beta\\)) with a lowercase b vector to indicate these are estimates rather than true population parameters. These estimated coefficients represent how much our dependent variable changes for each one-unit increase in an independent variable, holding all other variables constant. \\[ \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] 11) Predicted Values To complete our model, we replace Y with Ŷ (Y-hat) to indicate these are predicted values rather than actual observations. This notation clarifies that our model generates estimates based on our data rather than exact values. \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\] 12) Combining Equations By substituting our b vector into the original equation, we can express our predicted values (Ŷ) in terms of what’s known as the Hat Matrix. This matrix transformation connects our observed values to their predictions. \\[ \\hat{\\vec{Y}} = \\mathbf{X}\\vec{b} \\leftarrow \\vec{b} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\\\ \\hat{\\vec{Y}} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\vec{Y} \\] 13) The Hat Matrix The Hat Matrix transforms observed values into predicted values. Its diagonal elements (\\(h_{ii}\\)) are called leverage values, which measure how much influence each data point has on the model’s predictions. These values quantify how strongly each observation “pulls” the regression line toward itself. \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\] \\[ \\hat{\\vec{Y}} = \\mathbf{H}\\vec{Y} \\] Demonstrated in R Thankfully, we don’t have to go through that entire calculation process when we instead have the hatvalues(lmname) function in R Studio to calculate these for us! Additionally, we can graphically depict them as well with `plot(lmname, which=5) out.lm <- lm(Value ~ Point, data=outie) hatvalues(out.lm) %>% pander() 1 2 3 4 5 0.6 0.3 0.2 0.3 0.6 plot(out.lm, which=5) With this graph, another thing that it visually shows is the Cook’s Distance of each points, and its with points that have high leverage and large Cook’s Distance that we should reconsider for accuracy or possibly removing from the regression. But what is Cook’s Distance? Real Life Analogy Think of a group photo where everyone is standing together. Most people (leverage value close to 0) are just part of the group, blending in with others. However, imagine one person standing far off to the side by themselves (leverage value close to 1). When you try to frame the photo, you have to adjust the WHOLE composition just to include that one person - they’re “pulling” the frame towards them, just like a high-leverage point pulls the regression line towards itself. Cook’s Distances Cook’s Distance is a statistical diagnostic tool used in regression analysis to identify influential data points that might disproportionately affect the results of a regression model. It combines two key concepts when measuring the impact each individual point has on the regression estimates: Outlyingness : how unusual a data point is Leverage: how much influence a data point has on the regression line due to its position (as looked at previously) Click through the tabs to explore this concept more in depth Hide Exploration Mathematical Perspective Calculating Cook’s Distance mathematically can be visualized with the following steps: Given a data set Cars predicting car price based on different x/predictor values. mtcarz <- mtcars %>% select(mpg, cyl, disp, hp, drat, wt) set.seed(123) mtcarz$salesprice <- sample(15000:30000, nrow(mtcarz), replace = TRUE) datatable(mtcarz, options = list( pageLength = 3, lengthMenu = c(3,10,30))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"Mazda RX4\",\"Mazda RX4 Wag\",\"Datsun 710\",\"Hornet 4 Drive\",\"Hornet Sportabout\",\"Valiant\",\"Duster 360\",\"Merc 240D\",\"Merc 230\",\"Merc 280\",\"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\"Merc 450SLC\",\"Cadillac Fleetwood\",\"Lincoln Continental\",\"Chrysler Imperial\",\"Fiat 128\",\"Honda Civic\",\"Toyota Corolla\",\"Toyota Corona\",\"Dodge Challenger\",\"AMC Javelin\",\"Camaro Z28\",\"Pontiac Firebird\",\"Fiat X1-9\",\"Porsche 914-2\",\"Lotus Europa\",\"Ford Pantera L\",\"Ferrari Dino\",\"Maserati Bora\",\"Volvo 142E\"],[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],[6,6,4,6,8,6,8,4,4,6,6,8,8,8,8,8,8,4,4,4,4,8,8,8,8,4,4,4,8,6,8,4],[160,160,108,258,360,225,360,146.7,140.8,167.6,167.6,275.8,275.8,275.8,472,460,440,78.7,75.7,71.09999999999999,120.1,318,304,350,400,79,120.3,95.09999999999999,351,145,301,121],[110,110,93,110,175,105,245,62,95,123,123,180,180,180,205,215,230,66,52,65,97,150,150,245,175,66,91,113,264,175,335,109],[3.9,3.9,3.85,3.08,3.15,2.76,3.21,3.69,3.92,3.92,3.92,3.07,3.07,3.07,2.93,3,3.23,4.08,4.93,4.22,3.7,2.76,3.15,3.73,3.08,4.08,4.43,3.77,4.22,3.62,3.54,4.11],[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],[17462,17510,25418,23717,27482,17985,16841,24333,18370,28540,28555,28325,26637,19760,21745,24818,17756,20106,28402,24144,27635,24208,25204,28666,17887,21169,17566,24641,28960,24981,27498,17979]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>mpg<\\/th>\\n <th>cyl<\\/th>\\n <th>disp<\\/th>\\n <th>hp<\\/th>\\n <th>drat<\\/th>\\n <th>wt<\\/th>\\n <th>salesprice<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":3,\"lengthMenu\":[3,10,30],\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"mpg\",\"targets\":1},{\"name\":\"cyl\",\"targets\":2},{\"name\":\"disp\",\"targets\":3},{\"name\":\"hp\",\"targets\":4},{\"name\":\"drat\",\"targets\":5},{\"name\":\"wt\",\"targets\":6},{\"name\":\"salesprice\",\"targets\":7}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} **Computing the Model The model is then trained and creates a new row of predicted \\(\\hat{Y}\\) values with all the cars in the data set set.seed(373) mtcarz$salesprice_yhat <- sample(15000:30000, nrow(mtcarz), replace = TRUE) datatable(mtcarz, options = list( pageLength = 3, lengthMenu = c(3,10,30))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"Mazda RX4\",\"Mazda RX4 Wag\",\"Datsun 710\",\"Hornet 4 Drive\",\"Hornet Sportabout\",\"Valiant\",\"Duster 360\",\"Merc 240D\",\"Merc 230\",\"Merc 280\",\"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\"Merc 450SLC\",\"Cadillac Fleetwood\",\"Lincoln Continental\",\"Chrysler Imperial\",\"Fiat 128\",\"Honda Civic\",\"Toyota Corolla\",\"Toyota Corona\",\"Dodge Challenger\",\"AMC Javelin\",\"Camaro Z28\",\"Pontiac Firebird\",\"Fiat X1-9\",\"Porsche 914-2\",\"Lotus Europa\",\"Ford Pantera L\",\"Ferrari Dino\",\"Maserati Bora\",\"Volvo 142E\"],[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],[6,6,4,6,8,6,8,4,4,6,6,8,8,8,8,8,8,4,4,4,4,8,8,8,8,4,4,4,8,6,8,4],[160,160,108,258,360,225,360,146.7,140.8,167.6,167.6,275.8,275.8,275.8,472,460,440,78.7,75.7,71.09999999999999,120.1,318,304,350,400,79,120.3,95.09999999999999,351,145,301,121],[110,110,93,110,175,105,245,62,95,123,123,180,180,180,205,215,230,66,52,65,97,150,150,245,175,66,91,113,264,175,335,109],[3.9,3.9,3.85,3.08,3.15,2.76,3.21,3.69,3.92,3.92,3.92,3.07,3.07,3.07,2.93,3,3.23,4.08,4.93,4.22,3.7,2.76,3.15,3.73,3.08,4.08,4.43,3.77,4.22,3.62,3.54,4.11],[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],[17462,17510,25418,23717,27482,17985,16841,24333,18370,28540,28555,28325,26637,19760,21745,24818,17756,20106,28402,24144,27635,24208,25204,28666,17887,21169,17566,24641,28960,24981,27498,17979],[25461,15471,18337,29084,17468,19143,25668,29647,18987,29355,17776,15214,20299,22301,25535,19371,27034,26042,17403,17763,27521,21838,28917,15812,19691,22430,26959,17165,19788,16219,28616,29821]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>mpg<\\/th>\\n <th>cyl<\\/th>\\n <th>disp<\\/th>\\n <th>hp<\\/th>\\n <th>drat<\\/th>\\n <th>wt<\\/th>\\n <th>salesprice<\\/th>\\n <th>salesprice_yhat<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":3,\"lengthMenu\":[3,10,30],\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"mpg\",\"targets\":1},{\"name\":\"cyl\",\"targets\":2},{\"name\":\"disp\",\"targets\":3},{\"name\":\"hp\",\"targets\":4},{\"name\":\"drat\",\"targets\":5},{\"name\":\"wt\",\"targets\":6},{\"name\":\"salesprice\",\"targets\":7},{\"name\":\"salesprice_yhat\",\"targets\":8}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Now we will see if one specific car had an impact on the data set by excluding it from this new calculation Produces another y hat, but it is now y hat “i”, with “i” being that specific car/ data point that we took out (the row we excluded) - This process will repeat as many time as there are rows in the data set set.seed(287) mtcarz$salesprice_yhat_withoutspecificcar <- sample(15000:30000, nrow(mtcarz), replace = TRUE) datatable(mtcarz, options = list( pageLength = 3, lengthMenu = c(3,10,30))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"Mazda RX4\",\"Mazda RX4 Wag\",\"Datsun 710\",\"Hornet 4 Drive\",\"Hornet Sportabout\",\"Valiant\",\"Duster 360\",\"Merc 240D\",\"Merc 230\",\"Merc 280\",\"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\"Merc 450SLC\",\"Cadillac Fleetwood\",\"Lincoln Continental\",\"Chrysler Imperial\",\"Fiat 128\",\"Honda Civic\",\"Toyota Corolla\",\"Toyota Corona\",\"Dodge Challenger\",\"AMC Javelin\",\"Camaro Z28\",\"Pontiac Firebird\",\"Fiat X1-9\",\"Porsche 914-2\",\"Lotus Europa\",\"Ford Pantera L\",\"Ferrari Dino\",\"Maserati Bora\",\"Volvo 142E\"],[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],[6,6,4,6,8,6,8,4,4,6,6,8,8,8,8,8,8,4,4,4,4,8,8,8,8,4,4,4,8,6,8,4],[160,160,108,258,360,225,360,146.7,140.8,167.6,167.6,275.8,275.8,275.8,472,460,440,78.7,75.7,71.09999999999999,120.1,318,304,350,400,79,120.3,95.09999999999999,351,145,301,121],[110,110,93,110,175,105,245,62,95,123,123,180,180,180,205,215,230,66,52,65,97,150,150,245,175,66,91,113,264,175,335,109],[3.9,3.9,3.85,3.08,3.15,2.76,3.21,3.69,3.92,3.92,3.92,3.07,3.07,3.07,2.93,3,3.23,4.08,4.93,4.22,3.7,2.76,3.15,3.73,3.08,4.08,4.43,3.77,4.22,3.62,3.54,4.11],[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],[17462,17510,25418,23717,27482,17985,16841,24333,18370,28540,28555,28325,26637,19760,21745,24818,17756,20106,28402,24144,27635,24208,25204,28666,17887,21169,17566,24641,28960,24981,27498,17979],[25461,15471,18337,29084,17468,19143,25668,29647,18987,29355,17776,15214,20299,22301,25535,19371,27034,26042,17403,17763,27521,21838,28917,15812,19691,22430,26959,17165,19788,16219,28616,29821],[26556,18817,18975,20388,15578,21001,21370,16149,27032,27226,26736,16268,25129,23467,25371,16915,19120,29720,19537,20876,20796,19925,20529,20406,22023,20335,21062,24068,29670,17696,28905,23104]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>mpg<\\/th>\\n <th>cyl<\\/th>\\n <th>disp<\\/th>\\n <th>hp<\\/th>\\n <th>drat<\\/th>\\n <th>wt<\\/th>\\n <th>salesprice<\\/th>\\n <th>salesprice_yhat<\\/th>\\n <th>salesprice_yhat_withoutspecificcar<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":3,\"lengthMenu\":[3,10,30],\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9]},{\"orderable\":false,\"t"},{"title":" Predicting Grades for Math 325 ","url":"425Analyses/PredictingGrades.html","content":"Code Show All Code Hide All Code Predicting Grades for Math 325 Background In this study, we will create a multiple logistic regression model that predicts whether or not a student will get an A (or A-) for their final grade in Math 325, Intermediate Statistics. Hide Data Show Data Below is the data from past Math 325 students. The full data set consists of grades from every assignment within Math 325. library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) gradetrain <- read.csv(\"~/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/math325grades_train.csv\", stringsAsFactors=TRUE) gradetest <- read.csv(\"~/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/math325grades_test.csv\") gradetrain <- na.omit(gradetrain) %>% mutate(gradeA = ifelse(FinalGrade %in% c(\"A\", \"-A\"), 1, 0)) glmgrade <- glm(gradeA ~ SkillsQuizzesTotalCurrentScore + SkillsQuizzesTotalCurrentScore:CurrentScore + AssessmentQuizzesCurrentScore:CurrentScore, data=gradetrain, family=binomial) datatable(gradetrain) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"3\",\"8\",\"9\",\"10\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"19\",\"20\",\"22\",\"23\",\"24\",\"26\",\"27\",\"29\",\"30\",\"31\",\"32\",\"33\",\"35\",\"36\",\"39\",\"40\",\"41\",\"43\",\"46\",\"47\",\"49\",\"50\",\"53\",\"54\",\"55\",\"56\",\"57\",\"61\",\"62\",\"63\",\"66\",\"69\",\"70\",\"71\",\"74\",\"75\",\"80\",\"81\",\"82\",\"86\",\"87\",\"88\",\"89\",\"91\",\"93\",\"94\",\"96\",\"99\",\"100\",\"102\",\"103\",\"104\",\"105\",\"106\",\"108\",\"109\",\"112\",\"115\",\"119\",\"120\"],[\"Female\",\"Male\",\"Male\",\"Male\",\"Male\",\"Male\",\"Female\",\"Male\",\"Female\",\"Male\",\"Male\",\"Male\",\"Male\",\"Female\",\"Female\",\"Male\",\"Male\",\"Female\",\"Male\",\"Female\",\"Female\",\"Male\",\"Female\",\"Male\",\"Male\",\"Female\",\"Male\",\"Male\",\"Male\",\"Male\",\"Female\",\"Male\",\"Male\",\"Male\",\"Male\",\"Female\",\"Male\",\"Female\",\"Female\",\"Female\",\"Female\",\"Female\",\"Male\",\"Female\",\"Female\",\"Male\",\"Female\",\"Male\",\"Male\",\"Male\",\"Female\",\"Male\",\"Male\",\"Male\",\"Male\",\"Female\",\"Female\",\"Male\",\"Male\",\"Male\",\"Female\",\"Male\",\"Male\",\"Female\",\"Female\",\"Male\",\"Female\",\"Male\",\"Female\",\"Male\",\"Male\"],[\"Fall 2019\",\"Winter 2020\",\"Winter 2020\",\"Winter 2020\",\"Spring 2019\",\"Spring 2019\",\"Winter 2020\",\"Fall 2019\",\"Spring 2020\",\"Fall 2019\",\"Spring 2020\",\"Fall 2019\",\"Spring 2020\",\"Spring 2019\",\"Winter 2020\",\"Spring 2019\",\"Fall 2019\",\"Spring 2019\",\"Spring 2020\",\"Fall 2019\",\"Fall 2019\",\"Winter 2020\",\"Fall 2019\",\"Spring 2020\",\"Winter 2020\",\"Winter 2020\",\"Fall 2019\",\"Winter 2020\",\"Winter 2020\",\"Winter 2020\",\"Spring 2019\",\"Winter 2020\",\"Winter 2020\",\"Fall 2019\",\"Spring 2019\",\"Spring 2019\",\"Spring 2019\",\"Spring 2019\",\"Spring 2020\",\"Spring 2019\",\"Fall 2019\",\"Spring 2019\",\"Spring 2020\",\"Winter 2020\",\"Spring 2019\",\"Spring 2019\",\"Fall 2019\",\"Winter 2020\",\"Fall 2019\",\"Fall 2019\",\"Spring 2019\",\"Fall 2019\",\"Spring 2019\",\"Spring 2019\",\"Spring 2019\",\"Fall 2019\",\"Fall 2019\",\"Spring 2020\",\"Spring 2020\",\"Fall 2019\",\"Spring 2019\",\"Winter 2020\",\"Winter 2020\",\"Winter 2020\",\"Spring 2019\",\"Winter 2020\",\"Spring 2020\",\"Spring 2020\",\"Spring 2019\",\"Spring 2020\",\"Fall 2019\"],[7,20,34,27,12,33,36,25,12,45,1,23,10,38,23,22,21,3,18,12,35,8,36,6,17,24,19,37,30,28,4,18,11,18,36,11,15,45,17,40,16,35,14,21,44,42,13,22,5,37,23,33,14,27,26,22,46,24,19,14,34,15,2,31,20,33,30,2,13,15,4],[15,15,12,14,15,12,15,12,15,14.8,13,13.5,14,15,15,15,14.5,15,14.8,15,15,15,15,10,15,11,15,15,15,15,12,14.5,15,13.4,15,10,15,15,15,14,15,15,14,15,15,15,14.5,15,14,11,14.5,14.25,14.5,15,15,15,12,13,15,15,15,15,15,15,15,15,15,15,15,15,15],[15,15,12,11,15,13,15,12,15,15,10,13,11,15,15,15,13.75,15,13.8,15,15,13.8,15,10,15,13.25,15,15,14.5,15,14.5,15,15,13.8,15,13.5,15,15,15,15,15,14.8,15,15,15,15,9,15,14,5,13,14.6,15,14,15,14,11.5,13.5,15,12,13.8,10,15,15,15,14,15,14,15,15,15],[15,15,11,12,15,14,15,15,15,15,10,14.5,13,15,15,15,12,15,14.5,15,15,14,15,11,15,13.8,15,15,14.25,15,13,15,15,14,15,11,15,15,15,14,15,15,14.9,15,15,15,13.5,15,13.5,13,12.5,14.75,14.75,13.4,15,15,12,14.5,15,13,13.5,15,14.98,15,15,10,14.5,14.8,15,15,14.5],[15,15,13,12,13,11.5,15,13,14.7,15,0,11.5,8,15,15,15,12,14,15,15,15,12,15,11,15,14,15,15,13.4,15,14,15,13.8,15,15,13.5,15,15,15,8,15,13,15,15,15,15,14,15,14.25,11,13.5,14.5,14.9,14.9,15,13.5,9,14.9,15,9,10,15,14.25,15,15,11,15,15,13.5,15,12],[15,15,9,13.5,14,13,14.5,14,15,14.3,12,13.5,10,15,15,14.4,14,13.5,14,15,15,13.5,15,8,15,13,15,15,14.5,15,14,15,15,14.9,15,12.5,15,15,15,15,15,14.8,14.5,15,15,15,14,15,13,6,13.5,14.5,15,13,15,14,12,15,15,10,12,15,13.5,11,15,11,15,14.5,15,15,14.8],[15,15,12.5,13.5,14,11.5,13.8,14.5,14.5,13.5,12,13,8,15,15,15,13.5,13.8,14,15,13.8,13.5,15,11,15,10,13.8,15,11,15,14,15,15,15,15,13,15,15,15,13.8,15,15,14,15,15,14.5,13.5,15,13.5,3,13.5,14,14.5,11,15,13.8,12,14,11,13.25,13.8,15,13.5,11,15,10,15,15,15,15,12.5],[15,15,13.2,11,13.8,14,15,9,14.2,13.8,0,12,10,15,15,14.5,13.6,13.7,14,15,13,14,14.5,9,15,13,13.8,15,13,15,13.8,15,15,15,15,13,14.5,15,15,11,15,15,14.9,15,15,14.5,11,15,14,0,14,13,14.9,12,15,12.5,11.5,11,13,11,11.4,15,14.9,14.5,15,5,15,14,13.4,15,13.8],[15,15,13.25,13.5,13,12,13.5,6,14.2,13.4,11,13,7,13.5,15,14.5,13.6,15,14,15,12.5,12,15,11,15,9,13.5,15,13,15,13.5,15,15,15,15,14,13.5,15,15,11,15,15,14.8,15,15,15,11,14.5,15,0,13.5,12,14,13,15,13,13,14.1,15,11,13.5,13.4,14,12,15,0,15,15,11,15,12.5],[15,15,13,13.4,15,0,13,5,14.8,15,3,0,10,11,15,14.2,14.8,15,14,15,15,11,15,10,15,12,12,15,11,15,12,15,15,15,14,13,13.4,15,15,11,15,15,15,15,15,14,13.5,15,13.75,12,13.5,14,14,15,15,9,12,13,15,13.25,10.5,11,13.5,15,15,0,13,13.5,14,15,14],[10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,1,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,5.5,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,5.5,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,0,10,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,5.5,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,5.5,10,0,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,5.5,10,1,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,7.75,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,5.5,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10],[10,10,10,10,10,5.5,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,3.25,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,0,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,0,10,10,0,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,0,10,10,10,10,0,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,4,10,4,10,4,10,10,10,10,10,4,10,10,10,10,4,10,10,10,10,10,10,10,10,10,10,10,10,10,10,4,10,10,10,4,10,10,10,10,10,10,10,10,10,10,10,10,10,10,4,4,10,10,10,10,10,10,10,4,4,10,4,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,6,10,10,10,10,10,10,10,10,10,10,10,10,6,10,10,10,10,10,10,10,10,10,10,10,8,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9,10,10,10,10,10,9,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[1,0,0,1,0,0,2,1,0,1,1,2,1,2,0,1,1,2,2,1,1,1,0,0,0,1,2,1,1,1,1,2,1,0,2,0,2,0,2,0,0,1,2,0,2,0,1,2,1,1,1,1,1,0,2,0,0,0,1,2,0,1,2,0,0,0,3,1,0,1,1],[3,2,2,2,3,1,1,3,1,1,0,1,1,2,2,2,1,2,1,1,1,2,0,0,1,1,2,2,2,1,0,2,1,3,2,1,2,2,3,1,1,3,1,2,1,3,1,1,0,1,2,2,3,1,3,1,2,3,1,1,2,2,3,2,1,1,1,0,1,3,1],[2,0,1,2,3,2,2,3,2,2,2,2,2,3,2,2,2,2,3,2,3,2,2,0,2,2,3,3,3,2,2,2,2,2,3,2,2,3,3,1,2,3,3,2,2,2,2,1,3,3,0,3,2,0,3,2,1,2,2,3,1,3,3,1,2,2,3,3,3,1,2],[2,1,0,1,1,3,3,1,3,2,0,2,0,3,3,2,1,2,3,2,1,3,1,3,3,3,2,3,3,0,3,3,3,1,1,3,1,3,2,2,0,3,2,3,2,2,3,3,3,1,3,2,3,1,3,2,2,2,3,0,3,2,3,2,3,2,3,0,3,2,3],[2,0,0,2,2,2,1,1,0,1,0,1,0,3,1,2,1,2,3,2,1,2,2,3,3,1,2,3,2,0,2,2,3,2,1,1,2,2,2,2,0,3,3,2,3,2,3,3,1,2,2,2,1,1,3,3,0,1,3,3,2,3,2,2,3,0,3,3,2,3,2],[1,0,0,2,3,1,1,1,2,1,2,2,1,2,3,2,2,2,2,2,1,3,2,0,2,2,1,2,2,2,3,2,2,1,2,2,1,3,2,1,1,1,3,2,2,1,0,2,1,1,1,2,2,0,1,2,0,3,3,1,1,1,2,1,2,2,3,2,2,2,2],[2,3,3,3,2,2,3,1,0,2,1,1,1,3,3,2,1,3,2,1,3,3,3,3,3,1,1,3,3,3,3,3,3,3,2,3,0,3,3,1,3,2,3,3,3,2,2,3,2,0,2,3,3,1,3,1,0,2,3,2,2,3,3,3,2,3,3,3,3,3,3],[1,0,0,1,1,2,2,2,2,2,0,1,2,2,2,1,2,2,2,1,0,1,2,3,2,1,0,1,3,0,1,2,2,2,1,0,1,2,3,0,0,1,3,2,1,1,1,1,2,0,1,2,3,1,2,0,0,0,3,2,1,3,1,1,0,0,1,0,3,2,1],[3,0,1,1,2,1,1,3,2,1,0,2,2,2,2,2,1,2,1,2,0,2,1,2,2,0,2,2,1,1,3,1,2,2,1,2,1,3,2,1,2,1,1,2,2,3,1,1,1,0,2,1,2,2,2,2,1,2,3,2,1,1,2,3,3,1,3,2,3,2,1],[1,0,1,0,1,2,1,0,2,2,2,1,1,2,2,0,0,2,1,2,3,2,3,1,1,1,2,2,1,2,1,2,0,2,1,1,1,1,1,1,1,2,1,1,3,0,1,0,2,1,2,2,2,1,0,1,1,1,3,3,2,3,2,0,1,1,2,1,2,0,2],[0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,1,1,1,1,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0],[77,44,28,50,62,25,60,68,50,57,30,63,40,83,66,72,50,67,76,63,57,70,42,55,70,50,80,74,78,52,54,74,74,60,63,45,68,68,76,42,30,83,70,58,68,68,77,70,33,63,57,75,68,40,90,63,40,60,78,65,55,78,74,50,65,40,82,35,58,65,62],[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9.09,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9.09,10,10,10],[100,99.39,83,84.79000000000001,93.52,77.27,96.84999999999999,78.18000000000001,98.3,96.84999999999999,60.3,80,72.73,95.45,100,98.23999999999999,91.67,96.97,95.81999999999999,100,96.55,90.18000000000001,99.7,72.12,100,75.18000000000001,86.73,99.39,89.97,100,88.67,99.7,99.27,97.64,99.09,83.7,94.79000000000001,100,100,82.91,100,98.55,98.23999999999999,100,100,98.33,86.36,99.7,89.7,52.12,91.20999999999999,94.3,97.23999999999999,88.67,100,87.76000000000001,81.81999999999999,92,96.36,74.23999999999999,85.15000000000001,92.67,95.23,92.42,100,58.18,98.48,88.36,95.09,98.79000000000001,92.67],[100,99.39,83,84.79000000000001,93.52,77.27,96.84999999999999,78.18000000000001,98.3,96.84999999999999,60.3,80,72.73,95.45,100,98.23999999999999,91.67,96.97,95.81999999999999,100,96.55,90.18000000000001,99.7,72.12,100,75.18000000000001,86.73,99.39,89.97,100,88.67,99.7,99.27,97.64,99.09,83.7,94.79000000000001,100,100,82.91,100,98.55,98.23999999999999,100,100,98.33,86.36,99.7,89.7,52.12,91.20999999999999,94.3,97.23999999999999,88.67,100,87.76000000000001,81.81999999999999,92,96.36,74.23999999999999,85.15000000000001,92.67,95.23,92.42,100,58.18,98.48,88.36,95.09,98.79000000000001,92.67],[100,99.39,83,84.79000000000001,93.52,77.27,96.84999999999999,78.18000000000001,98.3,96.84999999999999,60.3,80,72.73,95.45,100,98.23999999999999,91.67,96.97,95.81999999999999,100,96.55,90.18000000000001,99.7,72.12,100,75.18000000000001,86.73,99.39,89.97,100,88.67,99.7,99.27,97.64,99.09,83.7,94.79000000000001,100,100,82.91,100,98.55,98.23999999999999,100,100,98.33,86.36,99.7,89.7,52.12,91.20999999999999,94.3,97.23999999999999,88.67,100,87.76000000000001,81.81999999999999,92,96.36,74.23999999999999,85.15000000000001,92.67,95.23,92.42,100,58.18,98.48,88.36,95.09,98.79000000000001,92.67],[100,99.39,83,84.79000000000001,93.52,77.27,96.84999999999999,78.18000000000001,98.3,96.84999999999999,60.3,80,72.73,95.45,100,98.23999999999999,91.67,96.97,95.81999999999999,100,96.55,90.18000000000001,99.7,72.12,100,75.18000000000001,86.73,99.39,89.97,100,88.67,99.7,99.27,97.64,99.09,83.7,94.79000000000001,100,100,82.91,100,98.55,98.23999999999999,100,100,98.33,86.36,99.7,89.7,52.12,91.20999999999999,94.3,97.23999999999999,88.67,100,87.76000000000001,81.81999999999999,92,96.36,74.23999999999999,85.15000000000001,92.67,95.23,92.42,100,58.18,98.48,88.36,95.09,98.79000000000001,92.67],[100,100,100,90,99.83,71.88,100,91.81999999999999,100,100,60,81.36,100,100,100,99.58,100,99.58,100,100,100,100,100,91,100,100,100,100,97.75,100,100,100,100,100,100,75,100,100,100,99.83,100,99.17,100,100,100,100,100,100,95.73,99.27,83.33,100,100,94.38,100,100,100,100,100,100,99.58,100,100,90,100,80,100,100,100,100,99.09],[100,100,100,90,99.83,71.88,100,91.81999999999999,100,100,60,81.36,100,100,100,99.58,100,99.58,100,100,100,100,100,91,100,100,100,100,97.75,100,100,100,100,100,100,75,100,100,100,99.83,100,99.17,100,100,100,100,100,100,95.73,99.27,83.33,100,100,94.38,100,100,100,100,100,100,99.58,100,100,90,100,80,100,100,100,100,99.09],[100,100,100,90,99.83,71.88,100,91.81999999999999,100,100,60,81.36,100,100,100,99.58,100,99.58,100,100,100,100,100,91,100,100,100,100,97.75,100,100,100,100,100,100,75,100,100,100,99.83,100,99.17,100,100,100,100,100,100,95.73,99.27,83.33,100,100,94.38,91.67,100,100,100,100,100,99.58,100,100,90,100,80,100,100,100,100,99.09],[100,100,100,90,99.83,71.88,100,91.81999999999999,100,100,60,81.36,100,100,100,99.58,100,99.58,100,100,100,100,100,91,100,100,100,100,97.75,100,100,100,100,100,100,75,100,100,100,99.83,100,99.17,100,100,100,100,100,100,95.73,99.27,83.33,100,100,94.38,100,100,100,100,100,100,99.58,100,100,90,100,80,100,100,100,100,99.09],[100,100,100,100,100,100,100,100,100,100,92.31,84.62,92.86,100,100,92.31,100,100,100,100,100,100,100,78.56999999999999,100,100,100,100,92.86,100,100,100,92.86,100,100,76.92,84.62,100,100,84.62,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,84.62,85.70999999999999,100,100,92.31,92.86,100,100,100,100,100,100,92.31,100,100],[100,100,100,100,100,100,100,100,100,100,92.31,84.62,92.86,100,100,92.31,100,100,100,100,100,100,100,78.56999999999999,100,100,100,100,92.86,100,100,100,92.86,100,100,76.92,84.62,100,100,84.62,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,84.62,85.70999999999999,100,100,92.31,92.86,100,100,100,100,100,100,92.31,100,100],[100,100,100,100,100,100,100,100,100,100,85.70999999999999,84.62,92.86,100,100,92.31,100,100,100,100,100,100,100,78.56999999999999,100,100,100,100,92.86,100,100,100,92.86,100,100,76.92,84.62,100,100,84.62,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,84.62,85.70999999999999,100,100,92.31,92.86,100,100,100,100,100,100,92.31,100,100],[100,100,100,100,100,100,100,100,100,100,85.70999999999999,84.62,92.86,100,100,92.31,100,100,100,100,100,100,100,78.56999999999999,100,100,100,100,92.86,100,100,100,92.86,100,100,76.92,84.62,100,100,84.62,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,84.62,85.70999999999999,100,100,92.31,92."},{"title":" Predicting the Weather ","url":"425Analyses/PredictingWeather.html","content":"Code Show All Code Hide All Code Predicting the Weather library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) Background For this study, we were tasked with predicting the “Actual Maximum Air Temperature” for this coming Monday, January 13th at BYU-Idaho. BYU-Idaho is located in the city of Rexburg, Idaho, and thus we will use this city’s weather recordings from timeanddate.com to make our predictions. leaflet() %>% addProviderTiles(providers$Esri.WorldTopoMap, group = \"World Map\") %>% addProviderTiles(providers$Esri.WorldImagery, group = \"Terrain Map\") %>% setView(lng = -111.7864, lat = 43.8225, zoom = 13) %>% # Set the view to Rexburg, Idaho addLayersControl( baseGroups = c(\"World Map\", \"Terrain Map\"), options = layersControlOptions(collapsed = FALSE) ) {\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addProviderTiles\",\"args\":[\"Esri.WorldTopoMap\",null,\"World Map\",{\"errorTileUrl\":\"\",\"noWrap\":false,\"detectRetina\":false}]},{\"method\":\"addProviderTiles\",\"args\":[\"Esri.WorldImagery\",null,\"Terrain Map\",{\"errorTileUrl\":\"\",\"noWrap\":false,\"detectRetina\":false}]},{\"method\":\"addLayersControl\",\"args\":[[\"World Map\",\"Terrain Map\"],[],{\"collapsed\":false,\"autoZIndex\":true,\"position\":\"topright\"}]}],\"setView\":[[43.8225,-111.7864],13,[]]},\"evals\":[],\"jsHooks\":[]} The specific data points/temperatures that were recorded from January 13th’s from previous years. The temperatures that were recorded was the high temperature at the beginning of the day (STARTMAXTEMP column) and the overall max temperature of that day(MAXTEMP column). Click the tabs below to see the data that was collected. Hide Data Show Data janweather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/JanWeather.csv\") datatable(janweather, options = list(pageLength = 10, lengthMenu = c(3, 10, 30))) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"],[\"13-Jan-24\",\"13-Jan-23\",\"13-Jan-22\",\"13-Jan-21\",\"13-Jan-20\",\"13-Jan-19\",\"13-Jan-18\",\"13-Jan-17\"],[0,25,18,30,27,3,32,3],[10,30,32,39,27,19,39,16]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>DATE<\\/th>\\n <th>STARTMAXTEMP<\\/th>\\n <th>MAXTEMP<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":10,\"lengthMenu\":[3,10,30],\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"DATE\",\"targets\":1},{\"name\":\"STARTMAXTEMP\",\"targets\":2},{\"name\":\"MAXTEMP\",\"targets\":3}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Analysis With the data collected above, we will be using a simple linear regression to predict what the max temperature will be on January 13, 2025. The red dot represents Based on the graph below, it seems to be that the higher the beginning max temperature of the day is, the higher the overall max temperature will be. Regardless, we must be careful to trust this final verdict until after we conduct our simple linear regression. Additionally, we can see the confidence interval of the true line and the prediction intervalof the average Max Temperature of the day. prediction <- data.frame( STARTMAXTEMP=16, MAXTEMP= 26, label = \"Prediction Point : 26°F\" ) janlm <- lm(MAXTEMP ~ STARTMAXTEMP, data=janweather) predi.c <- predict(janlm, data.frame(STARTMAXTEMP=16), interval= \"confidence\") predi.p <- predict(janlm, data.frame(STARTMAXTEMP=16), interval= \"prediction\") janweathery_plot <- ggplot(janweather, aes(x = STARTMAXTEMP, y = MAXTEMP)) + geom_point( aes( text = paste( \"Date:\", DATE, \"<br>\", \"Start Max Temp. of the Day:\", STARTMAXTEMP, \"\\u00b0F<br>\", \"Max Temp. of the Day:\", MAXTEMP, \"\\u00b0F\" ) ), size = 2, color = \"darkblue\" ) + geom_smooth(method = \"lm\", formula= y~x, se = FALSE, color = \"dodgerblue\") + labs( title = \"Weather Patterns from January 13th's of the Past\", x = \"Max Start Temperature of the Day (\\u00b0F)\", y = \"Max Temperature of the Day (\\u00b0F)\" ) + geom_point(data=prediction, aes(x=STARTMAXTEMP, y=MAXTEMP), size = 3, color= \"red\") + geom_text( data = prediction, aes(x = STARTMAXTEMP, y=MAXTEMP, label = label), nudge_x = -3.6, nudge_y = 1.5, color= \"red\", size = 3 ) + geom_segment(aes(x=16, xend=16, y=predi.p[2], yend=predi.p[3]), alpha = 0.1, color= \"lightgreen\", lwd = 3) + geom_segment(aes(x=16, xend=16, y=predi.c[2], yend=predi.c[3]), alpha = 0.1, color= \"pink\", lwd = 3) + theme_minimal() ggplotly(janweathery_plot, tooltip = \"text\") {\"x\":{\"data\":[{\"x\":[0,25,18,30,27,3,32,3],\"y\":[10,30,32,39,27,19,39,16],\"text\":[\"Date: 13-Jan-24 <br> Start Max Temp. of the Day: 0 °F<br> Max Temp. of the Day: 10 °F\",\"Date: 13-Jan-23 <br> Start Max Temp. of the Day: 25 °F<br> Max Temp. of the Day: 30 °F\",\"Date: 13-Jan-22 <br> Start Max Temp. of the Day: 18 °F<br> Max Temp. of the Day: 32 °F\",\"Date: 13-Jan-21 <br> Start Max Temp. of the Day: 30 °F<br> Max Temp. of the Day: 39 °F\",\"Date: 13-Jan-20 <br> Start Max Temp. of the Day: 27 °F<br> Max Temp. of the Day: 27 °F\",\"Date: 13-Jan-19 <br> Start Max Temp. of the Day: 3 °F<br> Max Temp. of the Day: 19 °F\",\"Date: 13-Jan-18 <br> Start Max Temp. of the Day: 32 °F<br> Max Temp. of the Day: 39 °F\",\"Date: 13-Jan-17 <br> Start Max Temp. of the Day: 3 °F<br> Max Temp. of the Day: 16 °F\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,0,139,1)\",\"opacity\":1,\"size\":7.559055118110237,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(0,0,139,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[0,0.4050632911392405,0.810126582278481,1.2151898734177216,1.620253164556962,2.0253164556962027,2.4303797468354431,2.8354430379746836,3.240506329113924,3.6455696202531644,4.0506329113924053,4.4556962025316453,4.8607594936708862,5.2658227848101262,5.6708860759493671,6.0759493670886071,6.481012658227848,6.8860759493670889,7.2911392405063289,7.6962025316455698,8.1012658227848107,8.5063291139240498,8.9113924050632907,9.3164556962025316,9.7215189873417724,10.126582278481013,10.531645569620252,10.936708860759493,11.341772151898734,11.746835443037975,12.151898734177214,12.556962025316455,12.962025316455696,13.367088607594937,13.772151898734178,14.177215189873417,14.582278481012658,14.987341772151899,15.39240506329114,15.797468354430379,16.202531645569621,16.60759493670886,17.0126582278481,17.417721518987342,17.822784810126581,18.227848101265824,18.632911392405063,19.037974683544302,19.443037974683545,19.848101265822784,20.253164556962027,20.658227848101266,21.063291139240505,21.468354430379748,21.873417721518987,22.278481012658226,22.683544303797468,23.088607594936708,23.49367088607595,23.898734177215189,24.303797468354428,24.708860759493671,25.11392405063291,25.518987341772153,25.924050632911392,26.329113924050631,26.734177215189874,27.139240506329113,27.544303797468356,27.949367088607595,28.354430379746834,28.759493670886076,29.164556962025316,29.569620253164558,29.974683544303797,30.379746835443036,30.784810126582279,31.189873417721518,31.594936708860757,32],\"y\":[13.682533279548201,13.983512134844078,14.284490990139956,14.585469845435835,14.886448700731712,15.18742755602759,15.488406411323467,15.789385266619345,16.090364121915222,16.391342977211103,16.69232183250698,16.993300687802858,17.294279543098735,17.595258398394613,17.89623725369049,18.197216108986368,18.498194964282245,18.799173819578122,19.100152674874003,19.401131530169881,19.702110385465758,20.003089240761636,20.304068096057513,20.60504695135339,20.906025806649268,21.207004661945149,21.507983517241023,21.808962372536904,22.109941227832778,22.410920083128659,22.711898938424532,23.012877793720413,23.313856649016291,23.614835504312168,23.915814359608049,24.216793214903923,24.517772070199804,24.818750925495678,25.119729780791559,25.420708636087433,25.721687491383314,26.022666346679191,26.323645201975069,26.62462405727095,26.925602912566823,27.226581767862704,27.527560623158578,27.828539478454459,28.129518333750337,28.430497189046214,28.731476044342095,29.032454899637969,29.333433754933846,29.634412610229724,29.935391465525605,30.236370320821479,30.53734917611736,30.838328031413234,31.139306886709115,31.440285742004995,31.741264597300869,32.04224345259675,32.343222307892624,32.644201163188505,32.945180018484379,33.24615887378026,33.547137729076141,33.848116584372015,34.149095439667896,34.45007429496377,34.751053150259651,35.052032005555525,35.353010860851406,35.653989716147287,35.95496857144316,36.255947426739034,36.556926282034915,36.857905137330796,37.15888399262667,37.459862847922551],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"name\":\"fitted values\",\"line\":{\"width\":3.7795275590551185,\"color\":\"rgba(30,144,255,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[16],\"y\":[26],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(255,0,0,1)\",\"opacity\":1,\"size\":11.338582677165356,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(255,0,0,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[12.4],\"y\":[27.5],\"text\":\"Prediction Point : 26°F\",\"hovertext\":\"\",\"textfont\":{\"size\":11.338582677165356,\"color\":\"rgba(255,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[16,16,null,16,16,null,16,16,null,16,16,null,16,16,null,16,16,null,16,16,null,16,16],\"y\":[14.469644585313679,36.672751542157066,null,14.469644585313679,36.672751542157066,null,14.469644585313679,36.672751542157066,null,14.469644585313679,36.672751542157066,null,14.469644585313679,36.672751542157066,null,14.469644585313679,36.672751542157066,null,14.469644585313679,36.672751542157066,null,14.469644585313679,36.672751542157066],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":11.338582677165356,\"color\":\"rgba(144,238,144,0.1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[16,16,null,16,16,null,16,16,null,16,16,null,16,16,null,16,16,null,16,16,null,16,16],\"y\":[21.854149654882669,29.288246472588078,null,21.854149654882669,29.288246472588078,null,21.854149654882669,29.288246472588078,null,21.854149654882669,29.288246472588078,null,21.854149654882669,29.288246472588078,null,21.854149654882669,29.288246472588078,null,21.854149654882669,29.288246472588078,null,21.854149654882669,29.288246472588078],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":11.338582677165356,\"color\":\"rgba(255,192,203,0.1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":43.762557077625573,\"r\":7.3059360730593621,\"b\":40.182648401826491,\"l\":37.260273972602747},\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724},\"title\":{\"text\":\"Weather Patterns from January 13th's of the Past\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.534246575342465},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-1.6000000000000001,33.600000000000001],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"10\",\"20\",\"30\"],\"tickvals\":[0,10,20,30],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"10\",\"20\",\"30\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.6529680365296811,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.68949771689498},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176002,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"Max Start Temperature of the Day (°F)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[8.5500000000000007,40.450000000000003],\"tickmode\":\"array\",\"ticktext\":[\"10\",\"20\",\"30\",\"40\"],\"tickvals\":[10,20,30,40],\"categoryorder\":\"array\",\"categoryarray\":[\"10\",\"20\",\"30\",\"40\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.6529680365296811,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.68949771689498},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176002,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"Max Temperature of the Day (°F)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":null,\"bordercolor\":null,\"borderwidth\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.68949771689498}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"8ac470155a7b\":{\"x\":{},\"y\":{},\"text\":{},\"type\":\"scatter\"},\"8ac41793be0\":{\"x\":{},\"y\":{}},\"8ac418b84f76\":{\"x\":{},\"y\":{}},\"8ac42f17195\":{\"x\":{},\"y\":{},\"label\":{}},\"8ac47f2abb0\":{\"x\":{},\"y\":{},\"xend\":{},\"yend\":{}},\"8ac468fe5061\":{\"x\":{},\"y\":{},\"xend\":{},\"yend\":{}}},\"cur_data\":\"8ac470155a7b\",\"visdat\":{\"8ac470155a7b\":[\"function (y) \",\"x\"],\"8ac41793be0\":[\"function (y) \",\"x\"],\"8ac418b84f76\":[\"function (y) \",\"x\"],\"8ac42f17195\":[\"function (y) \",\"x\"],\"8ac47f2abb0\":[\"function (y) \",\"x\"],\"8ac468fe5061\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} Simple Linear Regression Test By using a simple linear regression, our study is then represented by the mathematical model below: \\[\\underbrace{Y_i}_\\text{MAXTEMP} = \\overbrace{\\beta_0}^\\text{Intercept} + \\overbrace{\\beta_1}^\\text{Slope} \\underbrace{X_i}_\\text{STARTMAXTEMP}+ \\epsilon_i \\text{ where} \\sim N(0,\\sigma^2)\\] Part What is tells us \\(Y_i\\) The predictor variable (what we are predicting) \\(X_i\\) The explanatory variable (what we use to find the predictor variable) \\(\\beta_0\\) Intercept : Sets the starting point of the line \\(\\beta_i\\) Slope: the change in the average y-value for a one unit change in the x-value Between Intercept (\\(\\beta_0\\)) and Slope (\\(\\beta_1\\)), slope is the most meaningful variable to analyze out of the two. The slope will be able to tell us what our change in the overall max temperature would be based on the a one-unit increase in starting max temperature. Looking at intercept would only tell us what the overall max temperature would be like if the max starting temperature of the day is zero, so the intercept would not be of interest. Thus, our hypothesis and our level of significance is depicted as follows: \\[H_0 : \\beta_1 = 0\\] \\[H_a : \\beta_1 \\neq 0\\] \\[\\alpha = 0.05\\] Now that our model and regression has been more defined, we can conduct our linear regression as shown below as well as the updated version of our mathematical models with our estimated values. summary(janlm)%>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 13.68 2.583 5.297 0.001835 STARTMAXTEMP 0.743 0.1214 6.119 0.0008698 Fitting linear model: MAXTEMP ~ STARTMAXTEMP Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 8 4.275 0.8619 0.8389 \\[\\underbrace{\\hat{Y_i}}_\\text{Prediction MAXTEMP} = 13.68 + 0.743 \\underbrace{X_i}_\\text{STARTMAXTEMP}\\] Our mathematical model now states that, every increase of one degrees F (\\(X_i\\)) in starting max temperature results in a 0.743 degrees F increase in the average maximum temperature of that day(\\(Y_i\\)). The meaningfulness of our slope shows to be significant due to our low p-value of 0.0008698. To further confirm the truth in our findings, consult the tabs below to check the appropriateness of the model. Hide Diagnostic Plots Show Diagnostic Plots par(mfrow=c(1,3)) plot(janlm, which=1) qqPlot(janlm, id=FALSE, main= \"Q-Q plot\", col=\"darkblue\", col.lines = \"dodgerblue\", pch = 16) plot(janlm$residuals, main=\"Residuals vs Order\") Overall, everything checks out. The randomly scattered residuals from the residuals vs. fitted-values plot shows signs of good constant variance and linear relation. The dots in the Q-Q Plot are all within the bounds of normality. Additionally, the residuals in our final residuals vs. order plot shows no trends or order so the error terms can be assumed to be independent. Interpretation Due to the graphs and simple linear regression test, we are able to hopefully predict the maximum temperature of future days based on the starting max temperature of the day. The scatter plot we made showed that if the starting weather of the day was low or high, that would tell us that the overall temperature or that day would remain relatively around that low or high range. With the test, we were able to find that the impact of the starting max temperature of the day was meaningful to predicting the overall max temperature of the day due to our extremely low p-value (p-value = 0.0008698). For that reason, it is safe to say that every increase of one degrees F (\\(X_i\\)) in starting max temperature results in a 0.743 degrees F increase in the average maximum temperature of that day(\\(Y_i\\)). To further support these findings, they were checked by the diagnostic plots for their appropriateness and passed all three of them. Therefore, we are able to trust the predicted results as follows. Sources Chatgpt For helping me fix errors when I hit a road block Formatting My old weather analysis // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Regression Battleship - Creating your Data","url":"425Analyses/RegressionBattleshipCreatingYourShip.html","content":"Code Show All Code Hide All Code Regression Battleship - Creating your Data Paige Stewart library(pander) library(tidyverse) library(GGally) Instructions Using Desmos, design a “true linear regression model” that is 2D-Drawable, and follows all other Regression Battleship Rules (listed below), that is of the form \\[ Y_i = \\beta_0 + \\underbrace{\\quad\\quad\\quad\\ldots\\quad\\quad\\quad}_\\text{Your Model Goes Here} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) \\] Then, use a simulation in R and your linear regression model to obtain a sample of data saved as rbdata.csv. Your sample of data will be given to other students and your teacher, but this Rmd file (which contains the secret on how you made your data) will remain hidden until after the competition is complete. Your teacher and two of your peers will use the sample of data your provide, rbdata.csv, to try to guess the true linear regression model you used to create the data. The goal is to hide your model well enough that no one can find it, while keeping the R-squared of your data as high as possible. Official Rules Advanced Level Competition Competing in the Advanced Level will allow you the opportunity to earn full credit on the Regression Battleship portion of your grade in Math 425 (which is 15% of your Final Grade). However, if you compete at this level, you cannot ever discuss your actual model with your teacher. You can still ask for help from the TA, tutors, or other students that you are not competing against. And you can ask “vague” questions to your teacher as long as it doesn’t give too much away about your model. There are five official rules your model must abide by. If you break any of the rules, you will be disqualified from winning the competition and a grade penalty will result. Your csv file rbdata.csv must contain 11 columns of data. The first column must be your (1) y-variable (labeled as y). The other ten columns must be (10) x-variables (labeled as x1, x2, … , x10). Please use all lower-case letters. It does not matter which x-variables you use in your model, and you don’t need to use all 10 x-variables in your model. Your y-variable (or some transformation of the y-variable) must have been created from a linear regression model using only x-variables (or transformations of those x-variables) from within your data set. Be very careful with transformations. You must ensure that you do not break the rules of a linear regression if you choose to use transformations. If you choose transformations, only these functions are allowed when transforming X and Y variables: 1/Y^2, 1/Y, log(Y), sqrt(Y), sqrt(sqrt(Y)), Y^2, Y^3, 1/X^2, 1/X, log(X), sqrt(X), sqrt(sqrt(X)), X^2, X^3, X^4, and X^5. Don’t forget to check Rule #3 carefully if you choose transformations. set.seed(121) n = 800 a <- sample(c(\"square\", \"rectangle\", \"trapizoid\", \"paralleogram\"), 400, replace=TRUE) b <- sample(c(\"circle\", \"triangle\", \"octagon\", \"pentagon\", \"star\"), 400, replace= TRUE) x1 <- as.factor(sample(c(a,b), n, replace=FALSE)) # switch with da shapes for the green line x2 <- rexp(n, 10) # junk lol x3 <- sample(c(0,1), n, replace=TRUE) # junk's friend x4 <- sample(c(0,1,2,3,4), n, replace=TRUE) # the crazy lines x5 <- runif(n, -0.5, 1) # THE x x6 <- sample(c(a,b), n, replace=FALSE) x7 <- sample(c(a,b), n, replace=FALSE) x8 <- sample(c(0,1), n, replace=TRUE) # switch for the horizontal line x9 <- sample(c(a,b), n, replace=FALSE) x10 <- sample(c(a,b), n, replace=FALSE) x11 <- ifelse(x1 %in% c(\"square\", \"rectangle\", \"trapizoid\", \"paralleogram\"), 1, 0) # goes with x1, what we are actually using # red line beta0 <- 0.25 beta1 <- -1.6 beta2 <- 0.1 beta3 <- 1.8 #green line beta4 <- 0 beta5 <- 3.7 beta6 <- 9.9 beta7 <- -2.2 beta8 <- -0.6 beta9 <- -0.9 beta10 <- 0 beta11 <- 3.6 beta12 <- 2.9 beta13 <- 2.2 beta14 <- -10 sigma = 0.48 epsilon_i <- rnorm(n, 0, sigma) # x5 is da base line y = beta0 + beta1*x5 + beta2*x5^2 + beta3*x5^3 + beta4*x11 + beta5*x5*x11 + beta6*x5^2*x11 + beta7*x5^3*x11 + beta8*x5^4*x11 + beta9*x5^5*x11 + beta10*x4 + beta11*x4*x5 + beta12*x4*x5*x8 + beta13*x4*x5^2 + beta14*x4*x5^3 + epsilon_i x3=ifelse(y>-15, 0, 1) # CHANGE X6 TO X11 to not give it away!!! rbdata <- data.frame(y,x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11) battle <- lm(y ~ x5 + I(x5^2) + I(x5^3) + x11 + x5:x11 + I(x5^2):x11 + I(x5^3):x11 + I(x5^4):x11 + I(x5^5):x11 + x4 + x4:x5 + x4:x5:x8 + x4:I(x5^2) + x4:I(x5^3), data= rbdata) ship <- coef(battle) b0 <- ship[1] b1 <- ship[2] b2 <- ship[3] b3 <- ship[4] b4 <- ship[5] b5 <- ship[6] b6 <- ship[7] b7 <- ship[8] b8 <- ship[9] b9 <- ship[10] b10 <- ship[11] b11 <- ship[12] b12 <- ship[13] b13 <- ship[14] b14 <- ship[15] summary(battle) ## ## Call: ## lm(formula = y ~ x5 + I(x5^2) + I(x5^3) + x11 + x5:x11 + I(x5^2):x11 + ## I(x5^3):x11 + I(x5^4):x11 + I(x5^5):x11 + x4 + x4:x5 + x4:x5:x8 + ## x4:I(x5^2) + x4:I(x5^3), data = rbdata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.49213 -0.31397 0.00872 0.33988 1.44087 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 0.190877 0.062480 3.055 0.00233 ** ## x5 -1.577682 0.150384 -10.491 < 2e-16 *** ## I(x5^2) 0.813208 0.456275 1.782 0.07509 . ## I(x5^3) 1.136611 0.550155 2.066 0.03916 * ## x11 0.062182 0.066327 0.938 0.34878 ## x4 0.008266 0.021551 0.384 0.70141 ## x5:x11 3.475976 0.241839 14.373 < 2e-16 *** ## I(x5^2):x11 9.041042 0.951654 9.500 < 2e-16 *** ## I(x5^3):x11 -1.002297 1.108664 -0.904 0.36624 ## x11:I(x5^4) 1.321639 3.386218 0.390 0.69642 ## x11:I(x5^5) -3.221683 2.617485 -1.231 0.21875 ## x5:x4 3.619493 0.054622 66.265 < 2e-16 *** ## I(x5^2):x4 2.099632 0.153807 13.651 < 2e-16 *** ## I(x5^3):x4 -9.945012 0.186287 -53.385 < 2e-16 *** ## x5:x4:x8 2.911685 0.027309 106.621 < 2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.4805 on 785 degrees of freedom ## Multiple R-squared: 0.9863, Adjusted R-squared: 0.9861 ## F-statistic: 4043 on 14 and 785 DF, p-value: < 2.2e-16 ggpairs(rbdata, panel = panel.smooth) ## Warning in warn_if_args_exist(list(...)): Extra arguments: \"panel\" are being ## ignored. If these are meant to be aesthetics, submit them using the 'mapping' ## variable within ggpairs with ggplot2::aes or ggplot2::aes_string. ## Warning in cor(x, y): the standard deviation is zero ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning in cor(x, y): the standard deviation is zero ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Your sample size must be sufficiently large so that when the true model is fit to your data using lm(…), all p-values of terms found in the summary(...) output are significant at the \\(\\alpha = 0.05\\) level. The \\(R^2\\) value (“Multiple R-squared”, not the “Adjusted R-squared”) of your true model fit on your rbdata sample must be greater than or equal to \\(0.30\\). The higher your \\(R^2\\) value, the more impressive your model. Your true model must be 2D-drawable. This means that it can be drawn in both Desmos and with a single 2D scatterplot in R. library(ggplot2) ggplot(rbdata, aes(x = x5, y = y, color = as.factor(x11))) + # True red line stat_function( fun = function(x) beta0 + beta1*x + beta2*x^2 + beta3*x^3, color = \"red\", linetype = \"solid\", linewidth = 1 ) + # True green line stat_function( fun = function(x) (beta4 + beta10) + (beta5 + beta11)*x + (beta6 + beta12)*x^2 + (beta7 + beta13)*x^3 + (beta8+ beta14)*x^4 + beta9*x^5, color = \"darkgreen\", linetype = \"solid\", linewidth = 1 ) + stat_function(fun = function(x) 0.25 + 1*x, color = \"black\", linewidth = 1) + stat_function(fun = function(x) 0.25 + 2*x, color = \"pink\", linewidth = 1) + stat_function(fun = function(x) 0.25 + 3*x, color = \"blue\", linewidth = 1) + stat_function(fun = function(x) 0.25 + 4*x, color = \"purple\", linewidth = 1) + coord_cartesian(ylim = c(0, 0.4), xlim = c(-0.2, 0.2)) + labs(color = \"x11 Group\") + theme_minimal() # stat_fuction ( add the appropriate betas together) Intermediate Level Competition Competing in the Intermediate Level will only allow you to earn up to 88% of the full credit that is possible on the Regression Battleship portion of your grade in Math 425 (which is 15% of your Final Grade). However, getting 88% of the grade is better than failing the advanced level competition and getting 0% of the Regression Battleship grade. So choose this option if you are not feeling comfortable with your abilities to compete at the Advanced Level. The good news is that if you choose this option, your teacher can help you with your model to make sure everything is correct before you turn it in. For the Intermediate Level competition, there are also five official rules your model must abide by. If you break any of the rules, you will be disqualified from winning the Intermediate Level competition and a point penalty will be applied to your grade. Your csv file rbdata.csv must contain 6 columns of data. The first column must be your (1) y-variable (labeled as y). The other five columns must be (5) x-variables (labeled as x1, x2, x3, x4 , x5). Please use all lower-case letters. It does not matter which x-variables you use in your model, and you don’t need to use all 5 x-variables in your model. Your y-variable must have been created from a linear regression model using only x-variables from within your data set. No transformations of y-variables or x-variables are allowed in the Intermediate Level competition. Your sample size must be sufficiently large so that when the true model is fit to your data using lm(…), all p-values of terms found in the summary(...) output are significant at the \\(\\alpha = 0.05\\) level. The \\(R^2\\) value (“Multiple R-squared”, not the “Adjusted R-squared”) of your true model fit on your rbdata sample must be greater than or equal to \\(0.80\\). Your true model must be 2D-drawable. This means that it can be drawn in Desmos and with a single 2D scatterplot in R. Desmos Start by creating a picture of your true model in Desmos. Snip a screenshot of your completed model. Include a picture of your Desmos graph showing your true model. Code Use the R-chunks below to create your simulated sample of data from your true regression model. set.seed(122) #This ensures the randomness is the \"same\" everytime if you play the entire R-chunk as one entire piece of code. If you run lines separately, your data might not come out the same every time. You can pick any integer value you want for set.seed. Each choice produces a different sample, so you might want to play around with a few different choices. ## To begin, decide on your sample size. (You may have to revise it later to ensure all values in your lm(...) are significant.) n <- 50 ## Then, create 10 X-variables using functions like rnorm(n, mean, sd), rchisq(n, df), rf(n, df1, df2), rt(n, df), rbeta(n, a, b), runif(n, a, b) or sample(c(1,0), n, replace=TRUE)... ## To see what any of these functions do, run codes like hist(rchisq(n, 3)). These functions are simply allowing you to get a random sample of x-values. But the way you choose your x-values can have quite an impact on what the final scatterplot of the data will look like. x1 <- rep(0,n) #replace this x2 <- rep(0,n) #replace this x3 <- rep(0,n) #replace this x4 <- rep(0,n) #replace this x5 <- rep(0,n) #replace this x6 <- rep(0,n) #replace this x7 <- rep(0,n) #replace this x8 <- rep(0,n) #replace this x9 <- rep(0,n) #replace this x10 <- rep(0,n) #replace this ## Then, create betas, sigma, normal error terms and y #beta0 <- ... #beta1 <- ... #... sigma <- 1.72 #change to whatever positive number you want ################################ # You ARE NOT ALLOWED to change this part: epsilon_i <- rnorm(n, 0, sigma) ################################ #An example of how to make Y... # y <- beta0 + beta1*X1 + beta2*X2 + beta3*X4*X2 + epsilon_i y <- 0 #...edit this code and replace it with your model. Don't forget the + epsilon_i! ## Now, you need to load your x-variables and y-variable ## into a data set. # You can include Y' or X' instead of Y or X if you wish. # Remember, only these functions are allowed when transforming # variables: 1/Y^2, 1/Y, log(Y), sqrt(Y), sqrt(sqrt(Y)), Y^2, Y^3, 1/X^2, 1/X, log(X), sqrt(X), sqrt(sqrt(X)), X^2, X^3, X^4, X^5. ######################################################### # ILLEGAL: Y = (beta0 + beta1*X5)^2 + epsilon_i ######### ######################################################### # Legal: sqrt(Y) = beta0 + beta1*X5^2 + epsilon_i ####### ######################################################### # You can only transform individual terms, not groups of terms. # And the beta's cannot be part of the x-transformations. # This loads your data into a data set: rbdata <- data.frame(y, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10) #Now fit your model to make sure it comes out significant: mylm <- lm(y ~ 0, data=rbdata) #edit this code to be your true model summary(mylm) Call: lm(formula = y ~ 0, data = rbdata) Residuals: Min 1Q Median 3Q Max 0 0 0 0 0 No Coefficients Residual standard error: 0 on 50 degrees of freedom #all p-values must be significant #the R^2 value must be greater than or equal to 0.30. # Once you are done with creating your model, and have successfully # graphed it (see below), un-comment the following `write.csv` code, # then, PLAY this ENTIRE R-chunk to write your data to a csv. # write.csv(rbdata, \"rbdata.csv\", row.names=FALSE) # The above code writes the dataset to your \"current directory\" # To see where that is, use: getwd() in your Console. # Find the rbdata.csv data set and upload it to I-Learn. R Plot Provide a 2D scatterplot that shows both your true model (dashed lines) and estimated model (solid lines) on the same scatterplot. This should match your Desmos graph. Math Model Write out your “true” model in mathematical form. Make sure it matches your code. This could be “painful” if you chose a complicated model. \\[ \\text{EXAMPLE:} \\ Y_i = \\beta_0 + \\beta_1 X_{4i} + \\beta_2 X_{2i} + \\beta_3 X_{4i} X_{2i} + \\epsilon_i \\] Results Once the Regression Battleship competition is completed, you will be given instructions on how to complete this section. The basic idea is to compare the three guesses at your true model (from two peers, and your teacher) to decide who won (i.e., who had the closest guess). // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Untitled","url":"425Analyses/RegressionModelsGalore.html","content":"Untitled 2025-02-19 library(mosaic) ## Registered S3 method overwritten by 'mosaic': ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The 'mosaic' package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: 'mosaic' ## The following objects are masked from 'package:dplyr': ## ## count, do, tally ## The following object is masked from 'package:Matrix': ## ## mean ## The following object is masked from 'package:ggplot2': ## ## stat ## The following objects are masked from 'package:stats': ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## The following objects are masked from 'package:base': ## ## max, mean, min, prod, range, sample, sum library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ lubridate 1.9.3 ✔ tibble 3.2.1 ## ✔ purrr 1.0.2 ✔ tidyr 1.3.1 ## ✔ readr 2.1.5 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ mosaic::count() masks dplyr::count() ## ✖ purrr::cross() masks mosaic::cross() ## ✖ mosaic::do() masks dplyr::do() ## ✖ tidyr::expand() masks Matrix::expand() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ tidyr::pack() masks Matrix::pack() ## ✖ mosaic::stat() masks ggplot2::stat() ## ✖ mosaic::tally() masks dplyr::tally() ## ✖ tidyr::unpack() masks Matrix::unpack() ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors library(pander) library(DT) library(ggrepel) library(plotly) ## ## Attaching package: 'plotly' ## ## The following object is masked from 'package:mosaic': ## ## do ## ## The following object is masked from 'package:ggplot2': ## ## last_plot ## ## The following object is masked from 'package:stats': ## ## filter ## ## The following object is masked from 'package:graphics': ## ## layout library(dplyr) library(ggplot2) library(maps) ## ## Attaching package: 'maps' ## ## The following object is masked from 'package:purrr': ## ## map library(tmap) ## Breaking News: tmap 3.x is retiring. Please test v4, e.g. with ## remotes::install_github('r-tmap/tmap') library(leaflet) library(htmltools) ## ## Attaching package: 'htmltools' ## ## The following object is masked from 'package:pander': ## ## p library(car) ## Loading required package: carData ## ## Attaching package: 'car' ## ## The following object is masked from 'package:purrr': ## ## some ## ## The following objects are masked from 'package:mosaic': ## ## deltaMethod, logit ## ## The following object is masked from 'package:dplyr': ## ## recode library(mosaicData) library(ResourceSelection) ## ResourceSelection 0.3-6 2023-06-27 library(reshape2) ## ## Attaching package: 'reshape2' ## ## The following object is masked from 'package:tidyr': ## ## smiths library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) ## Warning: package 'knitr' was built under R version 4.4.2 library(kableExtra) ## Warning: package 'kableExtra' was built under R version 4.4.2 ## ## Attaching package: 'kableExtra' ## ## The following object is masked from 'package:dplyr': ## ## group_rows library(formattable) ## Warning: package 'formattable' was built under R version 4.4.2 ## ## Attaching package: 'formattable' ## ## The following object is masked from 'package:plotly': ## ## style library(haven) # Class Activity Expanding the Regression Model (QUADRATIC MODEL) : set.seed(12192021) n <- 89 X_i <- runif(n, 1.5, 6.5) # Betas and such beta0 <- -1 beta1 <- 6.6 beta2 <- -0.7 sigma <- 0.59 epsilon_i <- rnorm(n, -0.011, sigma) Y_i <- beta0 + beta1*X_i + beta2*X_i^2 + epsilon_i fabData <- data.frame(y=Y_i, x=X_i) fab.lm <- lm(y ~ x + I(x^2), data=fabData) summary(fab.lm) ## ## Call: ## lm(formula = y ~ x + I(x^2), data = fabData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.46180 -0.40042 0.03701 0.32967 1.33504 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -0.51889 0.48771 -1.064 0.29 ## x 6.34737 0.26885 23.610 <2e-16 *** ## I(x^2) -0.67289 0.03363 -20.006 <2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.5846 on 86 degrees of freedom ## Multiple R-squared: 0.9193, Adjusted R-squared: 0.9174 ## F-statistic: 489.6 on 2 and 86 DF, p-value: < 2.2e-16 plot(y ~ x, data=fabData, pch=19, size = .3, main=\"Quadratic Regression Relation Diagram\") ## Warning in plot.window(...): \"size\" is not a graphical parameter ## Warning in plot.xy(xy, type, ...): \"size\" is not a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): \"size\" is not a ## graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): \"size\" is not a ## graphical parameter ## Warning in box(...): \"size\" is not a graphical parameter ## Warning in title(...): \"size\" is not a graphical parameter curve(predict(fab.lm, newdata = data.frame(x=x)), add = TRUE, lwd = 1) curve(beta0 +beta1*x+beta2*x^2, lwd = 1, lty = 2, add = TRUE) # Class Activity - Different Types of Models (CUBIC MODEL): set.seed(3793) n <- 30 X_i <- runif(n, -2, 3.8) beta0 <- 2.7 beta1 <- -4.8 beta2 <- -1.9 beta3 <- 1.1 sigma <- 2 epsilon_i <- rnorm(n, -0.01, sigma) Y_i <- beta0 + beta1*X_i + beta2*X_i^2 + beta3*X_i^3 + epsilon_i fabData <- data.frame(y=Y_i, x=X_i) fab.lm <- lm(y ~ x + I(x^2) + I(x^3), data=fabData) summary(fab.lm) ## ## Call: ## lm(formula = y ~ x + I(x^2) + I(x^3), data = fabData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.9456 -1.2826 -0.0244 1.2938 3.9443 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 2.6060 0.7106 3.667 0.00111 ** ## x -5.6983 0.5047 -11.291 1.60e-11 *** ## I(x^2) -2.0373 0.4112 -4.954 3.79e-05 *** ## I(x^3) 1.2435 0.1415 8.790 2.89e-09 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 2.152 on 26 degrees of freedom ## Multiple R-squared: 0.8383, Adjusted R-squared: 0.8197 ## F-statistic: 44.94 on 3 and 26 DF, p-value: 1.988e-10 plot(y ~ x, data=fabData, pch=19, size = .3, main=\"Cubic Regression Relation Diagram\") ## Warning in plot.window(...): \"size\" is not a graphical parameter ## Warning in plot.xy(xy, type, ...): \"size\" is not a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): \"size\" is not a ## graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): \"size\" is not a ## graphical parameter ## Warning in box(...): \"size\" is not a graphical parameter ## Warning in title(...): \"size\" is not a graphical parameter curve(predict(fab.lm, newdata = data.frame(x=x)), add = TRUE, lwd = 1) curve(beta0 +beta1*x+beta2*x^2 + beta3*x^3, lwd = 1, lty = 2, add = TRUE) # Class Activity - Different Types of Models (TWO LINES MODEL) : set.seed(11833) n <- 40 X_i <- runif(n, 30, 52) x2 <- sample(c(0,1), n, replace = TRUE) beta0 <- 58 beta1 <- -0.22 beta2 <- -65 beta3 <- 1.45 sigma <- 2.4 epsilon_i <- rnorm(n, 0, sigma) Y0 <- beta0 + beta1*X_i + epsilon_i Y1 <- (beta0 +beta2) + (beta1 + beta3)*X_i + epsilon_i Y_i <- ifelse(x2 == 0, Y0, Y1) fabData <- data.frame(y=Y_i, x=X_i, x2=x2) fab.lm <- lm(y ~ x + x2 + x:x2, data=fabData) summary(fab.lm) ## ## Call: ## lm(formula = y ~ x + x2 + x:x2, data = fabData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.6120 -1.5767 -0.3176 1.2975 4.6172 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 55.00487 4.11655 13.362 1.60e-15 *** ## x -0.13437 0.09718 -1.383 0.175 ## x2 -62.94836 5.67412 -11.094 3.61e-13 *** ## x:x2 1.40043 0.13755 10.181 3.84e-12 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 2.712 on 36 degrees of freedom ## Multiple R-squared: 0.869, Adjusted R-squared: 0.8581 ## F-statistic: 79.63 on 3 and 36 DF, p-value: 5.869e-16 plot(y ~ x, data=fabData, col=as.factor(x2), pch = 19, size = 1.5, main = \"Two Lines Model\") ## Warning in plot.window(...): \"size\" is not a graphical parameter ## Warning in plot.xy(xy, type, ...): \"size\" is not a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): \"size\" is not a ## graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): \"size\" is not a ## graphical parameter ## Warning in box(...): \"size\" is not a graphical parameter ## Warning in title(...): \"size\" is not a graphical parameter b <- coef(fab.lm) x2 = 0 curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE, col= \"black\") x2 = 1 curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE, col=\"red\") x2=0 curve(beta0 + beta1*x + beta2*x2 +beta3*x*x2, add=TRUE, lty=2, col=\"black\") x2=1 curve(beta0 + beta1*x + beta2*x2 +beta3*x*x2, add=TRUE, lty=2, col=\"red\") # Skills Quiz - Different Types of Models (Problem 3 Part a) mtcars$disp2 <- mtcars$disp^2 drivin.lm <- lm(qsec ~ disp + disp2 + am + disp:am + disp2:am, data = mtcars) summary(drivin.lm) ## ## Call: ## lm(formula = qsec ~ disp + disp2 + am + disp:am + disp2:am, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.59239 -0.58978 0.02637 0.43534 2.35282 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 2.619e+01 1.728e+00 15.155 2.02e-14 *** ## disp -4.937e-02 1.270e-02 -3.888 0.000626 *** ## disp2 6.607e-05 2.133e-05 3.097 0.004646 ** ## am -3.663e+00 2.343e+00 -1.563 0.130076 ## disp:am -2.926e-03 2.335e-02 -0.125 0.901222 ## disp2:am 1.866e-05 5.143e-05 0.363 0.719707 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 1.033 on 26 degrees of freedom ## Multiple R-squared: 0.7197, Adjusted R-squared: 0.6658 ## F-statistic: 13.35 on 5 and 26 DF, p-value: 1.695e-06 ggplot(mtcars, aes(x = disp, y = qsec, color = factor(am))) + geom_point(size = 2) + geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), se = FALSE) + scale_color_manual(name = \"Transmission Type\", values = c(\"skyblue\", \"orange\"), labels = c(\"Automatic\", \"Manual\")) + theme_classic() + labs(title = \"Regression of qsec on disp and Transmission Type\", x = \"Displacement (disp)\", y = \"Quarter Mile Time (qsec)\") # Skills Quiz - Different Types of Models (Problem 4) set.seed(101) n <- 100 X_i <- runif(n, -2, 2) beta0.1 <- -2 beta1.1 <- 3 beta2.1 <- 4 beta0.2 <- 2 beta1.2 <- 5 beta2.2 <- -3 sigma <- 2 epsilon_i <- rnorm(n, 1, sigma) Y_i.1 <- beta0.1 + beta1.1*X_i + beta2.1*X_i^2 + epsilon_i Y_i.2 <- beta0.2 + beta1.2*X_i + beta2.2*X_i^2 + epsilon_i truth.1 <- function(x) -2 + 3*x + 4*x^2 truth.2 <- function(x) 2 + 5*x - 3*x^2 fabData <- data.frame(x = X_i, y = c(Y_i.1, Y_i.2), equation = rep(c(\"y.1\", \"y.2\"), each = n)) fab.lm.1 <- lm(y ~ x + I(x^2), data = fabData, subset = equation == \"y.1\") fab.lm.2 <- lm(y ~ x + I(x^2), data = fabData, subset = equation == \"y.2\") summary(fab.lm.1) ## ## Call: ## lm(formula = y ~ x + I(x^2), data = fabData, subset = equation == ## \"y.1\") ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4172 -1.3192 -0.2103 1.3501 4.2590 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -0.9483 0.3098 -3.061 0.00285 ** ## x 2.9766 0.1678 17.738 < 2e-16 *** ## I(x^2) 3.9480 0.1664 23.725 < 2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 1.993 on 97 degrees of freedom ## Multiple R-squared: 0.8994, Adjusted R-squared: 0.8973 ## F-statistic: 433.7 on 2 and 97 DF, p-value: < 2.2e-16 summary(fab.lm.2) ## ## Call: ## lm(formula = y ~ x + I(x^2), data = fabData, subset = equation == ## \"y.2\") ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4172 -1.3192 -0.2103 1.3501 4.2590 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 3.0517 0.3098 9.851 2.82e-16 *** ## x 4.9766 0.1678 29.656 < 2e-16 *** ## I(x^2) -3.0520 0.1664 -18.341 < 2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 1.993 on 97 degrees of freedom ## Multiple R-squared: 0.9269, Adjusted R-squared: 0.9254 ## F-statistic: 614.6 on 2 and 97 DF, p-value: < 2.2e-16 ggplot(fabData, aes(x = x, y = y, color = equation)) + geom_point(size = 1, pch=19) + stat_function(fun = truth.1, color = \"firebrick\", linetype = \"dashed\", size = 1) + stat_function(fun = truth.2, color = \"midnightblue\", linetype = \"dashed\", size = 1) + geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), se = FALSE) + scale_color_manual(name = \"Equation\", values = c(\"firebrick\", \"midnightblue\")) + theme_minimal() + labs(title = \"Quadratic Regression for Two Models\", x = \"X Values\", y = \"Y Values\") ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Residuals, Sums of Squares, & R Squared","url":"425Analyses/ResidualsSumsofSquaresRSquared.html","content":"Code Show All Code Hide All Code Residuals, Sums of Squares, & R Squared library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) Residual Terms What is a Residual? A residual is just the difference between: What you actually observed (\\(Y_i\\)) What your model predicted (\\(\\hat{Y_i}\\)) \\[r_i = Y_i - \\hat{Y_i}\\] Think of it as “how far off was my prediction of that jar of jelly beans?” Click between tabs for further explanations Hide Residual Explainations Why do residuals matter? They help check if your model is working well by looking for: Whether relationships between variables are truly linear If the spread of errors is consistent If errors follow a normal distribution If data points are independent of each other When looking at residual plots, you want to see points scattered randomly - like if someone threw a bunch of marbles on the floor (accidentally of course). If you see clear patterns, something might be wrong with your model. Real Life Comparison for Residuals Imagine you’re baking cookies: The recipe says they should take exactly 12 minutes to bake But the actual baking times might be: Batch 1: 13 minutes (residual = +1) Batch 2: 11 minutes (residual = -1) Batch 3: 12 minutes (residual = 0) The residual is how far off your actual baking time was from the predicted 12 minutes. Sometimes it’s over, sometimes under, and sometimes exactly right (just depends on how burnt you like your cookies, JK). This helps you see how accurate your recipe’s timing prediction is for each batch of cookies. What is a Sum of Squares Error (SSE)? A SSE is the measurement of how much the residuals(the observed value - the predicted value) deviate from the line(the law). This can also be explained as the amount of variability that is NOT explained by the model. We want this to be small, relative to SSTO (total variability) Can never be negative This is calculated by the following model: \\[SSE = \\underbrace{\\sum_{i=1}^n}_\\text{The sum of} (\\underbrace{Y_i}_\\text{Observed Value(The Dots)} - \\underbrace{\\hat{Y_i}}_\\text{Predicted Value (The Line)})^2 \\] Click between tabs for further explanations Hide SSE Explainations Why does the SSE matter? We want these differences to be small compared to how much your drive times vary overall (SSTO). If they’re small, it means our prediction is doing a good job!! And just like you can’t have a negative amount of variation (you can’t vary “negative minutes” from your prediction), these measures can’t be negative. Real Life Comparison of SSE Think of predicting how long it takes to drive to work: Your actual drive times vary (maybe 20, 25, or 30 minutes depending on things traffic, how fast you drive, who knows?), but your prediction model says it always takes 23 minutes The unexplained variability is all those differences between your actual times and your 23-minute prediction! (aka the SSE) What is a Sum of Squares Regression (SSR)? A SSR is the measurement of how much the regression line (the law) departs from the average y-value (overall mean). This can also be explained as the amount of variability EXPLAINED by the model by showing how far our predicted y values deviate from the overall mean. We want this to be large relative to SSTO Can never be negative This can be calculated by the following model: \\[SSR = \\underbrace{\\sum_{i = 1}^n}_\\text{The sum of} (\\underbrace{\\hat{Y_i}}_\\text{Predicted Y (The Line)} - \\underbrace{\\bar{Y}}_\\text{Average Y (Overall Mean)})^2\\] Click between tabs for further explanations Hide SSR Explainations Why does SSR matter? SSR matters because it tells us how good our predictions are. It shows how much of what we’re trying to predict can actually be explained by our model - A larger SSR means our predictions are more reliable and useful - It helps us decide if our prediction method is worth using Real life Comparison of SSR Imagine predicting pizza delivery times: The delivery app says: Small orders: 20 minutes Medium orders: 30 minutes Large orders: 40 minutes SSR measures how much these categories actually help EXPLAIN delivery times. For example: If order size really DOES determine delivery time, then SSR would be large order size is actually a useful factor for making predictions meaning your prediction system works well If delivery times are RANDOM regardless of order size, then SSR would be small - we might need to consider other factors like time of day or distance instead meaning your prediction system isn’t very helpful Just like you can’t have “negative accuracy” in predictions, SSR can’t be negative. The bigger the SSR compared to total variation (SSTO), the better your prediction model is working. What is a Sum of Squares Total (SSTO)? A SSTO is the measurement of how much the y-values departs from the average y- value. This can also be explained as the total variability of our model. Largest of the three values Can never be negative Key Relationship: SSTO = SSR + SSE -> Total Variation = Explained Variation + Unexplained Variation SSR/SSTO represents the proportion of variability explained by the model (R²) The smallest possible value for all three is 0 SSTO will always be the largest, as it represents total variability in the response variable (y). This is calculated by the following: \\[SSR + SSE = SSTO = \\underbrace{\\sum_{i=1}^n}_\\text{The sum of} (\\underbrace{Y_i}_\\text{Observed Y Values (The Dots)} - \\underbrace{\\bar{Y}}_\\text{Average Y (Overall Mean)})^2\\] Click between tabs for further explanations Hide SSTO Explainations Why does SSTO matter? The total variation (SSTO) helps us to know if our predictions are actually useful or just lucky guesses! A good model has large SSR (Explained Variation) and small SSE (Unexplained Variation) relative to SSTO (Total Variation) if it is the other way around, that means our model is not very good Real Life Comparison of SSTO Imagine you own a coffee shop and want to understand your daily sales patterns: Total Variation (SSTO): Your daily sales vary between 80-150 cups per day This is the total range of how much your sales go up and down This total variation can be broken into two parts: Explained Variation (SSR): Things you can predict like selling more coffee on cold days or your coffee shop is holding a fundraiser If cold days and planned fundraisers consistently mean more sales, this is a reliable pattern Unexplained Variation (SSE): Random things you can’t predict like a surprise business meeting nearby, or a bus full of high schoolers get dropped off here (yikes) These are the mystery factors affecting your sales The better your prediction model, the more of your total variation (SSTO) is explained by your model (SSR), and the less remains unexplained (SSE). What is R-squared? The R-squared is the proportion of variability in Y that can be explained by the regression. Definition breakdown: Proportion: meaning \\(R^2\\) is always between 0 and 1 (aka. 0% - 100%), thus representing how much variation is captured by the model Variability: how spread out the Y values are from their mean Explained: how much of that spread can be accounted for by the repression line \\[R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO} \\] Click between tabs for further explanations Hide R Squared Explainations Why does R Squared matter? It tells us how reliable our predictions are! - Additionally, it shows us how confident we can be in our predictions R-squared VS P-value We can further understand R-squared by how it differs from the p-value for slope: R-squared measures the how well the X variable explains the variation in Y P-value indicates whether the relationship is statistically significant Real Life Comparison for R Squared Imagine you’re analyzing how a cat’s playtime affects their sleep duration: Variability: Your cat’s daily sleep hours vary - some days they sleep a lot, other days less R-squared: If your analysis shows an R-squared of 0.80, this means that 80% of the changes in sleep duration can be explained by how much playtime they had Unexplained variation: The remaining 20% might be due to other factors like weather, visitors in the house, or feeding schedule P-value comparison: The p-value would tell you whether the relationship between playtime and sleep is statistically significant or just random chance. A low p-value would suggest that the relationship is real and not coincidental. This demonstrates the key concepts from the selection: proportion (80% explained), variability (fluctuating sleep patterns), and what can be explained by the regression (playtime’s effect). What is the Mean Squared Error (MSE) & the “Residual Standard Error”(RSE)? The MSE is the measurement of the average squared difference between predicted and actual values - Can be any non-negative number (0 to infinity) - Units are squared units of the original data (e.g., degrees Fahrenheit²) \\[MSE = \\frac{SSE}{n-p}\\] Relationship to R-squared MSE R-Squared measures squared prediction error measures proportion of variance in y explained between 0 and infinity between 0 and 1 (0% - 100%) units are squared units of the original data unitless The Residual Standard Error (RSE) is the square root of MSE. - Found in R regression summary output - Uses same units as original data (e.g., degrees Fahrenheit) \\[RSE = \\sqrt{MSE} = \\sqrt{\\frac{SSE}{n-p}}\\] Click between tabs for further explanations Hide MSE and RSE Explainations Why do the MSE and the RSE matter? Together, they indicate the fit of our model: Lower values of MSE and RSE indicate better model fit They help assess the accuracy of predictions in the original data’s scale Real Life Comparison of MSE and Residual Error Think of predicting daily temperatures: The MSE would be like measuring how far off your temperature predictions are on average, but the errors are squared - If you predict 75°F and it’s actually 73°F - that’s a difference of 2°F, which gets squared to 4°F² - The MSE would be the average of all these squared differences The Residual Standard Error (RSE) would convert this back to the original temperature units by taking the square root - So instead of 4°F², you’d get back to a value in °F - making it more intuitive to understand how far off your predictions typically are Lower values in both cases would mean your temperature predictions are more accurate!! - aka. you’re better at forecasting the actual temperatures that occur! (like a psychic) Application on Weather Prediction Analysis For this study, we were tasked with predicting the “Actual Maximum Air Temperature” for this coming Monday, January 13th at BYU-Idaho. BYU-Idaho is located in the city of Rexburg, Idaho, and thus we will use this city’s weather recordings from timeanddate.com to make our predictions. janweather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/JanWeather.csv\") prediction <- data.frame( STARTMAXTEMP=16, MAXTEMP= 26, label = \"Prediction Point : 26°F\" ) janweathery_plot <- ggplot(janweather, aes(x = STARTMAXTEMP, y = MAXTEMP)) + geom_point( aes( text = paste( \"Date:\", DATE, \"<br>\", \"Start Max Temp. of the Day:\", STARTMAXTEMP, \"\\u00b0F<br>\", \"Max Temp. of the Day:\", MAXTEMP, \"\\u00b0F\" ) ), size = 2, color = \"darkblue\" ) + geom_smooth(method = \"lm\", formula= y~x, se = FALSE, color = \"dodgerblue\") + labs( title = \"Weather Patterns from January 13th's of the Past\", x = \"Max Start Temperature of the Day (\\u00b0F)\", y = \"Max Temperature of the Day (\\u00b0F)\" ) + geom_point(data=prediction, aes(x=STARTMAXTEMP, y=MAXTEMP), size = 3, color= \"red\") + geom_text( data = prediction, aes(x = STARTMAXTEMP, y=MAXTEMP, label = label), nudge_x = -7, nudge_y = 3.6, color= \"red\", size = 3 ) + theme_minimal() ggplotly(janweathery_plot, tooltip = \"text\") {\"x\":{\"data\":[{\"x\":[0,25,18,30,27,3,32,3],\"y\":[10,30,32,39,27,19,39,16],\"text\":[\"Date: 13-Jan-24 <br> Start Max Temp. of the Day: 0 °F<br> Max Temp. of the Day: 10 °F\",\"Date: 13-Jan-23 <br> Start Max Temp. of the Day: 25 °F<br> Max Temp. of the Day: 30 °F\",\"Date: 13-Jan-22 <br> Start Max Temp. of the Day: 18 °F<br> Max Temp. of the Day: 32 °F\",\"Date: 13-Jan-21 <br> Start Max Temp. of the Day: 30 °F<br> Max Temp. of the Day: 39 °F\",\"Date: 13-Jan-20 <br> Start Max Temp. of the Day: 27 °F<br> Max Temp. of the Day: 27 °F\",\"Date: 13-Jan-19 <br> Start Max Temp. of the Day: 3 °F<br> Max Temp. of the Day: 19 °F\",\"Date: 13-Jan-18 <br> Start Max Temp. of the Day: 32 °F<br> Max Temp. of the Day: 39 °F\",\"Date: 13-Jan-17 <br> Start Max Temp. of the Day: 3 °F<br> Max Temp. of the Day: 16 °F\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,0,139,1)\",\"opacity\":1,\"size\":7.559055118110237,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(0,0,139,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[0,0.4050632911392405,0.810126582278481,1.2151898734177216,1.620253164556962,2.0253164556962027,2.4303797468354431,2.8354430379746836,3.240506329113924,3.6455696202531644,4.0506329113924053,4.4556962025316453,4.8607594936708862,5.2658227848101262,5.6708860759493671,6.0759493670886071,6.481012658227848,6.8860759493670889,7.2911392405063289,7.6962025316455698,8.1012658227848107,8.5063291139240498,8.9113924050632907,9.3164556962025316,9.7215189873417724,10.126582278481013,10.531645569620252,10.936708860759493,11.341772151898734,11.746835443037975,12.151898734177214,12.556962025316455,12.962025316455696,13.367088607594937,13.772151898734178,14.177215189873417,14.582278481012658,14.987341772151899,15.39240506329114,15.797468354430379,16.202531645569621,16.60759493670886,17.0126582278481,17.417721518987342,17.822784810126581,18.227848101265824,18.632911392405063,19.037974683544302,19.443037974683545,19.848101265822784,20.253164556962027,20.658227848101266,21.063291139240505,21.468354430379748,21.873417721518987,22.278481012658226,22.683544303797468,23.088607594936708,23.49367088607595,23.898734177215189,24.303797468354428,24.708860759493671,25.11392405063291,25.518987341772153,25.924050632911392,26.329113924050631,26.734177215189874,27.139240506329113,27.544303797468356,27.949367088607595,28.354430379746834,28.759493670886076,29.164556962025316,29.569620253164558,29.974683544303797,30.379746835443036,30.784810126582279,31.189873417721518,31.594936708860757,32],\"y\":[13.682533279548201,13.983512134844078,14.284490990139956,14.585469845435835,14.886448700731712,15.18742755602759,15.488406411323467,15.789385266619345,16.090364121915222,16.391342977211103,16.69232183250698,16.993300687802858,17.294279543098735,17.595258398394613,17.89623725369049,18.197216108986368,18.498194964282245,18.799173819578122,19.100152674874003,19.401131530169881,19.702110385465758,20.003089240761636,20.304068096057513,20.60504695135339,20.906025806649268,21.207004661945149,21.507983517241023,21.808962372536904,22.109941227832778,22.410920083128659,22.711898938424532,23.012877793720413,23.313856649016291,23.614835504312168,23.915814359608049,24.216793214903923,24.517772070199804,24.818750925495678,25.119729780791559,25.420708636087433,25.721687491383314,26.022666346679191,26.323645201975069,26.62462405727095,26.925602912566823,27.226581767862704,27.527560623158578,27.828539478454459,28.129518333750337,28.430497189046214,28.731476044342095,29.032454899637969,29.333433754933846,29.634412610229724,29.935391465525605,30.236370320821479,30.53734917611736,30.838328031413234,31.139306886709115,31.440285742004995,31.741264597300869,32.04224345259675,32.343222307892624,32.644201163188505,32.945180018484379,33.24615887378026,33.547137729076141,33.848116584372015,34.149095439667896,34.45007429496377,34.751053150259651,35.052032005555525,35.353010860851406,35.653989716147287,35.95496857144316,36.255947426739034,36.556926282034915,36.857905137330796,37.15888399262667,37.459862847922551],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"name\":\"fitted values\",\"line\":{\"width\":3.7795275590551185,\"color\":\"rgba(30,144,255,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[16],\"y\":[26],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(255,0,0,1)\",\"opacity\":1,\"size\":11.338582677165356,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(255,0,0,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[9],\"y\":[29.600000000000001],\"text\":\"Prediction Point : 26°F\",\"hovertext\":\"\",\"textfont\":{\"size\":11.338582677165356,\"color\":\"rgba(255,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":43.762557077625573,\"r\":7.3059360730593621,\"b\":40.182648401826491,\"l\":37.260273972602747},\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724},\"title\":{\"text\":\"Weather Patterns from January 13th's of the Past\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.534246575342465},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-1.6000000000000001,33.600000000000001],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"10\",\"20\",\"30\"],\"tickvals\":[0,10,20,30],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"10\",\"20\",\"30\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.6529680365296811,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.68949771689498},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176002,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"Max Start Temperature of the Day (°F)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[8.5500000000000007,40.450000000000003],\"tickmode\":\"array\",\"ticktext\":[\"10\",\"20\",\"30\",\"40\"],\"tickvals\":[10,20,30,40],\"categoryorder\":\"array\",\"categoryarray\":[\"10\",\"20\",\"30\",\"40\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.6529680365296811,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.68949771689498},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176002,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"Max Temperature of the Day (°F)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":null,\"bordercolor\":null,\"borderwidth\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.68949771689498}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"ada418d02837\":{\"x\":{},\"y\":{},\"text\":{},\"type\":\"scatter\"},\"ada44be41fa5\":{\"x\":{},\"y\":{}},\"ada4ab86e6f\":{\"x\":{},\"y\":{}},\"ada4ca86d4f\":{\"x\":{},\"y\":{},\"label\":{}}},\"cur_data\":\"ada418d02837\",\"visdat\":{\"ada418d02837\":[\"function (y) \",\"x\"],\"ada44be41fa5\":[\"function (y) \",\"x\"],\"ada4ab86e6f\":[\"function (y) \",\"x\"],\"ada4ca86d4f\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} This is our mathematical model: \\[\\underbrace{Y"},{"title":"Residuals, Sums of Squares, and R-Squared","url":"425Analyses/ResidualsTheoryAssignment.html","content":"Residuals, Sums of Squares, and R-Squared Table of Contents library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) library(rmarkdown) Click between each section to look at our Concepts or the Application in our Prediction Weather Analysis Residual Concepts What is a Residual? A residual is just the difference between: What you actually observed (\\(Y_i\\)) What your model predicted (\\(\\hat{Y_i}\\)) \\[r_i = Y_i - \\hat{Y_i}\\] Think of it as “how far off was my prediction of that jar of jelly beans?” Click between tabs for further explanations Hide Residual Explainations Why do residuals matter? They help check if your model is working well by looking for: Whether relationships between variables are truly linear If the spread of errors is consistent If errors follow a normal distribution If data points are independent of each other When looking at residual plots, you want to see points scattered randomly - like if someone threw a bunch of marbles on the floor (accidentally of course). If you see clear patterns, something might be wrong with your model. Real Life Comparison for Residuals Imagine you’re baking cookies: The recipe says they should take exactly 12 minutes to bake But the actual baking times might be: Batch 1: 13 minutes (residual = +1) Batch 2: 11 minutes (residual = -1) Batch 3: 12 minutes (residual = 0) The residual is how far off your actual baking time was from the predicted 12 minutes. Sometimes it’s over, sometimes under, and sometimes exactly right (just depends on how burnt you like your cookies, JK). This helps you see how accurate your recipe’s timing prediction is for each batch of cookies. data <- data.frame( BatchNumber = 1:10, ActualTime = c(13, 11, 12, 14, 10, 12, 15, 9, 13, 11), PredictedTime = rep(12, 10) ) # Fit the linear model model <- lm(ActualTime ~ BatchNumber, data = data) data$PredictedValue <- predict(model) # Create a plot with residuals visually connected ggplot(data, aes(x = BatchNumber, y = ActualTime)) + geom_point(color = \"pink\", size = 3) + # Data points geom_smooth(method = \"lm\", se = FALSE, color = \"gray\") + # Regression line geom_segment(aes(x = BatchNumber, y = ActualTime, xend = BatchNumber, yend = PredictedValue), color = \"pink\", linetype = \"solid\", size = 0.8) + # Residuals lines labs(title = \"Linear Regression: Actual Baking Time vs Batch Number\", x = \"Batch Number\", y = \"Actual Baking Time (minutes)\") + theme_minimal() ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## `geom_smooth()` using formula = 'y ~ x' What is a Sum of Squares Error (SSE)? A SSE is the measurement of how much the residuals(the observed value - the predicted value) deviate from the line(the law). This can also be explained as the amount of variability that is NOT explained by the model. We want this to be small, relative to SSTO (total variability) Can never be negative This is calculated by the following model: \\[SSE = \\underbrace{\\sum_{i=1}^n}_\\text{The sum of} (\\underbrace{Y_i}_\\text{Observed Value(The Dots)} - \\underbrace{\\hat{Y_i}}_\\text{Predicted Value (The Line)})^2 \\] Click between tabs for further explanations Hide SSE Explainations Why does the SSE matter? We want these differences to be small compared to how much your drive times vary overall (SSTO). If they’re small, it means our prediction is doing a good job!! And just like you can’t have a negative amount of variation (you can’t vary “negative minutes” from your prediction), these measures can’t be negative. Real Life Comparison of SSE Think of predicting how long it takes to drive to work: Your actual drive times vary (maybe 20, 25, or 30 minutes depending on things traffic, how fast you drive, who knows?), but your prediction model says it always takes 23 minutes The unexplained variability is all those differences between your actual times and your 23-minute prediction! (aka the SSE) What is a Sum of Squares Regression (SSR)? A SSR is the measurement of how much the regression line (the law) deviates from the average y-value (overall mean). This can also be explained as the amount of variability EXPLAINED by the model by showing how far our predicted y values deviate from the overall mean. We want this to be large relative to SSTO Can never be negative This can be calculated by the following model: \\[SSR = \\underbrace{\\sum_{i = 1}^n}_\\text{The sum of} (\\underbrace{\\hat{Y_i}}_\\text{Predicted Y (The Line)} - \\underbrace{\\bar{Y}}_\\text{Average Y (Overall Mean)})^2\\] Click between tabs for further explanations Hide SSR Explainations Why does SSR matter? SSR matters because it tells us how good our predictions are. It shows how much of what we’re trying to predict can actually be explained by our model - A larger SSR means our predictions are more reliable and useful - It helps us decide if our prediction method is worth using Real life Comparison of SSR Imagine predicting pizza delivery times: The delivery app says: Small orders: 20 minutes Medium orders: 30 minutes Large orders: 40 minutes SSR measures how much these categories actually help EXPLAIN delivery times. For example: If order size really DOES determine delivery time, then SSR would be large order size is actually a useful factor for making predictions meaning your prediction system works well If delivery times are RANDOM regardless of order size, then SSR would be small - we might need to consider other factors like time of day or distance instead meaning your prediction system isn’t very helpful Just like you can’t have “negative accuracy” in predictions, SSR can’t be negative. The bigger the SSR compared to total variation (SSTO), the better your prediction model is working. What is a Sum of Squares Total (SSTO)? A SSTO is the measurement of how much the y-values deviate from the average y- value. This can also be explained as the total variability of our model. Largest of the three values Can never be negative Key Relationship: SSTO = SSR + SSE -> Total Variation = Explained Variation + Unexplained Variation SSR/SSTO represents the proportion of variability explained by the model (R²) The smallest possible value for all three is 0 SSTO will always be the largest, as it represents total variability in the response variable (y). This is calculated by the following: \\[SSR + SSE = SSTO = \\underbrace{\\sum_{i=1}^n}_\\text{The sum of} (\\underbrace{Y_i}_\\text{Observed Y Values (The Dots)} - \\underbrace{\\bar{Y}}_\\text{Average Y (Overall Mean)})^2\\] Click between tabs for further explanations Hide SSTO Explainations Why does SSTO matter? The total variation (SSTO) helps us to know if our predictions are actually useful or just lucky guesses! A good model has large SSR (Explained Variation) and small SSE (Unexplained Variation) relative to SSTO (Total Variation) if it is the other way around, that means our model is not very good Real Life Comparison of SSTO Imagine you own a coffee shop and want to understand your daily sales patterns: Total Variation (SSTO): Your daily sales vary between 80-150 cups per day This is the total range of how much your sales go up and down This total variation can be broken into two parts: Explained Variation (SSR): Things you can predict like selling more coffee on cold days or your coffee shop is holding a fundraiser If cold days and planned fundraisers consistently mean more sales, this is a reliable pattern Unexplained Variation (SSE): Random things you can’t predict like a surprise business meeting nearby, or a bus full of high schoolers get dropped off here (yikes) These are the mystery factors affecting your sales The better your prediction model, the more of your total variation (SSTO) is explained by your model (SSR), and the less remains unexplained (SSE). What is R-squared? The R-squared is the proportion of variability in Y that can be explained by the regression. \\[R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO} \\] Correlation: ranges from -1.00 to +1.00 Strong correlation -> closer to +/- 1 Weaker correlation -> farther from +/- 1 (ex. 0) Direction of relationship: Positive R-squared: Variables increase or decrease together. Negative R-squared: Variables move in opposite directions - as one increases, the other decreases. Click between tabs for further explanations Hide R Squared Explainations Why does R Squared matter? It tells us how reliable our predictions are! - Additionally, it shows us how confident we can be in our predictions (using correlation and strength) R-squared VS P-value We can further understand R-squared by how it differs from the p-value for slope: R-squared measures the strength of the relationship P-value indicates whether the relationship is statistically significant Real Life Comparison for R Squared Imagine trying to predict ice cream sales based on temperature: R-squared tells you how much temperature actually explains ice cream sales: If R-squared = 0.80 (or 80%): - Temperature explains 80% of why ice cream sales go up or down - The other 20% might be due to other factors like holidays or promotions Correlation in this scenario works like this: Perfect Positive Correlation (+1.00): If every 1°F increase in temperature meant exactly 5 more ice creams sold 70°F = 100 ice creams 71°F = 105 ice creams 72°F = 110 ice creams Strong Positive Correlation (around +0.8): Temperature usually drives sales, but it’s not perfect Hot day (90°F) = around 200 ice creams But some hot days might only sell 180, others might sell 220 Other factors like weekends or holidays affect sales too No Correlation (0): If ice cream sales were completely random regardless of temperature Could sell 150 ice creams on both a 60°F day and a 90°F day Temperature tells us nothing about likely sales Negative Correlation (towards -1.00): This could be used to describe hot coffee sales instead! As temperature goes up, ice cream sales increase (positive correlation) but hot coffee sales decrease (negative correlation) The stronger the pattern, the closer to -1.00 Think of it as how confident you can be in your predictions based on the relationship between two things. What is the Mean Squared Error (MSE) & the “Residual Standard Error”(RSE)? The MSE is the measurement of the average squared difference between predicted and actual values - Can be any non-negative number (0 to infinity) - Units are squared units of the original data (e.g., degrees Fahrenheit²) \\[MSE = \\frac{SSE}{n-p}\\] Relationship to R-squared MSE R-Squared measures prediction error measures variance explained between 0 and infinity between 0 and 1 (0% - 100%) units are squared units of the original data unitless The Residual Standard Error (RSE) is the square root of MSE. - Found in R regression summary output - Uses same units as original data (e.g., degrees Fahrenheit) \\[RSE = \\sqrt{MSE} = \\sqrt{\\frac{SSE}{n-p}}\\] Click between tabs for further explanations Hide MSE and RSE Explainations Why do the MSE and the RSE matter? Together, they indicate the fit of our model: Lower values of MSE and RSE indicate better model fit They help assess the accuracy of predictions in the original data’s scale Real Life Comparison of MSE and Residual Error Think of predicting daily temperatures: The MSE would be like measuring how far off your temperature predictions are on average, but the errors are squared - If you predict 75°F and it’s actually 73°F - that’s a difference of 2°F, which gets squared to 4°F² - The MSE would be the average of all these squared differences The Residual Standard Error (RSE) would convert this back to the original temperature units by taking the square root - So instead of 4°F², you’d get back to a value in °F - making it more intuitive to understand how far off your predictions typically are Lower values in both cases would mean your temperature predictions are more accurate!! - aka. you’re better at forecasting the actual temperatures that occur! (like a psychic) Application on Weather Prediction Analysis For this study, we were tasked with predicting the “Actual Maximum Air Temperature” for this coming Monday, January 13th at BYU-Idaho. BYU-Idaho is located in the city of Rexburg, Idaho, and thus we will use this city’s weather recordings from timeanddate.com to make our predictions. janweather <- read.csv(\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/JanWeather.csv\") prediction <- data.frame( STARTMAXTEMP=16, MAXTEMP= 26, label = \"Prediction Point : 26°F\" ) janweathery_plot <- ggplot(janweather, aes(x = STARTMAXTEMP, y = MAXTEMP)) + geom_point( aes( text = paste( \"Date:\", DATE, \"<br>\", \"Start Max Temp. of the Day:\", STARTMAXTEMP, \"\\u00b0F<br>\", \"Max Temp. of the Day:\", MAXTEMP, \"\\u00b0F\" ) ), size = 2, color = \"darkblue\" ) + geom_smooth(method = \"lm\", formula= y~x, se = FALSE, color = \"dodgerblue\") + labs( title = \"Weather Patterns from January 13th's of the Past\", x = \"Max Start Temperature of the Day (\\u00b0F)\", y = \"Max Temperature of the Day (\\u00b0F)\" ) + geom_point(data=prediction, aes(x=STARTMAXTEMP, y=MAXTEMP), size = 3, color= \"red\") + geom_text( data = prediction, aes(x = STARTMAXTEMP, y=MAXTEMP, label = label), nudge_x = -7, nudge_y = 3.6, color= \"red\", size = 3 ) + theme_minimal() ggplotly(janweathery_plot, tooltip = \"text\") {\"x\":{\"data\":[{\"x\":[0,25,18,30,27,3,32,3],\"y\":[10,30,32,39,27,19,39,16],\"text\":[\"Date: 13-Jan-24 <br> Start Max Temp. of the Day: 0 °F<br> Max Temp. of the Day: 10 °F\",\"Date: 13-Jan-23 <br> Start Max Temp. of the Day: 25 °F<br> Max Temp. of the Day: 30 °F\",\"Date: 13-Jan-22 <br> Start Max Temp. of the Day: 18 °F<br> Max Temp. of the Day: 32 °F\",\"Date: 13-Jan-21 <br> Start Max Temp. of the Day: 30 °F<br> Max Temp. of the Day: 39 °F\",\"Date: 13-Jan-20 <br> Start Max Temp. of the Day: 27 °F<br> Max Temp. of the Day: 27 °F\",\"Date: 13-Jan-19 <br> Start Max Temp. of the Day: 3 °F<br> Max Temp. of the Day: 19 °F\",\"Date: 13-Jan-18 <br> Start Max Temp. of the Day: 32 °F<br> Max Temp. of the Day: 39 °F\",\"Date: 13-Jan-17 <br> Start Max Temp. of the Day: 3 °F<br> Max Temp. of the Day: 16 °F\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,0,139,1)\",\"opacity\":1,\"size\":7.559055118110237,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(0,0,139,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[0,0.4050632911392405,0.810126582278481,1.2151898734177216,1.620253164556962,2.0253164556962027,2.4303797468354431,2.8354430379746836,3.240506329113924,3.6455696202531644,4.0506329113924053,4.4556962025316453,4.8607594936708862,5.2658227848101262,5.6708860759493671,6.0759493670886071,6.481012658227848,6.8860759493670889,7.2911392405063289,7.6962025316455698,8.1012658227848107,8.5063291139240498,8.9113924050632907,9.3164556962025316,9.7215189873417724,10.126582278481013,10.531645569620252,10.936708860759493,11.341772151898734,11.746835443037975,12.151898734177214,12.556962025316455,12.962025316455696,13.367088607594937,13.772151898734178,14.177215189873417,14.582278481012658,14.987341772151899,15.39240506329114,15.797468354430379,16.202531645569621,16.60759493670886,17.0126582278481,17.417721518987342,17.822784810126581,18.227848101265824,18.632911392405063,19.037974683544302,19.443037974683545,19.848101265822784,20.253164556962027,20.658227848101266,21.063291139240505,21.468354430379748,21.873417721518987,22.278481012658226,22.683544303797468,23.088607594936708,23.49367088607595,23.898734177215189,24.303797468354428,24.708860759493671,25.11392405063291,25.518987341772153,25.924050632911392,26.329113924050631,26.734177215189874,27.139240506329113,27.544303797468356,27.949367088607595,28.354430379746834,28.759493670886076,29.164556962025316,29.569620253164558,29.974683544303797,30.379746835443036,30.784810126582279,31.189873417721518,31.594936708860757,32],\"y\":[13.682533279548201,13.983512134844078,14.284490990139956,14.585469845435835,14.886448700731712,15.18742755602759,15.488406411323467,15.789385266619345,16.090364121915222,16.391342977211103,16.69232183250698,16.993300687802858,17.294279543098735,17.595258398394613,17.89623725369049,18.197216108986368,18.498194964282245,18.799173819578122,19.100152674874003,19.401131530169881,19.702110385465758,20.003089240761636,20.304068096057513,20.60504695135339,20.906025806649268,21.207004661945149,21.507983517241023,21.808962372536904,22.109941227832778,22.410920083128659,22.711898938424532,23.012877793720413,23.313856649016291,23.614835504312168,23.915814359608049,24.216793214903923,24.517772070199804,24.818750925495678,25.119729780791559,25.420708636087433,25.721687491383314,26.022666346679191,26.323645201975069,26.62462405727095,26.925602912566823,27.226581767862704,27.527560623158578,27.828539478454459,28.129518333750337,28.430497189046214,28.731476044342095,29.032454899637969,29.333433754933846,29.634412610229724,29.935391465525605,30.236370320821479,30.53734917611736,30.838328031413234,31.139306886709115,31.440285742004995,31.741264597300869,32.04224345259675,32.343222307892624,32.644201163188505,32.945180018484379,33.24615887378026,33.547137729076141,33.848116584372015,34.149095439667896,34.45007429496377,34.751053150259651,35.052032005555525,35.353010860851406,35.653989716147287,35.95496857144316,36.255947426739034,36.556926282034915,36.857905137330796,37.15888399262667,37.459862847922551],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"name\":\"fitted values\",\"line\":{\"width\":3.7795275590551185,\"color\":\"rgba(30,144,255,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[16],\"y\":[26],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(255,0,0,1)\",\"opacity\":1,\"size\":11.338582677165356,\"symbol\":\"circle\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(255,0,0,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[9],\"y\":[29.600000000000001],\"text\":\"Prediction Point : 26°F\",\"hovertext\":\"\",\"textfont\":{\"size\":11.338582677165356,\"color\":\"rgba(255,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":43.762557077625573,\"r\":7.3059360730593621,\"b\":40.182648401826491,\"l\":37.260273972602747},\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724},\"title\":{\"text\":\"Weather Patterns from January 13th's of the Past\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.534246575342465},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-1.6000000000000001,33.600000000000001],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"10\",\"20\",\"30\"],\"tickvals\":[0,10,20,30],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"10\",\"20\",\"30\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.6529680365296811,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.68949771689498},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176002,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"Max Start Temperature of the Day (°F)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[8.5500000000000007,40.450000000000003],\"tickmode\":\"array\",\"ticktext\":[\"10\",\"20\",\"30\",\"40\"],\"tickva"},{"title":"Sampling Distributions Unveiled","url":"425Analyses/SamplingDistributionsUnveiled.html","content":"Code Show All Code Hide All Code Sampling Distributions Unveiled library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) Sampling Distributions of the Slope and intercept of a Regression Line Definition A sampling distribution is the distribution of a sample statistic calculated from repeated random samples from a population. It’s key to making inferences about a population based on limited sample information. Population Model Example: Macaron Amount VS. Joy People Feel Consider macaron diameter (Y) as a function of baking time (X): \\[\\underbrace{Y_i}_\\text{some text} = \\overbrace{\\beta_0}^\\text{some value} + \\overbrace{\\beta_1}^\\text{some value} \\underbrace{X_i}_\\text{some label} + \\epsilon_i \\text{ where} \\epsilon_i \\sim N(0, \\overbrace{\\sigma^2}^\\text{some value}\\] Where: β₀ = 35 (base macaron diameter in millimeters) β₁ = 0.8 (increase in diameter per minute of baking) ϵᵢ ~ N(0, 2²) (random error with standard deviation of 2mm) X = baking time in minutes (ranging from 12 to 18 minutes) Sampling Distribution Properties The sampling distributions of b₀ (intercept) and b₁ (slope) have these characteristics: Mean of Sampling Distributions E(b₀) = β₀ = 35 (unbiased estimator of base diameter) E(b₁) = β₁ = 0.8 (unbiased estimator of size change rate) Standard Error Effects The standard error (SE) of both b₀ and b₁ is affected by: 1. Sample Size (n) SE decreases as more batches of macarons are measured SE ∝ 1/√n 2. Error Standard Deviation (σ) SE increases with more variation in macaron sizes SE ∝ σ 3. Range of X-values SE decreases with wider range of baking times tested Testing more diverse baking times gives more precise estimates The standard error in regression output measures the typical deviation of the sample estimates (b₀ and b₁) from their true population values (β₀ and β₁). This helps determine the precision of our estimates and construct confidence intervals for predicting macaron sizes. P - Values Definition help us determine if our results are statistically significant by measuring how likely we would get these results by chance Standard Errors We use standard errors to calculate t-values, which tell us how far our estimates are from zero in terms of standard deviations Confidence Intervals Definition The ranges where we believe the truth lies A 95% confidence interval means if we repeated the study many times, about 95% of these intervals would contain the true value gofast2 <- lm(dist ~ sqrt(sqrt(speed)), data = cars) plot(dist ~ speed, data = cars) abline(gofast2) // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); $(document).ready(function () { window.initializeCodeFolding(\"hide\" === \"show\"); }); $(document).ready(function () { // temporarily add toc-ignore selector to headers for the consistency with Pandoc $('.unlisted.unnumbered').addClass('toc-ignore') // move toc-ignore selectors from section div to header $('div.section.toc-ignore') .removeClass('toc-ignore') .children('h1,h2,h3,h4,h5').addClass('toc-ignore'); // establish options var options = { selectors: \"h1,h2,h3,h4,h5\", theme: \"bootstrap3\", context: '.toc-content', hashGenerator: function (text) { return text.replace(/[.\\\\/?&!#<>]/g, '').replace(/\\s/g, '_'); }, ignoreSelector: \".toc-ignore\", scrollTo: 0 }; options.showAndHide = true; options.smoothScroll = true; // tocify var toc = $(\"#TOC\").tocify(options).data(\"toc-tocify\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Applied Linear Regression Notes","url":"425PaigesNotes.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Download Rmd Applied Linear Regression Notes (MATH 425) Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts Don’t forget to always load your libraries and to Knit often!! library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) library(alr4) Personal Notes OoOoooOOOoo let’s go assignments!! LETS GOOOOO!!!! Hint: Don’t worry about fully getting the code, always make sure you are actively listening and watching up front. Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester. As well as some personal notes on those assignments! Week 1 | Simple Linear Regression We were looking at “The Mathematical Model” and “Interpreting Model Parameters”! Wow! Skill Quiz - Simple Linear Regression Problem 1 Open the airquality dataset in R. Perform a regression of daily average Wind(\\(Y_i\\)) speed using the daily maximum Temperature (\\(X_i\\)) as the explanatory variable. Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{Wind} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{Temp} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0,\\sigma^2) \\] Part (b) Fit and summarize a simple linear regression model for this data. # Type your code there templm <- lm(Wind ~ Temp, data=airquality) summary(templm) %>% pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 23.23 2.112 11 4.901e-21 Temp -0.1705 0.02693 -6.331 2.642e-09 Fitting linear model: Wind ~ Temp Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 153 3.142 0.2098 0.2045 Part (c) Type out the estimated equation for this regression model using your estimates found in part (b). \\[ \\hat{Y}_i = 23.234 + (-0.170)X_i \\] Part (d) Plot the data and the estimated regression model. # Type your code here ggplot(airquality, aes(x=Temp, y=Wind)) + geom_point(size=1.5, color=\"lightblue\", alpha= 0.5) + geom_smooth(method=\"lm\", formula=y~x, se=FALSE, size=0.5,color=\"skyblue\")+ theme_minimal() Part (e) Use your model to predict the average daily average Wind speed for an outside temperature of 72 degrees F. \\[ \\hat{Y}_i = 23.234 + (-0.170)(72) \\] # Type your code here 23.234 + (-0.170)*(72) ## [1] 10.994 Part (f) Write out an interpretation of the slope and intercept of your model. Are both meaningful for this data? Slope Interpretation: An increase of 1 degree F in the daily maximum Temperature(\\(X_i\\)) results in a 0.17 mph decrease in the average daily average Wind ($Y_i) speed. Is the slope meaningful?: Yes, the regression seems to be a good fit to this data and the slope is significant so the interpretation of the slope is meaningful for this data. Intercept Interpretation: When the daily maximum Temperature is 0 degrees F, the average daily average Wind speed is estimated to be 23.234 mph. Is the intercept meaningful?: Yes, the intercept is meaningful for this data because it is significant and an outside temperature of 0 degrees F is a meaningful situation. Problem 2 Open the mtcars dataset in R. Fit a regression model to the data that can be used to explain average gas mileage of a vehicle (mpg) (\\(Y_i\\)) using the weight (wt) (\\(X_i\\)) of the vehicle. Part (a) Type out the mathematical equation for this regression model and label both \\(Y\\) and \\(X\\) in the equation. \\[ \\underbrace{Y_i}_\\text{mpg} = \\beta_0 + \\beta_1 \\underbrace{X_i}_\\text{wt} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0,\\sigma^2) \\] Part (b) Fit and summarize a simple linear regression model for this data. carslm <- lm(mpg ~ wt, data=mtcars) summary(carslm)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 Fitting linear model: mpg ~ wt Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 32 3.046 0.7528 0.7446 Part (c) Type out the estimated equation for this regression model using your estimates found in part (b). \\[ \\hat{Y}_i = 37.29 + (-5.34)X_i \\] Part (d) Plot the data and the estimated regression model. # Type your code here ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point(size=1.5, color=\"lightblue\", alpha= 0.5) + geom_smooth(method=\"lm\", formula=y~x, se=FALSE, size=0.5,color=\"skyblue\")+ theme_minimal() Part (e) Use your model to predict the average gas mileage (mpg) for a vehicle that weighs 3,000 lbs. (Hint: ?mtcars) **Must convert 3000 lbs by dividing by 1000, thus it would make it 3* \\[ \\hat{Y}_i = 37.29 + (-5.34)(3) \\] ANSWER: # Type your code here 37.29 + (-5.34)*(3) ## [1] 21.27 Part (f) Write out an interpretation of the slope and intercept of your model. Are both meaningful for this data? Slope Interpretation: An increase of 1,000 lbs in the weight (\\(X_i\\)) of a vehicle results in a 5.34 mpg decrease in the average gas mileage of such vehicles(\\(Y_i\\)). Is the slope meaningful? : While the slope is significant, it only looks to be meaningful for vehicles that weigh between 2.5 thousand and 4 thousand pounds. Otherwise this regression does not seem to be a good fit for this data based on the scatterplot. Intercept Interpretation: The average gas mileage of vehicles that weigh nothing (0 lbs) is estimated to be 37.29 mpg. Is the intercept meaningful?: No, the intercept is not meaningful for this data because a vehicle with weight zero is not possible Before we can really trust the interpretation of and predictions from a regression model, there are important diagnostic checks to perform on the regression. These diagnostics are even more important to perform when p-values or confidence intervals are used as part of the analysis. In future weeks of this course, we will focus in greater detail on the technical details of regression: hypothesis tests, confidence intervals, and diagnostic checks. However, for the sake of completeness, the following problems have run through these technical details, even though we lack full understanding about them for the time being. Problem 3 Use your regression for the airquality data set in Problem 1 to complete the following “technical details” for this regression. Part (a) Create a (1) residuals vs. fitted-values plot, (2) Q-Q Plot of the residuals, and (3) residuals vs. order plot. # Type your code here par(mfrow=c(1,3)) plot(templm, which=1) qqPlot(templm$residuals, main=\"Q-Q Plot\",pch= 19, id=FALSE) plot(templm$residuals, ylab= \"Residuals\", main=\"Residuals vs Order\") Part (b) Explain, as best you understand currently, what each of these three plots show for this regression. Explanation: Everything looks pretty good. The residuals vs. fitted-values plot shows constant variance and a nice linear relation. The Q-Q Plot shows possible problems with normality because some dots go out of bounds, but is fairly good. The residuals vs. order plot shows no problems with time trends, so the error terms can be assumed to be independent. Part (c) Report the p-value for the test of these hypotheses for your regression. Intercept Hypotheses \\[ H_0: \\beta_0 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_0 \\neq 0 \\] P-VALUE = < 2e-16 Slope Hypotheses \\[ H_0: \\beta_1 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_1 \\neq 0 \\] P-VALUE = < 2.64e-09 Comment on whether or not we should trust each p-value based on your plots in Part (a). Should trust each p-value based on your plots in Part (a)? Intercept Hypotheses p-value? : Yes, it can be trusted because the diagnostic plots all checked out. Slope Hypotheses p-value? : Yes, it can be trusted because the diagnostic plots all checked out. Problem 4 Use your regression for the mtcars data set in Problem 2 to complete the following “technical details” for this regression. Part (a) Create a (1) residuals vs. fitted-values plot, (2) Q-Q Plot of the residuals, and (3) residuals vs. order plot. # Type your code here par(mfrow=c(1,3)) plot(carslm, which=1) qqPlot(carslm$residuals, main=\"Q-Q Plot\",pch= 19, id=FALSE) plot(carslm$residuals, ylab= \"Residuals\", main=\"Residuals vs Order\") Part (b) Explain, as best you understand currently, what each of these three plots show for this regression. Explanation: Everything looks somewhat questionable. The residuals vs. fitted-values plot shows a lack of linearity, which makes it hard to judge constant variance. The Q-Q Plot shows possible problems with normality because some dots go out of bounds, but is fairly good. The residuals vs. order plot shows a possible problem with time trends due to the slight rainbow pattern. Part (c) Report the p-value for the test of these hypotheses for your regression. Intercept Hypotheses \\[ H_0: \\beta_0 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_0 \\neq 0 \\] P-VALUE = < 2e-16 Slope Hypotheses \\[ H_0: \\beta_1 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_1 \\neq 0 \\] P-VALUE = 1.29e-10 Comment on whether or not we should trust each p-value based on your plots in Part (a). Should trust each p-value based on your plots in Part (a)? Intercept Hypotheses p-value? : No, it should not be trusted because of the lack of linearity and the distance zero is from the current data. Slope Hypotheses p-value? : No, it should not be fully trusted, though there is likely some sort of trend in the data, because of some problems in the diagnostic plots. Assessment Quiz - Simple Linear Regression Run the following commands in R. library(car) View(Davis) ?Davis Reduce the data to just the data for the males. Then perform a regression with weight as the response variable and height as the explanatory variable. Which of the following provides the estimated average weight of males that are 180 cm tall? Davey <- filter(Davis, sex == \"M\") daveylm <- lm(weight ~ height, data=Davey) summary(daveylm)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) -101.3 29.86 -3.393 0.001046 height 0.9956 0.1676 5.939 5.922e-08 Fitting linear model: weight ~ height Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 88 10.07 0.2908 0.2826 -101.33 +(0.9956)*180 = -101.33 +(0.9956)*180 ## [1] 77.878 Run the following commands in R. View(USArrests) ?USArrests Perform a regression using this data that explains the average number of murder arrests in cities (per 100,000 in 1973) using the number of assault arrests (per 100,000) in a city. Select the answer that provides the correct estimate of \\(\\beta_1\\) in the formula: \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i \\sim N(0, \\sigma^2)\\) for the USArrests regression described above. arrestlm <- lm(Murder ~ Assault, data=USArrests) summary(arrestlm)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 0.6317 0.8548 0.739 0.4635 Assault 0.04191 0.004507 9.298 2.596e-12 Fitting linear model: Murder ~ Assault Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 50 2.629 0.643 0.6356 Which of the following statements is a correct statement about the graphic shown below? Note: the “Line of Equality” shows where the line would need to be if the reported heights were equal (on average) to the actual measured heights. Dots that fall on this line show men that knew their actual height before they were officially measured. Answer: The average actual height gets closer to the reported height for taller men, while shorter men seem more likely to under-report their height, on average. Class Activity - Estimating Means & Variablility Open the airquality dataset in R View(airquality) ?airquality Part 1: Estimating the Mean and Spread of Y Creating Histogram graph ggplot(airquality, aes(x=Temp))+ geom_histogram(binwidth=5, fill=\"lightblue\", color=\"skyblue\") + labs(title=\"Maximum daily temperature in defees Fahrenheit at La Gaurdia Airport, NY, USA\", x=\"Temperature in degrees F\", y=\"Number of Days in Temperature Range\") favstats(airquality$Temp) %>% pander() min Q1 median Q3 max mean sd n missing 56 72 79 85 97 77.88 9.465 153 0 Spread is SO important! This tells us the guess of this thing that we are looking at, how good or bad we were in our guess! essentially, 2 standard deviations are a good guess (takes up a good percentage of the data) Part 2: Estimating the Mean and Spread of Y for Categorical X weathering <- ggplot(airquality, aes(x=factor(Month), y=Temp)) + geom_boxplot(fill=\"lightblue\", color=\"skyblue\") + labs(title=\"Maximum daily temp in degrees F at La Guardia Airport, NY, USA\") plot(weathering) favstats(Temp ~ Month, data=airquality) ## Month min Q1 median Q3 max mean sd n missing ## 1 5 56 60.0 66 69.00 81 65.54839 6.854870 31 0 ## 2 6 65 76.0 78 82.75 93 79.10000 6.598589 30 0 ## 3 7 73 81.5 84 86.00 92 83.90323 4.315513 31 0 ## 4 8 72 79.0 82 88.50 97 83.96774 6.585256 31 0 ## 5 9 63 71.0 76 81.00 93 76.90000 8.355671 30 0 Part 3: Estimating the Mean and Spread of Y for Quantitative X dots : represents actual temperature line : represents prediction of temperature x : represents our explanatory variable (VERY important) ggplot(airquality, aes(x=Wind, y=Temp)) + geom_point(size=1.5, color=\"lightblue\", alpha= 0.5) + geom_smooth(method=\"lm\", formula=y~x, se=FALSE, size=0.5,color=\"skyblue\")+ theme_minimal() myweather <- lm(Temp ~ Wind, data = airquality) summary(myweather)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 90.13 2.052 43.92 6.69e-88 Wind -1.23 0.1944 -6.331 2.642e-09 Fitting linear model: Temp ~ Wind Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 153 8.442 0.2098 0.2045 Use your regression equation to compute the estimated mean Temp for a day (between May and September) with a morning average wind speed (7 AM to 10 AM) of 19 miles per hour: What is the standard error (estimated standard deviation) of the points around your regression equation? how much the dots will spread from the line (how much a dot will vary from our prediction) Hint: Look for the “residual standard error” in your regression summary output. Class Activity - The Regression Model \\(Y_i\\) : \\(\\sigma\\) : control the tightness of the data (68%) \\(\\epsilon\\) : explanatory data value for individuality (gives us our variability, the dots departure from the line .aka the law) Part 2 : Simulating Data from a Regression Model residual standard error of this regression came out to be 8.442 the sample size consisted of n = 153 data points \\(\\hat{Y}_i\\) : the line that we found in the middle of the data (depends on the value of x) However, the “true regression equation” (or “true law”) governing the relationship between the Mean Max Temp and the Morning Wind Speed is still unknown to us. Suppose however that God revealed this true law to be governed by the equation: the data doesn’t have slope, the law that created the dots have slope! the slope is interpreted as the change in the average y-value for a one unit change in the x-value. we can’t tell what an individual will do, but we can tell what the law will do based on the individual Week 2 | Simple Linear Regression Skill Quiz - Residuals, Sums of Squares, and R-squared datatable(Orange) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\"],[\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\"],[118,484,664,1004,1231,1372,1582,118,484,664,1004,1231,1372,1582,118,484,664,1004,1231,1372,1582,118,484,664,1004,1231,1372,1582,118,484,664,1004,1231,1372,1582],[30,58,87,115,120,142,145,33,69,111,156,172,203,203,30,51,75,108,115,139,140,32,62,112,167,179,209,214,30,49,81,125,142,174,177]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>Tree<\\/th>\\n <th>age<\\/th>\\n <th>circumference<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Tree\",\"targets\":1},{\"name\":\"age\",\"targets\":2},{\"name\":\"circumference\",\"targets\":3}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} orangelm <- lm(circumference ~ age, data=Orange) summary(orangelm)%>%pander()   Estimate Std. Error t value Pr(>|t|) (Intercept) 17.4 8.623 2.018 0.05179 age 0.1068 0.008277 12.9 1.931e-14 Fitting linear model: circumference ~ age Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 35 23.74 0.8345 0.8295 \\[\\hat{Y_i} = 17.40 + 0.106770 X_i\\] ggplot(Orange, aes(x = age, y = circumference))+ geom_point(color = \"orange\") + geom_smooth(method = \"lm\", formula = y~x, se = FALSE, color = \"chocolate\")+ theme_minimal() SSE <- round(sum(residuals(orangelm)^2), 2) SSR <- round(sum((fitted(orangelm)- mean(Orange$circumference))^2), 2) SSTO <- round(sum((Orange$circumference - mean(Orange$circumference))^2), 2) R2 <- round(SSR / SSTO, 2) correlation <- sqrt(R2) if(coef(orangelm)[2]<0) correlation <- -correlation correlation <- round(correlation, 2) cat(\"SSE:\", SSE, \"\\n\") ## SSE: 18594.74 cat(\"SSR:\", SSR, \"\\n\") ## SSR: 93771.54 cat(\"SSTO:\", SSTO, \"\\n\") ## SSTO: 112366.3 cat(\"R-squared (R2):\", R2, \"\\n\") ## R-squared (R2): 0.83 cat(\"Correlation (r):\", correlation, \"\\n\") ## Correlation (r): 0.91 predict(orangelm, data.frame(age=1095, interval=\"prediction\")) ## 1 ## 134.3132 datatable(mtcars) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"Mazda RX4\",\"Mazda RX4 Wag\",\"Datsun 710\",\"Hornet 4 Drive\",\"Hornet Sportabout\",\"Valiant\",\"Duster 360\",\"Merc 240D\",\"Merc 230\",\"Merc 280\",\"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\"Merc 450SLC\",\"Cadillac Fleetwood\",\"Lincoln Continental\",\"Chrysler Imperial\",\"Fiat 128\",\"Honda Civic\",\"Toyota Corolla\",\"Toyota Corona\",\"Dodge Challenger\",\"AMC Javelin\",\"Camaro Z28\",\"Pontiac Firebird\",\"Fiat X1-9\",\"Porsche 914-2\",\"Lotus Europa\",\"Ford Pantera L\",\"Ferrari Dino\",\"Maserati Bora\",\"Volvo 142E\"],[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],[6,6,4,6,8,6,8,4,4,6,6,8,8,8,8,8,8,4,4,4,4,8,8,8,8,4,4,4,8,6,8,4],[160,160,108,258,360,225,360,146.7,140.8,167.6,167.6,275.8,275.8,275.8,472,460,440,78.7,75.7,71.09999999999999,120.1,318,304,350,400,79,120.3,95.09999999999999,351,145,301,121],[110,110,93,110,175,105,245,62,95,123,123,180,180,180,205,215,230,66,52,65,97,150,150,245,175,66,91,113,264,175,335,109],[3.9,3.9,3.85,3.08,3.15,2.76,3.21,3.69,3.92,3.92,3.92,3.07,3.07,3.07,2.93,3,3.23,4.08,4.93,4.22,3.7,2.76,3.15,3.73,3.08,4.08,4.43,3.77,4.22,3.62,3.54,4.11],[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],[16.46,17.02,18.61,19.44,17.02,20.22,15.84,20,22.9,18.3,18.9,17.4,17.6,18,17.98,17.82,17.42,19.47,18.52,19.9,20.01,16.87,17.3,15.41,17.05,18.9"},{"title":"Analysis Of Variance (ANOVA)","url":"ANOVA.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Analysis Of Variance (ANOVA) function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } function openTab(evt, tabName) { var i, tabcontent, tablinks; tabcontent = document.getElementsByClassName(\"tabcontent\"); for (i = 0; i < tabcontent.length; i++) { tabcontent[i].style.display = \"none\"; } tablinks = document.getElementsByClassName(\"tablinks\"); for (i = 0; i < tablinks.length; i++) { tablinks[i].className = tablinks[i].className.replace(\" active\", \"\"); } document.getElementById(tabName).style.display = \"block\"; evt.currentTarget.className += \" active\"; } library(readr) An ANOVA is for testing the equality of several means simultaneously. A single quantitative response variable is required with one or more qualitative explanatory variables, i.e., factors. Note: A factor is defined as a qualitative variable containing at least two categories. The categories of the factor are referred to as the “levels” of the factor. One-way ANOVA Each experimental unit is assigned to exactly one factor-level combination. Another way to say this is “one measurement per individual” (no repeated measures) and “equal numbers of individuals per group”. Overview An ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA can be assumed to be normally distributed when seen in a Q-Q Plot.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out in a Residuals versus fitted-values plot.) Hypotheses For a One-way ANOVA \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] Mathematical Model A typical model for a one-way ANOVA is of the form \\[ Y_{ij} = \\mu_i + \\epsilon_{ij} \\] where \\(\\mu_i\\) is the mean for level (group) \\(i\\), and \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) is the error term for each point \\(j\\) within level (group) \\(i\\). R Instructions Console Help Command: ?aov() myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. X is a qualitative variable (should have class(X) equal to factor or character. If it does not, use as.factor(X) inside the aov(Y ~ as.factor(X),...) command. YourDataSet is the name of your data set. Perform the ANOVA myaov <- aov(Y ~ X, data=YourDataSet) summary(myaov) Diagnose ANOVA Assumptions par(mfrow=c(1,2)) plot(myaov, which=1:2) Example Code Hover your mouse over the example codes to learn more. Perform the ANOVA chick.aov <-  Saves the results of the ANOVA test as an object named ‘chick.aov’. aov( ‘aov()’ is a function in R used to perform the ANOVA. weight Y is ‘weight’, which is a numeric variable from the chickwts dataset.  ~  ‘~’ is the tilde symbol used to separate the Y and X in a model formula. feed, X is ‘feed’, which is a qualitative variable in the chickwts dataset, or more specifically, a factor with six levels: “casein”, “horsebean”, and so on… Use str(chickwts) to see this.  data = chickwts) ‘chickwts’ is a dataset in R. summary( ‘summary()’ shows the results of the ANOVA. chick.aov) ‘chick.aov’ is the name of the ANOVA.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## Df Sum Sq Mean Sq F value Pr(>F) ## feed 5 231129 46226 15.37 5.94e-10 *** ## Residuals 65 195556 3009 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Diagnose the ANOVA par( ‘par’ is a R function that can be used to set or query graphical parameters. mfrow = c(1,2)) The mfrow parameter controls “multiple frames on a row”. In this case, the c(1,2) specifies 1 row of 2 plots. This will cause the two diagnostic plots to be placed side-by-side. plot( ‘plot’ is a R function for the plotting of R objects. chick.aov, ‘chick.aov’ is the name of the ANOVA.  which = 1:2) The which=1:2 selects “which” of 6 available plots we want to have graphed. In this case, 1 shows the Residuals vs Fitted, and 2 shows the Normal QQ-plot. Both are needed to check the ANOVA assumptions.  Click to View Output  Click to View Output. Explanation Analysis of variance (ANOVA) is often applied to the scenario of testing for the equality of three or more means from (possibly) separate normal distributions of data. The normality assumption is required. No matter the sample size. If the distributions are skewed then a nonparametric test should be applied instead of ANOVA. One-Way ANOVA One-way ANOVA is when a completely randomized design is used with a single factor of interest. A typical mathematical model for a one-way ANOVA is of the form \\[ Y_{ik} = \\mu_i + \\epsilon_{ik} \\quad (\\text{sometimes written}\\ Y_{ik} = \\mu + \\alpha_i + \\epsilon_{ik}) \\] where \\(\\mu_i\\) is the mean of each group (or level) \\(i\\) of a factor, and \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) is the error term. The plot below demonstrates what these symbols represent. Note that the notation \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\) states that we are assuming the error term \\(\\epsilon_{ik}\\) is normally distributed with a mean of 0 and a standard deviation of \\(\\sigma\\). Hypotheses The aim of ANOVA is to determine which hypothesis is more plausible, that the means of the different distributions are all equal (the null), or that at least one group mean differs (the alternative). Mathematically, \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_m = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\quad \\text{for at least one}\\ i\\in\\{1,\\ldots,m\\}. \\] In other words, the goal is to determine if it is more plausible that each of the \\(m\\) different samples (where each sample is of size \\(n\\)) came from the same normal distribution (this is what the null hypothesis claims) or that at least one of the samples (and possibly several or all) come from different normal distributions (this is what the alternative hypothesis claims). Visualizing the Hypotheses The first figure below demonstrates what a given scenario might look like when all \\(m=3\\) samples of data are from the same normal distribution. In this case, the null hypothesis \\(H_0\\) is true. Notice that the variability of the sample means is smaller than the variability of the points. The figure below shows what a given scenario might look like for \\(m=3\\) samples of data from three different normal distributions. In this case, the alternative hypothesis \\(H_a\\) is true. Notice that the variability of the sample means, i.e., \\((\\bar{x}_1,\\bar{x}_2,\\bar{x}_3)\\), is greater than the variability of the points. Explaining the Name The above plots are useful in understanding the mathematical details behind ANOVA and why it is called analysis of variance. Recall that variance is a measure of the spread of data. When data is very spread out, the variance is large. When the data is close together, the variance is small. ANOVA utilizes two important variances, the between groups variance and the within groups variance. Between groups variance–a measure of the variability in the sample means, the \\(\\bar{x}\\)’s. Within groups variance–a combined measure of the variability of the points within each sample. The plot below combines the information from the previous plots for ease of reference. It emphasizes the fact that when the null hypothesis is true, the points should have a large variance (be really spread out) while the sample means are relatively close together. On the other hand, when the points are relative close together within each sample and the sample means have a large variance (are really spread out) then the alternative hypothesis is true. This is the theory behind analysis of variance, or ANOVA. Calculating the Test Statistic, \\(F\\) The ratio of the “between groups variation” to the “within groups variation” provides the test statistic for ANOVA. Note that the test statistic of ANOVA is an \\(F\\) statistic. \\[ F = \\frac{\\text{Between groups variation}}{\\text{Within groups variation}} \\] It would be good to take a minute and review the \\(F\\) distribution. The \\(p\\)-value for ANOVA thus comes from an \\(F\\) distribution with parameters \\(p_1 = m-1\\) and \\(p_2 = n-m\\) where \\(m\\) is the number of samples and \\(n\\) is the total number of data points. A Deeper Look at Variance It is useful to take a few minutes and explain the word variance as well as mathematically define the terms “within group variance” and “between groups variance.” Variance is a statistical measure of the variability in data. The square root of the variance is called the standard deviation and is by far the more typical measure of spread. This is because standard deviation is easier to interpret. However, mathematically speaking, the variance is the more important measurement. As mentioned previously, the variance turns out to be the key to determining which hypothesis is the most plausible, \\(H_0\\) or \\(H_a\\), when several means are under consideration. There are two variances that are important for ANOVA, the “within groups variance” and the “between groups variance.” Recall that the formula for computing a sample variance is given by \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})^2}{n-1} \\quad\\leftarrow \\frac{\\text{sum of squares}}{\\text{degrees of freedom}} \\] This formula has a couple of important pieces that are so important they have been given special names. The \\(n-1\\) in the denominator of the formula is called the “degrees of freedom.” The other important part of this formula is the \\(\\sum_{i=1}^n(x_i - \\bar{x})^2\\), which is called the “sum of squared errors” or sometimes just the “sum of squares” or “SS” for short. Thus, the sample variance is calculated by computing a “sum of squares” and dividing this by the “degrees of freedom.” It turns out that this general approach works for many different contexts. Specifically, it allows us to compute the “within groups variance” and the “between groups variance.” To introduce the mathematical definitions of these two variances, we need to introduce some new notation. Let \\(\\bar{y}_{i\\bullet}\\) represent the sample mean of group \\(i\\) for \\(i=1,\\ldots,m\\). Let \\(n_i\\) denote the sample size in group \\(i\\). Let \\(\\bar{y}_{\\bullet\\bullet}\\) represent the sample mean of all \\(n = n_1+n_2+\\cdots+n_m\\) data points. The mathematical calculations for each of these variances is given as follows. \\[ \\text{Between groups variance} = \\frac{\\sum_{i=1}^m (\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2}{m-1} \\leftarrow \\frac{\\text{Between groups sum of squares}}{\\text{Between groups degrees of freedom}} \\] \\[ \\text{Within groups variance} = \\frac{\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2}{n-m} \\leftarrow \\frac{\\text{Within groups sum of squares}}{\\text{Within groups degrees of freedom}} \\] A Fabricated Example The following table provides three samples of data: A, B, and C. These samples were randomly generated from normal distributions using a computer. The true means \\(\\mu_1, \\mu_2\\), and \\(\\mu_3\\) of the normal distributions are thus known, but withheld from you at this point of the example. A B C 13.15457 13.17463 16.66831 12.65225 12.16277 15.54719 13.73061 12.76905 16.63074 14.43471 13.38524 15.06726 13.79728 12.02690 15.57534 13.88599 13.24651 15.99915 12.77753 12.58386 15.58995 13.81536 12.64615 16.99429 13.03635 12.52055 15.47153 14.26062 14.03566 16.13330 An ANOVA will be performed with the sample data to determine which hypothesis is more plausible: \\[ H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\in \\{1,\\ldots,m\\} \\] To perform an ANOVA, we must compute the between groups variance and the within groups variance. This requires the Between groups sums of squares, within groups sums of squares, between groups degrees of freedom, and the within groups degrees of freedom. Note that to get the sums of squares, we first had to calculate \\(\\bar{y}_{1\\bullet}\\), \\(\\bar{y}_{2\\bullet}\\), \\(\\bar{y}_{3\\bullet}\\), and \\(\\bar{y}_{\\bullet\\bullet}\\) where the 1, 2, 3 corresponds to Samples A, B, and C, respectively. After some work, we find these values to be \\[ \\bar{y}_{1\\bullet} = 13.55, \\quad \\bar{y}_{2\\bullet} = 12.86 \\quad \\bar{y}_{3\\bullet} = 15.97 \\] and \\[ \\bar{y}_{\\bullet\\bullet} = \\frac{13.55+12.86+15.97}{3} = 14.13 \\] Using these values we can then compute the between groups sum of squares and the within groups sum of squares according to the formulas stated previously. This process is very tedious and will not be demonstrated. Only the results are shown in the following table which summarizes all the important information.   Degrees of Freedom Sum of Squares Variance F-value p-value Between groups 2 53.3 26.67 70.2 2e-11 Within groups 27 10.3 0.38 ANOVA Table In general, the ANOVA table is created by   Degrees of Freedom Sum of Squares Variance F-value p-value Between groups \\(m-1\\) \\(\\sum_{i=1}^m n_i(\\bar{y}_{i\\bullet}-\\bar{y}_{\\bullet\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) \\(\\frac{\\text{Between groups variance}}{\\text{Within groups variance}}\\) \\(F\\)-distribution tail probability Within groups \\(n-m\\) \\(\\sum_{i=1}^m\\sum_{k=1}^{n_i}(y_{ik}-\\bar{y}_{i\\bullet})^2\\) \\(\\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\) ANOVA Assumptions The requirements for an analysis of variance (the assumptions of the test) are two-fold and concern only the error terms, the \\(\\epsilon_{ik}\\). The errors are normally distributed. The variance of the errors is constant. Both of these assumptions were stated in the mathematical model where we assumed that \\(\\epsilon_{ik}\\sim N(0,\\sigma^2)\\). Checking ANOVA Assumptions To check that the ANOVA assumptions are satisfied, it is required to check the data in each group for normality using QQ-Plots. Also, the sample variance of each group must be relatively constant. The fastest way to check these two assumptions is by analyzing the residuals. An ANOVA residual is defined as the difference between the observed value of \\(y_{ik}\\) and the mean \\(\\bar{y}_{i\\bullet}\\). Mathematically, \\[ r_{ik} = y_{ik} - \\bar{y}_{i\\bullet} \\] One QQ-Plot of the residuals will provide the necessary evidence to decide if it is reasonable to assume that the error terms are normally distributed. Also, the constant variance can be checked visually by using what is known as a residuals versus fitted values plot. For the Fabricated Example above, the QQ-Plot and residuals versus fitted values plots show the two assumptions of ANOVA appear to be satisfied. Examples: chickwts (One-way) Two-way ANOVA Overview A two-way ANOVA is only appropriate when all of the following are satisfied. The sample(s) of data can be considered to be representative of their population(s). The data is normally distributed in each group. (This can safely be assumed to be satisfied when the residuals from the ANOVA are normally distributed.) The population variance of each group can be assumed to be the same. (This can be safely assumed to be satisfied when the residuals from the ANOVA show constant variance, i.e., are similarly vertically spread out.) Hypotheses With a two-way ANOVA there are three sets of hypotheses. Writing out the hypotheses can be very involved depending on whether you use the official “effects model” notation (very mathematically correct) or a simplified “means model” notation (which isn’t very mathematically correct, but gets the idea across in an acceptable way). Means Model Effects Model The first set of hypotheses are a “one-way” set of hypotheses for the first factor of the ANOVA. Factor: X1 with say, levels \\(A\\) and \\(B\\). \\[ H_0: \\mu_A = \\mu_B = \\mu \\] \\[ H_a: \\mu_A \\neq \\mu_B \\] The second set of hypotheses are also a “one-way” set of hypotheses, but for the second factor of the ANOVA. Factor: X2 with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\mu_C = \\mu_D = \\mu_E = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one}\\ i\\in\\{1=C,2=D,3=E\\} \\] The third set of hypotheses are the most interesting hypotheses in a two-way ANOVA. These are called the interaction hypotheses. They test to see if the levels of one of the factors, say \\(X1\\), impact \\(Y\\) differently for the differing levels of the other factor, \\(X2\\). The hypotheses read formally as \\[ H_0: \\text{The effect of the first factor on Y} \\\\ \\text{is the same for all levels of the second factor.} \\] \\[ H_a: \\text{The effect of the first factor on Y is not the same} \\\\ \\text{for all levels of the second factor.} \\] A mathematically correct way to state the two-way ANOVA model is with the equation \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\] In this model, \\(\\mu\\) is the grand mean (which is the average Y-value ignoring all information contained in the factors); \\(\\alpha_i\\) is the first factor \\(X1\\) with levels \\(A\\) and \\(B\\) (though there could be more levels in \\(X1\\) depending on your data); \\(\\beta_j\\) is the second factor with levels \\(C\\), \\(D\\), and \\(E\\) (though there could be fewer or more levels to this factor depending on your data); \\(\\alpha\\beta_{ij}\\) is the interaction of the two factors which has \\(2\\times3=6\\) (may differ for your data) levels; and \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) is the normally distributed error term for each point \\(k\\) found within level \\(i\\) of \\(X1\\) and level \\(j\\) of \\(X2\\). This model allows us to more formally state the hypotheses as First factor \\(X1\\) having say, levels \\(A\\) and \\(B\\). \\[ H_0: \\alpha_A = \\alpha_B = 0 \\] \\[ H_a: \\alpha_i \\neq 0 \\ \\text{for at least one}\\ i\\in\\{1=A,2=B\\} \\] Second factor \\(X2\\) with say, levels \\(C\\), \\(D\\), and \\(E\\). \\[ H_0: \\beta_C = \\beta_D = \\beta_E = 0 \\] \\[ H_a: \\beta_j \\neq 0 \\ \\text{for at least one}\\ j\\in\\{1=C,2=D,3=E\\} \\] Does the effect of the first factor (\\(X1\\)) change for the different levels of the second factor (\\(X2\\))? In other words, is there an interaction between the two factors \\(X1\\) and \\(X2\\)? \\[ H_0: \\alpha\\beta_{ij} = 0 \\ \\text{for all } i,j \\] \\[ H_a: \\alpha\\beta_{ij} \\neq 0 \\ \\text{for at least one } i,j \\] R Instructions Console Help Command: ?aov() myaov <- aov(Y ~ X1+X2+X1:X2, data=YourDataSet) Perform the ANOVA summary(myaov) View the ANOVA Results plot(myaov, which=1:2) Check ANOVA assumptions myaov is some name you come up with to store the results of the aov() test. Y must be a “numeric” vector of the quantitative response variable. X1 is a qualitative variable (should have class(X1) equal to factor or character. If it does not, use as.factor(X1) inside the aov() command. X2 is a second qualitative variable that should also be either a factor or a character vector. Note that factors X3, X4, and so on could also be added + to the model if desired, but this would create a three-way, or four-way AN"},{"title":"Chi-squared Tests","url":"ChiSquaredTests.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Chi-squared Tests function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } Association testing for two qualitative variables with at least two levels to each variable. Chi-squared Test of Independence Overview A method of comparing observed counts to the expected counts in a contingency table to decide if two qualitative variables are associated (alternative hypothesis) or are independent, i.e., not associated (null hypothesis). Typically, all expected counts are required to be 5 or greater for the test to be appropriate. However, the test is still appropriate if all expected counts are at least 1 and the average of the expected counts is at least 5. R Instructions Console Help Command: ?chisq.test() To Run the Test chisq.test(x) x must be a table or a matrix. To Make a Table If you have a data set that you want to turn into a table, use the table command. x The name of your created table to go in the chisq.test() code.  <-  This is the “left arrow” assignment operator that stores the results of your table() code into x. table( table is an R function used to tabulate how many times each value occrus in a given dataset. Dataset$FirstVariable, Use your dataset to call in a row variable.  Dataset$SecondVariable) Use your dataset to call in a column variable. x <- table(mtcars$am, mtcars$cyl) pander(x)   4 6 8 0 3 4 12 1 8 3 2 #Rename rows and columns of your table: rownames(x) <- c(\"Automatic\", \"Manual\") colnames(x) <- c(\"4 cyl\", \"6 cyl\", \"8 cyl\") pander(x)   4 cyl 6 cyl 8 cyl Automatic 3 4 12 Manual 8 3 2 Click open the example code above for details on how to rename rows and columns in a table. To Make a Matrix If you have tally counts of your observations, enter them into RStudio using a matrix. x The name of your created matrix to go in the chisq.test() code.  <-  This is the “left arrow” assignment operator that stores the results of your cbind() code into x. cbind( cbind stands for”column bind” and is a function that joins together different c() vectors to make them become columns of a table. `Farm 1` = The name of the first column you want in the matrix. To include spaces in the name, use the back-ticks ` and `.  c(Pigs = 10, Cats = 5, Dogs = 28, Roosters = 3), Specify each of the values by name in a c(…) function to be stored in your first column, in this case, Farm 1.  `Farm 2` = The name of the second column you want in the matrix.  c(Pigs = 15, Cats = 3, Dogs = 8, Roosters = 1) Each new column must contain entries for each named element. Technically, you can leave off the names in all but the first column as only the names from the first column will be used. pander(x) Print the matrix to the screen using pander(…).   Farm 1 Farm 2 Pigs 10 15 Cats 5 3 Dogs 28 8 Roosters 3 1 Or, notice how you could rbind instead with slightly different results… x The name of your created matrix to go in the chisq.test() code.  <-  This is the “left arrow” assignment operator that stores the results of your cbind() code into x. rbind( rbind stands for”row bind” and is a function that joins together different c() vectors to make them become rows of a table. `Farm 1` = The name of the first column you want in the matrix. To include spaces in the name, use the back-ticks ` and `.  c(Pigs = 10, Cats = 5, Dogs = 28, Roosters = 3), Specify each of the values by name in a c(…) function to be stored in your first column, in this case, Farm 1.  `Farm 2` = The name of the second column you want in the matrix.  c(Pigs = 15, Cats = 3, Dogs = 8, Roosters = 1) Each new column must contain entries for each named element. Technically, you can leave off the names in all but the first column as only the names from the first column will be used. pander(x) Print the matrix to the screen using pander(…).   Pigs Cats Dogs Roosters Farm 1 10 5 28 3 Farm 2 15 3 8 1 Diagnostics yourNamedTestResults This is some name you come up with that will become the R object that stores the results of your Chi-squated test chisq.test() command.  <-  This is the “left arrow” assignment operator that stores the results of your chisq.test() code into yourNamedTestResults. chisq.test( chisq.test() is an R function that stands for “Chi-Squared Test”. It performs the Chi-squared test for x. x) x is either a matrix or a table. Pearson’s Chi-squared test The name of the test being performed. data: x The data you are using for the Chi-Squared test. x is either a table or matrix that you created. X-squared = 9.2956, The test statistic of the test follows a chi-squared distribution with the given degrees of freedom. See the end of the “Making Inference” page of the book to learn more about the Chi-squared Parametric Distribution.  df = 3, The degrees of freedom of the test. It is found by (number of rows - 1)x(number of columns - 1)  p-value = 0.02561 The p-value of the test is found by computing the probability of being more extreme than given test statistic. In R, you can compute this yourself using the code pchisq(9.2956, 3, lower.tail=FALSE). yourNamedTestResults This is some name you come up with that will become the R object that stores the results of your Chi-squared test chisq.test() command. $ This allows you to access various elements from the test that was performed. expected The expected counts of the chi-squared test. These are computed for each observed count by using the equation (row tota)*(column total)/(total total). Pigs EXPLANATION.   Cats EXPLANATION.   Dogs EXPLANATION.   Roosters EXPLANATION. Farm 1 EXPLANATION.   15.753425 EXPLANATION.   5.041096 EXPLANATION. 22.68493 EXPLANATION. 2.520548 EXPLANATION. Farm 2 EXPLANATION.   9.246575 EXPLANATION.   2.958904 EXPLANATION. 13.31507 EXPLANATION. 1.479452 EXPLANATION. Interpretation: Pearson Residuals yourNamedTestResults This is some name you come up with that will become the R object that stores the results of your Chi-squated test chisq.test() command. $ This allows you to access various elements from the test that was performed. residuals This grabs the residuals for each observation in the Chi-squared test. Pigs EXPLANATION.   Cats EXPLANATION.   Dogs EXPLANATION.   Roosters EXPLANATION. Farm 1 EXPLANATION.   -1.449569 EXPLANATION.   -0.01830357 EXPLANATION. 1.115938 EXPLANATION. 0.3019936 EXPLANATION. Farm 2 EXPLANATION.   1.892065 EXPLANATION.   0.02389092 EXPLANATION. -1.456589 EXPLANATION. -0.3941801 EXPLANATION. Explanation To demonstrate the theory behind the Chi-squared Test, consider the following data about the survival of the Titanic passengers as a specific example. The rows of the following contingency table (a table of counts) show the Class of the passenger, while the columns show the Survival of the passenger. Note that each passenger is placed in exactly one Class and one level of Survival. Also, the two factors, Class and Survival, are qualitative variables with at least two levels. Thus, the data meets the basic requirements of the Chi-squared Test. No Yes 1st 122 203 2nd 167 118 3rd 528 178 Crew 673 212 The Hypotheses The hypotheses of the Chi-squared Test are written as verbal statements. \\(H_0\\): The row variable and column variable are independent. \\(H_a\\): The row variable and column variable are associated (not independent). For the Titanic data, these hypotheses would be written as \\(H_0\\): Class and Survival are independent. \\(H_a\\): Class and Survival are associated. The Model Recall that to obtain a \\(p\\)-value for a hypothesis test two things are required, 1) a test statistic, and 2) the distribution of the test statistic under the null hypothesis. The test statistic for the Chi-squared Test is given by the formula \\[ \\chi^2 = \\sum_{i=1}^m \\frac{(O_i - E_i)^2}{E_i} \\] where \\(O_i\\) represents each of the \\(m\\) observed counts and \\(E_i\\) represents the \\(m\\) expected counts. Note that \\(m = r\\times c\\), were \\(r\\) represents the number of rows in the contingency table and \\(c\\) represents the number of columns. The expected counts are obtained by the formula \\[ E_i = \\frac{\\text{(row total)(column total)}}{\\text{(total total)}} \\] The \\(\\chi^2\\) test statistic can be assumed to follow a chi-squared distribution with degrees of freedom \\(p = (r-1)\\times(c-1)\\) as long as the distribution of counts meets either of the following requirements. All expected counts are greater than five. OR All expected counts are greater than one, and the average of the expected counts is at least five. Where the \\(\\chi^2\\) statistic Comes From Consider the following graphic of the Titanic data. The data suggests that Survival of passengers appears to be associated with the Class of the passenger (the alternative hypothesis). This is due to the fact that more 1st class passengers survived than perished but far more 3rd class and Crew members perished than survived. It seems that the chances of survival were much greater for the 1st and 2nd class passengers than the 3rd class and Crew. Thus, survival appears to be associated with class. If the null hypothesis were true for the Titanic data, then we would expect to see roughly the same percentage of survival across all classes. In other words, if Class and Survival were independent, we would expect to see plots more like the theoretical ones depicted below. The theoretical plot above depicts the expected counts for each Class and Survival combination. To see where these expected counts come from, note that there are a total of 2,201 passengers recorded for the Titanic data (the sum of all values in the table). Further, there are a total of \\(122+203 = 325\\) passengers that were 1st class, \\(167 + 118 = 285\\) passengers in 2nd class, \\(528 + 178 = 706\\) passengers in 3rd class, and \\(673 + 212 = 885\\) Crew members. Thus, we have the following percentage of each class of passenger. Note that these percentages were obtained by taking the row totals and dividing them by the “total total”. Class Percentage 1st 0.1476602 2nd 0.1294866 3rd 0.3207633 Crew 0.4020900 Thus, if the null hypothesis is true, we would “expect” these percentages to remain consistent across both categories of Survival. Since there were \\(122+167+528+673 = 1490\\) passengers that did not survive and \\(203+118+178+212 =711\\) passengers that survived, we have the following expected counts. The counts under Survived were obtained by multiplying the number of passengers that survived, \\(711\\) by each of the percentages for the different classes. The counts under Perished were obtained by multiplying the number of passengers that perished, \\(1,490\\) by each of the percentages for the different classes. Class Percentage Survived Perished 1st 0.1476602 104.98637 220.0136 2nd 0.1294866 92.06497 192.9350 3rd 0.3207633 228.06270 477.9373 Crew 0.4020900 285.88596 599.1140 Notice that the above process followed the procedure of dividing the row totals by the total total and then multiplying that percentage to the column totals. Because the order of multiplication and division does not matter, we can phrase this as (row total)(column total) / (total total). These provide the counts we would expect if the null hypothesis were true. When the observed counts differ dramatically from the expected counts, we will reject the null hypothesis. To measure how dramatically the observed counts differ from the expected counts we use the formula \\[ \\chi^2 = \\sum_{i=1}^m \\frac{(O_i - E_i)^2}{E_i} \\] which is the formula that was stated previously. Notice that \\(\\chi^2\\) will become large when the observed counts differ dramatically from the expected counts, and will be relatively small when the expected counts and observed counts are similar. The \\(p\\)-value for the \\(\\chi^2\\) statistic can be calculated using a chi-squared distribution with degrees of freedom equal to the product of (the number of rows - 1) and (the number of columns - 1). Interpretation If the null hypothesis is true, then the interpretation is simple, the two variables are independent. End of story. However, when the null hypothesis is rejected and the alternative is concluded, it becomes interesting to interpret the results because all we know now is that the two variables are somehow associated. One way to interpret the results is to consider the individual values of \\[ \\frac{(O_i-E_i)^2}{E_i} \\] which, when square-rooted are sometimes called the Pearson residuals. \\[ \\sqrt{\\frac{(O_i-E_i)^2}{E_i}} = \\frac{(O_i-E_i)}{\\sqrt{E_i}} \\] The Pearson residuals allow a quick understanding of which observed counts are responsible for the \\(\\chi^2\\) statistic being large. They also show the direction in which the observed counts differ from the expected counts. For the Titanic data the Pearson residuals are given in the following table. Note that the most dramatic depatures in the observed counts (from the expected counts) were for the 1st Class survivors (Pearson residual = 9.566) and the 1st class passengers that perished (Pearson residual = -6.608). These tell us that far more 1st class passengers survived than we would have expected and far fewer perished than we expected. The next interesting scenario is for the Crew members, where more than expected perished and fewer than expected survived. A similar story to a lesser degree is true for the 3rd class passengers. The 2nd class passengers were similar to the 1st class passengers (more survived and less perished than expected) but to a far lesser degree. No Yes 1st -6.607873 9.565772 2nd -1.867159 2.702959 3rd 2.289965 -3.315027 Crew 3.018611 -4.369840 Examples: movies | StudentRatings | HairEyeColor Nonparametric Chi-squared Test Overview The test to perform when all observed counts are at least 1. The hypotheses and interpretation are the same as the Chi-Squared Test of Independence. The distribution of the test statistic is calculated in a nonparametric way. R Instructions Console Help Command: ?chisq.test() chisq.test( chisq.test() is an R function that stands for “Chi-Squared Test”. It performs the Chi-squared test for x. x, x is either a matrix or a table. simulate.p.value=TRUE) EXPLANATION. Pearson’s Chi-squared test with simulated p-value (based on 2000 replicates) EXPLANATION. data: x The data you are using for the Chi-Squared test. x is either a table or matrix that you created. X-squared = 9.2956, EXPLANATION.  df = NA, EXPLANATION.  p-value = 0.02799 EXPLANATION. Explanation The nonparametric chi-squared test is very similar to the chi-squared test. The hypotheses, expected counts, and test statistic are all exactly the same as the chi-squared test. The only difference is that the test statistic is not assumed to follow a parametric chi-squared distribution. The Hypotheses \\(H_0\\): The row variable and column variable are independent. \\(H_a\\): The row variable and column variable are associated. The Model The test statistic for the Nonparametric Chi-squared Test is the same as the Chi-Squared Test. \\[ \\chi^2 = \\sum_{i=1}^m \\frac{(O_i - E_i)^2}{E_i} \\] where \\(O_i\\) represents each of the \\(m\\) observed counts and \\(E_i\\) represents the \\(m\\) expected counts. Note that \\(m = r\\times c\\), were \\(r\\) represents the number of rows in the contingency table and \\(c\\) represents the number of columns. The expected counts are also still obtained by the formula \\[ E_i = \\frac{\\text{(row total)(column total)}}{\\text{(total total)}} \\] However, the \\(\\chi^2\\) test statistic is no longer assumed to follow a chi-squared distribution. Instead, a nonparametric distribution of the \\(\\chi^2\\) test statistic is generated using the following logic using the assumption that the null hypothesis is true. This is how it works. Calculating the P-Value If the null hypothesis of a Chi-squared Test is true, then the row variable and the column variable are independent. This means that the probability of a certain level of the column variable occurring is the same in all rows. This is the driving idea behind the method of the Nonparametric Chi-squared Test. Step 1: Randomly reassign values in each row to a different column so that they are somewhat close to the original expected counts, but random. In other words, try to mimic a possible outcome for what would have happened if the row variable and column variable really were independent. For the Titanic data, it would look something like this. Remember, this is not the actual data, it is a randomly made up version of the Titanic data that reflects what we might have observed if Survival was actually independent of Class. Yes No 1st 113 212 2nd 86 199 3rd 243 463 Crew 277 608 Step 2: Perform a Chi-squared Test of Independence of the rearranged data and record the value of the \\(\\chi^2\\) test statistic. For the made up data above, the Chi-squared Test of Independence gives the test statistic of \\[\\chi^2 = 3.1953669\\] Step 3: Repeat the process thousands of times, always keeping track of the \\(\\chi^2\\) value from each test. For 2,000 made up versions of the Titanic data (each similar to the orange table in Step 1), the 2,000 corresponding test statistics are summarized in the histogram below. Step 4: Calculate the number of \\(\\chi^2\\) statistics (from the thousands of simulated data sets) that are as extreme or more extreme than the original test statistic observed for the real data. Add 1 to this value (to avoid the possibility of getting a p-value of 0) and then divide by the number of test statistics that were simulated to get the p-value of the Nonparametric Chi-squared Test. For the Titanic data, the original Chi-squared Test statistic was \\(190.4011036\\). Notice that this statistic is dramatically larger than all of the test statistics that were simulated under the assumption that the null hypothesis was true. (See the histogram in Step 3 to see that no simulated test statistic was above 20, and very few were higher than 15!) Thus, for the Titanic data, no simulated test statistics were as extreme or more extreme than the observed test statistic. We add 1 to this, and divide by 2,000 to get that the p-value = \\((0 + 1)/2000 = 0.00005\\). This is sufficient evidence to reject the null hypothesis. Interpretation The interpretation of the Nonparametric Chi-squared Test is exactly the same as the Chi-squared Test. The only difference is in the calculation of the p-value. Fortunately, most software will automatically perform the Nonparametric Chi-squared Test, so all of the above calculations will be performed automatically. Examples: Cowlick // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\";"},{"title":"Data Sources","url":"DataSources.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Data Sources Kaggle UCI Machine Learning Repository U.S. Open Data Portal EU Open Data Portal Google Public Data Explorer U.S. Health Data // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Getting Started Tutorial","url":"GettingStartedTutorial.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Getting Started Tutorial function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } Student: “How do I learn R?” Teacher: “By using it.” Student: “But I don’t know how to use it.” Teacher: “Just try it anyway. Suddenly you’ll understand.” Most people that are new to using the “R Software” ask the question, “How do I learn R?” The answer is simple: “start using it.” Really. Seriously. Just start using it, even when you have no idea what you are doing, and suddenly you will start to learn R. So, here we go. The more you use it, the more you will know. This textbook (the “Statistics-Notebook”) follows a simple learning model: Hover your mouse over codeYes, just like that. By hovering over “Codes” you will get instructions on what that code does. to read about it. Click on a line of codeHovering is a good start, try clicking on this one. to see what it does. Try typing the code into RStudio yourself to actually start learning R. (This is the most important step! Avoid copying and pasting codes, and type them instead. The more you type codes yourself, even though it is slow and prone to mistakes, the more you will learn.) In summary, the most successful students in Math 325 follow the pattern: Example Codes For each of the following examples: (1) hover, (2) click, and (3) try. Before you begin working on these Example Codes, ensure you have RStudio open. It should look like this: Example 1 Remember, “Hover” the code first, then click, then try. View( The “View” R function (with a CAPTIAL ‘V’ in View) allows us to view a data set. When run, this function will open up a new tab in RStudio showing the data set. cars cars is a data set that is in R. R has datasets that are available for anyone to use. You can see them using the data() command. It would be good to explore View() for a few different datasets. ) Always be sure to end your function with closing parantheses.     Press Enter to run the code.  Click to Show Tutorial  Click to see a full tutorial on the “View()” command. Good work clicking on Example Code 1. But… Did you remember to first hover your mouse over the View(cars) example code? Good job if you did. If not, go do that right now before continuing. R allows you to work with data. So, the first step to understanding R is to open a dataset and begin exploring some data. If you type the data() command into your Console of RStudio you will see a list of “built in” data sets that come with R. Go ahead and type data() into your R Console (and then press Return or Enter) right now to try it yourself. The cars data set is one of the options that is available within the list from the data() command. This is a fun “historic” data set from the 1920’s. It shows the speed and stopping distance of cars from the 1920’s. To see the cars data set in RStudio, use the View command as follows. Step 1. Open RStudio. Step 2. Type the command View(cars) into your Console. Step 3. Press Enter or Return and the following output should appear within RStudio. Be sure to TRY IT yourself, if you haven’t already. Good work if you did. It’s the only way to learn R. Ask someone for help if you aren’t sure how to get started. Try View(airquality) as well. Example 2 mean( An R function “mean()” that will compute the mean of a quantitative column of data from a data set. cars cars is the name of a data set in R. Any data set can be used instead by simply typing the name of that data set instead of cars. $ The $ sign is a powerful operator in R. The $ sign allows you to access, or “purchase,” any column from a data set. Try typing cars$ into R and notice how a selection menu appears with options dist and speed. dist dist is one of the two columns from the cars data set. By typing cars$dist we are essentially pulling that column of data out of the data set, and then computing the mean of that column with mean(cars$dist). ) Closing parenthesis for the mean() function.     Press Enter to run the code.  Click to Show Output  Click to see output. Did you remember to first hover your mouse over the mean(cars$dist) example code? Good job if you did. If not, go do that right now before continuing. Recall that you “Viewed” the cars data set in the first example code. If you look carefully at this data set, you should notice that there are two columns labeled speed and dist in this data set. Each row of the data set represents a vehicle that was going a certain speed (in miles per hour) and once the breaks were applied, the distance the vehicle took to stop was also recorded (in feet). This data was recorded for 50 different vehicles. So it might make sense to compute something like the average (or mean) distance it took vehicles to stop. This is done with the mean( ) function. Notice how the $ sign is used to access a column called dist from the cars data set. Think of the data set cars as a store, and if you bring your money $ then you can “buy” the column dist from that data set. You may also be interested in trying any of the following: sd(cars$dist) var(cars$dist) median(cars$dist) min(cars$dist) max(cars$dist) length(cars$dist) sum(cars$dist) If you are curious, you can begin exploring the Numerical Summaries page of your Statistics-Notebook to learn more. But we won’t officially get to that as a class until later this week. So you don’t need to worry about it for now. Example 3 plot( The plot(...) function allows us to create a plot (usually a scatterplot) in R. dist dist is the name of a column from the cars data set. This is going to be the Y-variable of the plot. The Y-variable always comes first in the plot(Y ~ X) command.  ~  ~ is found on the upper-left key of your keyboard. It is called the “tilde” or “tilda” symbol. It is used to state a relationship between Y and X: Y ~ X. speed speed is the name of a column. This is going to be the X-variable of the plot. The X-variable always comes after the ~ in the plot(Y ~ X) command. ,  The comma , is used to separate each entry within a command. data=cars The data= statement is used to tell R which data set the columns of “dist” and “speed” come from. In this case, the cars data set. ,  The comma , is used to separate each entry within a command. col=“skyblue” The col= statement is used to tell R which color to use in the plot. Type colors() in your R Console to see what options there are. This code is using the \"skyblue\" color. Color names are always placed in quotes \" \". ,  The comma , is used to separate each entry within a command. pch=16 The pch= statement is used to tell R which plotting character to use in the plot. Type ?pch in your R Console to see what options there are. (You’ll need to scroll down in the help file that appears until you get to the 'pch values' section. ) Closing parenthesis for the plot(…) function.     Press Enter to run the code.  Click to Show Output  Click to View Output. Study the image below. Try changing the col=\"skyblue\" statement to col=\"firebrick\" instead. What do you notice? Try changing the pch=16 statement to pch=4 or any other number from 1 to 25. What do you notice? Hint: pressing the “up” arrow on your keyboard brings up the last command you typed into the Console. Good work! You’re all done for now. Completion Code: abc123R Return to the R Commands page of the Statistics-Notebook. This is “the end” of the Getting Started tutorial. To find the “completion code” you will need to study, and “click open” each of the example codes above. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Graphical Summaries","url":"GraphicalSummaries.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Graphical Summaries function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } function openTab(evt, tabName) { var i, tabcontent, tablinks; tabcontent = document.getElementsByClassName(\"tabcontent\"); for (i = 0; i < tabcontent.length; i++) { tabcontent[i].style.display = \"none\"; } tablinks = document.getElementsByClassName(\"tablinks\"); for (i = 0; i < tablinks.length; i++) { tablinks[i].className = tablinks[i].className.replace(\" active\", \"\"); } document.getElementById(tabName).style.display = \"block\"; evt.currentTarget.className += \" active\"; } There are many ways to display data. The fundamental idea is that the graphical depiction of data should communicate the truth the data has to offer about the situation of interest. Histograms 1 Quantitative Variable Overview Great for showing the distribution of data for a single quantitative variable when the sample size is large. Dotplots are a good alternative for smaller sample sizes. Gives a good feel for the mean and standard deviation of the data. R Instructions Base R ggplot2 plotly To make a histogram in R use the function: hist(object) object must be quantitative data. R refers to this as a “numeric vector.” Usually this will be the column of a dataset accessed with the $ sign by hist(dataSetName$columnName). Type ?hist in your R Console to open the help file in R. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic histogram hist An R function “hist” used to create a histogram. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the hist function.     Press Enter to run the code.  …  Click to View Output. Change Color hist(airquality$Temp,  This code was explained in the first example code. col=“skyblue” col= allows us to specify the color of the plot using a named color. The name of the color must be placed in quotations. Type “colors()” in R to see color options. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Add Titles hist(airquality$Temp This part was explained in the first example code. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. col=“skyblue”col= allows us to specify the color of the plot using a named color. The name of the color must be placed in quotations. Type “colors()” in R to see color options. ,  A comma must always be used to separate additional commands. xlab=“Temperature” xlab= stands for “x label.” Use it to specify the text to print on the plot under the x-axis. The desired text must always be in quotations. ,  A comma must always be used to separate additional commands. main=“La Guardia Airport Daily Mean Temperatures” main= lets us specify the “main” title to be placed above the plot. The desired text must always be placed in quotations. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. To make a histogram in R using the ggplot approach, first ensure library(ggplot2) is loaded. Then, ggplot(data, aes(x=column)) +   geom_histogram() data is the name of your dataset. column is a column of data from your dataset that is quantitative. The aesthetic helper function aes(x= ) is how you tell the gpplot to make the x-axis become your column of data. The geometry helper function geom_histogram() causes the ggplot to become a histogram. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic Histogram ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp “x=” declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_histogram() The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used.     Press Enter to run the code.  …  Click to View Output. Change Bin Width and Color ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp “x=” declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_histogram( The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used. binwidth=5,  The “binwidth” command controls the width of the bars in the histogram. fill=“skyblue”, The “fill” command controls the color of the insides of each bar. color=“black” The “color” command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function.     Press Enter to run the code.  …  Click to View Output. Add Titles ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice. aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become. x=Temp “x=” declares which variable will become the x-axis of the graphic. )Closing parenthsis for the aes function. )Closing parenthsis for the ggplot function.  +  The addition symbol + is used to add further elements to the ggplot.    geom_histogram( The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used. binwidth=5,  The “binwidth” command controls the width of the bars in the histogram. fill=“skyblue”, The “fill” command controls the color of the insides of each bar. color=“black” The “color” command controls the color of the edges of each bar. )Closing parenthsis for the geom_histogram function.  +  The addition symbol + is used to add further elements to the ggplot.    labs( The “labs” function is used to add labels to the plot, like a main title, x-label and y-label. title=“La Guardia Airport Daily Mean Temperature”,  The “title=” command allows you to control the main title at the top of the graphic. x=“Temperature”,  The “x=” command allows you to control the x-label of the graphic. y=“Number of Days” The “y=” command allows you to control the y-label of the graphic. )Closing parenthsis for the labs function.     Press Enter to run the code.  …  Click to View Output. Gallery See some ideas from past students… Show/Hide Gallery Hover to see code. Copy the code into R. Play with it. Modify it to create your own graph. ggplot(airquality, aes(x = Temp)) + geom_histogram(binwidth=5, fill = \"skyblue\", color = \"black\") + labs(title = \"La Guardia Airport Daily Mean Temperature\",    x = \"Temperature\", y = \"Number of Days\") + theme_bw() + theme(plot.title = element_text(size = 14, face = \"bold\", hjust = .5),    panel.grid.major = element_line(color = \"grey80\")) ggplot(airquality) + geom_histogram(aes(x = Temp, fill = Temp > median(Temp)), color = \"black\", binwidth = 2) + labs(title = \"La Guardia Airport Daily Mean Temperature\",    x = \"Temperature\", y = \"Number of Days\") + theme_bw() + theme(plot.title = element_text(size = 14, face = \"bold\", hjust = .5),    panel.grid.major = element_line(color = \"grey80\"),    legend.position = \"none\") + geom_vline(xintercept = 72, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_vline(xintercept = 85, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 71, y = 10, label = \"Q1\") + geom_text(x = 86, y = 10, label = \"Q3\") ggplot(airquality, aes(x = Temp)) + geom_histogram(aes(fill = Temp > median(Temp)), color = \"black\", binwidth = 2) + labs(title = \"La Guardia Airport Daily Mean Temperature\",    x = \"Temperature\", y = \"Number of Days\", fill = \"Temperature\") + theme_bw() + theme(plot.title = element_text(size = 10, face = \"bold\", hjust = .5),    panel.grid.major = element_line(color = \"grey80\")) + scale_fill_manual(values = c(\"steelblue2\",\"brown1\"),labels=c(\"< 79 Degrees\",\"> 79 Degrees\")) + geom_vline(xintercept = 72, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_vline(xintercept = 85, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 71, y = 10, label = \"Q1\") + geom_text(x = 86, y = 10, label = \"Q3\") ggplot(airquality, aes(x = Temp)) + geom_histogram(aes(fill = Temp > median(Temp)), color = \"black\", binwidth = 3) + labs(title = \"Temperature of La Guardia Airport by Month\",    x = \"Temperature\", y = \"Number of Days\", fill = \"Temperature\") + theme_bw() + theme(plot.title = element_text(size = 16, face = \"bold\", hjust = .5),    panel.grid.major = element_line(color = \"grey80\")) + scale_fill_manual(values = c(\"steelblue2\",\"brown1\"),labels=c(\"< 79 Degrees\",\"> 79 Degrees\")) + facet_grid(~Month) + geom_vline(xintercept = 77.88, linetype = \"dashed\", size = 1.5, color = \"black\") + geom_text(x = 69, y = 10, label = \"Mean = \\n 77.88\") To make a histogram in plotly first load library(plotly) Then, use the function: plot_ly(dataName, x=~columnName, type=\"histogram\") dataName is the name of a data set columnName must be the name of a column of quantitative data. R refers to this as a “numeric vector.” type=\"histogram\" tells the plot_ly(…) function to create a histogram. Visit plotly.com/r/histograms for more details. Example Code Hover your mouse over the example codes to learn more. Click on them to see what they create. Basic histogram plot_ly An R function “plot_ly” from library(plotly) used to create any plotly plot. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality,  “airquality” is a dataset. Type “View(airquality)” in R to see it. x= The x= allows us to declare which column of the data set will become the x-axis of the histogram. ~Temp,   “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. The ~ is required before column names inside all plot_ly(…) commands. type=“histogram” This option tells the plot_ly(…) function what “type” of graph to make. In this case, a histogram. )Closing parenthsis for the plot_ly function.     Press Enter to run the code.  …  Click to View Output. {\"x\":{\"visdat\":{\"100c2a7665f4\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"100c2a7665f4\",\"attrs\":{\"100c2a7665f4\":{\"x\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"histogram\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Temp\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[67,72,74,62,56,66,65,59,61,69,74,69,66,68,58,64,66,57,68,62,59,73,61,61,57,58,57,67,81,79,76,78,74,67,84,85,79,82,87,90,87,93,92,82,80,79,77,72,65,73,76,77,76,76,76,75,78,73,80,77,83,84,85,81,84,83,83,88,92,92,89,82,73,81,91,80,81,82,84,87,85,74,81,82,86,85,82,86,88,86,83,81,81,81,82,86,85,87,89,90,90,92,86,86,82,80,79,77,79,76,78,78,77,72,75,79,81,86,88,97,94,96,94,91,92,93,93,87,84,80,78,75,73,81,76,77,71,71,78,67,76,68,82,64,71,81,69,63,70,77,75,76,68],\"type\":\"histogram\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} Change Color plot_ly(airquality, x=~Temp, type=“histogram”,  This code was explained in the first example code. marker=list( this “list(…)” of options that will be specified will effect the bars of the histogram. color = “skyblue”,  this will change the color of the bars to skyblue. line = list(,  this opens a list of options to specify for the “lines” around the “markers.” color = “darkgray”,  this will change the color of the lines around the bars to darkgray. width = 2 this will change the width of the lines around the bars to 2 pixels. Too really see what this does, change it to something crazy like 10. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. {\"x\":{\"visdat\":{\"100cbf74e9e\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"100cbf74e9e\",\"attrs\":{\"100cbf74e9e\":{\"x\":{},\"marker\":{\"color\":\"skyblue\",\"line\":{\"color\":\"darkgray\",\"width\":2}},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"histogram\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Temp\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[67,72,74,62,56,66,65,59,61,69,74,69,66,68,58,64,66,57,68,62,59,73,61,61,57,58,57,67,81,79,76,78,74,67,84,85,79,82,87,90,87,93,92,82,80,79,77,72,65,73,76,77,76,76,76,75,78,73,80,77,83,84,85,81,84,83,83,88,92,92,89,82,73,81,91,80,81,82,84,87,85,74,81,82,86,85,82,86,88,86,83,81,81,81,82,86,85,87,89,90,90,92,86,86,82,80,79,77,79,76,78,78,77,72,75,79,81,86,88,97,94,96,94,91,92,93,93,87,84,80,78,75,73,81,76,77,71,71,78,67,76,68,82,64,71,81,69,63,70,77,75,76,68],\"marker\":{\"color\":\"skyblue\",\"line\":{\"color\":\"darkgray\",\"width\":2}},\"type\":\"histogram\",\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} Add Titles plot_ly(airquality$Temp, type=“histogram”,  This code was explained in the first example code. marker=list( this “list(” of options that will be specified will effect the bars of the histogram. color = “skyblue”,  this will change the color of the bars to skyblue. line = list(,  this opens a list of options to specify for the “lines” around the “markers.” color = “darkgray”,  this will change the color of the lines around the bars to darkgray. width = 10 this will change the width of the lines around the bars to 10 pixels, which is rather large really. Using a width=2 is probably better. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis.  %>% The pipe operator passes the completed plot_ly(…) code into the layout(…) function. layout( The layout(…) function is used for specifying details about the axes and their labels. title=“La Guardia Airport Daily Mean Temperatures” This declares a main title for the top of the graph. xaxis=list( This declares a list of options to be specified for the xaxis. The same can be done for the yaxis(…). title=“Temperature in Degrees F” This declares a title underneath the x-axis. ) Functions always end with a closing parenthesis. ) Functions always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. {\"x\":{\"visdat\":{\"100c66333271\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"100c66333271\",\"attrs\":{\"100c66333271\":{\"x\":{},\"marker\":{\"color\":\"skyblue\",\"line\":{\"color\":\"darkgray\",\"width\":10}},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"histogram\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"title\":\"La Guardia Airport Daily Mean Temperatures\",\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Temperature in Degrees F\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[67,72,74,62,56,66,65,59,61,69,74,69,66,68,58,64,66,57,68,62,59,73,61,61,57,58,57,67,81,79,76,78,74,67,84,85,79,82,87,90,87,93,92,82,80,79,77,72,65,73,76,77,76,76,76,75,78,73,80,77,83,84,85,81,84,83,83,88,92,92,89,82,73,81,91,80,81,82,84,87,85,74,81,82,86,85,82,86,88,86,83,81,81,81,82,86,85,87,89,90,90,92,86,86,82,80,79,77,79,76,78,78,77,72,75,79,81,86,88,97,94,96,94,91,92,93,93,87,84,80,78,75,73,81,76,77,71,71,78,67,76,68,82,64,71,81,69,63,70,77,75,76,68],\"marker\":{\"color\":\"skyblue\",\"line\":{\"color\":\"darkgray\",\"width\":10}},\"type\":\"histogram\",\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"bas"},{"title":"Statistics Notebook","url":"index.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Statistics Notebook This page, and all pages of this notebook, are meant to be customized to become a useful Guide to Statistical Analysis in R for your current and future self. function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } function openTab(evt, tabName) { var i, tabcontent, tablinks; tabcontent = document.getElementsByClassName(\"tabcontent\"); for (i = 0; i < tabcontent.length; i++) { tabcontent[i].style.display = \"none\"; } tablinks = document.getElementsByClassName(\"tablinks\"); for (i = 0; i < tablinks.length; i++) { tablinks[i].className = tablinks[i].className.replace(\" active\", \"\"); } document.getElementById(tabName).style.display = \"block\"; evt.currentTarget.className += \" active\"; } Sections Paige’s Notes : Notes taken throughout different statistic based classes. R Help : R Studio commands, structuring tips, and tricks. Describing Data : R Studio commands for graphically and numerically describing data. Making Inference : Deep dives into the different statistical tests. Analyses : Statistical analyses that utilizes the tools of collecting, analyzing, and interpreting data to result in informed decision-making. function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } function openTab(evt, tabName) { var i, tabcontent, tablinks; tabcontent = document.getElementsByClassName(\"tabcontent\"); for (i = 0; i < tabcontent.length; i++) { tabcontent[i].style.display = \"none\"; } tablinks = document.getElementsByClassName(\"tablinks\"); for (i = 0; i < tablinks.length; i++) { tablinks[i].className = tablinks[i].className.replace(\" active\", \"\"); } document.getElementById(tabName).style.display = \"block\"; evt.currentTarget.className += \" active\"; } Search Bar (async function () { // fetch the prebuilt index (we'll generate this file in a sec) const res = await fetch('search-index.json'); const docs = await res.json(); const miniSearch = new MiniSearch({ fields: ['title', 'content'], // index these fields storeFields: ['title', 'url'], // return these fields }); miniSearch.addAll(docs); const q = document.getElementById('q'); const resultsEl = document.getElementById('results'); const countEl = document.getElementById('count'); function render(results) { resultsEl.innerHTML = results.map(r => { const title = r.title || r.url; const url = r.url; return `<li style=\"margin:8px 0\"> <a href=\"${url}\">${title}<\/a><br/> <small>${url}<\/small> <\/li>`; }).join(''); countEl.textContent = results.length ? `${results.length} result(s)` : ''; } q.addEventListener('input', () => { const query = q.value.trim(); if (!query) { resultsEl.innerHTML = ''; countEl.textContent = ''; return; } const results = miniSearch.search(query, { fuzzy: 0.2, prefix: true }); render(results); }); })(); Table of Contents These are the statistical tools used to explore and interpret different data types. One Quantitative Response Variable Y Graphics Y is a single quantitative variable of interest. This would be like “heights” of BYU-Idaho students. Test(s) used for this category: One Sample T-test Paired Sample T-test Wilcoxon Signed- Rank Test Used for one quantitative variable. Pretty simple tbh. Questions that this answers: How long are 4th grader’s feet? What is the average length of feet in the KidsFeet dataset? These are the best graphics to use: Examples : What is the mean temperature at Airport? (One Sample) can be done with histogram or dot plot! plot_ly(airquality, x=~Temp, type=\"histogram\", marker=list(color=\"skyblue\", line=list(color=\"darkgray\", width=10))) %>% layout(title=\"La Guardia Airport Daily Mean Temperatures\", xaxis=list(title=\"Temperature in Degrees F\")) {\"x\":{\"visdat\":{\"100c4d31ab2\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"100c4d31ab2\",\"attrs\":{\"100c4d31ab2\":{\"x\":{},\"marker\":{\"color\":\"skyblue\",\"line\":{\"color\":\"darkgray\",\"width\":10}},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"histogram\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"title\":\"La Guardia Airport Daily Mean Temperatures\",\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Temperature in Degrees F\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[67,72,74,62,56,66,65,59,61,69,74,69,66,68,58,64,66,57,68,62,59,73,61,61,57,58,57,67,81,79,76,78,74,67,84,85,79,82,87,90,87,93,92,82,80,79,77,72,65,73,76,77,76,76,76,75,78,73,80,77,83,84,85,81,84,83,83,88,92,92,89,82,73,81,91,80,81,82,84,87,85,74,81,82,86,85,82,86,88,86,83,81,81,81,82,86,85,87,89,90,90,92,86,86,82,80,79,77,79,76,78,78,77,72,75,79,81,86,88,97,94,96,94,91,92,93,93,87,84,80,78,75,73,81,76,77,71,71,78,67,76,68,82,64,71,81,69,63,70,77,75,76,68],\"marker\":{\"color\":\"skyblue\",\"line\":{\"color\":\"darkgray\",\"width\":10}},\"type\":\"histogram\",\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}stripchart(airquality$Temp, method=\"stack\", xlab= \"Temperature (F)\", main=\"La Guardia Airport Daily Mean Temperature\") You can change the method to “overplot”, “jitter”, or “stack” What is the median temperature of La Guardia Airport? (One Sample) boxplot (airquality$Temp, ylab=\"Temperature (F)\", main=\"La Guardia Airport Temperature Median\") Tests One Sample t Test A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable. Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average? Is human body temperature really 98.6&deg; F on average? Do I spend less than $3 a day,on average, purchasing snacks? Requirements This test is only appropriate when both of the following are satisfied. The sample is representative of the population. (Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal. This is a safe assumption when either: the population data can be assumed to be normally distributed using a Q-Q Plot OR the size of the sample (n) that was taken from the population is large (at least n > 30, but “large” really depends on how badly the data is skewed). If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population. If the requirements are not met, then that doesn’t mean the results of the test are necessarily bad, but there is no guarantee that they are good. Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits R Instructions Console Help Command: ?t.test() t.test(NameOfYourData$Y, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set, like mtcars or KidsFeet. Y must be a “numeric” vector of quantitative data. YourNull is the numeric value from your null hypothesis for \\(\\mu\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(NameOfYourData$Y) Example Code Hover your mouse over the example codes to learn more. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,  ‘mpg’ is Y, a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,  The numeric value from the null hypothesis is 20 meaning \\(\\mu=20\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1−α.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. One Sample t-test EXPLANATION. data: mtcars$mpg EXPLANATION. t = 0.08506, EXPLANATION.  df = 31, EXPLANATION.  p-value = 0.9328 EXPLANATION. alternative hypothesis: true mean is not equal to 20 EXPLANATION. 95 percent confidence interval: EXPLANATION.  17.91768 EXPLANATION.  22.26357 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION.  20.09062 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg) ‘mpg’ is a quantitative variable (numeric vector) from the mtcars dataset.  Click to Show Output  Click to View Output. ## [1] 20 18 Explanation When we want to check if a claim about the average of a group(population mean \\(\\mu\\)) is true, we often use a test called the “one sample t test”. This test works well when we can assume that the data follows a normal pattern and that we’ve picked our sample randomly from the bigger group we’re interested in. In the figure below, the null hypothesis \\(H_0: \\mu = \\mu_0\\) is represented by the normal distribution (gray) centered at \\(\\mu_0\\). - Note that \\(\\mu_0\\) is just some specified number. - This shows how the null hypothesis represents the assumption about the center of the distribution of the data. After a hypothesis (null) is established and an alternative hypothesis similarly declared, a simple random sample of data of size \\(n\\) is obtained from the population of interest. - In the plot above, this is depicted by the points (blue dots) which are centered around their sample mean \\(\\bar{x}\\). Above the points (blue dots) is another bell-shaped curve (blue dashed line). This curve shows that the alternative hypothesis might fit the data better than the null hypothesis. The role of the one sample t test is to measure the probability of a sample mean being as extreme or more extreme from the hypothesized value of \\(\\mu_0\\) than the one observed assuming the null hypothesis is true. - This probability is of course the p-value of the test. - This works because the sampling distribution of the sample mean has been assumed to be normal. In this case, the distribution of the test statistic t, \\[ t = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\] is known to follow a t distribution with \\(n-1\\) degrees of freedom. *The mathematics that provide this result are phenominal! You can consult any advanced statistical textbook for the details. The p-value of the one sample t test represents the probability that the test statistic \\(t\\) is as extreme or more extreme than the one observed according to a t-distribution with \\(n-1\\) degrees of freedom. If the probability (the p-value) is close enough to zero (smaller than \\(\\alpha\\)), then it is determined that the most plausible hypothesis is the alternative hypothesis, and thus the null is “rejected” in favor of the alternative. Paired Samples t Test The paired samples t test is used when a value is hypothesized for the popluation mean of the differences, \\(\\mu_d\\), obtained from paired observations. Overview Questions The Paired Samples t Test can be used to answer questions like: From pre-test to post-test is there an improvement on average in the subjects? How much taller are husbands than their wives, on average? Do hospital patients that are carefully matched together according to reason for being in the hospital, age, gender, ethnicity, height, and weight show increased stay times in the hospital when infected with a nosocomial infection compared to those who were not infected? Requirements The test is only appropriate when both of the following are satisfied. The sample of differences is representative of the population differences. The sampling distribution of the sample mean of the differences \\(\\bar{d}\\) (\\(\\bar{x}\\) of the differences) can be assumed to be normal. (This second requirement can be assumed to be satisfied when (a) the differences themselves can be assumed to be normal from a Q-Q Plot, or (b) when the sample size \\(n\\) of the differences is large.) Hypotheses Math Code $$ H_0: \\mu_d = 0 $$ $$ H_a: \\mu_d \\neq 0 $$ \\(H_0: \\mu_d = \\text{some number, but typically 0}\\)\\(H_a: \\mu_d \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepPaired studentPaired R Instructions Console Help Command: ?t.test() Option 1: t.test(NameOfYourData$Y1, NameOfYourData$Y2, paired = TRUE, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set like sleep or mtcars or KidsFeet. Y1 must be a “numeric” vector that represents the quantitative data from the first sample of data. Y2 must be a “numeric” vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y1 - Y2) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the “group1” data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the “group2” data from the sleep data set t.test( ‘t.test’ is an R function that performs one and two sample t-tests. sleep2$extra,  A numeric vector that represents the hours of extra sleep that the group had with drug 2. sleep1$extra,  A numeric vector that represents the hours of extra sleep that the same group had with drug 1. paired=TRUE,  Indicates that this is a paired t-Test. This will cause the subtraction of sleep2$extra - sleep1$extra to be performed to obtain the paired differences. To cause the subtraction to occur in the other order, reverse the order sleep1$extra, sleep2$extra occur in the t.test(…) function. mu = 0,  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. Paired t-test EXPLANATION. data: sleep2$extra and sleep1$extra EXPLANATION. t = 4.0621, EXPLANATION.  df = 9, EXPLANATION.  p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  0.7001142 EXPLANATION.  2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of the differences EXPLANATION.   1.58 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. sleep2$extra The hours of extra sleep that the group had with drug 2.  -  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. ) Closing parenthesis for qqPlot(…) function.  Click to Show Output  Click to View Output. ## [1] 9 5 Option 2: Compute the differences yourself instead of using paired=TRUE. differences = NameOfYourData$Y1 - NameOfYourData$Y2 t.test(differences, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a “numeric” vector that represents the quantitative data from the first sample of data. Y2 must be a “numeric” vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. differences are the resulting differences obtained from subtracting Y1 - Y2. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(differences) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the “group1” data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the “group2” data from the sleep data set differences <-  Saved the computed differences to an object called ‘differences’. sleep2$extra The hours of extra sleep that the group had with drug 2.  -  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. differences,  ‘differences’ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2. mu = 0,  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. One Sample t-test EXPLANATION. data: differences EXPLANATION. t = 4.0621, EXPLANATION.  df = 9, EXPLANATION.  p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  0.7001142 EXPLANATION.  2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION.   1.58 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. differences) ‘differences’ are the resulti"},{"title":"Kruskal-Wallis Test","url":"Kruskal.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Kruskal-Wallis Test function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } The nonparametric equivalent to one-way ANOVA. Kruskal-Wallis Rank Sum Test Allows for deciding if several samples come from the same population or if at least one sample comes from a different population. Overview It is assumed that the various populations are of approximately the same form, but are shifted or translated. Can also be used when data is ordered (ordinal) but does not have an exact measurement. Best used when the distribution of the data is not normal. ANOVA is appropriate when the data is normal. Hypotheses Typically, the hypotheses of the Kruskal-Wallis test are: \\[ H_0: \\text{All samples are from the same distribution.} \\] \\[ H_a: \\text{At least one sample's distribution is stochastically different.} \\] However, according to the original authors, what the alternative to the Kruskal-Wallis test really is, “is a tendency for observations in at least one of the populations to be larger (or smaller) than all the observations together, when paired randomly. In many cases, this is practically equivalent to the mean of at least one population differing from the others.” Thus, it could also be appropriate to use the ANOVA style hypotheses \\[ H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu \\] \\[ H_a: \\mu_i \\neq \\mu \\ \\text{for at least one} \\ i \\] R Instructions Console Help Command: ?kruskal.test() kruskal.test(x ~ g, data=YourDataSet) x is a numeric vector of data values that represents the quantatitive response variable. g is a qualitative grouping variable defining which groups each value in x belongs to. It must either be a character vector or a factor vector. YourDataSet is the name of your data set. Example Code Hover your mouse over the example codes to learn more. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. weight ‘weight’ is a numeric variable from the chickwts dataset that represents the quantatitive response variable.  ~  ‘~’ is the tilde symbol used to separate the left- and right-hand side in a model formula. feed, ‘feed’ is a qualitative grouping variable in the chickwts dataset.  data = chickwts) ‘chickwts’ is a dataset in R.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to View Output  Click to View Output. ## ## Kruskal-Wallis rank sum test ## ## data: weight by feed ## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07 Alternatively, you could use the following approach as well. kruskal.test(list(x, y, z)) x is a numeric vector of data values that represents the quantatitive response variable for the first group. y is a numeric vector of data values that represents the quantatitive response variable for the second group. z is a numeric vector of data values that represents the quantatitive response variable for the third group. Note that more than three vectors of data could be included inside of list(). Example Code Hover your mouse over the example codes to learn more. feed1 <- filter(chickwts, feed == “horsebean”) This splits out the first group of feed (horsebean) from the chickwts dataset. feed2 <- filter(chickwts, feed == “linseed”) This splits out the second group of feed (linseed) from the chickwts dataset. feed3 <- filter(chickwts, feed == “soybean”) This splits out the third group of feed (soybean) from the chickwts dataset. feed4 <- filter(chickwts, feed == “sunflower”) This splits out the fourth group of feed (sunflower) from the chickwts dataset. feed5 <- filter(chickwts, feed == “meatmeal”) This splits out the fifth group of feed (meatmeal) from the chickwts dataset. feed6 <- filter(chickwts, feed == “casein”) This splits out the sixth group of feed (casein) from the chickwts dataset. kruskal.test( The function that performs a Kruskal-Wallis rank sum test. list(feed1$weight,feed2$weight,feed3$weight,feed4$weight,feed5$weight,feed6$weight)) A list of numeric vecotrs that represent the quantitative response variable for each group.  Click to View Output  Click to View Output. Load library(tidyverse) to run this code in R. ## ## Kruskal-Wallis rank sum test ## ## data: list(feed1$weight, feed2$weight, feed3$weight, feed4$weight, feed5$weight, feed6$weight) ## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07 Explanation In many cases it is of interest to perform a hypothesis test about whether or not several samples come from the same population. The Kruskal-Wallis Rank Sum Test allows for this by extending the idea behind the Wilcoxon Rank Sum Test to allow for more than two samples. The following explanation of how the test works comes mainly from the original 1952 article, Use of Ranks in One-Criterion Variance Analysis, 1952, but is expanded here for increased clarity. Calculating the Test Statistic The following artificial data will be used to demonstrate how the Kruskal-Wallis Test statistic is calculated. (Taken from the original article.) Daily Bottle-Cap Production of Three Machines (Artificial data.) Standard Modification1 Modification2 340 339 347 345 333 343 330 344 349 342 355 338 Step 1: Obtain \\(C\\) samples, each of size \\(n_i\\). Note that the notation \\(n_i\\) allows for each sample to be a different size. In the bottle-cap data, \\(C=3\\) and \\(n_1 = 5\\), \\(n_2 = 3\\), and \\(n_3 = 4\\). Step 2: Order the \\(N=n_1 + \\cdots + n_C\\) values from all samples from smallest to largest. Rank the values from smallest to largest with the ranks \\(1,\\ldots,N\\). Award any tied values the average of the ranks of those values that are tied. In the bottle-cap data we have the following. Rank 1 2 3 4 5 6 7 8 9 10 11 12 Value 330 333 338 339 340 342 343 344 345 347 349 355 Machine S M1 S M1 S S M2 M1 S M2 M2 M2 Step 3: Sum the ranks for each sample. Denote the sum of the ranks for sample \\(i\\) by \\(R_i\\). In the bottle-cap data, \\(R_1 = 1 + 3 + 5 + 6 + 9 = 24\\) \\(R_2 = 2 + 4 + 8 = 14\\) \\(R_3 = 7 + 10 + 11 + 12 = 40\\) Step 4: Compute the mean rank for each sample, \\(\\bar{R_i} = \\frac{R_i}{n_i}\\). In the bottle-cap data, \\(\\bar{R}_1 = \\frac{R_1}{n_1} = \\frac{24}{5} = 4.8\\) \\(\\bar{R}_2 = \\frac{R_2}{n_2} = \\frac{14}{3} = 4.667\\) \\(\\bar{R}_3 = \\frac{R_3}{n_3} = \\frac{40}{4} = 10.0\\) Step 5: If there are no ties, calculate the test statistic \\(H\\) by the formula \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] See the original article for the formula to use when there are ties present in the data. In the bottle-cap data, \\(H = 5.656\\). To understand the logic behind the test statistic \\(H\\), note that the null hypothesis assumes that each of the \\(C\\) samples are taken from the same population. Under this assumption, the ranks assigned to each sample should represent a uniform sample of the ranks \\(1,\\ldots,N\\). If the null hypothesis were true, we would expect the average of the ranks from each group, the \\(\\bar{R}_i\\), to be roughly about the same. More precisely, using the properties of the discrete uniform distribution (see wikipedia), we expect each \\(\\bar{R}_i\\) to be close to \\(\\frac{1}{2}(N+1)\\), which is the mean of the discrete uniform distribution. That \\(\\bar{R}_i\\) should be close to this value comes from the fact that the mean of a distribution of means is equal to the population mean. Further, the variance of \\(\\bar{R}_i\\) is given by the population variance, \\(\\frac{1}{12}(N^2-1)\\) in this case, divided by the sample size \\(n_i\\). Finally, the \\(\\frac{N-1}{N}\\) is a correction factor that will not be explained here. The full article explains the details about how a multivariate normal distribution is used to uncover the formula for \\(H\\). Thus, understanding \\(H\\) completely will require an advanced degree in statistical theory. However, the short story is that the distribution of the test statistic \\(H\\) is approximately a chi squared distribution with \\(C-1\\) degrees of freedom and has the form of a sum of values consisting of the (observed mean - expected mean)^2/(variance of the observed mean). Step 6: Calculate the p-value of \\(H\\) using a chi squared distribution with \\(C-1\\) degrees of freedom. \\[ H = \\frac{N-1}{N}\\sum_{i=1}^C \\frac{n_i[\\bar{R}_i-\\frac{1}{2}(N+1)]^2}{\\frac{1}{12}(N^2-1)} \\] In the bottle-cap data, \\(p = 0.059\\), which represents the probability of \\(H\\) being as extreme or more extreme than the observed value of 5.656, assuming the null hypothesis is true. Examples: wages, pigweights // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Linear Regression","url":"LinearRegression.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Linear Regression function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } function openTab(evt, tabName) { var i, tabcontent, tablinks; tabcontent = document.getElementsByClassName(\"tabcontent\"); for (i = 0; i < tabcontent.length; i++) { tabcontent[i].style.display = \"none\"; } tablinks = document.getElementsByClassName(\"tablinks\"); for (i = 0; i < tablinks.length; i++) { tablinks[i].className = tablinks[i].className.replace(\" active\", \"\"); } document.getElementById(tabName).style.display = \"block\"; evt.currentTarget.className += \" active\"; } Determine which explanatory variables have a significant effect on the mean of the quantitative response variable. Simple Linear Regression Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable \\(Y\\) and a single quantitative explanatory variable \\(X\\). Overview Mathematical Model The true regression model assumed by a regression analysis is given by Math Code $$ \\underbrace{Y_i}_\\text{Some Label} = \\overbrace{\\beta_0}^\\text{y-int} + \\overbrace{\\beta_1}^\\text{slope} \\underbrace{X_i}_\\text{Some Label} + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2) $$ \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(=\\) This states that we are assuming \\(Y_i\\) was created, or is “equal to” the formula that will follow on the right-hand-side of the equation. \\(\\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X_i \\ }_\\text{true regression relation}\\) The true regression relation is a line, a line that is typically unknown in real life. It can be likened to “God’s Law” or “Natural Law”. Something that governs the way the data behaves, but is unkown to us. \\(+\\) This plus sign emphasizes that the actual data, the \\(Y_i\\), is created by adding together the value from the true line \\(\\beta_0 + \\beta_1 X_i\\) and an individual error term \\(\\epsilon_i\\), which allows each dot in the regression to be off of the line by a certain amount called \\(\\epsilon_i\\). \\(\\overbrace{\\epsilon_i}^\\text{error term}\\) Error term for each individual \\(i\\). The error terms are “random” and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law. \\(\\quad \\text{where}\\) Some extra comments are needed about \\(\\epsilon_i\\)… \\(\\ \\overbrace{\\epsilon_i \\sim N(0, \\sigma^2)}^\\text{error term normally distributed}\\) The error terms \\(\\epsilon_i\\) are assumed to be normally distributed with constant variance. Pay special note that the \\(\\sigma\\) does not have an \\(i\\) in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line. The estimated regression line obtained from a regression analysis, pronounced “y-hat”, is written as Math Code $$ \\underbrace{\\hat{Y}_i}_\\text{Some Label} = \\overbrace{b_0}^\\text{est. y-int} + \\overbrace{b_1}^\\text{est. slope} \\underbrace{X_i}_\\text{Some Label} $$ \\(\\hat{Y}_i\\) The estimated average y-value for individual \\(i\\) is denoted by \\(\\hat{Y}_i\\). It is important to recognize that \\(Y_i\\) is the actual value for individual \\(i\\), and \\(\\hat{Y}_i\\) is the average y-value for all individuals with the same \\(X_i\\) value. \\(=\\) The formula for the average y-value, \\(\\hat{Y}_i\\) is equal to what follows… \\(\\underbrace{\\overbrace{\\ b_0 \\ }^\\text{y-intercept} + \\overbrace{b_1}^\\text{slope} X_i \\ }_\\text{estimated regression relation}\\) Two things are important to notice about this equation. First, it uses \\(b_0\\) and \\(b_1\\) instead of \\(\\beta_0\\) and \\(\\beta_1\\). This is because \\(b_0\\) and \\(b_1\\) are the estimated y-intercept and slope, respectively, not the true y-intercept \\(\\beta_0\\) and true slope \\(\\beta_1\\). Second, this equation does not include \\(\\epsilon_i\\). In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values. Note: see the Explanation tab The Mathematical Model for details about these equations. Hypotheses Math Code $$ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses} $$ $$ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses} $$ \\[ \\left.\\begin{array}{ll} H_0: \\beta_1 = 0 \\\\ H_a: \\beta_1 \\neq 0 \\end{array} \\right\\} \\ \\text{Slope Hypotheses}^{\\quad \\text{(most common)}}\\quad\\quad \\] \\[ \\left.\\begin{array}{ll} H_0: \\beta_0 = 0 \\\\ H_a: \\beta_0 \\neq 0 \\end{array} \\right\\} \\ \\text{Intercept Hypotheses}^{\\quad\\text{(sometimes useful)}} \\] If \\(\\beta_1 = 0\\), then the model reduces to \\(Y_i = \\beta_0 + \\epsilon_i\\), which is a flat line. This means \\(X\\) does not improve our understanding of the mean of \\(Y\\) if the null hypothesis is true. If \\(\\beta_0 = 0\\), then the model reduces to \\(Y_i = \\beta_1 X + \\epsilon_i\\), a line going through the origin. This means the average \\(Y\\)-value is \\(0\\) when \\(X=0\\) if the null hypothesis is true. Assumptions This regression model is appropriate for the data when five assumptions can be made. Linear Relation: the true regression relation between \\(Y\\) and \\(X\\) is linear. Normal Errors: the error terms \\(\\epsilon_i\\) are normally distributed with a mean of zero. Constant Variance: the variance \\(\\sigma^2\\) of the error terms is constant (the same) over all \\(X_i\\) values. Fixed X: the \\(X_i\\) values can be considered fixed and measured without error. Independent Errors: the error terms \\(\\epsilon_i\\) are independent. Note: see the Explanation tab Residual Plots & Regression Assumptions for details about checking the regression assumptions. Interpretation The slope is interpreted as, “the change in the average y-value for a one unit change in the x-value.” It is not the average change in y. It is the change in the average y-value. The y-intercept is interpreted as, “the average y-value when x is zero.” It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful. R Instructions Console Help Command: ?lm() Perform the Regression mylm This is some name you come up with that will become the R object that stores the results of your linear regression lm(...) command.  <-  This is the “left arrow” assignment operator that stores the results of your lm() code into mylm name. lm( lm(…) is an R function that stands for “Linear Model”. It performs a linear regression analysis for Y ~ X. Y  Y is your quantitative response variable. It is the name of one of the columns in your data set. ~  The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X. X, X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value.  data = NameOfYourDataset NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X. ) Closing parenthesis for the lm(…) function. summary(mylm) The summary command allows you to print the results of your linear regression that were previously saved in mylm name.  Click to Show Output  Click to View Output. Example output from a regression. Hover each piece to learn more. Call: lm(formula = dist ~ speed, data = cars) This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…). Residuals: Residuals are the vertical difference between each point and the line, \\(Y_i - \\hat{Y}_i\\). The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. min   -29.069 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -9.525 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -2.272 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -2.272 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-2.272) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   9.215 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   43.201 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your lm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details.   Std. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   t value To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   Pr(>|t|) The “Pr” stands for “Probability” and the “(> |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero. To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. (Intercept) This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   -17.5791 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the average y-value when X is zero.   6.7584 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. -2.601 This is the test statistic t for the test of \\(\\beta_0 = 0\\). It is calculated by dividing the “Estimate” of the intercept (-17.5791) by its standard error (6.7584). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of -17.5791 is -2.601 standard errors (6.7584) from zero, which is a fairly surprising distance as shown by the p-value. 0.0123 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). speed This is always the name of your X-variable in your lm(Y ~ X, …).   3.9324 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the average y-value as X is increased by 1 unit.   0.4155 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. 9.464 This is the test statistic t for the test of \\(\\beta_1 = 0\\). It is calculated by dividing the “Estimate” of the slope (3.9324) by its standard error (0.4155). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from zero, which is a really surprising distance as shown by the smallness of the p-value. 1.49e-12 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pt(-abs(your t-value), df of your regression)*2 *** This is called a “star”. Three stars means significant at the 0.01 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. Residual standard error: This is the estimate of \\(\\sigma\\) in the regression model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). It is the square root of the MSE.  15.38 For this particular regression, the estimate of \\(\\sigma\\) is 15.38. Squaring this number gives you the MSE, which is the estimate of \\(\\sigma^2\\).  on 48 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 50 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 50-2 = 48. Multiple R-squared: This is \\(R^2\\), the percentage of variation in \\(Y\\) that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.  0.6511, In this particular regression, 65.11% of the variation in stopping distance dist is explained by the regression model using speed of the car.  Adjusted R-squared: The adjusted R-squared will always be at least slightly smaller than \\(R^2\\). The closer to R-squared that it is, the better. When it differs dramatically from \\(R^2\\), it is a sign that the regression model is over-fitting the data.  0.6438 In this case, the value of 0.6438 is quite close to the original \\(R^2\\) value, so there is no fear of over-fitting with this particular model. That is good. F-statistic: The F-statistic is found as the ratio of the MSR/MSE where MSR = SSR/(p-1) and MSE = SSE/(n-p) where n is the sample size and p is the number of parameters in the regression model.  89.57 This is the value of the F-statistic for the lm(dist ~ speed, data=cars) regression. Note that SSE = sum( cars.lm$res^2 ) = 11353.52 with n - p = 50 - 2 = 48 degrees of freedom for this data. Further, SSR = sum( (cars.lm$fit - mean(cars$dist))^2 ) = 21185.46 with p - 1 = 1 degree of freedom. So MSR = 21185.46 and MSE = 11353.52 / 48 = 236.5317. So MSR / MSE = 21185.46 / 236.5317 = 89.56711.  on 1 and 48 DF, The 1 degree of freedom is the SSR degrees of freedom (p-1). The 48 is the SSE degrees of freedom (n-p).  p-value: 1.49e-12 The p-value for an F-statistic is found by the code pf(89.56711, 1, 48, lower.tail=FALSE), which gives the probability of being more extreme than the observed F-statistic in an F distribution with 1 and 48 degrees of freedom. Check Assumptions 1, 2, 3, and 5 par( The par(…) command stands for “Graphical PARameters”. It allows you to control various aspects of graphics in Base R. mfrow= This stands for “multiple frames filled by row”, which means, put lots of plots on the same row, starting with the plot on the left, then working towards the right as more plots are created. c( The combine function c(…) is used to specify how many rows and columns of graphics should be placed together. 1, This specifies that 1 row of graphics should be produced. 3 This states that 3 columns of graphics should be produced. ) Closing parenthesis for c(…) function. ) Closing parenthesis for par(…) function. plot( This version of plot(…) will actually create several regression diagnostic plots by default. mylm, This is the name of an lm object that you created previously. which= This allows you to select “which” regression diagnostic plots should be drawn. 1 Selecting 1, would give the residuals vs. fitted values plot only. : The colon allows you to select more than just one plot. 2 Selecting 2 also gives the Q-Q Plot of residuals. If you wanted to instead you could just use which=1 to get the residuals vs fitted values plot, then you could use qqPlot(mylm$residuals) to create a fancier Q-Q Plot of the residuals. ) Closing parenthesis for plot(…) function. plot( This version of plot(…) will be used to create a time-ordered plot of the residuals. The order of the residuals is the original order of the x-values in the original data set. If the original data set doesn’t have an order, then this plot is not interesting. mylm The lm object that you created previously. $ This allows you to access various elements from the regression that was performed. residuals This grabs the residuals for each observation in the regression. ) Closing parenthesis for plot(…) function.  Click to Show Output  Click to View Output. Plotting the Regression Line Base R ggplot2 To add the regression line to a scatterplot use the abline(...) command: plot( The plot(…) function is used to create a scatterplot with a y-axis (the vertical axis) and an x-axis (the horizontal axis). Y  This is the “response variable” of your regression. The thing you are interested in predicting. This is the name of a “numeric” column of data from the data set called YourDataSet. ~  The tilde “~” is used to relate Y to X and can be found on the top-left key of your keyboard. X,  This is the explanatory variable of your regression. It is the name of a “numeric” column of data from YourDataSet. . data= The data= statement is used to specify the name of the data set where the columns of “X” and “Y” are located. YourDataSet This is the name of your data set, like KidsFeet or cars or airquality. ) Closing parenthesis for plot(…) function. abline( This stands for “a” (intercept) “b” (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line. mylm This is the name of an lm(…) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(…) function will locate these two values from within mylm and use them to add a line to your current plot(…). ) Closing parenthesis for abline(…"},{"title":"Logistic Regression","url":"LogisticRegression.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Logistic Regression function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } function openTab(evt, tabName) { var i, tabcontent, tablinks; tabcontent = document.getElementsByClassName(\"tabcontent\"); for (i = 0; i < tabcontent.length; i++) { tabcontent[i].style.display = \"none\"; } tablinks = document.getElementsByClassName(\"tablinks\"); for (i = 0; i < tablinks.length; i++) { tablinks[i].className = tablinks[i].className.replace(\" active\", \"\"); } document.getElementById(tabName).style.display = \"block\"; evt.currentTarget.className += \" active\"; } Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\). The explanatory variables can be either quantitative or qualitative. Simple Logistic Regression Model Regression for a qualitative binary response variable \\((Y_i = 0\\) or \\(1)\\) using a single (typically quantitative) explanatory variable. Overview The probability that \\(Y_i = 1\\) given the observed value of \\(x_i\\) is called \\(\\pi_i\\) and is modeled by the equation Math Code $$ P(Y_i = 1|\\, x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} = \\pi_i $$ \\(P(\\) The “P” stands for “Probability that…” \\(Y_i\\) The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to \\(n\\), the sample size. \\(= 1\\) Equals 1… This states that we are assuming that the probability that the response variable \\(Y_i\\) is a 1 for the current individual. \\(| x_i)\\) Given \\(x_i\\)… in other words, the “|” says “given” and \\(x_i\\) means the x-value of the current individual. \\(=\\) Equals sign. \\(\\displaystyle\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\) The logistic regression equation where \\(e=2.71828...\\) is the “natural constant” number and \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is teh slope. \\(= \\pi_i\\) The \\(\\pi_i\\) stands for the probability of individual \\(i\\) having a y-value equal to 1 given their \\(x_i\\) value. It is the short hand notation for \\(P(Y_i = 1 |x_i)\\). (It is NOT the number 3.14…) The coefficents \\(\\beta_0\\) and \\(\\beta_1\\) are difficult to interpret directly. Typicall \\(e^{\\beta_0}\\) and \\(e^{\\beta_1}\\) are interpreted instead. The value of \\(e^{\\beta_0}\\) or \\(e^{\\beta_1}\\) denotes the relative change in the odds that \\(Y_i=1\\). The odds that \\(Y_i=1\\) are \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: challenger | mouse R Instructions Console Help Command: ?glm() Perform a Logistic Regression YourGlmName This is some name you come up with that will become the R object that stores the results of your logistic regression glm() command.  <-  This is the “left arrow” assignment operator that stores the results of your glm() code into YourGlmName. glm( glm( is an R function that stands for “General Linear Model”. It works in a similar way that the lm( function works except that it requires a family= option to be specified at the end of the command. Y  Y is your binary response variable. It must consist of only 0’s and 1’s. Since TRUE’s = 1’s and FALSE’s = 0’s in R, Y could be a logical statement like (Price > 100) or (Animal == “Cat”) if your Y-variable wasn’t currently coded as 0’s and 1’s. ~  The tilde symbol ~ is used to tell R that Y should be treated as a function of the explanatory variable X. X, X is the explanatory variable (typically quantitative) that will be used to explain the probability that the response variable Y is a 1.  data = NameOfYourDataset,NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be called Y and another column would be called X.  family=binomial) The family=binomial command tells the glm( function to perform a logistic regression. It turns out that glm can perform many different types of regressions, but we only study it as a tool to perform a logistic regression in this course. summary(YourGlmName) The summary command allows you to print the results of your logistic regression that were previously saved in YourGlmName. Example output from a regression. Hover each piece to learn more. Call: glm(formula = am ~ disp, family = binomial, data = mtcars) This is simply a statement of your original glm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the glm(…). Deviance Residuals: Deviance residuals are a measure of how far the fitted probability for \\(\\pi_i\\) has differed from the actual outcome of \\(Y_i\\) in terms of the log of the fitted probability space. (This is a fairly complicated idea.) Min   -1.5651 “min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line. 1Q   -0.6648 “1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile. Median   -0.2460 “Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.2460 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.2460) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum. 3Q   0.7276 “3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally. Max   2.2691 “Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed. Coefficients: Notice that in your glm(…) you used only \\(Y\\) and \\(X\\). You did type out any coefficients, i.e., the \\(\\beta_0\\) or \\(\\beta_1\\) of the regression model. These coefficients are estimated by the glm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.   Estimate To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details.   Std. Error To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.   z value The test statistic is a regular old z-score. It is most reliable when the sample size is “large.” It is a measurement of the number of standard errors the estimate is from 0.   Pr(>|z|) This is the p-value, the probability of observing a test statistic more extreme than Z. (Intercept) This always says “Intercept” for any glm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.   2.630849 This is the estimate of the y-intercept, \\(\\beta_0\\). It is called \\(b_0\\). It is the value of the log of the odds that \\(Y_i=1\\) when \\(x_i\\) is zero. Remember to use \\(e^{b_0}\\) to interpret this values actual effect on the odds.   1.050170 This is the standard error of \\(b_0\\). It tells you how much \\(b_0\\) varies from sample to sample. The closer to zero, the better. 2.505 The test statistic for testing the hypothesis that \\(\\beta_0 = 0\\). 0.01224 This is the p-value of the test of the hypothesis that \\(\\beta_0 = 0\\). It measures the probability of observing a z-score as extreme as the one observed. To compute it yourself in R, use pnorm(-abs(your z-value))*2. * This is called a “star”. One star means significant at the 0.1 level of \\(\\alpha\\). disp This is always the name of your X-variable in your glm(Y ~ X, …).   -0.014604 This is the estimate of the slope, \\(\\beta_1\\). It is called \\(b_1\\). It is the change in the log of the odds that \\(Y_i = 1\\) as X is increased by 1 unit. Remember to use \\(e^{b_1}\\) to compute the actual effect on the odds.   0.005168 This is the standard error of \\(b_1\\). It tells you how much \\(b_1\\) varies from sample to sample. The closer to zero, the better. -2.826 This is the test statistic for testing the hypothesis that \\(\\beta_1 = 0\\). 0.00471 This is the p-value of the test of the hypothesis that \\(\\beta_1 = 0\\). To compute it yourself in R, use pnorm(-abs(your z-value))*2 ** This is called a “star”. Three stars means significant at the 0.001 level of \\(\\alpha\\). --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ’*’ 0.05 ‘.’ 0.1 ‘ ’ 1 These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above. (Dispersion parameter for binomial family taken to be 1) This is a simplifying assumption of the logistic regression. Overdispersion is a common problem with logistic regression data, but is typically ignored. Unless you become an expert in statistics, this is not something you need to worry about.   Null Deviance: The deviance of the null model. This is the model that excludes any information from the x-variable, i.e., \\(\\beta_1=0\\).  43.230  on 31 degrees of freedom The residual degrees of freedom. The higher this number, the more reliable the p-values will be from the logistic regression. Residual deviance: The sum of log of the squared residuals. Essentially the resulting statistic of a goodness of fit test measuring how well the data works with the logistic regression model. Using pchisq(residual deviance, df residual deviance, lower.tail=FALSE) gives the p-value for this goodness of fit test. However, the residual deviance only follows a chi-squared distribution with df residual deviance when there are many repeated x-values, and all x-values have at least a few replicates.  29.732 This can be calculated by sum(log(myglm$res^2)).  on 30 degrees of freedom This is \\(n-p\\) where \\(n\\) is the sample size and \\(p\\) is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), so 32-2 = 30. AIC: As stated in the R Help file for ?glm, “A version of Akaike’s An Information Criterion…” The AIC is useful for comparing different models for the same Y-variable. The glm model with the lowest AIC (which can go negative) is the best model.  33.732 The AIC for this particular model is 33.732. So if a different model (using the same Y-variable as this model) can get a lower AIC, it is a better model. Number of Fisher Scoring iterations: If you have taken a class in Numerical Analysis, this tells you how many iterations of the maximization algorithm were required before converging to the “Estimates” of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) found in the summary.  5 This implementation of glm required 5 Fisher Scoring iterations to converge. Fewer iterations hints that the model is a better fit than when many iterations are required. Diagnose the Goodness-of-Fit There are two ways to check the goodness of fit of a logistic regression model. Option 1: Hosmer-Lemeshow Goodness-of-Fit Test (Most Common) To check the goodness of fit of a logistic regression model where there are few or no replicated \\(x\\)-values use the Hosmer-Lemeshow Test. library(ResourceSelection) This loads the ResourceSelection R package so that you can access the hoslem.test() function. You may need to run the code: install.packages(“ResourceSelection”) first. hoslem.test( This R function performs the Hosmer-Lemeshow Goodness of Fit Test. See the “Explanation” file to learn about this test. YourGlmName YourGlmName is the name of your glm(…) code that you created previously. $y,  ALWAYS type a “y” here. This gives you the actual binary (0,1) y-values of your logistic regression. The goodness of fit test will compare these actual values to your predicted probabilities for each value in order to see if the model is a “good fit.” YourGlmName YourGlmName is the name you used to save the results of your glm(…) code. $fitted,  ALWAYS type “fitted” here. This gives you the fitted probabilities \\(\\pi_i\\) of your logistic regression. g=10) The “g=10” is the default option for the value of g. The g is the number of groups to run the goodness of fit test on. Just leave it at 10 unless you are told to do otherwise. Ask your teacher for more information if you are interested. ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: myglm$y, myglm$fitted ## X-squared = 5.7327, df = 8, p-value = 0.6771 Note that the null hypothesis of the goodness-of-fit test is that “the logistic regression is a good fit.” So we actually don’t want to “reject the null” in this case. So a large p-value here means our logistic regression fits the data satisfactorily. A small p-value implies a poor fit and the results of the logistic regression should not be fully trusted. Option 2: Deviance Goodness-of-fit Test (Less Common) In some cases, there are many replicated \\(x\\)-values for all x-values, i.e., each value of x is repeated more than 50 times. Though this is rare, it is good to use the deviance goodness-of-fit test whenever this happens. pchisq( The pchisq command allows you to compute p-values from the chi-squared distribution. residual deviance,  The residual deviance is shown at the bottom of the output of your summary(YourGlmName) and should be typed in here as a number like 25.3. df for residual deviance,  The df for the residual deviance is also shown at the bottom of the output of your summary(YourGlmName). lower.tail=FALSE) This command ensures you find the probability of the chi-squared distribution being as extreme or more extreme than the observed value of residual deviance. ## ## Call: ## glm(formula = am ~ disp, family = binomial, data = mtcars) ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.630849 1.050170 2.505 0.01224 * ## disp -0.014604 0.005168 -2.826 0.00471 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.732 on 30 degrees of freedom ## AIC: 33.732 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.479439 The null hypothesis of the goodness-of-fit test is that the logistic regression is a good fit of the data. So a large p-value (like 0.479) is good because it allows us to trust the results of our logistic regression. When the p-value becomes very small, we must “reject the null” and conclude a poor fit, which implies that we should not trust the results of the logistic regression. Plot the Regression Base R ggplot2 plot( The plot function allows us to draw a scatterplot where the y-axis is only 0’s or 1’s and the x-axis is quantitative. Y Y should be some logical statement like Fail > 0 or sex == “B” or height < 68. In other words, Y needs to be a collection of 0’s and 1’s.  ~  The tilde is read “Y on X.” X, X is some quantitative variable.  data= Tell the plot which data set to use. X and Y are columns of that data set. YourDataSet The name of your data set. ) Closing parenthesis for plot(…) function. curve( A function in R that draws a “curve” on a plot. Using add=TRUE puts the curve onto the current plot. exp( This allows you to compute e to the power of something. So exp(1) = e, exp(2) = e^2 and so on. \\(b_0\\) + This is the “Intercept” estimate from your logistic regression summary output.  \\(b_1\\) This is the slope estimate from your logistic regression summary output. *x The curve function demands you ALWAYS use a lower-case “x” in this function. ) Closing parenthesis. / The division symbol. (1 + Required for the formula.  exp(\\(b_0\\) + \\(b_1\\)*x) This needs to match exactly the first version of this statement. ), Closing parenthesis for the denominator.  add = TRUE) This makes the curve be added to the current plot. If it is left out, the curve will be drawn in a new plot. ggplot( The ggplot(…) function is used to create a basic ggplot frame. data=YourDataSetName, Use this to specify the name of your data set.  aes( The aes(…) function stands for “aesthetics” and tells the ggplot which variables to match up with the x-axis and y-axis of the graph as well as other visual things like the type of plotting characters, size of plotting characters, color, fill, and so on. x=X, Use x=nameOfYourXvariable to declare the x-axis of your graph. y=Y Use y=nameOfYourYvariable to declare the y-axis of your graph. If your y-variable is a logical expression, like height>60 then you must use y=as.numeric(height>60). ) Closing parenthesis for aes(…) function. ) Closing parenthesis for ggplot(…) function.  + The plus sign adds a new layer to the ggplot.   geom_point( Tells the plot to add the physical geometry of “points” to the plot. ) Closing parenthesis for the geom_point() function.  + Add another layer to the plot.   geom_smooth( Add a smoothing line to the graph. method=“glm”, This adds a general linear model to the graph. method.args = list(family=“binomial”), This tells the method=“glm” to choose specifically the “binomial” model, otherwise known as the “logistic regression” model.  se=FALSE Turn off the displaying of the confidence band around the logistic regression. You can turn this on if you know what it means. ) Closing parenthesis for the geom_smooth() function.  + Add another layer to the ggplot.   theme_bw() Give the graph a basic black and white theme. Other themes are possible, see ?theme_ in your Console. ## `geom_smooth()` using formula ## = 'y ~ x' Predict Probabilities To predict the probability that \\(Y_i=1\\) for a given \\(x\\)-value, use the code predict( The predict() function allows us to use the regression model that was obtained from glm() to predict the probability that \\(Y_i = 1\\) for a given \\(X_i\\). YourGlmName,  YourGlmName is the name of the object you created when you performed your logistic regression using glm(). newdata =  The newdata = command allows you to specify the x-values for which you want to obtain predicted probabilities that \\(Y_i=1\\). data.frame(xVariableName = someNumericValue),  The xVariableName is the same one you used in your glm(y ~ x, …) statement for “x”. Input any desired value for the “someNumericValue” spot. Then, the predict code uses the logistic regression model equation to calculate a predicted probability that \\(Y_i = 1\\) for the given \\(x_i\\) value that you specify. type = “response”) The type = “response” options specifies that you want predicted probabilities. There are other options available. See ?predict.glm for details. myglm <- glm(am ~ disp, data = mtcars, family = binomial) predict(myglm, newdata = data.frame(disp = 200), type = \"response"},{"title":"Making Inference","url":"MakingInference.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Making Inference function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } It is common to only have a sample of data from some population of interest. Using the information from the sample to reach conclusions about the population is called making inference. When statistical inference is performed properly, the conclusions about the population are almost always correct. Hypothesis Testing One of the great focal points of statistics concerns hypothesis testing. Science generally agrees upon the principle that truth must be uncovered by the process of elimination. The process begins by establishing a starting assumption, or null hypothesis (\\(H_0\\)). Data is then collected and the evidence against the null hypothesis is measured, typically with the \\(p\\)-value. The \\(p\\)-value becomes small (gets close to zero) when the evidence is extremely different from what would be expected if the null hypothesis were true. When the \\(p\\)-value is below the significance level \\(\\alpha\\) (typically \\(\\alpha=0.05\\)) the null hypothesis is abandoned (rejected) in favor of a competing alternative hypothesis (\\(H_a\\)). Click for an Example The current hypothesis may be that the world is flat. Then someone who thinks otherwise sets sail in a boat, gathers some evidence, and when there is sufficient evidence in the data to disbelieve the current hypothesis, we conclude the world is not flat. In light of this new knowledge, we shift our belief to the next working hypothesis, that the world is round. After a while, someone gathers more evidence and shows that the world is not round, and we move to the next working hypothesis, that it is oblate spheroid, i.e., a sphere that is squashed at its poles and swollen at the equator. This process of elimination is called hypothesis testing. The process begins by establishing a null hypothesis (denoted symbolically by \\(H_0\\)) which represents the current opinion, status quo, or what we will believe if the evidence is not sufficient to suggest otherwise. The alternative hypothesis (denoted symbolically by \\(H_a\\)) designates what we will believe if there is sufficient evidence in the data to discredit, or “reject,” the null hypothesis. See the BYU-I Math 221 Stats Wiki for another example. Managing Decision Errors When the \\(p\\)-value approaches zero, one of two things must be occurring. Either an extremely rare event has happened or the null hypothesis is incorrect. Since the second option, that the null hypothesis is incorrect, is the more plausible option, we reject the null hypothesis in favor of the alternative whenever the \\(p\\)-value is close to zero. It is important to remember that rejecting the null hypothesis could however be a mistake.   \\(H_0\\) True \\(H_0\\) False Reject \\(H_0\\) Type I Error Correct Decision Accept \\(H_0\\) Correct Decision Type II Error Type I Error, Significance Level, Confidence and \\(\\alpha\\) A Type I Error is defined as rejecting the null hypothesis when it is actually true. (Throwing away truth.) The significance level, \\(\\alpha\\), of a hypothesis test controls the probability of a Type I Error. The typical value of \\(\\alpha = 0.05\\) came from tradition and is a somewhat arbitrary value. Any value from 0 to 1 could be used for \\(\\alpha\\). When deciding on the level of \\(\\alpha\\) for a particular study it is important to remember that as \\(\\alpha\\) increases, the probability of a Type I Error increases, and the probability of a Type II Error decreases. When \\(\\alpha\\) gets smaller, the probability of a Type I Error gets smaller, while the probability of a Type II Error increases. Confidence is defined as \\(1-\\alpha\\) or the opposite of a Type I error. That is the probability of accepting the NULL when it is in fact true. Type II Errors, \\(\\beta\\), and Power It is also possible to make a Type II Error, which is defined as failing to reject the null hypothesis when it is actually false. (Failing to move to truth.) The probability of a Type II Error, \\(\\beta\\), is often unknown. However, practitioners often make an assumption about a detectable difference that is desired which then allows \\(\\beta\\) to be prescribed much like \\(\\alpha\\). In essence, the detectable difference prescribes a fixed value for \\(H_a\\). We can then talk about the power of of a hypothesis test, which is 1 minus the probability of a Type II Error, \\(\\beta\\). See Statistical Power in Wikipedia for a starting source if your are interested. This website provides a novel interactive visualization to help you understand power. It does require a little background on Cohen’s D. Sufficient Evidence Statistics comes in to play with hypothesis testing by defining the phrase “sufficient evidence.” When there is “sufficient evidence” in the data, the null hypothesis is rejected and the alternative hypothesis becomes the working hypothesis. There are many statistical approaches to this problem of measuring the significance of evidence, but in almost all cases, the final measurement of evidence is given by the \\(p\\)-value of the hypothesis test. The \\(p\\)-value of a test is defined as the probability of the evidence being as extreme or more extreme than what was observed assuming the null hypothesis is true. This is an interesting phrase that is at first difficult to understand. The “as extreme or more extreme” part of the definition of the \\(p\\)-value comes from the idea that the null hypothesis will be rejected when the evidence in the data is extremely inconsistent with the null hypothesis. If the data is not extremely different from what we would expect under the null hypothesis, then we will continue to believe the null hypothesis. Although, it is worth emphasizing that this does not prove the null hypothesis to be true. Evidence not Proof Hypothesis testing allows us a formal way to decide if we should “conclude the alternative” or “continue to accept the null.” It is important to remember that statistics (and science) cannot prove anything, just show evidence towards. Thus we never really prove a hypothesis is true, we simply show evidence towards or against a hypothesis. Calculating the \\(p\\)-Value Recall that the \\(p\\)-value measures how extremely the data (the evidence) differs from what is expected under the null hypothesis. Small \\(p\\)-values lead us to discard (reject) the null hypothesis. A \\(p\\)-value can be calculated whenever we have two things. A test statistic, which is a way of measuring how “far” the observed data is from what is expected under the null hypothesis. The sampling distribution of the test statistic, which is the theoretical distribution of the test statistic over all possible samples, assuming the null hypothesis was true. Visit the Math 221 textbook for an explanation. A distribution describes how data is spread out. When we know the shape of a distribution, we know which values are possible, but more importantly which values are most plausible (likely) and which are the least plausible (unlikely). The \\(p\\)-value uses the sampling distribution of the test statistic to measure the probability of the observed test statistic being as extreme or more extreme than the one observed. All \\(p\\)-value computation methods can be classified into two broad categories, parametric methods and nonparametric methods. Parametric Methods Parametric methods assume that, under the null hypothesis, the test statistic follows a specific theoretical parametric distribution. Parametric methods are typically more statistically powerful than nonparametric methods, but necessarily force more assumptions on the data. Parametric distributions are theoretical distributions that can be described by a mathematical function. There are many theoretical distributions. (See the List of Probability Distributions in Wikipedia for details.) Four of the most widely used parametric distributions are: The Normal Distribution Click for Details One of the most important distributions in statistics is the normal distribution. It is a theoretical distribution that approximates the distributions of many real life data distributions, like heights of people, heights of corn plants, baseball batting averages, lengths of gestational periods for many species including humans, and so on. More importantly, the sampling distribution of the sample mean \\(\\bar{x}\\) is normally distributed in two important scenarios. The parent population is normally distributed. The sample size is sufficiently large. (Often \\(n\\geq 30\\) is sufficient, but this is a general rule of thumb that is sometimes insufficient.) Mathematical Formula \\[ f(x | \\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} \\] The symbols \\(\\mu\\) and \\(\\sigma\\) are the two parameters of this distribution. The parameter \\(\\mu\\) controls the center, or mean of the distribution. The parameter \\(\\sigma\\) controls the spread, or standard deviation of the distribution. Graphical Form Comments The usefulness of the normal distribution is that we know which values of data are likely and which are unlikely by just knowing three things: that the data is normally distributed, \\(\\mu\\), the mean of the distribution, and \\(\\sigma\\), the standard deviation of the distribution. For example, as shown in the plot above, a value of \\(x=-8\\) would be very probable for the normal distribution with \\(\\mu=-5\\) and \\(\\sigma=2\\) (light blue curve). However, the value of \\(x=-8\\) would be very unlikely to occur in the normal distribution with \\(\\mu=3\\) and \\(\\sigma=3\\) (gray curve). In fact, \\(x=-8\\) would be even more unlikely an occurance for the \\(\\mu=0\\) and \\(\\sigma=1\\) distribution (dark blue curve). The Chi Squared Distribution Click for Details The chi squared distribution only allows for values that are greater than or equal to zero. While it has a few real life applications, by far its greatest use is theoretical. The test statistic of the chi squared test is distributed according to a chi squared distribution. Mathematical Formula \\[ f(x|p) = \\frac{1}{\\Gamma(p/2)2^{p/2}}x^{(p/2)-1}e^{-x/2} \\] The only parameter of the chi squared distribution is \\(p\\), which is known as the degrees of freedom. Larger values of the parameter \\(p\\) move the center of the chi squared distribution farther to the right. As \\(p\\) goes to infinity, the chi squared distribution begins to look more and more normal in shape. Note that the symbol in the denominator of the chi squared distribution, \\(\\Gamma(p/2)\\), is the Gamma function of \\(p/2\\). (See Gamma Function in Wikipedia for details.) Graphical Form Comments It is important to remember that the chi squared distribution is only defined for \\(x\\geq 0\\) and for positive values of the parameter \\(p\\). This is unlike the normal distribution which is defined for all numbers \\(x\\) from negative infinity to positive infinity as well as for all values of \\(\\mu\\) from negative infinity to positive infinity. The t Distribution Click for Details A close friend of the normal distribution is the t distribution. Although the t distribution is seldom used to model real life data, the distribution is used extensively in hypothesis testing. For example, it is the sampling distribution of the one sample t statistic. It also shows up in many other places, like in regression, in the independent samples t test, and in the paired samples t test. Mathematical Formula \\[ f(x|p) = \\frac{\\Gamma\\left(\\frac{p+1}{2}\\right)}{\\Gamma\\left(\\frac{p}{2}\\right)}\\frac{1}{\\sqrt{p\\pi}}\\frac{1}{\\left(1 + \\left(\\frac{x^2}{p}\\right)\\right)^{(p+1)/2}} \\] Notice that, similar to the chi squared distribution, the t distribution has only one parameter, the degrees of freedom \\(p\\). As the single parameter \\(p\\) is varied from \\(p=1\\), to \\(p=2\\), …, \\(p=5\\), and larger and larger numbers, the resulting distribution becomes more and more normal in shape. Note that the expressions \\(\\Gamma\\left(\\frac{p+1}{2}\\right)\\) and \\(\\Gamma(p/2)\\), refer to the Gamma function. (See Gamma Function in Wikipedia for details.) Graphical Form Comments When the degrees of freedom \\(p=30\\), the resulting t distribution is almost indistinguishable visually from the normal distribution. This is one of the reasons that a sample size of 30 is often used as a rule of thumb for the sample size being “large enough” to assume the sampling distribution of the sample mean is approximately normal. The F Distribution Click for Details Another commonly used distribution for test statistics, like in ANOVA and regression, is the F distribution. Technically speaking, the F distribution is the ratio of two chi squared random variables that are each divided by their respective degrees of freedom. Mathematical Formula \\[ f(x|p_1,p_2) = \\frac{\\Gamma\\left(\\frac{p_1+p_2}{2}\\right)}{\\Gamma\\left(\\frac{p_1}{2}\\right)\\Gamma\\left(\\frac{p_2}{2}\\right)}\\frac{\\left(\\frac{p_1}{p_2}\\right)^{p_1/2}x^{(p_1-2)/2}}{\\left(1+\\left(\\frac{p_1}{p_2}\\right)x\\right)^{(p_1+p_2)/2}} \\] where \\(x\\geq 0\\) and the parameters \\(p_1\\) and \\(p_2\\) are the “numerator” and “denominator” degrees of freedom, respectively. Graphical Form Comments The effects of the parameters \\(p_1\\) and \\(p_2\\) on the F distribution are complicated, but generally speaking, as they both increase the distribution becomes more and more normal in shape. Nonparametric Methods Nonparametric methods place minimal assumptions on the distribution of data. They allow the data to “speak for itself.” They are typically less powerful than the parametric alternatives, but are more broadly applicable because fewer assumptions need to be satisfied. Nonparametric methods include Rank Sum Tests and Permutation Tests. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"Numerical Summaries","url":"NumericalSummaries.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Numerical Summaries function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } There are many ways to numerically summarize data. The fundamental idea is to describe the center, or most probable values of the data, as well as the spread, or the possible values of the data. Mean \\[ \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} \\] Measure of Center | 1 Quantitative Variable Overview The “balance point” or “center of mass” of quantitative data. It is calculated by taking the numerical sum of the values divided by the number of values. Typically used in tandem with the standard deviation. Most appropriate for describing the most typical values for relatively normally distributed data. Influenced by outliers, so it is not appropriate for describing strongly skewed data. R Instructions To calculate a mean in R use the code: mean(object) object must be a quantitative variable, what R calls a “numeric vector.” Usually this is a column from a data set. Use na.rm=TRUE if there are missing values in object so that the code reads mean(object, na.rm=TRUE). Example Code Hover your mouse over the example codes to learn more. mean “mean” is an R function used to calculate the mean of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the mean function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 77.88235 Note that the single number showing above is the average Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveTemp =  “AveTemp” is just a name we made up. It will contain the results of the mean(…) function. mean( “mean” is an R function used to calculate the mean. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month aveTemp 5 65.55 6 79.1 7 83.9 8 83.97 9 76.9 Note that R calculated the mean Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, note that to get the “nicely formatted” table, you would have to use library(pander) airquality %>% group_by(Month) %>% summarise(aveTemp = mean(Temp)) %>% pander() A note about missing values in data… mean “mean” is an R function used to calculate the mean of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Ozone “Ozone” is a quantitative variable (numeric vector) from the “airquality” dataset. ,  The comma allows us to specify optional commands. na.rm=TRUE Missing values are called “NA” in R. If data contains missing values, mean(...) will give “NA” as the result unless we “remove” (rm) the “NA” (na) values. )Closing parenthsis for the mean function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 42.12931 Note that the single number showing above is the average Ozone from the airquality dataset. Because the Ozone column had missing values, we had to use the option na.rm=TRUE to get the mean to calculate. If we had left it off, we would have gotten an “NA” result: mean(airquality$Ozone) ## [1] NA Explanation The mathematical formula used to compute the mean of data is given by the formula to the left. Although the formula looks complicated, all it states is “add all the data values up and divide by the total number of values.” Read on to learn what all the symbols in the formula represent. Symbols in the Formula \\(\\bar{x}\\) is read “x-bar” and is the symbol typically used for the sample mean, the mean computed on a sample of data from a population. \\(\\Sigma\\), the capital Greek letter “sigma,” is the symbol used to imply “add all of the data values up.” The \\(x_i\\)’s are the data values. The \\(i\\) in the \\(x_i\\) is stated to go from \\(i=1\\) all the way up to \\(n\\). In other words, data value 1 is represented by \\(x_1\\), data value 2: \\(x_2\\), \\(\\ldots\\), up through the last data value \\(x_n\\). In general, we just write \\(x_i\\). \\(n\\) represents the sample size, or number of data values. Population Mean When all of the data from a population is available, the population mean is calculated instead of the sample mean. The mathematical formula for the population mean is the same as the formula for the sample mean, but is written with slightly different notation. \\[ \\mu = \\frac{\\sum_{i=1}^N x_i}{N} \\] Notice that the symbol for the population mean is \\(\\mu\\), pronounced “mew,” another Greek letter. (Review your Greek alphabet.) The only other difference between the two formulas is that the sample mean uses a sample of data, denoted by \\(n\\), while the population mean uses all the population data, denoted by \\(N\\). Physical Interpretation The mean is sometimes described as the “balance point” of the data. The following example will demonstrate. Say there are \\(n=5\\) data points with the following values. \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) The sample mean is calculated as follows. \\[ \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} = \\frac{2 + 5 + 6 + 7 + 10}{5} = 6 \\] If these values were plotted, and an “infinitely thin bar” connected the points, then the bar would “balance” at the mean (the triangle) as shown below. Middle of the Deviations The above plot demonstrates that there are equal, but opposite, “sums of deviations” to either side of the mean. Note that a deviation is defined as the distance from the mean to a given point. Thus, \\(x_1\\) has a deviation of -4 from the mean, \\(x_2\\) a deviation of -1, \\(x_3\\) a deviation of 0, \\(x_4\\) a deviation of 1, and \\(x_5\\) a deviation of 4. To the left there is a sum of deviations equal to -5 and on the right, a sum of deviations equal to 5. This can be verified to hold for any scenario. Effect of Outliers The mean can be strongly influenced by outliers, points that deviate abnormally from the mean. This is shown below by changing \\(x_5\\) to be 20. Note that the deviation of \\(x_5\\) is 12, and the sum of deviations to the left of the mean (\\(\\bar{x}=8\\)) is \\(-1 + -2 + -3 + -6 = -12\\). The mean of the altered data \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 20\\) is now \\(\\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} = \\frac{2 + 5 + 6 + 7 + 20}{5} = 8\\). Median \\[ \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} \\] \\(\\uparrow\\) even \\(n\\) odd \\(\\downarrow\\) \\[ x_{((n+1)/2)} \\] Measure of Center | 1 Quantitative Variable Overview The “middle data point,” i.e., the 50\\(^{th}\\) percentile. Half of the data is below the median and half is above the median. Typically used in tandem with the five-number summary to describe skewed data because it is not heavily influenced by outliers, i.e., it is robust. Can also be used with normally distributed data, but the mean and standard deviation are more useful measures in such cases. R Instructions To calculate a median in R use the code: median(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code median “median” is an R function used to calculate the median of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the median function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 79 Note that the single number showing above is the median Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. medTemp =  “medTemp” is just a name we made up. It will contain the results of the median(…) function. median( “median” is an R function used to calculate the median. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month medTemp 5 66 6 78 7 84 8 82 9 76 Note that R calculated the median Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(medTemp = median(Temp)) %>% pander() Explanation The mathematical formula used to compute the median of data depends on whether \\(n\\), the number of data points in the sample, is even or odd. If \\(n\\) is even, then there is no “middle” data point, so the middle two values are averaged. \\[ \\text{Median} = \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} \\] If \\(n\\) is odd, then the middle data point is the median. \\[ \\text{Median} = x_{((n+1)/2)} \\] Symbols in the Formula There is no generally accepted symbol for the median. Sometimes a capital \\(M\\) or even lower-case \\(m\\) is used, but generally the word median is just written out. \\(x_{(n/2)}\\) represents the data value that is in the \\((n/2)^{th}\\) position in the ordered list of values. It only exists when \\(n\\) is even. \\(x_{(n/2+1)}\\) represents the data value that immediately follows the \\((n/2)^{th}\\) value in the ordered list of values. It only exists when \\(n\\) is even. \\(x_{((n+1)/2)}\\) represents the data value that is in the \\(((n+1)/2)^{th}\\) position in the ordered list of values. It only exists when \\(n\\) is odd. \\(n\\) represents the sample size, or number of data values in the sample. Population Median When all of the data from a population is available, the population median is calculated by the above formulas with the slight change that \\(N\\), the total number of data values in the population, instead of \\(n\\), the number of values in the sample, is used. If \\(N\\) is even, then there is no “middle” data point, so the middle two values are averaged. \\[ \\text{Median} = \\frac{x_{(N/2)}+x_{(N/2+1)}}{2} \\] If \\(N\\) is odd, then the middle data point is the median. \\[ \\text{Median} = x_{((N+1)/2)} \\] Physical Interpretation The median is the \\(50^{th}\\) percentile of the data. Say there are \\(n=5\\) data points in the sample with the following values. \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) The sample median is calculated as follows. Note that \\(n=5\\) is odd. \\[ \\text{Median} = x_{((n+1)/2)} = x_{((5+1)/2)} = x_{(3)} = 6 \\] When these values are plotted it is clear that exactly 50% of the data (excluding the median) is to either side of the median. Second Example Say there was a sixth value in the data set equal to 10, so that \\(n=6\\) is even. \\(x_1 = 2\\) \\(x_2 = 5\\) \\(x_3 = 6\\) \\(x_4 = 7\\) \\(x_5 = 10\\) \\(x_6 = 10\\) \\[ \\text{Median} = \\frac{x_{(n/2)}+x_{(n/2+1)}}{2} = \\frac{x_{(6/2)}+x_{(6/2+1)}}{2} = \\frac{x_{(3)}+x_{(4)}}{2} = \\frac{6+7}{2} = 6.5 \\] Effect of Outliers The median is not greatly influenced by outliers. It is said to be robust. This is shown below by changing \\(x_6\\) to be 20, which does not change the value of the median. Mode Most Frequent Value Measure of Center | 1 Quantitative or Qualitative Variable Overview The most commonly occurring value. There may be more than one mode. Seldom used, but sometimes useful. R Instructions R will not calculate a mode directly. However, to tabulate the number of times each value occurs in a dataset, use the code: table(object) object can be quantitative or qualitative, but should contain at least one repeated value or table() is not useful. Example Code Hover your mouse over the example codes to learn more. table “table” is an R function used to count how many times each observation occurs in a list of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Month “Month” is a qualitative variable (technically a numeric vector) from the “airquality” dataset that contains repeated values. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## ## 5 6 7 8 9 ## 31 30 31 31 30 Note that the modes would be 5, 7, and 8 because these months all have the most (31) days in them. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. aveTemp = mean(Temp),  Computes the mean of the Temp column. medTemp = median(Temp),  Computes the median of the Temp column. sampleSize = n( ) Counts how many times each Month (the group_by statement) occurs in the dataset. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month aveTemp medTemp sampeSize 5 65.55 66 31 6 79.1 78 30 7 83.9 84 31 8 83.97 82 31 9 76.9 76 30 Note that R calculated the median Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(aveTemp = mean(Temp), medTemp = median(Temp), sampeSize = n()) %>% pander() Minimum \\[ x_{(1)} \\] Measure of Spread | 1 Quantitative Variable Overview The smallest occurring data value. One of the numerical summaries in the five-number summary. Typically not useful on its own. Gives a good feel for the spread in the left tail of the distribution when used with the five-number summary. R Instructions To calculate a minimum in R use the code: min(object) object must be a quantitative variable, what R calls a “numeric vector.” Example Code Hover your mouse over the example codes to learn more. min “min” is an R function used to calculate the minimum of data. ( Parenthesis to begin the function. Must touch the last letter of the function. airquality “airquality” is a dataset. Type “View(airquality)” in R to see it. $ The $ allows us to access any variable from the airquality dataset. Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. )Closing parenthsis for the function.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. ## [1] 56 Note that the single number showing above is the minimum Temp from the airquality dataset. library(tidyverse) tidyverse is an R Package that is very useful for working with data. airquality airquality is a dataset in R.  %>%  The pipe operator that will send the airquality dataset down inside of the code on the following line.    group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column. Month “Month” is a column from the airquality dataset that can be treated as qualitative. ) Functions must always end with a closing parenthesis.  %>%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.    summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data. minTemp =  “minTemp” is just a name we made up. It will contain the results of the median(…) function. min( “min” is an R function used to calculate the minimum. Temp Temp is a quantitative variable (numeric vector) from the airquality dataset. ) Functions must always end with a closing parenthesis. ) Functions must always end with a closing parenthesis.     Press Enter to run the code.  …  Click to View Output. Month minTemp 5 56 6 65 7 73 8 72 9 63 Note that R calculated the minimum Temp for each month in Month from the airquality dataset. May (5), June (6), July (7), August (8), and September (9), respectively. Further, to get the nicely formatted table you must use: library(pander) airquality %>% group_by(Month) %>% summarise(minTemp = min(Temp)) %>% pander() Maximum \\[ x_{(n)} \\] Measure of Spread | 1 Quantitative Variable Overview The"},{"title":"Intermediate Statistics Notes","url":"Paige-sNotes.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Code Show All Code Hide All Code Intermediate Statistics Notes (MATH 325) function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } function openTab(evt, tabName) { var i, tabcontent, tablinks; tabcontent = document.getElementsByClassName(\"tabcontent\"); for (i = 0; i < tabcontent.length; i++) { tabcontent[i].style.display = \"none\"; } tablinks = document.getElementsByClassName(\"tablinks\"); for (i = 0; i < tablinks.length; i++) { tablinks[i].className = tablinks[i].className.replace(\" active\", \"\"); } document.getElementById(tabName).style.display = \"block\"; evt.currentTarget.className += \" active\"; } Cheat Sheets R Colors RStudio Cheat Sheets R Base Commands Cheat Sheet Keyboard Shortcuts Don’t forget to always load your libraries and to Knit often!! library(mosaic) library(tidyverse) library(pander) library(DT) library(ggrepel) library(plotly) library(dplyr) library(ggplot2) library(maps) library(tmap) library(leaflet) library(htmltools) library(car) library(mosaicData) library(ResourceSelection) library(reshape2) library(RColorBrewer) library(scatterplot3d) library(readr) library(prettydoc) library(knitr) library(kableExtra) library(formattable) library(haven) This is the templet for the Hover Box Thingy Hover over words box! [insert description here]       …  Click to View Output. There are three parts to the help box: The starter code < a href=“javascript:showhide(‘[Insert classifying name]’)”> < div class=“hoverchunk” > The description code This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions! < span class=“tooltipr” > [What shows up in the box] < span class=“tooltiprtext” > [The description that shows up when you hover] < /span>< /span > The ending code the r chunk you put right after will then show up after you click the help box! & nbsp;& nbsp;& nbsp;& nbsp; < span class=“tooltiprtext” >Press Enter to run the code. < /span> & nbsp;…& nbsp; < span class=“tooltiprtext” >Click to View Output.< /span > < /span > < /div > < /a > < div id=“[The classifying name that you put in the starter code!]” style=“display:none;” > *When using, make sure you fix all the spaces when using and use “< /div >” to show where the drop down is meant to end! Personal Notes OoOoooOOOoo let’s go assignments!! Weekly Assignments The sections below show the different Skill Quizzes and Class Activities done throughout the semester. As well as some personal notes on those assignments! Week 1 | Welcome to the Course & Introduction to RStudio We were setting up RStudio and doing cool little graphic thingys! Skill Quiz - Introduction to R Look at data sets, computing means, and creating graphs. Mean of Distance from “cars” data set View(cars) mean(cars$dist) ## [1] 42.98 Scatter plot of “cars” data set plot(dist~speed, data=cars, col=\"skyblue\",pch=16) Creating new data set with “<- The Assignment Operator” create an airquality2 data set that is a copy of the airquality data set add a new column to the airquality2 data set called “Celcius” that converts Temp from degrees Fahrenheit (F) to Ceclcius (C) using the equation: Celcius = (Temp − 32) * 5/9. Don’t forget to use airquailty$ appropriately in that equation! Hint: Typing “?airquality” will allow a help file for the airquality data set to appear! write.csv(airquality,\"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/airquality.cvs\") airquality2 <- airquality airquality2$Celcius <- round((airquality2$Temp-32)*5/9,1) View(airquality2) datatable(airquality2, options=list(lengthMenu =c(3,10,30)), extensions=\"Responsive\") {\"x\":{\"filter\":\"none\",\"vertical\":false,\"extensions\":[\"Responsive\"],\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\"],[41,36,12,18,null,28,23,19,8,null,7,16,11,14,18,14,34,6,30,11,1,11,4,32,null,null,null,23,45,115,37,null,null,null,null,null,null,29,null,71,39,null,null,23,null,null,21,37,20,12,13,null,null,null,null,null,null,null,null,null,null,135,49,32,null,64,40,77,97,97,85,null,10,27,null,7,48,35,61,79,63,16,null,null,80,108,20,52,82,50,64,59,39,9,16,78,35,66,122,89,110,null,null,44,28,65,null,22,59,23,31,44,21,9,null,45,168,73,null,76,118,84,85,96,78,73,91,47,32,20,23,21,24,44,21,28,9,13,46,18,13,24,16,13,23,36,7,14,30,null,14,18,20],[190,118,149,313,null,null,299,99,19,194,null,256,290,274,65,334,307,78,322,44,8,320,25,92,66,266,null,13,252,223,279,286,287,242,186,220,264,127,273,291,323,259,250,148,332,322,191,284,37,120,137,150,59,91,250,135,127,47,98,31,138,269,248,236,101,175,314,276,267,272,175,139,264,175,291,48,260,274,285,187,220,7,258,295,294,223,81,82,213,275,253,254,83,24,77,null,null,null,255,229,207,222,137,192,273,157,64,71,51,115,244,190,259,36,255,212,238,215,153,203,225,237,188,167,197,183,189,95,92,252,220,230,259,236,259,238,24,112,237,224,27,238,201,238,14,139,49,20,193,145,191,131,223],[7.4,8,12.6,11.5,14.3,14.9,8.6,13.8,20.1,8.6,6.9,9.699999999999999,9.199999999999999,10.9,13.2,11.5,12,18.4,11.5,9.699999999999999,9.699999999999999,16.6,9.699999999999999,12,16.6,14.9,8,12,14.9,5.7,7.4,8.6,9.699999999999999,16.1,9.199999999999999,8.6,14.3,9.699999999999999,6.9,13.8,11.5,10.9,9.199999999999999,8,13.8,11.5,14.9,20.7,9.199999999999999,11.5,10.3,6.3,1.7,4.6,6.3,8,8,10.3,11.5,14.9,8,4.1,9.199999999999999,9.199999999999999,10.9,4.6,10.9,5.1,6.3,5.7,7.4,8.6,14.3,14.9,14.9,14.3,6.9,10.3,6.3,5.1,11.5,6.9,9.699999999999999,11.5,8.6,8,8.6,12,7.4,7.4,7.4,9.199999999999999,6.9,13.8,7.4,6.9,7.4,4.6,4,10.3,8,8.6,11.5,11.5,11.5,9.699999999999999,11.5,10.3,6.3,7.4,10.9,10.3,15.5,14.3,12.6,9.699999999999999,3.4,8,5.7,9.699999999999999,2.3,6.3,6.3,6.9,5.1,2.8,4.6,7.4,15.5,10.9,10.3,10.9,9.699999999999999,14.9,15.5,6.3,10.9,11.5,6.9,13.8,10.3,10.3,8,12.6,9.199999999999999,10.3,10.3,16.6,6.9,13.2,14.3,8,11.5],[67,72,74,62,56,66,65,59,61,69,74,69,66,68,58,64,66,57,68,62,59,73,61,61,57,58,57,67,81,79,76,78,74,67,84,85,79,82,87,90,87,93,92,82,80,79,77,72,65,73,76,77,76,76,76,75,78,73,80,77,83,84,85,81,84,83,83,88,92,92,89,82,73,81,91,80,81,82,84,87,85,74,81,82,86,85,82,86,88,86,83,81,81,81,82,86,85,87,89,90,90,92,86,86,82,80,79,77,79,76,78,78,77,72,75,79,81,86,88,97,94,96,94,91,92,93,93,87,84,80,78,75,73,81,76,77,71,71,78,67,76,68,82,64,71,81,69,63,70,77,75,76,68],[5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],[19.4,22.2,23.3,16.7,13.3,18.9,18.3,15,16.1,20.6,23.3,20.6,18.9,20,14.4,17.8,18.9,13.9,20,16.7,15,22.8,16.1,16.1,13.9,14.4,13.9,19.4,27.2,26.1,24.4,25.6,23.3,19.4,28.9,29.4,26.1,27.8,30.6,32.2,30.6,33.9,33.3,27.8,26.7,26.1,25,22.2,18.3,22.8,24.4,25,24.4,24.4,24.4,23.9,25.6,22.8,26.7,25,28.3,28.9,29.4,27.2,28.9,28.3,28.3,31.1,33.3,33.3,31.7,27.8,22.8,27.2,32.8,26.7,27.2,27.8,28.9,30.6,29.4,23.3,27.2,27.8,30,29.4,27.8,30,31.1,30,28.3,27.2,27.2,27.2,27.8,30,29.4,30.6,31.7,32.2,32.2,33.3,30,30,27.8,26.7,26.1,25,26.1,24.4,25.6,25.6,25,22.2,23.9,26.1,27.2,30,31.1,36.1,34.4,35.6,34.4,32.8,33.3,33.9,33.9,30.6,28.9,26.7,25.6,23.9,22.8,27.2,24.4,25,21.7,21.7,25.6,19.4,24.4,20,27.8,17.8,21.7,27.2,20.6,17.2,21.1,25,23.9,24.4,20]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>Ozone<\\/th>\\n <th>Solar.R<\\/th>\\n <th>Wind<\\/th>\\n <th>Temp<\\/th>\\n <th>Month<\\/th>\\n <th>Day<\\/th>\\n <th>Celcius<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"lengthMenu\":[3,10,30],\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Ozone\",\"targets\":1},{\"name\":\"Solar.R\",\"targets\":2},{\"name\":\"Wind\",\"targets\":3},{\"name\":\"Temp\",\"targets\":4},{\"name\":\"Month\",\"targets\":5},{\"name\":\"Day\",\"targets\":6},{\"name\":\"Celcius\",\"targets\":7}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"responsive\":true}},\"evals\":[],\"jsHooks\":[]}PRACTICE : Scatter plot in Base R plot(Ozone~ Temp, data=airquality, xlab=\"Daily Maximum Temperature\", ylab=\"Mean Ozone in Parts per Billion (from 1300 to 1500 hours\", main=\"Exponential Growth in Ozone with Increasing Temp\", col=\"firebrick\",pch=16 ) pch Options! Week 2 | Describing Data in RStudio Talking about how to give good feedback and more coolio graphics in this week! Skill Quiz - Describing Data with R Numerical Summaries that MEASURE CENTER: Proportion Mean Mode Median Mean - Gives a good feel for the most typical values of quantitative data as long as the data is at least approximately normally distributed. Median - Gives a good feel for the most typical values of any style of distribution of quantitative data, but is especially useful for skewed data. Standard Deviation - Typically used to describe the spread of relatively normally distribued data. Five-Number Summaryt - Gives a good feel for how the possible values of quanititative data spread out around the more probable values. Practice Find the mean and standard deviation of the “Wind” speed variable in the airquality dataset. pander(summary(airquality$Wind)) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.7 7.4 9.7 9.958 11.5 20.7 Create a Scatter plot of Temp VS. Wind plot(Wind~Temp, data=airquality, xlab= \"Daily Average Temperature\", ylab = \"Daily Average Wind Speed\", main= \"La Guardia Airport Warmer Weather Shows Less Wind\", col= \"gray\", pch= 19) Create a Histogram and Box plot of Solar.R hist(airquality$Solar.R, xlab= \"Daily Mean Radiation in Langleys (from 0800 to 1200 hours)\", main= \"Central Park, NYC Daily Average Radiation\", col= \"orange\") boxplot(Solar.R~Month, data=airquality, xlab= \"Month of the Year\", ylab= \"Radiation in Langleys (Averaged from 0800 to 1200 hours)\", main= \"Daily Mean Radiation High in July\",col=c(\"gray\",\"gray\",\"orangered\",\"gray\",\"gray\")) Graph Types Histogram - Show the distribution of heights for a sample of n=400 first grade boys. Box Plot - Compare the distribution of body weights of American adults for different ethnicities where there is a sample size of n=100 for each ethnicity. Dot Plots - Compare the distribution of bird beak lengths for different species of bird where there is a sample size of n=3 for each species. Class Activity - Principles of Good Graphics PRACTICE Histogram, Box plot, and Scatter plot with airquality data set hist(airquality$Wind, col=\"steelblue\", xlab=\"Daily Average Wind Speeds (mph)\", main=\"La Guardia Airport from May to September, 1973\") boxplot(Wind~Month, data=airquality, names=c(\"5\",\"6\",\"7\",\"8\",\"9\"), col=c(\"steelblue1\",\"steelblue2\",\"steelblue3\",\"steelblue3\",\"steelblue2\")) plot(Ozone~Temp, data=airquality, col= \"steelblue\", pch=19) Week 3 | Intro to Data Wrangling & Visulization Looking at the different categories of data that is found in the Index Page and how to discern quantitative from categorical data! Skills Quiz - Data Wrangling & Visualization Quick reminder! How do you view a dataset? View([insert the dataset here]) How do you open the help file ?([insert the dataset here]) Quanitative vs. Qualitative Quantitative Examples: length & width Qualitatve Examples: name, birthmonth, birthyear, sex, biggerfoot, & domhand PRACTICE : Use KidsFeet dataset Make a table displaying gender table(KidsFeet$sex)%>% pander() B G 20 19 Make a table displaying which foot is bigger Which foot is commonly bigger? : Left Foot table(KidsFeet$biggerfoot)%>% pander() L R 22 17 Make a table displaying which birthmonth is the most common Which birth month is most common among these sampled children? : March table(KidsFeet$birthmonth)%>% pander() 1 2 3 4 5 6 7 8 9 10 11 12 2 3 8 3 2 4 3 2 5 2 2 3 Make a table that shows “are girls or boys more likely to be left handed?” table(KidsFeet$domhand, KidsFeet$sex)%>% pander()   B G L 5 3 R 15 16 Make a table showing the summaries of children’s foot lengths according to their gender KidsFeet %>% group_by(sex) %>% summarise(Min=min(length), Q1 = quantile(length, 0.25), Median = (median(length)), Q3 = quantile(length,0.75), Max = max(length), Mean = mean(length), SD = sd(length))%>% pander() sex Min Q1 Median Q3 Max Mean SD B 22.9 24.35 24.95 25.8 27.5 25.11 1.217 G 21.6 23.65 24.2 25.1 26.7 24.32 1.33 For this particular sample of data, which gender has the longest feet on average? Boys Which gender shows the most consistency in length of feet among children in this sample? Boys PRACTICE : Use airquality dataset run an appropriate command to obtain the mean daily temperature at LaGuardia Airport for each month, separately. airquality %>% group_by(Month) %>% summarise(`Mean Temperature`= mean(Temp)) %>% pander() Month Mean Temperature 5 65.55 6 79.1 7 83.9 8 83.97 9 76.9 Observations: Which month experiences the coolest average temperature? May By how many degrees do the average temperatures of July and August differ? 83.97-83.9 = 0.07 Between which two consecutive months is there the largest difference in average temperature? May and June What are the BEST graphics for this data set? These 3 would be useful in depicting the above information, because they nicely display the month and their averages (1) Box plot boxplot(Temp ~ Month, data=airquality, xlab=\"Month\", ylab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\") (2) Scatter plot plot(Temp ~ Month, data=airquality, xlab=\"Month\", ylab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\") (3) Dot Plot stripchart(Temp ~ Month, data=airquality, ylab=\"Month\", xlab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\", method=\"stack\") What are the WORST graphics for depicting this information? these two are the worst because they don’t clearly show the months in order to allow us to compare the data of each month. (1) Scatter plot but the x axis is Day, not very clear variable plot(Temp ~ Day, data=airquality, xlab=\"Day of the Month\", ylab=\"Temperature\", main=\"LaGuardia Airport (May to September 1973)\", pch=16, col=\"slategray\") (2) Histogram gives you the average of the whole time frame but not the specific times hist(airquality$Temp, xlab=\"Daily Temperature\", main=\"LaGuardia Airport (May to September 1973)\", col=\"slategray\") PRACTICE : Use Orange dataset View(Orange) Orange2 <- Orange %>% group_by(age)%>% summarise(MedianCir = median(circumference)) datatable(Orange2, options=list(lengthMenu =c(3,10,30)), extensions=“Responsive”) What are the BEST graphics for this data set? Scatter plot with median growth line visualizes the relationship/ captures the overall trends between two continuous variables plot(circumference ~ age, data=Orange, ylab=\"Trunk Circumference (mm)\", xlab=\"Age of Trees (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\", pch=15) Orange.m <- median(circumference ~ age, data=Orange) lines(names(Orange.m), Orange.m, col=\"ivory3\") legend(\"topleft\", legend=\"Median Growth\", lty=1, col='ivory3', bty='n') Box plot shows how the trunk circumference varies across different ages boxplot(circumference ~ age, data=Orange, ylab=\"Trunk Circumference (mm)\", xlab=\"Age of Trees (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\") Dot plot shows the distribution of data and helps visualize all the individual data points stripchart(circumference ~ age, data=Orange, ylab=\"Trunk Circumference (mm)\", xlab=\"Age of Trees (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\", pch=15, method=\"stack\", vertical=TRUE) Question: During which age interval did the most rapid overall median growth occur (in the circumference of the orange trees that were sampled)? 664 to 1004 days (third interval) What are the WORST graphics for this data set? Box plot x-axis is sectioned out by multiple variables instead of just one, thus, not really showing us anything of value boxplot(Orange, xlab=\"Age of Tree (days)\", main=\"Trunk Circumference of Orange Trees\", col=\"ivory3\") PRACTICE : Use the Riders data set Consider the Riders dataset in R. (You may need to load library(mosaicData).) How many total riders were observed on each day of the week? (Hint: the sum() function works the same way as mean()…) Riders2 <- Riders %>% group_by(day)%>% summarise(`Total Number of Riders Observed`= sum(riders)) datatable(Riders2) {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"],[\"Friday\",\"Monday\",\"Saturday\",\"Sunday\",\"Thursday\",\"Tuesday\",\"Wednesday\"],[5269,4698,4623,5168,5593,4423,4012]],\"container\":\"<table class=\\\"display\\\">\\n <thead>\\n <tr>\\n <th> <\\/th>\\n <th>day<\\/th>\\n <th>Total Number of Riders Observed<\\/th>\\n <\\/tr>\\n <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":2},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"day\",\"targets\":1},{\"name\":\"Total Number of Riders Observed\",\"targets\":2}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}PRACTICE : Use the mtcars dataset How would you describe the dataset? First, type ?mtcars and look at the “Description” of the data set information Description: The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models). How many variables are in the mtcars data set? First, View()the data, then read how many columns are in the data set in the bottom left hand corner : 11 How many observations are in the mtcars data set? (Hint: try View(…)) First, View() the data, then read the amount of entries in the data set in the bottom left hand corner : 32 How many vehicles are represented in the dataset for 4, 6, and 8 cylinder vehicles? (Hint: use the table(…) function.) table(mtcars$cyl)%>% pander() 4 6 8 11 7 14 According to the mtcars data, on average, vehicles with 4 cylinders get the best (highest) gas mi"},{"title":"Permutation Tests","url":"PermutationTests.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Permutation Tests function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } A nonparametric approach to computing the p-value for any test statistic in just about any scenario. Overview In almost all hypothesis testing scenarios, the null hypothesis can be interpreted as follows. \\(H_0\\): Any pattern that has been witnessed in the sampled data is simply due to random chance. Permutation Tests depend completely on this single idea. If all patterns in the data really are simply due to random chance, then the null hypothesis is true. Further, random re-samples of the data should show similar lack of patterns. However, if the pattern in the data is real, then random re-samples of the data will show very different patterns from the original. Consider the following image. In that image, the toy blocks on the left show a clear pattern or structure. They are nicely organized into colored piles. This suggests a real pattern that is not random. Someone certainly organized those blocks into that pattern. The blocks didn’t land that way by random chance. On the other hand, the pile of toy blocks shown on the right is certainly a random pattern. This is a pattern that would result if the toy blocks were put into a bag, shaken up, and dumped out. This is the idea of the permutation test. If there is structure in the data, then “mixing up the data and dumping it out again” will show very different patterns from the original. However, if the data was just random to begin with, then we would see a similar pattern by “mixing up the data and dumping it out again.” The process of a permutation test is: Compute a test statistic for the original data. Re-sample the data (“shake it up and dump it out”) thousands of times, computing a new test statistic each time, to create a sampling distribution of the test statistic. Compute the p-value of the permutation test as the percentage of test statistics that are as extreme or more extreme than the one originally observed. In review, the sampling distribution is created by permuting (randomly rearranging) the data thousands of times and calculating a test statistic on each permuted version of the data. A histogram of the test statistics then provides the sampling distribution of the test statistic needed to compute the p-value of the original test statistic. R Instructions Any permutation test can be performed in R with a for loop. #Step 1 Compute a test statistic for the original data. myTest <- …perform the initial test… This could be a t.test, wilcox.test, aov, kruskal.test, lm, glm, chisq.test, or any other R code that results in a test statistic. It could even simply be the mean or standard deviation of the data. observedTestStat <- …get the test statistic… Save the test statistic of your test into the object called observedTestStat. For tests that always result in a single test statistic like a t.test, wilcox.test, kruskal.test, and chisq.test it is myTest$statistic. For an aov, lm, or glm try printing summary(myTest)[] to see what values you are interested in using. observedTestStat Print the value of the test statistic of your test to the screen. This is the value that we now need to use to compute the P-value from by finding the probability that a randomly computed test statistic would be “more extreme” than this originally observed value. #Step 2 Re-sample the data (“shake it up and dump it out”) thousands of times, computing a new test statistic each time, to create a sampling distribution of the test statistic. N <- 2000       N is the number of times you will reuse the data to create the sampling distribution of the test statistic. A typical choice is 2000, but sometimes 10000, or 100000 reuses are needed before useful answers can be obtained. permutedTestStats <-  This is a storage container that will be used to store the test statistics from each of the thousands of reuses of the data. rep(NA, N) The rep() function repeats a given value N times. This particular statement repeats NA’s or “missing values” N times. This gives us N “empty” storage spots inside of permutedTestStats that we can use to store the N test statistics from the N reuses of the data we will make in our for loop. for The for loop is a programming tool that lets us tell R to run a certain code over and over again for a certain number of times.  (i in   In R, the for loop must be followed by a space, then an opening parenthesis, then a variable name (in this case the variable is called “i”), then the word “in” then a list of values. 1:N The 1:N gives R the list of values 1, 2, 3, … and so on all the way up to N. These values are passed into i one at a time and the code inside the for loop is performed first for i=1, then again for i=2, then again for i=3 and so on until finally i=N. At that point, the for loop ends. ) Required closing parenthesis on the for (i in 1:N) statement. { This bracket opens the code section of the for loop. Any code placed between the opening { and closing } brackets will be performed over and over again for each value of i=1, i=2, … up through i=N.    Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized. permutedTest <- …perform test with permutedData… The same test that was performed on the original data, should be performed again but with randomly permuted data. The easiest way to permute data is with sample(y-variable-name) inside your test. See the Explanation tab for details.    Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized. permutedTestStats This is the storage container that was built prior to the for (i in 1:N) code. Inside the for loop, this container is filled value by value using the square brackets [i]. [i] The square brackets [i] allows us to access the “i”th position of permutedTestStats. Remember, since this code is inside of the for loop, i=1 the first time through the code, then i=2 the second time through the code, i=3 the third time through, and so on up until i=N the Nth time through the code.  <- …get test statistic… The test statistic from permutedTest is accessed here and stored into permutedTestStats[i]. } The closing } bracket ends the code that is repeated over and over again inside the for loop. hist(permutedTestStats) Creating a histogram of the sampling distribution of the test statistics obtained from the reused and permuted data allows us to visually compare the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. abline(v=observedTestStat) This adds the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. #Step 3 Compute the p-value of the permutation test as the percentage of test statistics that are as extreme or more extreme than the one originally observed. sum(permutedTestStats >= observedTestStat)/N This computes a “greater than” p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the right hand side of the histogram. sum(permutedTestStats <= observedTestStat)/N This computes a “less than” p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the left hand side of the histogram. myTest <- ...perform the initial test... This could be a t.test, wilcox.test, aov, kruskal.test, lm, glm, chisq.test, or any other R code that results in a test statistic. It could even simply be the mean or standard deviation of the data. observedTestStat <- ...get the test statistic... Save the test statistic of your test into the object called observedTestStat. For tests that always result in a single test statistic like a t.test, wilcox.test, kruskal.test, and chisq.test it is myTest$statistic. For an aov, lm, or glm try printing summary(myTest)[] to see what values you are interested in using. N <- 2000 N is the number of times you will reuse the data to create the sampling distribution of the test statistic. A typical choice is 2000, but sometimes 10000, or 100000 reuses are needed before useful answers can be obtained. permutedTestStats <- rep(NA, N) This is a storage container that will be used to store the test statistics from each of the thousands of reuses of the data. The rep() function repeats a given value N times. This particular statement repeats NA’s or “missing values” N times. This gives us N “empty” storage spots inside of permutedTestStats that we can use to store the N test statistics from the N reuses of the data we will make in our for loop. for (i in 1:N)\\{ The for loop is a programming tool that lets us tell R to run a certain code over and over again for a certain number of times. In R, the for loop must be followed by a space, then an opening parenthesis, then a variable name (in this case the variable is called “i”), then the word “in” then a list of values. The 1:N gives R the list of values 1, 2, 3, … and so on all the way up to N. These values are passed into i one at a time and the code inside the for loop is performed first for i=1, then again for i=2, then again for i=3 and so on until finally i=N. At that point, the for loop ends. There is a required closing parenthesis on the for (i in 1:N) statement. Any code placed between the opening { and closing } brackets will be performed over and over again for each value of i=1, i=2, … up through i=N. Two spaces in front of every line inside of the opening { and closing } brackets helps keep your code organized. permutedData <- ...randomly permute the data... This is the most important part of the permutation test and takes some thinking. The data must be randomly reorganized in a way consistent with the null hypothesis. What that means exactly is specific to each scenario. Read the Explanation tab for further details on the logic you should use here. permutedTest <- ...perform test with permutedData... The same test that was performed on the original data, should be performed again on the randomly permuted data. permutedTestStats[i] <- ...get test statistic... This is the storage container that was built prior to the for (i in 1:N) code. Inside the for loop, this container is filled value by value using the square brackets [i]. The square brackets [i] allows us to access the “i”th position of permutedTestStats. Remember, since this code is inside of the for loop, i=1 the first time through the code, then i=2 the second time through the code, i=3 the third time through, and so on up until i=N the Nth time through the code. The test statistic from permutedTest is accessed here and stored into permutedTestStats[i]. } The closing } bracket ends the code that is repeated over and over again inside the for loop. hist(permutedTestStats) Creating a histogram of the sampling distribution of the test statistics obtained from the reused and permuted data allows us to visually compare the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. abline(v=observedTestStat) This adds the observedTestStat to the distribution of test statistics to visually see the percentage of test statistics that are as extreme or more extreme than the observed test statistic value. This is the p-value. sum(permutedTestStats >= observedTestStat)/N This computes a “greater than” p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the right hand side of the histogram. sum(permutedTestStats <= observedTestStat)/N This computes a “less than” p-value. A two-sided p-value could be obtained by multiplying this value by 2 if the observed test statistic was on the left hand side of the histogram. Explanation The most difficult part of a permutation test is in the random permuting of the data. How the permuting is performed depends on the type of hypothesis test being performed. It is important to remember that the permutation test only changes the way the p-value is calculated. Everything else about the original test is unchanged when switching to a permutation test. Independent Samples t Test For the independent sample t Test, we will use the data from the independent sleep analysis. In that analysis, we were using the sleep data to test the hypotheses: \\[ H_0: \\mu_\\text{Extra Hours of Sleep with Drug 1} - \\mu_\\text{Extra Hours of Sleep with Drug 2} = 0 \\] \\[ H_a: \\mu_\\text{Extra Hours of Sleep with Drug 1} - \\mu_\\text{Extra Hours of Sleep with Drug 2} \\neq 0 \\] We used a significance level of \\(\\alpha = 0.05\\) and obtained a P-value of \\(0.07939\\). Let’s demonstrate how a permutation test could be used to obtain this same p-value. (Technically you only need to use a permutation test when the requirements of the original test were not satisfied. However, it is also reasonable to perform a permutation test anytime you want. No requirements need to be checked when performing a permutation test.) # First run the initial test and gain the test statistic: myTest <- t.test(extra ~ group, data = sleep, mu = 0) observedTestStat <- myTest$statistic # Now we run the permutations to create a distribution of test statistics N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- t.test(sample(extra) ~ group, data = sleep, mu = 0) permutedTestStats[i] <- permutedTest$statistic } # Now we show a histogram of that distribution hist(permutedTestStats, col = \"skyblue\") abline(v = observedTestStat, col = \"red\", lwd = 3) #Greater-Than p-value: Not the correct one in this case sum(permutedTestStats >= observedTestStat)/N # Less-Than p-value: Not the correct one for this data sum(permutedTestStats <= observedTestStat)/N # Two-Sided p-value: This is the one we want based on our alternative hypothesis. 2*sum(permutedTestStats <= observedTestStat)/N Note The Wilcoxon Rank Sum test is run using the same code except with myTest <- wilcox.test(y ~ x, data=...) instead of t.test(...) in both Step’s 1 and 2. Other Examples Paired Samples t Test (and Wilcoxon Signed-Rank) (click to show/hide) Paired Data Example See the Sleep Paired t Test example for the background and context of the study. Here is how to perform the test as a permutation test instead of a t test. The question that this sleep data can answer concerns which drug is more effective at increasing the amount of extra sleep an individual receives. The associated hypotheses would be \\[ H_0: \\mu_d = 0 \\] \\[ H_a: \\mu_d \\neq 0 \\] where \\(\\mu_d\\) denotes the true mean of the differences between the observations for each drug obtained from each individual. Differences would be obtained by \\(d_i = \\text{extra}_{1i} - \\text{extra}_{2i}\\). To perform a permutation test of the hypothesis that the drugs are equally effective, we use the following code. # Perform the initial test: myTest <- with(sleep, t.test(extra[group==1], extra[group==2], paired = TRUE, mu = 0)) # Get the test statistic from the test: observedTestStat <- myTest$statistic # Obtain the permutation sampling distribution N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permuteValues <- sample(c(-1,1), size=10, replace=TRUE) permutedTest <- with(sleep, t.test(permuteValues*(extra[group==1] - extra[group==2]), mu = 0)) #Note, t.test(group1 - group2) is the same as t.test(group1, group2, paired=TRUE). permutedTestStats[i] <- permutedTest$statistic } hist(permutedTestStats) abline(v=observedTestStat, col='skyblue', lwd=3) # Greater than p-value: (not what we want here) sum(permutedTestStats >= observedTestStat)/N ## [1] 1 # Less than p-value: sum(permutedTestStats <= observedTestStat)/N ## [1] 0.002 # Correct two sided p-value for this study: 2*sum(permutedTestStats <= observedTestStat)/N ## [1] 0.004 Note: ANOVA (click to show/hide) One-Way ANOVA For this example, we will use the data from the chick weights analysis. # Again, we run the initial test and find the test statistic myTest <- aov(weight ~ feed, data = chickwts) observedTestStat <- summary(myTest)[[1]]$`F value`[1] # For this permutation, we need to shake up the groups similar to the Independent Sample example N <- 2000 permutedTestStats <- rep(NA, N) for (i in 1:N){ permutedTest <- aov(sample(weight) ~ feed, data = chickwts) permutedTestStats[i] <- summary(permutedTest)[[1]]$`F value`[1] } # The histogram of this distribution gives an interesting insight into the results hist(permutedTestStats, col = \"skyblue\", xlim = c(0,16)) abline(v = observedTestStat, col = \"red\", lwd = 3) # Here is the greater-than p-value (since the F-distribution is right skewed # this is the only p-value of interest.) sum(permutedTestStats >= observedTestStat)/N Two-Way ANOVA For the two-way ANOVA, I will use the data from the warpbreaks analysis. # The initial test is done in the same way as one-way ANOVA but there is a little more to find the test statistic myTest <- aov(breaks ~ wool + tension + wool:tension, data=warpbreaks) # This first test statistic is the comparison between the two types of wool observedTestStatW <- summary(myTest)[[1]]$`F value`[1] # This second test statistic is the comparison between the three types of tension observedTestStatT <- summary(myTest)[[1]]$`F value`[2] # The third test statistic is the comparison of the interaction of wool types and tension observedTestStatWT <- summary(myTest)[[1]]$`F value`[3] # Now comes three different permutations for the test. First is for wool, second is for tension, and third is the interaction N <- 2000 permutedTestStatsW <- rep(NA, N) permutedTestStatsT <- rep(NA, N) permutedTestStatsWT <- rep(NA, N) for (i in 1:N){ permutedTest <- aov(sample(breaks) ~ wool + tension + wool:tension, data=warpbreaks) permutedTestStatsW[i] <- summary(permutedTest)[[1]]$`F value`[1] permutedTestStatsT[i] <- summary(permutedTest)[[1]]$`F value`[2] permutedTestStatsWT[i] <- summary(permutedTest)[[1]]$`F value`[3] } # We likewise need three differenct plots to show the distribution. First is wool, second is tension, and third is the interaction hist(permutedTestStatsW, col = \"skyblue\", xlim = c(3,14)) abline(v = observedTestStatW, col = \"red\", lwd = 3) hist(permutedTestStatsT, col = \"skyblue\") abline(v = observedTestStatT, col = \"red\", lwd = 3) hist(permutedTestStatsWT, col = \"skyblue\") abline(v = observedTestStatWT, col = \"red\", lwd = 3) # Greater-than p-value: the three situations are in order sum(permutedTestStatsW >= observedTestStatW)/N sum(permutedTestStatsT >= observedTestStatT)/N sum(permutedTestStatsWT >= observedTestStatWT)/N # Less-than p-value: again, they are in order sum(permutedTestStatsW <= observedTestStatW)/N sum(permutedTestStatsT <= observedTestStatT)/N sum(permutedTestStatsWT <"},{"title":"R Commands","url":"RCommands.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus R Commands function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } “For the things we have to learn before we can do them, we learn by doing them.” ― Aristotle, The Nicomachean Ethics Getting Started Hover your mouse here to begin. Good work! This book requires that you interact with it to learn. Hovering is the first step. Now click right here on these words to get started. ? The Help Command Getting help in R is easy. Usage ?something This command pulls up the help file for whatever you write in the place of something. Examples Click to view. Hover to learn. ? The quick way to access the help function in R. cars The name of a dataset can be typed to open the help file for that dataset.     Press Enter to run the code.  Click to Show Output  Click to View Output. ? The quick way to access the help function in R. data The name of an R function, like data can also be used to open the help file for that function.     Press Enter to run the code.  Click to Show Output  Click to View Output. ? The quick way to access the help function in R. mean The mean function computes the mean of a column of quantitative data. Typing the name of an R function, like mean can also be used to open the help file for that function.     Press Enter to run the code.  Click to Show Output  Click to View Output. $ The Selection Operator Once you have a dataset, you need to be able to access columns from it. Usage DataSetName$ColumnName The $ operator allows you to access the individual columns of a dataset. Tip: think of the data set as a “store” from which you “purchase” a column using “money”: $. Example Code airquality The airqaulity dataset. This could be the name of any dataset instead of airquality. $ Grabs the column, or variable, from the dataset to be used. This is typically used when computing say the mean (or other statistic) of a single column of the data. Wind The name of any column of the dataset can be entered after the dollar sign. In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality).     Press Enter to run the code.  Click to Show Output  Click to View Output. This allows you to compute things about that column, like the mean or standard deviation. mean( The mean function computes the mean of a column of quantitative data. airquality The airquality dataset. This could be the name of any dataset instead of airquality. $ Grabs the column, or variable, from the dataset to be used. This is typically used when computing say the mean (or other statistic) of a single column of the data. Wind The name of any column of the dataset can be entered after the dollar sign. In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality). ) Closing parenthesis to the mean() function.     Press Enter to run the code.  Click to Show Output  Click to View Output. 9.958 sd( The sd function computes the standard deviation of a column of quantitative data. airquality The airqaulity dataset. This could be the name of any dataset instead of airquality. $ Grabs the column, or variable, from the dataset to be used. This is typically used when computing say the mean (or other statistic) of a single column of the data. Wind The name of any column of the dataset can be entered after the dollar sign. In the airquality dataset, this includes: Ozone, Solar.R, Wind, Temp, Month, or Day as shown by View(airquality). ) Closing parenthesis to the sd() function.     Press Enter to run the code.  Click to Show Output  Click to View Output. 3.523 See Numerical Summaries for more stats functions like mean() and sd(). <- The Assignment Operator Being able to save your work is important! Usage    Keyboard Shortcut: Alt - NameYouCreate <- some R commands <- (Less than symbol < with a hyphen -) is called the assignment operator and lets you store the results of the some R commands into an object called NameYouCreate. NameYouCreate is any name that begins with a letter, but can use numbers, periods, and underscores thereafter. To use spaces in the name, you must use `your Name` encased in back-ticks, but this is not recommended. Example Code cars2 First we name the object we are creating. In this case, we are making a copy of the cars dataset, so it is logical to call it cars2, but it could be bob, c2 or any name you wanted to use. Just be careful to not use names that are already in use!   <-   The <- assignment operator will take whatever is on the right hand side and save it into the name written on the left hand side. cars In this case the cars dataset is being copied to cars2 so that we can change cars2 without changing the original cars dataset.     Press Enter to run the code. cars2 The new copy of the cars dataset that we just created $ftpersec The $ selection operator can be used to create a new column in a dataset when used with the <- assignment operator.  <-  The <- assignment operator will take the results of the right-hand-side and save them into the name on the left-hand-side. cars2$speed * 5280 / 3600 This calculation converts the miles per hour of the cars2 speed column into feet per seconds because there are 5280 feet in a mile and 60 minutes in an hour and 60 seconds in a minute. View(cars2) The cars2 dataset now contains a 3rd column called feetpersec. Compare this to the original cars dataset to see how it changed.  Click to Show Output  Click to View Output. c( ) The Combine Function Think of this function as the “back-pack” function, just like putting different books into one back-pack. Usage c(value 1, value 2, value 3, ... ) The c( ) function combines values into a single object called a “vector”. values 1, 2, 3, ... can be numbers or characters, i.e., words, but must be all of one type or the other. Example Code Classlist <- Classlist is a new object being created using the assignment operator <- that will contain the four names listed above.  c( The combine function c( ) is being used in this case to group character values representing names of students into a single object named “Classlist”. “Jackson”, “Jared”, “Jill”, “Jane”) These are the values we are grouping into the object named Classlist.     Press Enter to run the code. Ages <-   The assignment operator <- is being used to create the object called Ages that will contain the ages of each student on the Classlist. c( The R function “c()” allows us to group together values in order to save them into an object. 8, 9, 7, 8 The values, separated by comma’s, that are being grouped together. In this case, numbers are being grouped together. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code. Colors <-   The assignment operator <- is being used to create the object called Colors that will have one color for each student on the Classlist. c( The R function “c()” allows us to group together values in order to save them into an object. “red”, “blue”, “green”, “yellow” The values, separated by comma’s, that are being grouped together. In this case, characters are being grouped together. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. table( ) This is a way to quickly count how many times each value occurs in a column or columns. Usage table(NameOfDataset$columnName) table(NameOfDataset$columnName1, NameOfDataset$columnName2) The table( ) function counts how many times each value in a column of data occurs. NameOfDataset is the ane of a data set, like cars or airquality or KidsFeet. columnName is the name of a column from the data set. columnName1 and columnName2 are two different names of columns from the data set. Example Code speedCounts <-speedCounts is a new object being created using the assignment operator <- that will contain the counts of how many times each “speed” occurs in the cars data set speed column.  table( The table function table( ) is being used in this case to count how many times each speed occurs in the cars data set speed column. cars This is the name of the data set. $ The $ is used to access a given column from the data set. speed This is the name of the column we are interested in from the cars data set. ) Always close off your functions in R with a closing parathesis. speedCounts Typing the name of an object will print the results to the screen.     Press Enter to run the code.  Click to Show Output  Click to View Output. ## ## 4 7 8 9 10 11 12 13 14 15 16 17 18 19 20 22 23 24 25 ## 2 2 1 1 3 2 4 4 4 3 2 3 4 3 5 1 1 4 1 Notice how the speed of “4” occurs 2 times, same for the speed of 7, but the speed of 8 only occurs 1 time and so on with the other speeds. The first row of the output is the value from the speed column. The number on the second line shows how many times that value occurred in the speed column. library(mosaic) library(mosaic) is needed to access the KidsFeet data set that is used in this example. If you don’t have the mosaic library, you will need to run install.packages(\"mosaic\") to install it first. From then on, you can open mosaic to use it with the command library(mosaic). You need only install packages once. You must library them each time you wish to use them. birthdays <-birthdays is a new object being created using the assignment operator <- that will contain the counts of how many birthdays occur in each month for each gender in the KidsFeet dataset.  table( The table function table( ) is being used in this case to count how many birthdays occur in each month for children of each gender. KidsFeet This is the name of the data set. $ The $ is used to access a given column from the data set. sex This is the name of the column we are interested in becoming the rows of our final table. ,  Comma separating the two columns of the data set you want to table. KidsFeet This is the name of the data set. $ The $ is used to access a given column from the data set. birthmonth This is the name of the column we are interested in becoming the columns of our final table. ) Always close off your functions in R with a closing parathesis. birthdays Typing the name of an object will print the results to the screen.     Press Enter to run the code.  Click to Show Output  Click to View Output. ## ## 1 2 3 4 5 6 7 8 9 10 11 12 ## B 1 2 3 2 1 1 2 2 2 1 1 2 ## G 1 1 5 1 1 3 1 0 3 1 1 1 The left column contains the “sex” values of “B” and “G” (Boy and Girl). The top row contains the birthmonths (1 through 12). The numbers within the row of the table next to the “B” show how many Boys had birthdays in each month of the year. The numbers within the row of the table next to the “G” show how many Girls had birthdays in each month of the year. filter( ) Used to reduce a dataset to a smaller set of rows than the original dataset contained. Usage filter(NameOfDataset, columnName filteringRules) filter() is the function that filters out certain rows of the dataset. NameOfDataset is the name of a dataset, like cars or airquality or KidsFeet. columnName is the name of one of the columns from the dataset. You can use colnames(NameOfDataset) or View(NameOfDataset) to see the names. filteringRules consists of some Logical Expression (see table below) that selects only the rows from the original dataset that meet the criterion. Filtering Rule Logical Expression Equals one “thing” columnName == something Equals Any Of Several “things” columnName %in% c(something1,something2,...) Not Equal (one thing) columnName != something Not Equals Any of (several things) !columnName %in% c(something1,something2,...) Less Than columnName < value Less Then or Equal to columnName <= value Greater Than columnName > value Greater Than or Equal to columnName >= value AND expression1 & expression2 OR expression1 | expression2 Equals NA is.na(columnName) Not NA !is.na(columnName) Example Code library(tidyverse) The tidyverse library is needed to access the filter function used in the following example codes. library(mosaic) The mosaic library is needed to access the KidsFeet data set used in the following example codes. Equals one “thing”… Kids87 <-  Kids87 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. birthyear A quantitative column of the KidsFeet dataset that we want to use to reduce the dataset.  == 87 This “filtering rule” filters the data down to just those children who had a birthyear equal to 87. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. KidsBoys <-  KidsBoys is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. sex A categorical column of the KidsFeet dataset that we want to use to reduce the dataset.  == “B” This “filtering rule” filters the data down to just those children who are boys. Words must be quoted “B” but values are just typed directly. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. Equals Any of Several “things”… KidsSummer <-  KidsSummer is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. birthmonth The column of the KidsFeet dataset that we want to use to reduce the dataset.  %in% c(6,7,8) This is the “filtering rule”. It will filter the data down to just those children who were born during the summer, i.e., birthmonth equal to either 6, 7, or 8. Notice how the c( ) function is being used to combine the values of 6, 7, and 8 together into a single list of numbers. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. Does not equal one thing… KidsNotJosh <-  KidsNotJosh is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. name The column of the KidsFeet dataset that we want to use to reduce the dataset.  != “Josh” This is the “filtering rule”. It will filter the data down to just those children who are NOT named “Josh”. In this case, it removed just two students who were named “Josh”. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. Less than… KidsLength24 <-  KidsLength24 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. length The column of the KidsFeet dataset that we want to use to reduce the dataset.  < 24 This is the “filtering rule”. It will filter the data down to just those children who have a foot length less than 24 cm. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. Less than or equal to… KidsLessEq24 <-  KidsLessEq24 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. length The column of the KidsFeet dataset that we want to use to reduce the dataset.  <= 24 This is the “filtering rule”. It will filter the data down to just those children who have a foot length less than or equal to 24 cm. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. Greater than… KidsWider9 <-  KidsNotJosh is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. width The column of the KidsFeet dataset that we want to use to reduce the dataset.  > 9 This is the “filtering rule”. It will filter the data down to just those children who have a foot width greater than 9 cm. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. Greater than or equal to… KidsWiderEq9 <-  KidsWiderEq9 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet dataset by filtering according to certain criteria. Click on this code to see the original and filtered datasets. width The column of the KidsFeet dataset that we want to use to reduce the dataset.  >= 9 This is the “filtering rule”. It will filter the data down to just those children who have a foot width greater than or equal to 9 cm. ) Always close off your functions in R with a closing parathesis.     Press Enter to run the code.  Click to Show Output  Click to View Output. The “and” statement… GirlsWide9 <-  GirlsWide9 is a name we made up. The assignment operator <- will save the reduced version of the KidsFeet dataset created by the filter(...) function into this name. filter(KidsFeet,   “filter” is a function from library(tidyverse) that reduces the number of rows in the KidsFeet "},{"title":"","url":"README.html","content":"Statistics Notebook R Help R Commands R Markdown Hints R Cheatsheets & Notes Data Sources Describing Data Graphical Summaries Numerical Summaries Making Inference Making Inference t Tests Wilcoxon Tests ANOVA Kruskal-Wallis Linear Regression Logistic Regression Chi Squared Tests Randomization Testing Analyses Analysis Rubric Good Example Analysis Poor Example Analysis Rent Analysis Stephanie Analysis Statistics-Notebook This began as the textbook for Intermediate Statistics (Math 325) at BYU-Idaho. It aims to become a personal notebook for performing and interpreting statistical analyses in R. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"R Markdown Hints","url":"RMarkdownHints.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus R Markdown Hints Think of an R Markdown File, or Rmd for short, as a command center. You write commands, then Knit the file, and an html output file is created according to your commands. Overview Click Here to Learn More Carefully read through all parts of this image to learn… Close The above tabs (blue bottons that read “Click Here to Learn More” and “Close”) were created with the code: ## {.tabset .tabset-pills .tabset-fade} ### Click Here to Learn More Carefully read through all parts of this image to learn... ![](./Images/Rmd2html.png) ### Close ## Creating Links To make a link use the code [Name of Link](addressForLink). Linking to parts of your textbook: [Numerical Summaries](NumericalSummaries.html) becomes Numerical Summaries [Boxplots](GraphicalSummaries.html#boxplots) becomes Boxplots [R Commands](RCommands.html) becomes R Commands Linking to outside resources: [R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) becomes R Colors Creating Headers There are six available sizes of headings you can use in an Rmd file (left in image) that show up as shown below (right in image). Emphasizing Words To italisize a word use the asterisk (Shift 8) *italisize*. To bold a word use the double asterisk **bold**. The back tic can be used tohighlightwords by placing back tics on each side of a word: highlight `. Bullet Points Simple Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: * This is the first item. * This is the second. * This is the third. Numbered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: 1. This is the first item. 2. This is the second. 3. This is the third. Lettered Lists To achieve the result: This is the first item. This is the second. This is the third. Use the code: To achieve the result: A) This is the first item. B) This is the second. C) This is the third. Nested Lists What is \\(2+2\\)? 4 8 What is \\(3\\times5\\)? 14 15 1. What is $2+2$? a. **4** b. 8 2. What is $3\\times5$? a. 14 b. **15** Math Equations Use the dollar signs $x=5$ to write \\(x=5\\) or $z=\\frac{x-\\mu}{\\sigma}$ to write \\(z=\\frac{x-\\mu}{\\sigma}\\). For a nicely centered equation use the double dollar signs $$ $$ on separate lines $$ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} $$ to get \\[ z = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] Or $$ H_0: \\mu_\\text{Group 1} = \\mu_\\text{Group 2} $$ $$ H_a: \\mu_\\text{Group 1} \\neq \\mu_\\text{Group 2} $$ to get \\[ H_0: \\mu_{\\text{Group 1}} = \\mu_{\\text{Group 2}} \\] \\[ H_a: \\mu_{\\text{Group 1}} \\neq \\mu_{\\text{Group 2}} \\] Symbol list: Symbol LaTeX Math Code \\(\\alpha\\) $\\alpha$ \\(\\beta\\) $\\beta$ \\(\\sigma\\) $\\sigma$ \\(\\epsilon\\) $\\epsilon$ \\(\\bar{x}\\) $\\bar{x}$ \\(\\hat{Y}\\) $\\hat{Y}$ \\(=\\) $=$ \\(\\ne\\) $\\ne$ or $\\neq$ \\(>\\) $>$ \\(<\\) $<$ \\(\\ge\\) $\\ge$ \\(\\le\\) $\\le$ \\(\\{ \\}\\) $\\{ \\}$ \\(\\text{Type just text}\\) $\\text{Type just text}$ \\(\\overbrace{Y_i}^\\text{label}\\) $\\overbrace{Y_i}^\\text{label}$ \\(\\underbrace{Y_i}_\\text{label}\\) $\\underbrace{Y_i}_\\text{label}$ Here is a list of all supported LaTeX commands. Insert a Picture To add a picture to your document, say some notes you took down on paper from class, Use the code: ![](./Images/insertPictureNotes.jpg) to get… Tables There are many ways to make tables in R Markdown. Here is a simple way to make a “pipe” table. | Name | Age | Gender | |---------------|---------------|--------------| | Jill | 8 | Female | | Jack | 9 | Male | Name Age Gender Jill 8 Female Jack 9 Male Themes Notice in the YAML (at the top of the RMD file) there is a line that reads: “theme: cerulean” Other possible themes are “default”, “cerulean”, “journal”, “flatly”, “readable”, “spacelab”, “united”, and “cosmo”. You can also change the highlighting by adding the line “highlight: tango” to the YAML as follows. --- title: \"Markdown Hints\" output: html_document: theme: cerulean highlight: tango --- Other highlighting options are “default”, “tango”, “pygments”, “kate”, “monochrome”, “espresso”, “zenburn”, “haddock”, and “textmate”. More Information Go to the rmarkdown.rstudio.com website for more information on how to use R Markdown. // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); $(document).ready(function () { // temporarily add toc-ignore selector to headers for the consistency with Pandoc $('.unlisted.unnumbered').addClass('toc-ignore') // move toc-ignore selectors from section div to header $('div.section.toc-ignore') .removeClass('toc-ignore') .children('h1,h2,h3,h4,h5').addClass('toc-ignore'); // establish options var options = { selectors: \"h1,h2,h3\", theme: \"bootstrap3\", context: '.toc-content', hashGenerator: function (text) { return text.replace(/[.\\\\/?&!#<>]/g, '').replace(/\\s/g, '_'); }, ignoreSelector: \".toc-ignore\", scrollTo: 0 }; options.showAndHide = true; options.smoothScroll = true; // tocify var toc = $(\"#TOC\").tocify(options).data(\"toc-tocify\"); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();"},{"title":"jQuery UI Example Page","url":"site_libs/jqueryui-1.13.2/index.html","content":"Welcome to jQuery UI! This page demonstrates the widgets and theme you selected in Download Builder. Please make sure you are using them with a compatible jQuery version. YOUR COMPONENTS: Accordion First Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet. Second Phasellus mattis tincidunt nibh. Third Nam dui erat, auctor a, dignissim quis. Autocomplete Button A button element An icon-only button Checkboxradio Choice 1 Choice 2 Choice 3 Controlgroup Rental Car Compact carMidsize carFull size carSUVLuxuryTruckVanStandard Automatic Insurance # of cars Book Now! Tabs First Second Third Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Phasellus mattis tincidunt nibh. Cras orci urna, blandit id, pretium vel, aliquet ornare, felis. Maecenas scelerisque sem non nisl. Fusce sed lorem in enim dictum bibendum. Nam dui erat, auctor a, dignissim quis, sollicitudin eu, felis. Pellentesque nisi urna, interdum eget, sagittis et, consequat vestibulum, lacus. Mauris porttitor ullamcorper augue. Dialog Open Dialog Overlay and Shadow Classes Lorem ipsum dolor sit amet, Nulla nec tortor. Donec id elit quis purus consectetur consequat. Nam congue semper tellus. Sed erat dolor, dapibus sit amet, venenatis ornare, ultrices ut, nisi. Aliquam ante. Suspendisse scelerisque dui nec velit. Duis augue augue, gravida euismod, vulputate ac, facilisis id, sem. Morbi in orci. Nulla purus lacus, pulvinar vel, malesuada ac, mattis nec, quam. Nam molestie scelerisque quam. Nullam feugiat cursus lacus.orem ipsum dolor sit amet, consectetur adipiscing elit. Donec libero risus, commodo vitae, pharetra mollis, posuere eu, pede. Nulla nec tortor. Donec id elit quis purus consectetur consequat. Nam congue semper tellus. Sed erat dolor, dapibus sit amet, venenatis ornare, ultrices ut, nisi. Aliquam ante. Suspendisse scelerisque dui nec velit. Duis augue augue, gravida euismod, vulputate ac, facilisis id, sem. Morbi in orci. Nulla purus lacus, pulvinar vel, malesuada ac, mattis nec, quam. Nam molestie scelerisque quam. Nullam feugiat cursus lacus.orem ipsum dolor sit amet, consectetur adipiscing elit. Donec libero risus, commodo vitae, pharetra mollis, posuere eu, pede. Nulla nec tortor. Donec id elit quis purus consectetur consequat. Nam congue semper tellus. Sed erat dolor, dapibus sit amet, venenatis ornare, ultrices ut, nisi. Aliquam ante. Suspendisse scelerisque dui nec velit. Duis augue augue, gravida euismod, vulputate ac, facilisis id, sem. Morbi in orci. Nulla purus lacus, pulvinar vel, malesuada ac, mattis nec, quam. Nam molestie scelerisque quam. Nullam feugiat cursus lacus.orem ipsum dolor sit amet, consectetur adipiscing elit. Donec libero risus, commodo vitae, pharetra mollis, posuere eu, pede. Nulla nec tortor. Donec id elit quis purus consectetur consequat. Nam congue semper tellus. Sed erat dolor, dapibus sit amet, venenatis ornare, ultrices ut, nisi. Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Framework Icons (content color preview) Slider Datepicker Progressbar Selectmenu SlowerSlowMediumFastFasterSpinner Menu Item 1 Item 2 Item 3 Item 3-1 Item 3-2 Item 3-3 Item 3-4 Item 3-5 Item 4 Item 5 Tooltip Tooltips can be attached to any element. When you hover the element with your mouse, the title attribute is displayed in a little box next to the element, just like a native tooltip. Highlight / Error Hey! Sample ui-state-highlight style. Alert: Sample ui-state-error style. $( \"#accordion\" ).accordion(); var availableTags = [ \"ActionScript\", \"AppleScript\", \"Asp\", \"BASIC\", \"C\", \"C++\", \"Clojure\", \"COBOL\", \"ColdFusion\", \"Erlang\", \"Fortran\", \"Groovy\", \"Haskell\", \"Java\", \"JavaScript\", \"Lisp\", \"Perl\", \"PHP\", \"Python\", \"Ruby\", \"Scala\", \"Scheme\" ]; $( \"#autocomplete\" ).autocomplete({ source: availableTags }); $( \"#button\" ).button(); $( \"#button-icon\" ).button({ icon: \"ui-icon-gear\", showLabel: false }); $( \"#radioset\" ).buttonset(); $( \"#controlgroup\" ).controlgroup(); $( \"#tabs\" ).tabs(); $( \"#dialog\" ).dialog({ autoOpen: false, width: 400, buttons: [ { text: \"Ok\", click: function() { $( this ).dialog( \"close\" ); } }, { text: \"Cancel\", click: function() { $( this ).dialog( \"close\" ); } } ] }); // Link to open the dialog $( \"#dialog-link\" ).click(function( event ) { $( \"#dialog\" ).dialog( \"open\" ); event.preventDefault(); }); $( \"#datepicker\" ).datepicker({ inline: true }); $( \"#slider\" ).slider({ range: true, values: [ 17, 67 ] }); $( \"#progressbar\" ).progressbar({ value: 20 }); $( \"#spinner\" ).spinner(); $( \"#menu\" ).menu(); $( \"#tooltip\" ).tooltip(); $( \"#selectmenu\" ).selectmenu(); // Hover states on the static widgets $( \"#dialog-link, #icons li\" ).hover( function() { $( this ).addClass( \"ui-state-hover\" ); }, function() { $( this ).removeClass( \"ui-state-hover\" ); } );"},{"title":"t Tests","url":"tTests.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus t Tests function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } Much of statistical inference concerns the location of the population mean \\(\\mu\\) for a given parametric distribution. Some of the most common approaches to making inference about \\(\\mu\\) utilize a test statistic that follows a t distribution. One Sample t Test A one sample t test is used when there is a hypothesized value for the population mean \\(\\mu\\) of a single quantitative variable. Overview Questions The one sample t test can be used to answer questions like: How long does it take to drive from Rexburg, ID to Salt Lake City, UT on average? Is human body temperature really 98.6&deg; F on average? Do I spend less than $3 a day, on average, purchasing snacks? Requirements This test is only appropriate when both of the following are satisfied. The sample is representative of the population. (Having a simple random sample is the best way to do this.) The sampling distribution of the sample mean \\(\\bar{x}\\) can be assumed to be normal. This is a safe assumption when either (a) the population data can be assumed to be normally distributed using a Q-Q Plot or (b) the size of the sample (n) that was taken from the population is large (at least n > 30, but “large” really depends on how badly the data is skewed). If the requirements listed above are satisfied, then the results of the test can be trusted to give meaningful inference about the population. If the requirements are not met, then that doesn’t mean the results of the test are necessarily bad, but there is no guarantee that they are good. Hypotheses Math Code $$ H_0: \\mu = 5.2 $$ $$ H_a: \\mu \\neq 5.2 $$ \\(H_0: \\mu = \\text{some number}\\) \\(H_a: \\mu \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number}\\) Examples: analysis resubmits R Instructions Console Help Command: ?t.test() t.test(NameOfYourData$Y, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set, like mtcars or KidsFeet. Y must be a “numeric” vector of quantitative data. YourNull is the numeric value from your null hypothesis for \\(\\mu\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(NameOfYourData$Y) Example Code Hover your mouse over the example codes to learn more. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,  ‘mpg’ is Y, a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,  The numeric value from the null hypothesis is 20 meaning \\(\\mu=20\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1−α.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. One Sample t-test EXPLANATION. data: mtcars$mpg EXPLANATION. t = 0.08506, EXPLANATION.  df = 31, EXPLANATION.  p-value = 0.9328 EXPLANATION. alternative hypothesis: true mean is not equal to 20 EXPLANATION. 95 percent confidence interval: EXPLANATION.  17.91768 EXPLANATION.  22.26357 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION.  20.09062 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg) ‘mpg’ is a quantitative variable (numeric vector) from the mtcars dataset.  Click to Show Output  Click to View Output. ## [1] 20 18 Explanation In many cases where it is of interest to test a claim about a single population mean \\(\\mu\\), the one sample t test is used. This is an appropriate decision whenever the sampling distribution of the sample mean can be assumed to be normal and the data represents a simple random sample from the population. In the figure below, the null hypothesis \\(H_0: \\mu = \\mu_0\\) is represented by the normal distribution (gray) centered at \\(\\mu_0\\). Note that \\(\\mu_0\\) is just some specified number. This shows how the null hypothesis represents the assumption about the center of the distribution of the data. After a hypothesis (null) is established and an alternative hypothesis similarly declared, a simple random sample of data of size \\(n\\) is obtained from the population of interest. In the plot above, this is depicted by the points (blue dots) which are centered around their sample mean \\(\\bar{x}\\). Above the points (blue dots) is shown a second normal distribution (blue dashed line) which represents the idea that the alternative hypothesis allows for a normal distribution which is potentially more consistent with the data than the one specified under the null hypothesis. The role of the one sample t test is to measure the probability of a sample mean being as extreme or more extreme from the hypothesized value of \\(\\mu_0\\) than the one observed assuming the null hypothesis is true. This probability is of course the p-value of the test. This works because the sampling distribution of the sample mean has been assumed to be normal. In this case, the distribution of the test statistic t, \\[ t = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\] is known to follow a t distribution with \\(n-1\\) degrees of freedom. (The mathematics that provide this result are phenominal! You can consult any advanced statistical textbook for the details.) The p-value of the one sample t test represents the probability that the test statistic \\(t\\) is as extreme or more extreme than the one observed according to a t-distribution with \\(n-1\\) degrees of freedom. If the probability (the p-value) is close enough to zero (smaller than \\(\\alpha\\)) then it is determined that the most plausible hypothesis is the alternative hypothesis, and thus the null is “rejected” in favor of the alternative. Paired Samples t Test The paired samples t test is used when a value is hypothesized for the popluation mean of the differences, \\(\\mu_d\\), obtained from paired observations. Overview Questions The Paired Samples t Test can be used to answer questions like: From pre-test to post-test is there an improvement on average in the subjects? How much taller are husbands than their wives, on average? Do hospital patients that are carefully matched together according to reason for being in the hospital, age, gender, ethnicity, height, and weight show increased stay times in the hospital when infected with a nosocomial infection compared to those who were not infected? Requirements The test is only appropriate when both of the following are satisfied. The sample of differences is representative of the population differences. The sampling distribution of the sample mean of the differences \\(\\bar{d}\\) (\\(\\bar{x}\\) of the differences) can be assumed to be normal. (This second requirement can be assumed to be satisfied when (a) the differences themselves can be assumed to be normal from a Q-Q Plot, or (b) when the sample size \\(n\\) of the differences is large.) Hypotheses Math Code $$ H_0: \\mu_d = 0 $$ $$ H_a: \\mu_d \\neq 0 $$ \\(H_0: \\mu_d = \\text{some number, but typically 0}\\)\\(H_a: \\mu_d \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepPaired studentPaired R Instructions Console Help Command: ?t.test() Option 1: t.test(NameOfYourData$Y1, NameOfYourData$Y2, paired = TRUE, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set like sleep or mtcars or KidsFeet. Y1 must be a “numeric” vector that represents the quantitative data from the first sample of data. Y2 must be a “numeric” vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y1 - Y2) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the “group1” data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the “group2” data from the sleep data set t.test( ‘t.test’ is an R function that performs one and two sample t-tests. sleep2$extra,  A numeric vector that represents the hours of extra sleep that the group had with drug 2. sleep1$extra,  A numeric vector that represents the hours of extra sleep that the same group had with drug 1. paired=TRUE,  Indicates that this is a paired t-Test. This will cause the subtraction of sleep2$extra - sleep1$extra to be performed to obtain the paired differences. To cause the subtraction to occur in the other order, reverse the order sleep1$extra, sleep2$extra occur in the t.test(…) function. mu = 0,  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. Paired t-test EXPLANATION. data: sleep2$extra and sleep1$extra EXPLANATION. t = 4.0621, EXPLANATION.  df = 9, EXPLANATION.  p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  0.7001142 EXPLANATION.  2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of the differences EXPLANATION.   1.58 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. sleep2$extra The hours of extra sleep that the group had with drug 2.  -  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. ) Closing parenthesis for qqPlot(…) function.  Click to Show Output  Click to View Output. ## [1] 9 5 Option 2: Compute the differences yourself instead of using paired=TRUE. differences = NameOfYourData$Y1 - NameOfYourData$Y2 t.test(differences, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a “numeric” vector that represents the quantitative data from the first sample of data. Y2 must be a “numeric” vector that represents the quantitative data from the second sample of data. This vector must be in the same order as the first sample so that the pairing can take place. differences are the resulting differences obtained from subtracting Y1 - Y2. YourNull is the numeric value from your null hypothesis for \\(\\mu_d\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(differences) Example Code Hover your mouse over the example codes to learn more. sleep1 <- filter(sleep, group==1) This splits out the “group1” data from the sleep data set. sleep2 <- filter(sleep, group==2) This splits out the “group2” data from the sleep data set differences <-  Saved the computed differences to an object called ‘differences’. sleep2$extra The hours of extra sleep that the group had with drug 2.  -  Subtract the hours of extra sleep with drug 1 from the hours of extra sleep with drug 2 to get the difference. sleep1$extra The hours of extra sleep that the same group had with drug 1. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. differences,  ‘differences’ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2. mu = 0,  The numeric value from the null hypothesis 0 meaning the null hypothesis is \\(\\mu_d=0\\). alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\mu_d\\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. One Sample t-test EXPLANATION. data: differences EXPLANATION. t = 4.0621, EXPLANATION.  df = 9, EXPLANATION.  p-value = 0.002833 EXPLANATION. alternative hypothesis: true mean is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  0.7001142 EXPLANATION.  2.4598858 EXPLANATION. sample estimates: EXPLANATION. mean of x EXPLANATION.   1.58 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. differences) ‘differences’ are the resulting differences of the hours of extra sleep with drug 1 and the hours of extra sleep with drug 2.  Click to Show Output  Click to View Output. ## [1] 9 5 Explanation The paired samples t test considers the single mean of all the differences from the paired values. Thus, the paired samples t test essentially becomes a one sample t test on the differences between paired observations. Hence the requirement is that the sampling distribution of the sample mean of the differences, \\(\\bar{d}\\), can be assumed to be normally distributed. (It is also required that the obtained differences represent a simple random sample of the full population of possible differences.) The paired samples t test is similar to the independent samples t test scenario, except that there is extra information that allows values from one sample to be paired with a value from the other sample. This pairing of values allows for a more direct analysis of the change or difference individuals experience between the two samples. The points in the plot below demonstrate how points are paired together, and the only thing of interest are the differences between the paired points. Independent Samples t Test The independent samples t test is used when a value is hypothesized for the difference between two (possibly) different population means, \\(\\mu_1 - \\mu_2\\). Overview Questions The Independent Samples t Test can be used to answer questions like: Are boys taller than girls on average? Do students who show up to class everyday get higher scores on average than those who don’t? Do you take more steps on average on weekdays or on weekends? Requirements The test is only appropriate when both of the following are satisfied. Both samples are representative of the population. (Simple random samples are the best way to do this.) The sampling distribution of the difference of the sample means \\((\\bar{x}_1 - \\bar{x}_2)\\) can be assumed to be normal. (This is a safe assumption when the sample size of each group is \\(30\\) or greater or when the population data from each group can be assumed to be normal with a Q-Q Plot.) Hypotheses Math Code $$ H_0: \\mu_\\text{label 1} - \\mu_\\text{label 2} = 13.2 $$ $$ H_a: \\mu_\\text{label 1} - \\mu_\\text{label 2} \\neq 13.2 $$ \\(H_0: \\mu_1 - \\mu_2 = \\text{some number, but typically 0}\\) \\(H_a: \\mu_1 - \\mu_2 \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{some number, but typically 0}\\) Examples: sleepInd student1 student2 R Instructions Console Help Command: ?t.test() There are two ways to perform the test. Option 1: t.test(Y ~ X, data = YourData, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) Y must be a “numeric” vector from YourData that represents the data for both samples. X must be a “factor” or “character” vector from YourData that represents the group assignment for each observation. There can only be two groups specified in this column of data. YourNull is the numeric value from your null hypothesis for \\(\\mu_1-\\mu_2\\). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Testing Assumptions library(car) qqPlot(Y ~ X, data=YourData) Example Code Hover your mouse over the example codes to learn more. t.test( ‘t.test’ is an R function that performs one and two sample t-tests. length  ‘length’ is a quantitative variable (numeric vector). ~  ‘~’ is the tilde symbol. sex,  ‘sex’ is a ‘factor’ or ‘character’ vector that represents the group assignment for each observation. There are two groups. data=KidsFeet,  ‘KidsFeet’ is a dataset in library(mosaic). Type View(KidsFeet) to view it. mu = 0,  The numeric value from the null hypothesis for μ1-μ2 is 0 meaning the null hypothesis is \\(\\mu1-\\mu2 = 0\\) alternative = “two.sided”,  The alternative is “two-sided” meaning the alternative hypothesis is \\(\\mu1-\\mu2 \\neq 0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to \\(1-\\alpha\\)     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  Click to Show Output  Click to View Output. Welch Two Sample t-test EXPLANATION. data: length by sex EXPLANATION. t = 1.9174, EXPLANATION.  df = 36.275, EXPLANATION.  p-value = 0.06308 EXPLANATION. alternative hypothesis: true difference in means is not equal to 0 EXPLANATION. 95 percent confidence interval: EXPLANATION.  -0.04502067 EXPLANATION.  1.61291541 EXPLANATION. sample estimates: EXPLANATION. mean in group B mean in group G EXPLANATION.   25.10500 EXPLANATION.   24.32105 EXPLANATION. qqPlot( ‘qqPlot’ is a R function from library(car) that creates a qqPlot. length  ‘length’ is a quantitative variable (numeric vector). ~  ‘~’ is the tilde symbol. sex,  ‘sex’ is a “factor” or “character” vector that represents the group assignment for each observation. There are two groups. data=KidsFeet) ‘KidsFeet’ is a dataset in library(mosaic). Type View(KidsFeet) to view it.  Click to Show Output  Click to View Output. Option 2: t.test(NameOfYourData$Y1, NameOfYourData$Y2, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) NameOfYourData is the name of your data set. Y1 must be a “numeric” vector that represents the quantitative data from the first sample. Y2 mus"},{"title":"Wilcoxon Tests","url":"WilcoxonTests.html","content":"Statistics Notebook Paige's Notes Intermediate Statistics (MATH 325) Experimental Design (MATH 326) Applied Linear Regression (MATH 425) R Help R Commands R Markdown Hints Data Sources Example Analyses Describing Data Graphical Summaries Numerical Summaries Making Inference Overview t Tests Wilcoxon Tests Kruskal-Wallis Test ANOVA Linear Regression Logistic Regression Chi-Squared Tests Randomization Analyses 325 Analyses Good Example Analysis Poor Example Analysis Rent Stephanie High School Seniors (Independent Samples t-Test) Recalling Words (Wilcoxon Test) My Two-way ANOVA Food (Kruskal-Wallis Test) My Simple Linear Regression Weather (Multiple Linear Regression) My Logistic Regression My Chi-squared Test Testing Center (Consulting Project) 425 Analyses Predicting Rexburg, Idaho Weather Residuals, Sums of Squares, and R-Squared Acura TL Selling Prices Predicting Grades House Selling Prices Outlier Theory Assignment Homework Focus Wilcoxon Tests function showhide(id) { var e = document.getElementById(id); e.style.display = (e.style.display == 'block') ? 'none' : 'block'; } Wilcoxon tests allow for the testing of hypotheses about the value of the the median without assuming the test statistic follows any parametric distribution. They are often seen as nonparametric alternatives to the various t tests. However, they can also be used on ordinal data (data that is not quite quantitative, but is ordered) unlike t tests which require quantitative data. Wilcoxon Signed-Rank Test For testing hypotheses about the value of the median of (1) one sample of quantitative data or (2) one set of differences from paired data. Overview The nonparametric equivalent of the paired-samples t test as well as the one-sample t test. Best for smaller sample sizes where the distribution of the data is not normal. The t test is more appropriate when the data is normal or when the sample size is large. While the test will work in most scenarios it suffers slightly when ties (repeated values) are present in the data. If many ties are present in the data, the test is not appropriate. If only a few ties are present, the test is still appropriate. Hypotheses Originally created to test hypotheses about the value of the median, but works as well for the mean when the distribution of the data is symmetrical. One Sample of Data Math Code $$ H_0: \\text{Median} = \\text{(Some Number)} $$ $$ H_a: \\text{Median} \\neq \\text{(Same Number)} $$ \\(H_0: \\text{Median} = \\text{(Some Number)}\\) \\(H_a: \\text{Median} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ \\text{(Some Number)}\\) Paired Samples of Data Math Code $$ H_0: \\text{median of differences} = 0 $$ $$ H_a: \\text{median of differences} \\neq 0 $$ \\(H_0: \\text{median of differences} = 0\\) \\(H_a: \\text{median of differences} \\ \\left\\{\\underset{<}{\\stackrel{>}{\\neq}}\\right\\} \\ 0\\) Examples: sleep, CornHeights R Instructions Console Help Command: ?wilcox.test() Paired Data wilcox.test(Y1, Y2, mu = YourNull, alternative = YourAlternative, paired = TRUE, conf.level = 0.95) Y1 must be a “numeric” vector. One set of measurements from the pair. Y2 also a “numeric” vector. Other set of measurements from the pair. YourNull is the numeric value from your null hypothesis for the median of differences from the paired data. Usually zero. YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests. sleep$extra[sleep$group==1],  The hours of extra sleep that the group had with drug 2. sleep$extra[sleep$group==2],  The hours of extra sleep that the same group had with drug 1. mu = 0,  The numeric value from the null hypothesis for the median of differences from the paired data is 0 meaning the null hypothesis is \\(\\text{median of differences} = 0\\). paired=TRUE,  This command forces a “paired” samples test to be performed. alternative = “two.sided”,  The alternative hypothesis is “two.sided” meaning the alternative hypothesis is \\(\\text{median of differences} \\neq0\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1 - \\(\\alpha\\).     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Wilcoxon signed rank test with continuity correction The phrase “with continuity correction” implies that instead of using the “exact” distribution of the test statistic a “normal approximation” was used instead to compute the p-value. Further, a small correction was made to allow for the change from the “discrete” exact distribution to the “continuous normal distribution” when calculating the p-value. data: sleep$extra[sleep$group == 1] and sleep$extra[sleep$group == 2] This statement of the output just reminds you of the code you used to perform the test. The important thing is recognizing that the first group listed is “Group 1” and the second group listed is “Group 2.” This is especially important when using alternative hypotheses of “less” or “greater” as the order is always “Group 1” is “less” than “Group 2” or “Group 1” is “greater” than “Group 2.” V = 0, This is the test statistic of the test, i.e., the sum of the ranks from the positive group minus the minimum sum of ranks possible.  p-value = 0.009091 This is the p-value of the test. If no warning is displayed when the test is run, then this is the “exact” p-value from the non-parametric Wilcoxon Test Statistic distribution. Sometimes a message will appear stating “Cannot compute exact p-value with ties” or other similar messages. In those cases, the p-value is still considered valid even though it is obtained through a normal approximation to the exact distribution. alternative hypothesis: true location shift is not equal to 0 This reports that the alternative hypothesis was “two-sided.” If the alternative had been “less” or “greater” the wording would change accordingly. One Sample wilcox.test(object, mu = YourNull, alternative = YourAlternative, conf.level = 0.95) object must be a “numeric” vector. YourNull is the numeric value from your null hypothesis for the median (even though it says “mu”). YourAlternative is one of the three options: \"two.sided\", \"greater\", \"less\" and should correspond to your alternative hypothesis. The value for conf.level = 0.95 can be changed to any desired confidence level, like 0.90 or 0.99. It should correspond to \\(1-\\alpha\\). Example Code Hover your mouse over the example codes to learn more. wilcox.test( ‘wilcox.test’ is a function for non-parametric one and two sample tests. mtcars ‘mtcars’ is a dataset. Type ‘View(mtcars)’ in R to view the dataset. $ The $ allows us to access any variable from the mtcars dataset. mpg,  ‘mpg’ is a quantitative variable (numeric vector) from the mtcars dataset. mu = 20,  The numeric value from the null hypothesis is 20 meaning \\(\\mu = 20\\). alternative = “two.sided”,  The alternative is “two.sided” meaning the alternative hypothesis is \\(\\mu\\neq20\\). conf.level = 0.95) This test has a 0.95 confidence level which corresponds to 1−α.     Press Enter to run the code if you have typed it in yourself. You can also click here to view the output.  …  Click to View Output. Wilcoxon signed rank test with continuity correction This reports on the type of test performed. The phrase “with continuity correction” implies the normal approximation was used when calculating the p-value of the test. data: mtcars$mpg This print-out reminds us that the mpg column of the mtcars data was used as “Y” in the test. V = 249, The test statistic of the test.  p-value = 0.7863 The p-value of the test. alternative hypothesis: true location is not equal to 20 The words “not equal” tell us this was a two-sided test. Had it been a one-sided test, either the word “less” or the word “greater” would have appeared instead of “not equal.” Explanation In many cases it is of interest to perform a hypothesis test about the location of the center of a distribution of data. The Wilcoxon Signed Rank Test allows a nonparametric approach to doing this. The Wilcoxon Signed-Rank Test covers two important scenarios. One sample of data from a population. (Not very common.) The differences obtained from paired data. (Very common.) The Wilcoxon methods are most easily explained through examples, beginning with the paired data for which the method was originally created. Scroll down for the One Sample Example if that is what you are really interested in. However, it is still recommended that you read the paired data example first. Paired Data Example Note: the data for this example comes from the original 1945 paper Individual Comparison by Ranking Methods by Frank Wilcoxon. Background Height differences “between cross- and self- fertilized corn plants of the same pair” were collected. The experiment hypothesized that the center of the distribution of the height differences would be zero, with the alternative being that the center was not zero. The result of the data collection was 15 height differences: Differences: 14, 56, 60, 16, 6, 8, -48, 49, 24, 28, 29, 41, -67, 23, 75 Step 1 The first step of the Wilcoxon Signed Rank Test is to order the differences from smallest magnitude to largest magnitude. Negative signs are essentially ignored at this point and only magnitudes of the numbers matter. Sorted Differences: 6, 8, 14, 16, 23, 24, 28, 29, 41, -48, 49, 56, 60, -67, 75 Step 2 The next step is to rank the ordered values. Negative signs are attached to the ranks corresponding to negative numbers.   Differences: 6 8 14 16 23 24 28 29 41 -48 49 56 60 -67 75 Ranks: 1 2 3 4 5 6 7 8 9 -10 11 12 13 -14 15 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=15\\). Step 3 The ranks are then put into two groups. Negative Ranks Positive Ranks -10, -14 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15 Step 4 One of the groups is summed, usually the group with the fewest observations. Only the absolute values of the ranks are summed. Sum of Negative Ranks: \\(\\left|-10\\right| + \\left|-14\\right| = 24\\) The sum of the ranks becomes the test statistic of the Wilcoxon Test. The test statistic is sometimes called \\(W\\) or \\(V\\) or \\(U\\). Step 5 The \\(p\\)-value of the test is then obtained by computing the probability of the test statistic being as extreme or more extreme than the one obtained. This is done by first computing the probability of all possible values the test statistic could have obtained using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=15\\) ranks, the possible sums of ranks range from 0 to 120 and hit every integer in between, i.e., \\(1, 2, 3, \\ldots, 120\\). (Note, if summing the negative ranks these sums would technically all be negative.) To verify that \\(120\\) is the largest sum possible for \\(n=15\\) ranks, note that: \\(1+15 = 16\\), \\(2 + 14 = 16\\), \\(3+13 = 16\\), \\(4+12=16\\), \\(5+11=16\\), \\(6+10=16\\), \\(7+9=16\\), and finally that \\(8 = \\frac{16}{2}\\). Thus, there are 7 sums of 16 and one sum of \\(\\frac{16}{2}\\). This could be said in a mathematically equivalent way by stating there are \\(\\frac{14}{2}\\) sums of 16 and one sum of \\(\\frac{16}{2}\\). By multiplication this gives \\[ \\frac{14}{2}\\cdot\\frac{16}{1} + \\frac{1}{1}\\cdot\\frac{16}{2} = \\frac{14\\cdot16 + 1\\cdot16}{2} = \\frac{15\\cdot16}{2} = \\frac{n(n+1)}{2} = 120 \\] The probability of each sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 32,768 total different groups of ranks possible when there are \\(n=15\\) ranks.) For example, a sum of 1 can happen only one way, only the rank of 1 is in the group. A sum of 2 can also only happen 1 way. The sum of 3 however, can happen two ways: we could have the ranks of 1 and 2 in the group, or just the rank of 3 in the group. A similar counting technique is then implemented for each possible sum. After all the calculations are performed, the distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(24\\) (or its opposite of \\(120-24=96\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of (the absolute value of) negative ranks as extreme or more extreme than \\(24\\) is \\(p=0.04126\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would reject the null hypothesis that the center of the distribution of differences is zero. We conclude that the center of the distribution is greater than zero because the sum of negative ranks is much smaller than we expected under the zero center hypothesis (the null). Thus, there is sufficient evidence to conclude that the centers of the distributions of “cross- and self-fertilized corn plants” heights are not equal. One is greater than the other. Notice how the following dot plot shows that the differences are in favor of the cross-fertilized plants (the first group in the subtraction) being taller. This is true even though two self-fertilized plants were much taller than their cross-fertilized counterpart (the two negative differences). Comment If the distribution of differences is symmetric, then the hypotheses can be written as \\[ H_0: \\mu = 0 \\] \\[ H_a: \\mu \\neq 0 \\] If the distribution is skewed, then the hypotheses technically refer to the median instead of the mean and should be written as \\[ H_0: \\text{median} = 0 \\] \\[ H_a: \\text{median} \\neq 0 \\] One Sample Example The idea behind the one sample Wilcoxon Signed Rank test is nearly identical to the paired data. The only change is that the median must be subtracted from all observed values to obtain the differences. Note that the mean is equal to the median when data is symmetric. Background Suppose we are interested in testing to see if the median hourly wage of BYU-Idaho students during their off-track employment is equal to the minimum wage in Idaho, $7.25 an hour as of January 1st, 2015. Five randomly sampled hourly wages from BYU-Idaho Math 221B students provides the following data. Wages: $6.00, $9.00, $8.10, $18.00, $10.45 The differences are then obtained by subtracting the hypothesized value for the median (or mean if the data is symmetric) from all observations. Differences: -1.25, 1.75, 0.85, 10.75, 3.20 Note: from this point down, the wording of this example is identical to the paired data example (above) with the numbers changed to match \\(n=5\\). It is useful to continue reading to reinforce the idea of the Wilcoxon Signed Rank Test, but no new knowledge will be presented. Step 1 The first step of the Wilcoxon Signed Rank Test is to order the differences from smallest magnitude to largest magnitude. Negative signs are essentially ignored at this point and only magnitudes of the numbers matter. Sorted Differences: 0.85, -1.25, 1.75, 3.20, 10.75 Step 2 The next step is to rank the ordered values. Negative signs are attached to the ranks corresponding to negative numbers. Ranks: 1, -2, 3, 4, 5 Note that the ranks will always be of the form \\(1, 2, \\ldots, n\\). In this case, \\(n=5\\). Step 3 The ranks are then put into two groups. Negative Ranks Positive Ranks -2 1, 3, 4, 5 Step 4 One of the groups is summed, usually the group with the fewest observations. Sum of Negative Ranks: \\(\\left|-2\\right| = 2\\) Step 5 The \\(p\\)-value of the test is then obtained by computing the probabilities of all possible sums using mathematical counting techniques. This is a very tedious process that only a mathematician would enjoy pursuing. However, the end result is fairly easily understood. If you are interested, read the details. Details When there are \\(n=5\\) ranks, the possible sums of ranks range from 0 to 15 and hit every integer in between, i.e., \\(1, 2, 3, \\ldots, 15\\). (Note, if summing the negative ranks these sums would technically all be negative.) To verify that \\(15\\) is the largest sum possible for \\(n=5\\) ranks, note that: \\(1+5 = 6\\), \\(2 + 4 = 6\\), and finally \\(3 = \\frac{6}{2}\\). Thus, there are 2 sums of 6 and one sum of \\(\\frac{6}{2}\\). This could be said in a mathematically equivalent way by stating there are \\(\\frac{4}{2}\\) sums of 6 and one sum of \\(\\frac{6}{2}\\). By multiplication this gives \\[ \\frac{4}{2}\\cdot\\frac{6}{1} + \\frac{1}{1}\\cdot\\frac{6}{2} = \\frac{4\\cdot6 + 1\\cdot6}{2} = \\frac{5\\cdot6}{2} = \\frac{n(n+1)}{2} = 15 \\] The probability of each sum occurring is computed by counting all of the ways a certain sum can occur (combinations) and dividing by the total number of sums possible. (There are 32 total different groups of ranks possible when there are \\(n=5\\) ranks.) For example, a sum of 1 can happen only one way, only the rank of 1 is in the group. A sum of 2 can also only happen 1 way. The sum of 3 however, can happen two ways: we could have the ranks of 1 and 2 in the group, or just the rank of 3 in the group. A similar counting technique is then implemented for each possible sum. After all the calculations are performed, the distribution of possible sums looks like what is shown in the following plot, where the red bars show those sums that are as extreme or more extreme than a sum of \\(2\\) (or its opposite of \\(15-2=13\\)). Computing the probabilities of all possible sums creates a distribution of the test statistic (shown in the plot above). Note that the test statistic is obtained in Step 4 (above) by taking the sum of the ranks. Once the distribution of the test statistic is established, the \\(p\\)-value of the test can be calculated as the combined probability of possible sums that are as extreme or more extreme than the one observed. For this example, it turns out that the probability of getting a sum of negative ranks as extreme or more extreme than \\(-2\\) is \\(p=0.1875\\) (the sum of the probabilities of the red bars in the plot above). Thus, at the \\(\\alpha=0.05\\) level we would fail to reject the null hypothesis that the center of the distribution of differences is zero. We will continue to assume the null hypothesis was true, that the median off-track hourly wage of BYU-Idaho students is the same as the Idaho minimum wage. Final Comment Notice that when the sample size is large the distribution of the test statistic can be approximated by a normal distribution. Most software applications use this approximation when the sample size is over \\(50\\), i.e., for \\(n\\geq50\\) because computing all the possible sums of ranks becomes incredibly time consuming. Thus, for large sample sizes the results will be almost identical whether the Wilcoxon Tests or a t Test is used. Wilcoxon Rank Sum (Mann-Whitney) Test For testing the equality of the medians of two (possibly different) distributions of a quantitative variable. Overview The nonparametric equivalent of the Independent Samples t Test. Can also be used when data is ordered (ordinal) but does not have an exact measurement. For example, first place, second place, and so on. The Independent Samples t Test is more appropriate when the distributions are normal, or when the sample size for each sample is large. The test is negatively af"}]
