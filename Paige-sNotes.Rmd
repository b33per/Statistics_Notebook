---
title: "Intermediate Statistics Notes"
subtitle: "(MATH 325)"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 5
    code_folding: hide
    css: "styles.css"
        
---

<script type="text/javascript">
 function showhide(id) {
    var e = document.getElementById(id);
    e.style.display = (e.style.display == 'block') ? 'none' : 'block';
 }
 
 function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}
 
</script>


***

### Cheat Sheets

* [R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf)

* [RStudio Cheat Sheets](https://www.rstudio.com/resources/cheatsheets/)

* [R Base Commands Cheat Sheet](https://iqss.github.io/dss-workshops/R/Rintro/base-r-cheat-sheet.pdf)

* [Keyboard Shortcuts](https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts)

***



### Don't forget to always load your libraries and to Knit often!!

```{r, message=FALSE, warning=FALSE}
library(mosaic)
library(tidyverse)
library(pander)
library(DT)
library(ggrepel)
library(plotly)
library(dplyr)
library(ggplot2)
library(maps)
library(tmap)
library(leaflet)
library(htmltools)
library(car)
library(mosaicData)
library(ResourceSelection)
library(reshape2)
library(RColorBrewer)
library(scatterplot3d)
library(readr)
library(prettydoc)
library(knitr)
library(kableExtra)
library(formattable)
library(haven)

```

***



This is the templet for the Hover Box Thingy

<a href="javascript:showhide('SampleHover')">
<div class="hoverchunk">
<span class="tooltipr">
Hover over words box!
  <span class="tooltiprtext">
  [insert description here]
  </span></span><span class="tooltipr">
&nbsp;&nbsp;&nbsp;&nbsp;  
</span><span class="tooltipr" style="float:right;">
&nbsp;...&nbsp; 
  <span class="tooltiprtext">Click to View Output.</span>
</span>
</div>
</a>
<div id="SampleHover" style="display:none;">

There are three parts to the help box: 

(1) <u>The starter code</u>

< a href="javascript:showhide('[**Insert classifying name**]')">
< div class="hoverchunk" >


<br>

(2) <u>The description code</u>
- This is the pattern of the reoccurring code that you need to look out for when writing in the help box descriptions! 

< span class="tooltipr" >
[**What shows up in the box**]
  < span class="tooltiprtext" >
  [**The description that shows up when you hover**]
  < /span>< /span >
  
<br>

(3) <u>The ending code</u>
- the r chunk you put right after will then show up after you click the help box!

& nbsp;& nbsp;& nbsp;& nbsp;  
< span class="tooltiprtext" >Press Enter to run the code. </span >
< /span><span class="tooltipr" style="float:right;" >
& nbsp;...& nbsp; 
  < span class="tooltiprtext" >Click to View Output.< /span >
< /span >
< /div >
< /a >
< div id="[**The classifying name that you put in the starter code!**]" style="display:none;" >

*When using, make sure you fix all the spaces when using and use "< /div >" to show where the drop down is meant to end! 

<br>


</div>

<br>

***


# Personal Notes

<br>

<center>

OoOoooOOOoo let's go assignments!!
</div>

<center>

![](C:/Users/paige/OneDrive/Pictures/Screenshots 1/Screenshot 2024-11-05 233723.png)
</center>


<br>

***

## Weekly Assignments

<br>

The sections below show the different Skill Quizzes and Class Activities done throughout the semester. 
As well as some personal notes on those assignments!
</div>
 

<br>

***

### Week 1 | Welcome to the Course & Introduction to RStudio

<br>


We were setting up RStudio and doing cool little graphic thingys! 

<br>

***

#### Skill Quiz - Introduction to R 



***

**Look at data sets, computing means, and creating graphs.**



- Mean of Distance from "cars" data set

```{r message=FALSE, warning=FALSE}

View(cars)

mean(cars$dist)
```

- Scatter plot of "cars" data set
```{r}
plot(dist~speed, data=cars, col="skyblue",pch=16)
```

***

**Creating new data set with "<- The Assignment Operator"**

(1) create an airquality2 data set that is a copy of the airquality data set

(2) add a new column to the airquality2 data set called "Celcius" that converts Temp from degrees Fahrenheit (F) to Ceclcius (C) using the equation: Celcius = (Temp − 32) * 5/9. Don't forget to use airquailty$ appropriately in that equation!

(3) Hint: Typing "?airquality" will allow a help file for the airquality data set to appear! 

```{r}
write.csv(airquality,"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/airquality.cvs")

airquality2 <- airquality

airquality2$Celcius <- round((airquality2$Temp-32)*5/9,1)

View(airquality2)

datatable(airquality2, options=list(lengthMenu =c(3,10,30)), extensions="Responsive")

```

***



***PRACTICE*** :

- Scatter plot in Base R
```{r }
plot(Ozone~ Temp, data=airquality, 
     xlab="Daily Maximum Temperature",
     ylab="Mean Ozone in Parts per Billion (from 1300 to 1500 hours",
     main="Exponential Growth in Ozone with Increasing Temp",
     col="firebrick",pch=16 )
```

**pch Options!**
```{r, echo=FALSE, fig.height=1}
par(mai=c(.5,0,0,0))
plot(1:25, rep(1,25), pch=1:25, xaxt='n', yaxt='n', xlab="", ylab="")
axis(1, at=1:25, labels=1:25, cex.axis=.7)
```



***

### Week 2 | Describing Data in RStudio

<br>


Talking about how to give good feedback and more coolio graphics in this week!
</div>

<br>

***


#### Skill Quiz - Describing Data with R


***

<u>Numerical Summaries that <span style="background-color:yellow;">MEASURE CENTER</span>:</u>

(1) Proportion
(2) Mean
(3) Mode
(4) Median



<span style="background-color:lightblue;">Mean</span> - Gives a good feel for the most typical values of quantitative data as long as the data is at least approximately normally distributed.

<span style="background-color:lightblue;">Median</span> - 	
Gives a good feel for the most typical values of any style of distribution of quantitative data, but is especially useful for skewed data.

<span style="background-color:lightblue;">Standard Deviation</span> - Typically used to describe the spread of relatively normally distribued data.

<span style="background-color:lightblue;">Five-Number Summaryt</span> - 		
Gives a good feel for how the possible values of quanititative data spread out around the more probable values. 

***


***Practice***


- Find the mean and standard deviation of the "Wind" speed variable in the airquality dataset.


```{r}
pander(summary(airquality$Wind))

```




***

- Create a Scatter plot of Temp VS. Wind
```{r}
plot(Wind~Temp, data=airquality,
     xlab= "Daily Average Temperature",
     ylab = "Daily Average Wind Speed",
     main= "La Guardia Airport Warmer Weather Shows Less Wind",
     col= "gray", pch= 19)
```

***

- Create a Histogram and Box plot of Solar.R 
```{r}
hist(airquality$Solar.R, 
     xlab= "Daily Mean Radiation in Langleys (from 0800 to 1200 hours)",
     main= "Central Park, NYC Daily Average Radiation", col= "orange")
```

***

```{r}
boxplot(Solar.R~Month, data=airquality, 
        xlab= "Month of the Year",
        ylab= "Radiation in Langleys (Averaged from 0800 to 1200 hours)",
        main= "Daily Mean Radiation High in July",col=c("gray","gray","orangered","gray","gray"))
```


***


**Graph Types**


<span style="background-color:lightblue;">Histogram</span>
- Show the distribution of heights for a sample of `n=400` first grade boys.


<span style="background-color:lightblue;">Box Plot</span>
- Compare the distribution of body weights of American adults for different ethnicities where there is a sample size of `n=100` for each ethnicity.


<span style="background-color:lightblue;">Dot Plots</span> - Compare the distribution of bird beak lengths for different species of bird where there is a sample size of `n=3` for each species.




***

#### Class Activity - Principles of Good Graphics


***

***PRACTICE***


- Histogram, Box plot, and Scatter plot with airquality data set
```{r}
hist(airquality$Wind, col="steelblue", xlab="Daily Average Wind Speeds (mph)", main="La Guardia Airport from May to September, 1973")
```

```{r}
boxplot(Wind~Month, data=airquality, names=c("5","6","7","8","9"), col=c("steelblue1","steelblue2","steelblue3","steelblue3","steelblue2"))
```

```{r}
plot(Ozone~Temp, data=airquality, col= "steelblue", pch=19)
```

***

### Week 3 | Intro to Data Wrangling & Visulization

<br>


Looking at the different categories of data that is found in the [Index Page](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/index.html) and how to discern quantitative from categorical data!
</div>

<br>

***

#### Skills Quiz - Data Wrangling & Visualization

***

**Quick reminder!**

- How do you view a dataset?

*View([insert the dataset here])*<br><br>


- How do you open the help file

*?([insert the dataset here])*<br><br>


<u>Quanitative vs. Qualitative</u>

- Quantitative Examples:
    - length & width
    
- Qualitatve Examples: 
    - name, birthmonth, birthyear, sex, biggerfoot, & domhand

***

***PRACTICE*** : Use **KidsFeet dataset**<br><br>


- Make a table displaying **gender**

```{r}
table(KidsFeet$sex)%>%
pander()
```

<br><br>

***

- Make a table displaying **which foot is bigger**
    - Which foot is commonly bigger? : **Left Foot**

```{r}
table(KidsFeet$biggerfoot)%>%
pander()

```

<br><br>

***

- Make a table displaying **which birthmonth is the most common**
    - Which birth month is most common among these sampled children? : **March**

```{r}
table(KidsFeet$birthmonth)%>%
pander()
```

<br><br>

***

- Make a table that shows **"are girls or boys more likely to be left handed?"**

```{r}
table(KidsFeet$domhand, KidsFeet$sex)%>%
pander()
```

<br><br>

***

- Make a table showing the **summaries of children's foot lengths according to their gender**

```{r}
KidsFeet %>%
  group_by(sex) %>%
  summarise(Min=min(length),
            Q1 = quantile(length, 0.25),
            Median = (median(length)),
            Q3 = quantile(length,0.75),
            Max = max(length),
            Mean = mean(length),
            SD = sd(length))%>%
  pander()
```

<br><br>

- For this particular sample of data, which gender has the longest feet on average?
    - **Boys**<br><br>


- Which gender shows the most consistency in length of feet among children in this sample?
    - **Boys**  


***

***PRACTICE*** : Use **airquality dataset**<br><br>


- run an appropriate command to obtain the mean daily temperature at LaGuardia Airport for each month, separately.

```{r}
airquality %>%
  group_by(Month) %>%
  summarise(`Mean Temperature`= mean(Temp)) %>%
pander()
```

<u>Observations:</u>

- Which month experiences the coolest average temperature?
    - **May**<br><br>
    
- By how many degrees do the average temperatures of July and August differ?
    - 83.97-83.9 = **0.07**<br><br>
    
- Between which two *consecutive* months is there the largest difference in average temperature?
    - **May and June**


***


<br><br>

What are the **BEST** graphics for this data set?<br>

- These 3 would be useful in depicting the above information, because they nicely display the month and their averages<br><br>


<u>**(1) Box plot**</u>

```{r}
boxplot(Temp ~ Month, data=airquality, xlab="Month", ylab="Temperature", main="LaGuardia Airport (May to September 1973)", pch=16, col="slategray")
```

<u>**(2) Scatter plot**</u>

```{r}
plot(Temp ~ Month, data=airquality, xlab="Month", ylab="Temperature", main="LaGuardia Airport (May to September 1973)", pch=16, col="slategray")
```


<u>**(3) Dot Plot**</u>

```{r}
stripchart(Temp ~ Month, data=airquality, ylab="Month", xlab="Temperature", main="LaGuardia Airport (May to September 1973)", pch=16, col="slategray", method="stack")
```

***


What are the **WORST** graphics for depicting this information?

- these two are the worst because they don't clearly show the months in order to allow us to compare the data of each month.<br><br><br>



**(1) Scatter plot**

- but the x axis is Day, not very clear variable
```{r}
plot(Temp ~ Day, data=airquality, xlab="Day of the Month", ylab="Temperature", main="LaGuardia Airport (May to September 1973)", pch=16, col="slategray")
```


**(2) Histogram**

- gives you the average of the whole time frame but not the specific times
```{r}
hist(airquality$Temp, xlab="Daily Temperature", main="LaGuardia Airport (May to September 1973)", col="slategray")
```



***


***PRACTICE*** : Use **Orange dataset**


View(Orange)

Orange2 <- Orange %>%
  group_by(age)%>%
  summarise(MedianCir = median(circumference))

datatable(Orange2, options=list(lengthMenu =c(3,10,30)), extensions="Responsive")




***


What are the **BEST** graphics for this data set?<br>

```{r include=FALSE}
plot.new()
text(0.5,0.5,"Yippe")
```

(1) Scatter plot with median growth line

- visualizes the relationship/ captures the overall trends between two continuous variables


`plot(circumference ~ age, data=Orange, ylab="Trunk Circumference (mm)", xlab="Age of Trees (days)", main="Trunk Circumference of Orange Trees", col="ivory3", pch=15)
Orange.m <- median(circumference ~ age, data=Orange)
lines(names(Orange.m), Orange.m, col="ivory3")
legend("topleft", legend="Median Growth", lty=1, col='ivory3', bty='n')`




(2) Box plot

- shows how the trunk circumference varies across different ages


`boxplot(circumference ~ age, data=Orange, ylab="Trunk Circumference (mm)", xlab="Age of Trees (days)", main="Trunk Circumference of Orange Trees", col="ivory3")`


(3) Dot plot

- shows the distribution of data and helps visualize all the individual data points


`stripchart(circumference ~ age, data=Orange, ylab="Trunk Circumference (mm)", xlab="Age of Trees (days)", main="Trunk Circumference of Orange Trees", col="ivory3", pch=15, method="stack", vertical=TRUE)`


<u>Question:</u>

- During which age interval did the **most rapid overall median growth occur** (in the circumference of the orange trees that were sampled)?
    - **664 to 1004 days (third interval)**

***


What are the **WORST** graphics for this data set?

- Box plot 
    - x-axis is sectioned out by multiple variables instead of just one, thus, not really showing us anything of value 


`boxplot(Orange, xlab="Age of Tree (days)", main="Trunk Circumference of Orange Trees", col="ivory3")`


***

***PRACTICE*** : Use the **Riders** data set

- Consider the Riders dataset in R. (You may need to load library(mosaicData).)
    - **How many total riders were observed on each day of the week?** (Hint: the sum() function works the same way as mean()...)
    
```{r}
Riders2 <- Riders %>%
  group_by(day)%>%
  summarise(`Total Number of Riders Observed`= sum(riders))

datatable(Riders2)
```

***

***PRACTICE*** : Use the **mtcars dataset**

- How would you describe the dataset?
    - First, type ?mtcars and look at the "Description" of the data set information<br><br>
    
<u>Description:</u>
The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).<br><br>


- How many **variables** are in the mtcars data set?
    - First, View()the data, then read how many columns are in the data set in the bottom left hand corner : **11**<br><br>
    
    
- How many **observations** are in the mtcars data set? (Hint: try View(...))
    - First, View() the data, then read the amount of entries in the data set in the bottom left hand corner : **32**<br><br>
    
    
- How many vehicles are represented in the dataset for 4, 6, and 8 cylinder vehicles? (Hint: use the table(...) function.)

```{r}
table(mtcars$cyl)%>%
  pander()
```

***

- According to the mtcars data, on average, vehicles with 4 cylinders get the best (highest) gas mileage. **What is the average mpg for automatic and manual transmission vehicles with 4 cylinders?** (Round answers to the nearest tenth.)
    - 0 = Automatic, 1 = Manual
  

```{r}
mtcars %>%
  filter(cyl == "4")%>%
  group_by(am)%>%
  summarise(`Mean Gas Mileage form 4 Cylinder mtcars Vehicles(mpg)`= round(mean(mpg),1))%>%
  mutate(Transmission = ifelse(am == 0, "Automatic","Manual"))%>%
  pander()



```

***

- According to the mtcars data, on average, vehicles with 8 cylinders have the best (fastest) quarter mile time. **What is the mean quarter mile time (qsec) for automatic and manual transmission vehicles with 8 cylinders?** (Round answers to nearest tenth.)

```{r}
mtcars %>%
  filter(cyl == "8")%>%
  group_by(am)%>%
  summarise(`Mean Quarter Mile Time for 8 Cylinder mtcars Vechiles (sec)`= round(mean(qsec),1))%>%
  mutate(Transmission = ifelse(am == 0, "Automatic","Manual"))%>%
  pander()
```

***


- According to the mtcars data, **how many thousands of pounds does the heaviest 6 cylinder car with an** ***automatic transmission*** **weigh?** (Round to the nearest tenth.)

```{r}
mtcars %>%
  filter(cyl == "6", am == "0") %>%
  summarise(MaxWTAutomatic = round(max(wt),1))%>%
  pander()
```

- **How many** ***more*** **thousands of pounds does it weigh than the heaviest 6 cylinder car with a** ***manual transmission?*** (Round to the nearest tenth.)
    - 3.5 - 2.9 = **0.6**

```{r}
mtcars %>%
  filter(cyl == "6", am == "1") %>%
  summarise(MaxWTManual = round(max(wt),1))%>%
  pander()
```

***


#### Assesment Quiz - Intro to Data Wrangling & Visualization


***

(1) Use the mtcars dataset in R to compute the **mean "Gross horsepower" of both automatic and manual** transmission 1974 Motor Trend vehicles.
```{r}
mtcars %>%
  group_by(am)%>%
  summarise(mean(hp))%>%
  pander()
  
```



***


(2) Use the mtcars dataset in R to make a graph that allows you to see how the quarter mile time **(qsec)** of 1974 Motor Trend vehicles is effected by the number of carburetors **(carb)** in the vehicle.

- Since both qsec and carb are quantitiative, a scatterplot is the best graphic!
    - This helps show that the **average qsec time** (remember, average is the middle of the dots) drops (or **gets faster**) as the number of **carburetors increases**

```{r}
plot(qsec ~ carb, data=mtcars)
```
On average, the more carburetors a vehicle has, the faster its quarter mile time.


***


(3) Run the following codes in R. Then select the statement that most appropriately interprets the resulting graph.

- The graph produced by the code given shows **gas mileage on the y-axis and quarter mile times on the x-axis**. Also, as indicated by the legend, the color of **the points is determined by whether the vehicle is automatic or manual transmission**.
    - Since **both** transmission types show **positive moderate correlations**, we can conclude that ***higher quarter mile times (which means slower vehicles) correlate with higher gas mileages.***

```{r}
palette(c("skyblue","firebrick"))

  plot(mpg ~ qsec, data=mtcars, col=as.factor(am), pch=16, xlab="Quarter Mile Time (seconds)", ylab="Miles per Gallon", main="1974 Motor Trend Vehicles")
  legend("topright", pch=16, legend=c("automatic","manual"), title="Transmission", bty='n', col=palette())
```

The graph suggests that if you want a vehicle that **gets really good gas mileage**, you should go with a **manual transmission** that has a **really slow** quarter mile time.

***

#### Class Activity - Types of Data, R Commands, and Graphics


***


- <span style="background-color:lightblue;">Y Variable</span> : the data we are after/ interested in


- Example with a Histogram

```{r}
hist(KidsFeet$birthmonth, breaks=seq(0.5,12.5,1))
```

***


- <span style="background-color:lightblue;">Quantitative Data</span> : data that is of units of measurement


Examples: length, width

```{r}
KidsFeet %>%
  group_by(sex) %>%
  summarise(mean(length),max(length)) %>%
  pander()
```


***

- <span style="background-color:lightblue;">Categorical Data</span> : a trait that cannot be measured 


Examples: sex, birth year, birth month

```{r}
table(KidsFeet$sex) %>%
  pander()
```


***

#### Class Activity - Reviewing It All




***

- Load and view the **starwars dataset**

```{r}

datatable(starwars, options=list(lengthMenu =c(3,10,30)), extensions="Responsive")

```


***

***PRACTICE*** : Use **starwars** data set
<br><br>



1. Use the starwars data set to create a meaningful **histogram** and supporting numerical summaries.


```{r}
hist(starwars$height)
```


***

```{r}
summary(starwars$height)%>%
  pander()
  
```

- you could also use favstats ( ), it gives you the standard deviation! 

***


2. Use the starwars data set to create a meaningful **boxplot** (preferably side-by-side boxplots) and supporting numerical summaries.

```{r}
boxplot(height~eye_color, data=starwars)
```

- this table was using favstats!!

```{r}
favstats(height~eye_color, data=starwars)%>%
  pander()
```


***


3. Use the starwars data set to change your boxplot from above to a **dot plot**. Which is more meaningful for the data you selected?

```{r}
stripchart(height~species, data=starwars, method="stack", pch=19)
```

- same numerical summary as boxplot!

***


4. Use the starwars data set to create a meaningful **scatterplot** and compute the **correlation** of the data shown in the plot.

```{r}
plot(mass ~ height,data=starwars, pch=8)
```



- x and y can change, and the correlation stays the same
    - 1st one is height by mass and the 2nd one is mass by height

```{r}
cor(starwars$height, starwars$mass, use="pairwise.complete.obs")
```

```{r}
cor(starwars$mass, starwars$height, use="pairwise.complete.obs")
```


***


5. Use the starwars data set to create a meaningful **bar chart** and produce a table of counts to support the plot.

```{r}
barplot(table(starwars$hair_color))
```


- use a table! (Base R or favstats)

```{r}
table(starwars$hair_color)%>%
pander()
```


***

### Week 4 | Making Inference with t Tests

<br>


Studying from the [Making Inference](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/MakingInference.html and [t Test](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/tTests.html) pages and work on learning the t Tests!

There is are the: 

- One Sample t Test
- Paired Samples t Test
- Independent Samples t Test

They are all looking at MEAN!
</div>

<br>

***

#### Skills Quiz - t Tests


***



- **Inference** : the process of deciding whether patterns and trends in a sample of data from a population can be assumed to be true for the full population.
    - **does the group define the crowd?**

- A pattern can appear one way in a sample of data, but be completely different for the full population. 
    - That is why we use **statistical inference** to decide when it is safe (and when it is not) to conclude that a pattern in a sample holds in the full population.
    

***

- **P-value** : the most commonly used method of deciding when to reject a null hypothesis.
    - The things needed to compute a p-value are: 
        (1) A test statistic
        (2) A probabiltiy distribution of the test statistics that would be possible if the null hypothesis is true
        

-  If a researcher was using a signficance level of 0.05 and obtained a p-value of 0.419, they can safely conclude that the null hypothesis is true and that the alternative hypothesis is the false.
    - If P is high, keep the guy!


***

- **Decision Errors** : they are two types of errors, Type I or II, that indicate to us whether or not we made a mistake in:
    (1) Rejecting the null when it was true (Type I)
    (2) Accepting the null when it was false (Type II)


<br>


*For example:* In a typical U.S. Court of Law, the **null hypothesis** is that the person on trial is innocent.
- Thus, convicting an innocent man of a crime would be an example of a **Type I Error**
- And letting a guilty man go free would be an example of a **Type II Error**
    

***


<u>Parametric Distributions</u> 

- These are the 4 distributions you need to be familiar with: 
    (1) The Normal Distribution 
    (2) The t Distribution
    (3) The F Distribution 
    (4) The Chi-squared Distribution
    

<br> 

***


***PRACTICE*** :

<br>


(1) **In which distribution would an observation of x=5 be the most likely to occur?**


```{r}
 curve(dnorm(x,-5,2), -12, 20, col="skyblue", lwd=2, ylab="", n=1000, ylim=c(0,.81))
 curve(dchisq(x,5), from=0, add=TRUE, col="firebrick", lwd=2, n=1000)
 curve(dt(x,5), add=TRUE, col="skyblue4", lwd=2, n=1000)
 curve(df(x,10,10), from=0, add=TRUE, col="salmon", lwd=2, n=1000)
 legend("topright", legend=c("Normal(-5,2)","Chi-squared(8)","t(5)","f(2,5)"), lwd=2, lty=1, col=c("skyblue","firebrick","skyblue4","salmon"), bty='n')
```

- **Answer** : A chi-squared distribution with p = 8 

<br>


***



(2) **What set of sample values are most likely to be obtained if a sample of n = 5 observations is sampled from the normal distribution drawn by the below code in R?**

```{r}
curve(dnorm(x, -5, 2), from=-12, to=2, lwd=2, col="skyblue", ylab="Normal Distribution with Mean of -5 and Standard Deviation of 2")

abline(h=0, v=-5, lty=2)
```

- **Answer** : -8, 0, -5, -6, -3

***

<u>t Tests</u>

<br>

There 3 different t Tests: 

(1) <u>Paired Samples</u>

- requires that the sampling distribution of the sample mean of the differences is normally distributed

$$ H_0: \mu_\text{post - pre} = 0 $$
$$ H_a: \mu_\text{post - pre} \neq 0 $$

<br>


(2) <u>One Sample t Test</u>

$$ H_0: \mu = 5.8 $$

$$ H_a: \mu \neq 5.8 $$

<br>


(3) <u>Independent Samples t Test</u>
- requires two things: 

    (1) **The sample size of each group** : Both samples are **representitive** of the population
        - Simple Random Samples are the best way to do this

        
    (2) **The normality of the data in each group using a Q-Q Plot for each group's data** : The sampling distribution of the difference of the sample means (x¯1−x¯2) can be assumed to be normal.
         - This is a safe assumption when the sample size of each group is **30 or greater** or when the population data from each group can be assumed to be normal with a **Q-Q Plot**


$$ H_0: \mu_\text{Group 1} - \mu_\text{Group 2} = 0 $$

$$ H_a: \mu_\text{Group 1} - \mu_\text{Group 2} \neq 0 $$

<br> 

***



***PRACTICE*** : 

Use a subset of the CO2 dataset in R to answer the following questions. 

<br> 

(1) **Filter the dataset so that you are considering only the chilled plants where conc = 250.**

```{r echo=TRUE}
CO2.chilled.250 <- filter(CO2, Treatment == "chilled" & conc == 250)

```


<br>


(2) **What are the mean uptake values for each Type of plant (Quebec and Mississippi) for plants that are chilled at a concentration of 250?**

```{r}
CO2.chilled.250 %>%
  group_by(Type) %>%
  summarise(`Mean Uptake`=mean(uptake)) %>%
  pander()
```

<br>


(3) **A side-by-side <u>dot plot</u> is best for presenting this data visually because of the small sample sizes.**

- Shown in ***Base R*** graphics

```{r}
stripchart(CO2.chilled.250$uptake ~ CO2.chilled.250$Type, method="stack", xlab="Uptake", main="Quebec shows higher uptake values than Mississippi when plants are chilled at an amdient CO2 of 250", col=c("red","lightblue"), pch=15)
```


- Shown in ***ggplot2*** graphics

```{r}
ggplot(CO2.chilled.250, aes(x=uptake, fill=Type)) +
  geom_dotplot(dotsize = 3, binwidth = 0.5) +
  scale_fill_manual(values=c("Quebec"="lightblue","Mississippi"="coral")) +
  labs(title="Quebec shows higher uptake values than Mississippi when plants are chilled at an ambient CO2 of 250", x="uptake", y="count" )
  
```


- These two sample means are **clearly different**
    - However, these results are based on small sample Mississippi and Quebec plants, or if it was just a figment of this sample of data
    
***
    
<br>


To determine if we should conclude that this pattern holds for the full population, conduct an appropriate hypothesis test for the hypotheses:

$$ H_0: \mu_{Miss} - \mu_{Queb} = 0 $$
$$ H_a: \mu_{Miss} - \mu_{Queb} \neq 0 $$

<br>

<u>μMiss:</u>
- represents the (unknown) true mean CO2 uptake of ALL Mississippi plants that are chilled at a concentration of 250

<u>μQueb:</u>
- represents the true (unknown) mean CO2 uptake of ALL Quebec plants that are chilled at a concentration of 250


- **Answer** : Use an *Independent t Test*

<br> 

**(4) Report the test statistic, the parametric distribution being used for the test statistic (including degrees of freedom if appropriate), and p-value of the test.**

```{r}
CO2.Q <- filter(CO2.chilled.250, Type == "Quebec")

CO2.M <- filter(CO2.chilled.250, Type == "Mississippi")

t.test(CO2.Q$uptake, CO2.M$uptake, mu=0, alternative="two.sided", conf.level = 0.95) %>%
  pander()
```

This shows there is **sufficent** evidence to conclude that Quebec plants (that are chilled at a concentration of 250) truly do have a different mean uptake than Mississippi plants(that are chilled at a concentration of 250)
- In other words, it is **safe** to conclude that the pattern we have seen in the sample data holds for the full population. 


*Note that since our sample data shows that Quebec has a higher mean than Mississippi, and the p-value of the two sided test is significant, we can actually conclude that Quebec plants have higher CO2 uptake than Mississippi plants, on average. 

<br>


(5) **What requirements should be verified for this particular hypothesis test?**


(a) **The sample of Mississippi and Quebec plants need to be representative of the population.** 
- Hopefully they were selected randomly so that this would be satisfied.

(b) **The sampling distribution of the difference of the sample means  x¯1−x2¯ can be assumed to be normal.**
- This is a safe assumption when the sample size of each group is 30 or greater or when the population data from each group can be assumed to be normal.



*Note: before reporting our results, it is important to verify that the requirements of this hypothesis test were satisfied.


<br> 

(6) **How many plants of each Type (Mississippi and Quebec) are in the sample of data?**

- **Answer** : Look at the CO2.chilled.250 data set and count how many of each you got!
    - Mississippi Sample Size: 3
    - Quebec Sample Size : 3 
    

*Problem* : These are small sample sizes. You will need to create **Q-Q Plots** of each sample of data to determine if the data in each group is normal or not.

<br>


(7) **Make a Q-Q Plot of each group.**

```{r}
qqPlot(uptake ~ Type, data = CO2.chilled.250)
```


* Note that the sample sizes are very small (n=3 in each group). However, normality does not appear to be violated as none of the points go outside of the red dashed lines
- Thus, the requirements of the hypothesis test appear to be satisfied for these data.
    - We are safe to report the results of our hypothesis test.



***




#### Assessment Quiz - t Tests


(1) In a typical year, there are 52 weeks. However, 52 x 7 = 364, and as most of us know, there are 365 days in a year. This means that every year, at least one day gets to happen more than 52 times.

 

**Use appropriate R commands and the Births78 dataset to determine which day of the week in 1978 occurred 53 times.**



```{r}
Births78 %>%
  group_by(wday) %>%
  summarise(n()) %>%
  pander()
```
- **Answer** : Sunday



<br>


(2) Use the Births78 dataset in RStudio to test the following hypotheses.

 

H0:μWednesday−μThursday=0
Ha:μWednesday−μThursday≠0
 

**Find the p-value of the test.**

```{r}
Birf <-filter(Births78,wday %in% c("Wed","Thu"))

t.test(births ~ wday, data=Birf, mu=0, alternative="two.sided", conf.level=0.95)
```
- **Answer** : P-value = 0.8855

(3) **What two things are required to compute a p-value?**

- **Answer** : A test statistic and a sampling distribution of the test statistic.



***

#### Class Activity - Making Inference


<br>

**Studying from the [Making Inference](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/MakingInference.html) page!**
    
    
<br>

***


<u>Managing Decision Errors</u>


- hypothesis -> beliefs
    - null (initial) BELIEF, alternative BELIEF
    
- Goal: to discover **truth**

| &nbsp; | $H_0$ True | $H_0$ False |
|--------|------------|-------------|
| **Reject** $H_0$ | Type I Error | Correct Decision |
| **Accept** $H_0$ | Correct Decision | Type II Error |

<br>

***

***Alternatively, you can think about it like:***

| &nbsp; | $H_0$ True **love** | $H_0$ False **love** |
|--------|------------|-------------|
| Reject **your love** $H_0$ | Type I Error | Correct Decision |
| Accept **your love** $H_0$ | Correct Decision | Type II Error |

- Type I Error -> Action (Throwing away truth)
    - effected by alpha; small a, small error vice versa
-Type II Error -> (Failing to move to truth)
    - effected by beta; small b, small error vice versa
    
    
Alpha and Beta work inversely of eachother! 
- high a -> low b
- low a -> high b

<br>

***


<u>P-value</u>

- The P-value measures how much the evidence differs from what we expected under the null hypothesis(belief)
    - P = "probability" 
        - P is low, Ho must go! 
        - P is high, keep the guy! 


- null gives us a frame of reference from where the data COULD fall

***


***What two things are needed to get a p-value?***

<span style="background-color:yellow;">(1)Test Statistic</span>

<span style="background-color:yellow;">(2) Sampling Distribution of the Test Statistic</span>

<br>

***

<u>4 Parametric Methods</u>

(1) The Normal Distribution

(2) The Chi Squared Distribution

(3) The t Distribution

(4) The F Distribution 

<br>

- curve really tall -> really likely

- curve dips down -> very unlikely


***

<u>t Tests</u>

***Practice***

- Conduct a One Sample t Test: What is the average drive from Walmart to BYU-I Campus?

```{r message=FALSE}
t.test(cars$speed, mu=5, alternative="two.sided", conf.level=0.95) %>%
  pander
```

- Alternative hypothesis can be: 
    - "greater"
    - "less"
    - "two.sided"
    
- conf.level corresponds with significance level
    - conf. level = 1 - a
    - significance level = 1 - conf. level


***


#### Class Activity - t Tests


- P-value measures the probability of the test statistic (what is happening in front of you)
    - if something is impossible, then we should reject the belief(the null) since the possibility of that belief happening is close to 0


- Hypothesis = "WE BELIEVE"

<br>


***Practice:***

Finding P-values with t Tests!

<br>

<u>**One-Sample t Test**</u>

- What is the average foot size of 4th graders?

```{r}
t.test(KidsFeet$length, mu=28, alternative="two.sided", conf.level=0.95) %>%
  pander()

```

- We have sufficient evidence to believe that kids feet are not averaged aroun 28 cm. Instead, they are averaged around 24 to 25 cm.  


***


<u>**Paired Samples t Test**</u>

- Is the length measurement of someone's foot the same measurement of 3 width's of their foot? 

```{r}
t.test(KidsFeet$length, KidsFeet$width*3, paired=TRUE, mu=0, alternative="two.sided",conf.level=0.95) %>%
  pander()
```

- We have sufficent evidence to suggest that there is a difference between the measurement of length and the measurement of width, thus the measurements of 3 widths of their foot is not accurate. 



***


<u>**Independent Sample t Test**</u>


- In the 4th grade, who's feet length is greater, boys or girls?


```{r}
t.test(length ~ sex, data=KidsFeet) %>%
  pander()
```

- If you change "t.test" to "boxplot", you can provide a visual to support your claim! 

```{r}
boxplot(length ~ sex, data=KidsFeet)
```

- We have insufficent evidence to suggest that boys or girls feet are longer than the other. 

***




**Significance Value Setting**

- Love rejecting things? Use 0.1
    - looking for suspects

- You a strict with you value? Use 0.05

- Testing medicine effectiveness? Use 0.01
    - on trial
    
    
***



**Using qqPlot**

- Q stands for quantity! 
   - they are used for **testing assumptions**
   
   
- all you do is change "t.test" to "qqplot" 

```{r}
qqPlot(length ~ sex, data=KidsFeet)
```

- blue region(bounds of normality) is the tolerance space that allows us to call the data "normal" if the data appears within it (test results are great)
    - when there is data out of bounds, that makes the data not normal (mega skew)
    
    
- In order to fix out of bounds data, you need to increase your sample size

- When there is a heavy tailed (s shape), you need to decrease your sample size
    - When one side is dippin down low, it is skewed

***


### Week 5 | Wilcoxon Tests

<br>

Studying from the [Wilcoxon Tests](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/WilcoxonTests.html) page! (the first nonparameteric test we encounter!)

There are the: 

- Wilcoxon Signed-Rank Test
- Wilcoxon Rank Sum (Mann-Whiteny) Test
</div>

<br>

***


#### Skills Quiz - Wilcoxon Tests


- are NON-PARAMETRIC tests 
    - uses **sums of ranks and mathematical counting techniques** (a NON-PARAMETRIC distribution) to calculate the **p-value**
    - t test (that are PARAMETRIC) rely on **t distributions** (which are PARAMETRIC distributions) to calculate the p-value while Wilcoxon tests
    
<br>

- the distribution of the Wilcoxon Test Statistic can be **usefully approximated** by a normal distribution when the **sample size** of the data being used in the test is **large**
    - However, this distribution of the test statistic **can never be exactly normal because the test statistic can only ever be a whole number**
    
<br>

- **ignores the specific values** of the data and only utilize the relative positions of the data, i.e., **the ranks** (hence the reason they are often called Rank-Based Tests.)   

<br><br>


<u> **1) Wilcoxon Signed-Rank Test** </u>

<br>

- was originally created to test hypotheses about the **value of the median**, but can be used to test hypotheses about the ***mean when data is SYMMETRIC***

- A useful custom graphic that shows all of the data as well as a five-number summary of the data is a **box plot overlaid with a dot plot**

- The style of tests that can be performed with this test are: 

(1) One Sample: testing hypotheses about the **center of a distribution** (where the center is subtracted from each value).

(2) Paired Samples: testing hypotheses about the **center of the distribution** of differences.

<br> 

**Examples**

a. The [CornHeights](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/Analyses/Wilcoxon%20Tests/Examples/CornHeightsPairedWilcoxon.html) Analysis Example is shown below: 
    
```{r}
corn <- c(14, 56, 60, 16, 6, 8, -48, 49, 24, 28, 29, 41, -67, 23, 75)

boxplot(corn, horizontal=TRUE, col="cornsilk",
        main="Differences in Corn Plant Heights",
        xlab="Cross-Fertilized Height minus Self-Fertilized Height")
stripchart(corn, pch=16, method="stack", 
           col="darkgray", add=TRUE)
```
 
Here is the number summary and Wilcoxon test that goes along with it! 

```{r}
pander(summary(corn))
```

```{r}
pander(wilcox.test(corn, mu = 0, alternative = "two.sided", conf.level = 0.95))
```

*Quesitons based on the data above*: 


(1) What is the minimum value in that plot?

    - -67

(2) What is the p-value of this test?

    - 0.04126

<br>

b. The [Sleep](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/Analyses/Wilcoxon%20Tests/Examples/SleepPairedWilcoxon.html) Analysis Example is shown below:

```{r}
sleepWide <- sleep %>%
  spread(key=group, value=extra, sep="") %>%
  mutate(g1_minus_g2 = group1 - group2)
pander(sleepWide)

ggplot(sleepWide, aes(x=g1_minus_g2)) +
  geom_dotplot(binwidth = 0.1) +
  theme_bw()
```

```{r message=FALSE, warning=FALSE}
pander(with(sleep,
     wilcox.test(extra[group==1], extra[group==2], mu = 0, alternative = "two.sided", paired = TRUE, conf.level = 0.95)))
```

***


*Quesitons for the data set above:* 

(1) What does the null and alternative hypothesess look like?

$$
  H_0: \text{median of the differences} = 0
$$

$$
  H_a: \text{median of the differences} \neq 0
$$

<br>

(2) What is the p-value of this data? 

    - 0.009091

***


***PRACTICE:***

<br> 

<u> ***Part A: Background*** </u>

An online website reports that the 2010 Honda Odyssey should get 17 miles per gallon (mpg) during city driving conditions. A family has kept track of their gas mileage data for their 2010 Honda Odyssey sincle June 2020. They want to know if they are typically getting 17 mpg or not, and have provided us with a systematic sample of their mpg records. Run the following code in R to input their data into R.

```{r}
Honda_mpg <- data.frame(date = c("06/01/20", "08/19/20", "09/14/20", "01/07/21", "07/22/21", "08/16/21", "10/24/21", "12/31/21", "03/12/22", "05/04/22", "07/09/22", "10/12/22"), milesDriven = c(274.4, 266.8, 317.0, 113.4, 326.2, 62.6, 239.8, 242.0, 195.6, 106.4, 379.3, 325.4), gallonsUsed = c(12.42, 12.255, 15.224, 7.953, 16.378, 3.285, 13.230, 10.467, 9.310, 4.721, 16.958, 16.221))

datatable(Honda_mpg, options=list(lengthMenu =c(3,10,30)), extensions="Responsive")
```

***


<u>***Part B: Hypothesis***</u>

<br>

1b. Use a mutate statement to create an mpg column in the data set that divides the milesDriven by the gallonsUsed. (Keep the name of the dataset Honda_mpg as you add this new column.)

```{r}
Honda_mpgG <- Honda_mpg %>%
  mutate(mpg= milesDriven/gallonsUsed)

datatable(Honda_mpgG, options=list(c(3,10,30)))
```

2b.  We will use this mpg column to see if the family is getting 17 mpg, on median. (The median is being used here so that potential outliers do not overly sway the results.) **What are the hypotheses that are most appropriate for using the Wilcoxon Signed-Rank test on the mpg data?**


$$ H_0: \text{Median mpg} = 17 $$

$$ H_a: \text{Median mpg} \neq 17 $$

- We will use a significance level of: 

$$ α=0.05 $$

***


<u> ***Part C: Graphic and Anaysis*** </u>

<br>

1c. To visualize what the Wilcoxon Signed-Rand Test is testing,**create a dotplot in R of the mpg data that includes a vertical line representing the null hypothesis of our *test.**

```{r}
ggplot(Honda_mpgG, aes(x=mpg))+
    geom_dotplot(binwidth = .1) +   
  geom_segment(aes(x=17, xend=17, y=0, yend=0.5), color="skyblue") +
  geom_text(aes(x=17, y=.6), label="Median Hypothesized to be 17 mpg", color="skyblue") + 
  labs(title="2010 Honda Odyssey Gas Mileages", y="", x="Miles Per Gallon Vehicle Achieved on Systematically Selected Dates") + 
  theme(axis.ticks.y=element_blank(), axis.text.y=element_blank())
```

- Just looking at the plot, **the median looks to be higher than 17 mpg.**
    - due to the fact that most of the data points are more to the right
    
    
2c. **Conduct the actual hypothesis test in R using a Wilcoxon Signed-Rank test** to test if the mpg data has a median of 17:

```{r}
pander(wilcox.test(Honda_mpgG$mpg, mu = 17, alternative = "two.sided"))
```

3c. What is the value of the test statistic?
    - 74
    
4c.  What is the probability that the test statistic would be as extreme or more extreme than this value?
    - 0.00659
    
5c. What conclusion does the probability give us? 
    - There is sufficient evidence to reject the null hypothesis (p-value less than the significance level). We will conclude that the true median gas mileage of this vehicle is something different than 17 mpg.
    
<br>
    
6c.  Compute the **median** of this sample of gas mileages: 

```{r}
pander(summary(Honda_mpgG$mpg))
```

***It appears that this Honda Odyssey is getting closer to 21 miles per gallon on median rather than the listed 17 miles per gallon, which is great!***


***


<u> ***Part D: Wait... but like why does this work?*** </u>

<br>

We will now work through the test by hand to show how the test actually works!
-  it is useful for you to do it by hand once to gain some understanding on how rank-based tests, like this test, work

```{r}
ggplot(Honda_mpgG, aes(x=mpg)) +
  geom_dotplot(binwidth = .1) +
  geom_segment(aes(x=17, xend=17, y=0, yend=0.5), color="skyblue") +
  geom_text(aes(x=17, y=.6), label="Median Hypothesized to be 17 mpg", color="skyblue") +
  labs(title="2010 Honda Odyssey Gas Mileages", y="", x="Miles Per Gallon Vehicle Achieved on Systematically Selected Dates") +
  theme(axis.ticks.y=element_blank(), axis.text.y=element_blank())
```

**Pre-Step 1:** calculate what are called the "differences"

- The differences are obtained by subtracting the hypothesized value for the ***median*** in all observations
    - this is done with the following code: 
    
```{r}
pander(Honda_mpgG$mpg-17)

```

- Note: there is only **one negative difference** = only one value is below the hypothesized median of 17 mpg!
    - looking at the graph, it is there! 


***


**Step 1 & Step 2:**

<br>

- Write out the missing differences (using the list of differences that you just computed) in the sorted list of differences below. 
    - Be sure to place them in order from smallest in magnitude to the largest magnitude. 
    - Keep the negative sign on the one difference that is negative. 
        - Then, rank the differences!
        
Sorted Differences: 1.125742, **2.056317**, **-2.741230**, 2.916962,  3.060416, **3.822386**, 4.009667, 4.770706, 5.093398, 5.367024, 5.537598, 6.120283. 

|&nbsp;           |  |  |  |    |   |   |   |   |   |    |   |   |   |    |   |
|-----------------|--|--|--|----|---|---|---|---|---|----|---|---|---|----|---|
|**Differences**: |1.125742 | 2.056317 | **-2.741230** | 2.916962 | 3.060416 | 3.822386 | 4.009667 | 4.770706 | 5.093398 | 5.367024 | 5.537598 | 6.120283 | 
|**Ranks**:       |  1 | 2 | **-3** | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 |

*Remember: if there is a tie between two ranks, then the ranks of the two values are averaged together. Fortunately, there were no ties in this data.

***


- Now, we have two options: 

(1)  Add up the sum of the *positive* ranks

(2) Add up the sum of the *absolute value* of the negative ranks.

    - Since there is only one negative rank, that seems like the easiest way to proceed, which would give us a sum of  |−3|=3
    - HOWEVER!! We need to match how R computed the test statistic, summing up the positive ranks (1+2+4+5+6+7+8+9+10+11+12 = 75)


Looking back at the test in R, **what is the letter that R uses for the Wilcoxon Test statistic?**
- **V**
    - This shows that the test statistic of the Wilcoxon Test in R is the sum of the ranks from the **positive ranks**!!

```{r message=FALSE, warning=FALSE}
pander(wilcox.test(Honda_mpgG$mpg, mu = 17, alternative = "two.sided"))
```


***


<u>Calculating the p-value:</u> 

<br>

- The p-value is calculated by adding up the **possibility** of all the possible sums of ranks that are as extreme or more extreme than the observed sum of ranks!

<br>

(1) Come up with all of the possible values that the test statistic could have been obtained
    - Two possibilities could occur: 
    
    a. **None** of the values were less than 17 mpg (all ranks would belong to the **positive group**)
        -  the rank of -3 would now become a positive 3, this increasing the total sum of positive ranks from what it was to being the total of all ranks
        - The sum of the **positive ranks** would have been: **78**
        
    b.**All** of the values were less than 17 mpg (all ranks would belong to the **negative group**) 
        - The sum of the positive ranks would have been: **0**
        
<br>        
        
- Therefore, the test statistic of the Wilcoxon Test could be any *whole number* between **0 and 78**

<br>

(2) Compute the probability of each such value occurring (This is known as the distribution of the test statistic!)
    - then, ta-dah! p-value! 

<br>

***



***MORE PRATICE***

<br>

As shown in the help file of this dataset, ?Davis, this data is about individuals who were "engaged in regular exercise." 
    - Load *Davis* data set and filter the data to be **just men**
    - Question: **is it true that men who work out regularly over or under report their weight?**

```{r}
Davy <- filter(Davis, sex == "M")

datatable(Davy, options=list(3,10,30))
```

<br>
 
 
***


<u>Hypothesis:</u>

<br>

- Let the null hypothesis be that the median of the differences in actual and reported weights are equal to zero. Let the alternative hypothesis be that they are different from zero.
    - Make sure the order is Actual - Reported weight
        - we are comparing each man's actual weight to their reported weight

$$ H_0: \text{median of the differences} = 0 $$
$$ H_a: \text{median of the differences} \neq 0 $$

***


- **compute the differences between the actual weight and reported weight for each person in the dataset using a mutate statement**
    - Name the new column containing these differences something like "weight_diff"

```{r}
Davis2 <- Davy %>%
  mutate(weight_diff = weight - repwt)

hist(Davis2$weight_diff, col="gray", main="Difference in Individual's Acutal and Reported Weight (kg)", xlab="Positive values mean under-reported weight (Acutal - Reported)", ylab="Number of Individuals")
```

<u>Observations of the Graph</u>

<br>

1 ) recognize that any value to the left of zero shows men whose weight difference is negative
    - **their reported weight was a larger number than their actual weight**
        - the order of subtraction we used, actual - reported, proves this
        
2 ) more than half the data is to the left of 0
    - more than half the men had a negative weight difference, or that they **said they weighed more than they acutally did**
    - the median weight difference is **-0.5 pounds**, showing the median reporting is half a pound higher than what the men actually weighed

***

<u>Wilcoxon Signed-Rank Test of *Davis* dataset</u>

```{r}
pander(wilcox.test(Davis2$weight_diff, mu=0, alternative="two.sided", conf.level= 0.95))
```

<br>

***

<u>Conclusion</u>

<br>

**FAIL TO REJECT THE NULL AT THE 0.05 LEVEL OF SIGNIFICANCE**

<br>

There is **insufficient evidence** to conclude that men who work out regularly over report their weight by 0.5 lbs on median. 

<br><br>

***



<u> **2) Wilcoxon Rank Sum Test** </u>

<br>

- the **nonparametric** equivalent of the **Independent Samples t Test**
    - Quantitative Y | Categorical X (2 groups)

- The style of test(s) that can be performed with this test are:

    (1) Independent Samples: test of the difference in the **location of the centers** of two distributions

<br>

**Example:**

- The [BugsSpray](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/Analyses/Wilcoxon%20Tests/Examples/BugSprayWilcoxonRankSum.html) Analysis Example is shown below: 

```{r}
bugspray <- data.frame(Killed = c(68,68,59,72,64,67,70,74,
                                  60,67,61,62,67,63,56,58),
                       Spray = rep(c("A","B"), each=8))
```

| Spray Concentration | Percent Killed |
|---------------------|----------------|
| A | 68, 68, 59, 72, 64, 67, 70, 74 |
| B | 60, 67, 61, 62, 67, 63, 56, 58 |


    - here are the graphics and number summaries
    
```{r, echo=FALSE}
boxplot(Killed ~ Spray, data=bugspray, boxwex=.3,
        xlim=c(.5,2.5), col="gray", main="Bugspray A More Effective", xlab="Brand of Bug Spray", ylab="Percent of Bug Colony Exterminated")
stripchart(Killed ~ Spray, data=bugspray, method="stack", 
           vertical=TRUE, pch=16, col="steelblue4", cex=1.25,
           add=TRUE)
```

```{r}
bugspray %>%
  group_by(Spray) %>%
  summarise(min = min(Killed), median = median(Killed), mean = mean(Killed), max = max(Killed), sd = sd(Killed), `Number of Trials` = n()) %>%
pander(caption="Summary of Bug Spray Effectiveness")
```

```{r, comment=NA, echo=FALSE, warning=FALSE}
pander(wilcox.test(Killed ~ Spray, data=bugspray, mu = 0, alternative = "two.sided", conf.level = 0.95))
```

***


*Questions based on the data above?*

(1) What is the test statsic? (W)

    - 55
    
(2) What is the probabilty of getting a test statistic at least this extreme if the null hypothesis is true?

    - p = 0.01771


***



**PRACTICE***

<br>

- we will be using the *Duncan* data set filtered to only show "wc" and "prof" types of occupations
    - Question: **Are prestige scores typically higher for professional and managerial type occupations than for white-collar occupations?**
    
    
```{r}
DuncanDO <- filter(Duncan, type %in% c("wc","prof"))

datatable(DuncanDO, options= list(lengthMenu = c(3,10,30)))
```
    
<br>
  

<u>**Hypotheses:**</u>



$$ H_0: \text{Median}_\text{prof} - \text{Median}_\text{wc} = 0 $$
$$ H_a: \text{Median}_\text{prof} - \text{Median}_\text{wc} > 0 $$ 

<br> 

- below is the Wilcoxon Rank Sum Test

```{r}
pander(wilcox.test(DuncanDO$prestige[DuncanDO$type == "prof"], DuncanDO$prestige[DuncanDO$type == "wc"], mu =0, alternative = "greater", conf.level = 0.95))
```

<br>

***


<u>Conclusion:</u>

<br>

**REJECT THE NULL**

The is **sufficent evidence** evidence to conclude that prestige scores are typically higher for professional and managerial occupations than for white-collar occupations. To quantify the difference in prestige scores and how much higher they are for professional and managerial occupations than for white-collar occupations, we can create side-by-side boxplots and a table of the five-number summary.

<br>

***


<u>Box plot of the *Duncan Prestige scores between professors and white collar workers* data set:</u>

```{r}
boxplot(prestige ~ type, data=droplevels(DuncanDO), col= c("dodgerblue3", "white"), xlab="Type of Profession", ylab="Prestige Score (Higher is More Prestigious)", main= "Duncan Prestige of Occupations Data")
```

***



<u>Numerical Summary Table of *Duncan Prestige scores between professors and white collar workers*</u>


```{r}
pander(favstats( prestige ~ type, data=droplevels(DuncanDO)))
```

From the results, we see that professional type occupations have a median prestige score that is **more than double** the median prestige score of white-collar occupations!

<br>

***


***MORE PRACTICE***

<br>

This data was collected from the 2008-09 academic school year for colleges in the U.S. "as part of the on-going effort of the college's administration to monitor salary differences between male and female faculty members.
    - Question : **"Which professors earns a higher median wage, male or female?"**
    
<br>

- When filtering the data, columns of data that are a "factor" can be frustrating to work with (as seen in the previous question). So sometimes it is easier to mutate(...) the data and change columns that are "factors" to "character".

```{r}
Salaries2 <- Salaries %>%
  mutate(rank = as.character(rank))
```

***


- Now, **graph in R in order to explore any differences in salaries of male and female Professors at colleges across the U.S. in 2008-09.**

```{r}
boxplot(salary ~ sex, data=Salaries2, col="wheat", ylab="Annual Salary (USD)", xlab="", main="Do Male Prof's Earn Higher Median Wage?")
```

<br>

***



<u>Nummerical Summaries Table for Salaries for **Male and Female Professors**</u>

```{r}
SalariesProf <- filter(Salaries2, rank == "Prof")

pander(favstats(salary ~ sex, data=SalariesProf))
```

- Run an Wilcoxon Test in R to **determine if salary is stochastically different for one of the genders of Professors.**

```{r}
wilcox.test(salary ~ sex, data=SalariesProf, mu=0, alternative="two.sided", conf.level=0.95)
```


***


<u>Conclusion: </u>

<br>

**FAIL TO REJECT THE NULL**

<br>

There is **insufficient evidence** to conclude that salary is stochastically different for one of the genders of professors. In looking back at our numerical summaries and graphical summary, we see that it appears that male Professors are being paid a median salary that is **$3738** more than the median female salary. Further research into why this discrepancy exists would be warranted though this discrepancy itself does not prove anything further than a pay gap in median salaries of Professors. We can't claim that discrimination or any other reason in particular has caused the pay gap without further research. But this difference certainly warrants further research into the "why" behind the gap. The gap has been shown to exist by the analysis above. We just don't have any explanation as to why it is there.


***


#### Assessment Quiz - Wilcoxon Tests


(1) Use the *Salaries* dataset in R to find the number of male and female assistant professors in the dataset.

- This can be done in two different ways! 


1. Use the favstats() command


```{r}
SalariesAss <- filter(Salaries, rank == "AsstProf")

pander(favstats(salary ~ sex, data=SalariesAss))
```

2. Use the group_by() and summarise() commands

```{r}
Salaries %>%
  group_by(rank, sex) %>%
  summarise(n()) %>%
  pander()
```



***


(2) Create an appropriate graphic using the Salaries dataset in R that would allow you to compare the distribution of salaries for faculty in discipline A ("theoretical") and discipline B ("applied") departments.


```{r}
boxplot(salary ~ discipline, data=SalariesAss, col="white", main="Faculty From U.S. Colleges", xlab="Discipline", ylab="Salaries")

```


<span style=“background-color:yellow;”>The graphics that can me used for Wilcoxon Rank Sum Test are BOXPLOTS & DOT PLOTS, not SCATTER PLOTS</span>



***


(3) Perform an appropriate Wilcoxon Test of the following hypotheses using the Salaries dataset in R.

 

H0:median discipline A salaries−median discipline B salaries=0
Ha:median difference of salaries≠0


Select the response below showing the correct test statistic, p-value, and conclusion of the test based on a significance level of 0.05.


```{r}
pander(wilcox.test(salary ~ discipline, data=SalariesAss))
```

<span style=“background-color:yellow;”>The correct test is the WILCOXON RANK SUM TEST. Since the p-value of 0.0007516 is less than 0.05, we reject the null hypothesis.</span>




***


#### Class Activity - Wilcoxon Tests


<u>What is the Wilcoxon Test?</u>

- not concerned about the distance between values
- we care about the **location shift/ difference in ranks**
         - sample size, standard deviation does NOT matter! 
         

***


<u>Steps to Wilcoxon Rank Sum (Mann-Whitney Test):</u>

(1) Order Data

(2) Rank it

(3) Pull out ranks from each group

(4) Add the ranks

(5) What is the possibility of each sum being possible? 

(6) Use Frank Wilcoxon spikey distribution to determine the extremities of the data
    - **Finite and discrete** (normal distribution is **infinite and continuous**)
    
    
<br>

- When they detect **any ties in ranks**, they shift from a **finite** realm to a more **continuous** realm in order to compute it properly
    
    
    
```{r}
A <- c(68, 68, 59, 72, 64, 67, 70, 74)

B <- c(60, 67, 61, 62, 67, 63, 56, 58)

wilcox.test(A,B)
```


***

<u>What is the question(s) this test would answer?</u>

**Examples:**

- Do the two genders tend to pick their favorite numbers differently? 

- Do the test scores of students from school X tend to be higher than the test score of students from school Y?


*can stretch to situations where averages(means) don't necessary matter/ are interesting
    
<br>

*Wilcoxon test secret -> hands*
 
<br>

Null Hypothesis = No shift (fingers overlap)

Alternative = shift (fingers are not overlapped)
- 0 shift -> full overlap in ranks
- No 0 shift -> no overlap in ranks



- stochastically -> GENERALLY speaking

***


<u>Why is the Wilcoxon test used?</u>

- deduces it down to ranks without having to analyze numerical  
    - studying medians


```{r}
wilcox.test(length ~ sex, data=KidsFeet)
```

- test statistics -> the sum of the ranks from one of the groups is ____.


- p-value -> tells us that the p-value of the test statistic



*does not make any mathematical assumptions about the data (nonparametric test) 




***


#### Class Activity - Wilcoxon Tests, Boxplots, and P-Values


***Review***

a. Which function would you use to add a new column to your dataset?

    - mutate()
    
<br>

b. What is the goal of statistics?
    - to quantify things (the chance of this happening if this is the truth!)

    - test statistic shows us the measurement of how far we are from the null
    
<br>

c. Which plots work best to display one quantitative variable that is separated into 2 or more groups?

    - box plot & dot plot
    - Box plots + Dot plots are great to show off this kind of data (because it shows the median!)
        - Median line closer to the **bottom** of the box plot = **Right Skewed**
        - Median line is closer to the **top** of the box plot = **Left Skewed**
    
<br>


d. What two things are needed to calculate a p-value?

    - a test statistic and a sampling distribution of that test statistics
    
<br>

e. When would an independent samples t test be inappropriate to use but a Wilcoxon Rank Sum Test be appropriate?

    - Independent Samples t Test : looks at **average**
        - needs **raw data** (times, measurements, etc.)
    
    - Wilcoxon Rank Sum Test : looks at **median**
        - needs **ranks** (first place, second place, etc.)
    
<br>   
  
f. What is an appropriate set of hypotheses to use for an independent samples t test? For a Wilcoxon Rank Sum Test?

    - Independent Samples t Test : mu

    - Wilcoxon Rank Sum Test : Median or "stochastically"
    
<br>

***


<u>Using the Wilcoxon Test Applet</u>

This is the Wilcoxon Test Applet! : [Wilcoxon Test Applet](https://shiny.byui.edu/StatClass/)


*"Salt makes it hard to realize just where the salt shakers are." - Brother Saunders*

- using p-value and graphs together to make the right decision
    - graph is critical, valuable piece to visualize the data
    - but the p-value supports/ debunks the graph 

***



***PRACTICE***

(1) Explain what it means for the null hypothesis of a Wilcoxon Rank Sum Test to be true.

    - 0 location shift (hands overlap)

<br>

(2) Explain what it means for the alternative hypothesis of a Wilcoxon Rank Sum Test to be true.

    - Location shift (no hands overlap)

<br>

(3) Explain how boxplots for small sample sizes can be misleading.

    - highly sensitive to sample size, don't reveal the sample size
        - overlay dot and box plot to show sample size as well as number summary! 

<br>

(4) Explain how the p-value helps us judge whether or not the null hypothesis or alternative hypothesis is the truth based on the evidence shown in a box plot.

    - help us to show and give a definitive  comparison between the two groups 

 
<br>

***


### Week 6 | Two-way ANOVA

<br>


Studying from the [ANOVA](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/ANOVA.html) page and its one quantitative column and the two categorical columns!

There are the: 

- One-Way ANOVA Test
- Two-Way ANOVA Test

</div>

<br>

***


#### Skills Quiz - ANOVA


- What is an example of a "factor"?

    - Hair color! 
    
    
***

    

- Run the code "View(warpbreaks)" in R. Use that data set to identify each of the following as either a "factor" or a "level" of a factor.

```{r}
datatable(warpbreaks,options=list(c(3,10,30)))
```

- What are the **factors** for this data set? 

    (1) Tension
    
    (2) Wool
    
<br>
    
- What are the **levels** of factors? 
    
    (1) Medium (from the Tension factor)
    
    (2) High (from the Tension factor)
    
    (3) A (from the Wool factor)
    
    (4) Low (from the Tension Factor)
    
    (5) B (from the Wool factor)

- Why is the "Breaks" column not a factor? 

    - The column contains **quanitative** values than the qualitative values
    
<br>

- How do you change something into a factor?

    - use the function ***as.factor()*** and insert your desired column! 
    
    
<br>

***


***One-way ANOVA***

- What does each element of this equation mean?

$$ Y_{ik} = \mu_i + \epsilon_{ik} $$

- The true population mean for group i

$$\mu_i$$
    
- The data points.

$$Y_ik$$

- The error term for the  kth data point of the  ith group. In other words, how far that data point is from the true mean  μi.
    
$$\epsilon_ik$$
    
***


- the ***null hypothesis*** of ANOVA is that all means are **equal**

- the ***alternative hypothesis*** is that **at least one mean differs**

***


- The **most important** idea behind ANOVA is to understand the ***two terms***:
    
    - **Between** groups variance
    
        - which measures the variability of the **sample means**
    
    - **Within** groups variance
        
        - which measures the variablitiy of the **data within each group**
        
        
***


- The null hypothesis of ANOVA is assumed to hold true when these two variances are roughly equal
    - It is rejected when the **between** groups variance is significantly larger than the **within** groups variance as measured by the pvalue obtained from: 
        
        (1) the ANOVA **F statisitc** 
        
        (2) An F Distribution with p1 and p2 degress of freedom
        
        
<br>

- The F Statistic of ANOVA is found dividing the **between groups** variance by the **within groups** variance

    - the between group variance is also sometimes called a Mean Square or Mean Sq for short
    
    
<br>

- The p-value of the ANOVA test:

    -  **Becomes small when the F statistic becomes large**
    
<br>
    

- The F statisitc becomes large when the means are showing greater variance than the data
    
        - as this contradicts the possibility of all means being sampled from the same distribution

    
***


***Practice***

- Test your ability to read ANOVA output from R  using the *Friendly* data set


<u>**Part 1: Writing out our Hypothesis**</u>

- The ANOVA hypotheses for the *Friendly* data set are shown below

    - Group 1 is SFR, Group 2 is Before, and Group 3 is Meshed
    

$$ H_0: \mu_1 = \mu_2 = \mu_3 = \mu $$

$$ H_a: \mu_i \neq \mu \ \text{for at least one} \ i$$

- What do these say? 

    - **These hypotheses simply state that the mean words recalled under the three different conditions, SFR, Before, and Meshed are the same (the null) or that at least one differs (the alternative).**
    
    
***


<u>**Part 2: Conducting an ANOVA test**</u>

- This is the ANOVA test for the *Friendly* data set

```{r}
friendly.aov <- aov(correct ~ condition, data=Friendly)

summary(friendly.aov) %>%
  pander()
```

- The p-value of the ANOVA test is **0.0232** showing that there is **sufficient evidence** to conclude that **at least one of the means differs from the others.**


<br>

- Note that this ANOVA p-value is the probability that the test statistics from an F distribution with **2 and 27 degrees of freedom** is as extreme or more extreme than the **observed value of 4.341**, assuming the null hypothesis is true

<br>

    - This p-value is only meaningful if two important assumptions can be made: 
    
    (1) that the error term  ϵik is normal and 
    (2) that the variance  σ2 of the error term is constant, i.e., the same in all groups. 

***


<u>**Part 3: Checking Requirements**</u>

- Two plots were created in R to check the two requirements of ANOVA for the *Friendly* data set 

```{r}
par(mfrow=c(1,2))

plot(friendly.aov, which=1:2)
```


(1) The Residual Plot

    - shows the **constant (equal) variance** assumption 
    - is reasonable when the points for each group have roughly the same spread (Pinky test!)
    
(2) The Q-Q Plot of the Residuals

    - shows the **normal error terms (residuals)** assumption
    - is reasonable when the points hold to the line rather well
 
 
***


<u>**Part 4: Conclusion**</u>

- In the *Friendly* data set, the conclusion of this ANOVA test is that one of the means differs
    - This is not a very informative conclusion, but it does give us permission to explore the data.
    
  
- Consider the following plot in R that shows the means of each group with a line as well as the points from each group (jittered or wiggled a little in the horizontal direction so that they don't overlap).
    
```{r}
xyplot(correct ~ condition, data=Friendly, jitter.x=TRUE, type=c("p","a"))
```

- From this plot (and the p-value of the ANOVA test) we can determine that the **SFR** has a significantly **lower** mean than the other two groups, which happen to both be equal to each other

    - The plot thus gives us greater insight than the ANOVA test provided, but without the ANOVA, we could not trust the results in the plot. A meaningful plot and hypothesis test always go hand in hand.
    
<br>

***


***Two-Way ANOVA***

<br>

<u>***There are FIVE main elements included in TWO-WAY ANOVA:***</u>

<br> 

(1) The Background

    - This includes the data used in the analysis
    
<br>
    
(2) The *three* sets of hypotheses of the two-way ANOVA

    - These include two one-way style sets of hypotheses as well as the interaction hypothesis.
    
<br>
    
(3) The R code for performing and summarizing the two-way ANOVA

<br>

(4) The two diagnostic plots that verify the appropriateness of performing a two-way ANOVA on the warpbreaks data.

    - When an ANOVA is appropriate, we can have confidence in our results. When the ANOVA is not appropriate, we cannot be confident that our results are meaningful, but we usually still interpret the results anyways. We just have to be honest that our analysis might not be a good way to analyze the data.
    
<br>
    
(5) The graphical and numerical summaries that allow us to make more detailed conclusions about what is going on in the data.

    - Remember, in an ANOVA analysis, when we reject the null hypotheses that "all means are equal" we don't know which means differ from the others, so the graphics and the numerical summaries are very important for helping us see what is going on in the data. 
    - Of course, if the p-value was large and we failed to reject the null hypothesis, then the graphical or numerical summary for that particular hypothesis is not really needed as "all means are equal" in that case.
    
***


***MORE PRACTICE***

- We will use the `ToothGrowth` data set for a practice ANOVA analysis! 

<br> 

***


***BACKGROUND***

<br>

The `ToothGrowth` data set is measuring the effect of **vitamin C** dosage levels (0.5, 1, and 2 mg/day) and delivery methods (using either orange juice or ascorbic acid) on length of growth in teeth (odontoblasts, cells responsible for tooth growth) in **guinea pigs**. 

    - all this information will can be found using the ?(dataset name) in the Console

```{r}
datatable(ToothGrowth, options = list(c(3,10,30)))
```

<br>

***


***HYPOTHESES***

<br>
 - Test the following hypothese all at the same time with a single Two-Way ANOVA Test in R. 
 
<br>

(1) Hypotheses about the effects on tooth growth due to the supplement type:

$$ H_0: \mu_{VC} = \mu_{OJ} $$

$$ H_a : \mu_{VC} \neq \mu_{OJ} $$ 
 
 <br>
 
 (2) Hypotheses about the effects on tooth growth due to the dosage level:
 
$$H_0 : \mu_{0.5} = \mu_{1.0} = \mu_{2.0} = \mu$$
 
$$ H_a : \mu_i \neq \mu \text{for at least on dosage level i = 0.5, 1.0, or 2.0} $$

<br>

(3) Hypotheses about the effects on tooth growth due to the interaction of supplement type and dosage level:

$$ H_0 : \text{the effect of dosage on tooth growth is the same for all levels of supplement type.} $$

$$ H_a: \text{the effect of dosage on tooth growth is not the same for all levels of supplement type.}$$

<br>

***


***TWO-WAY ANOVA RESULTS***

- Perform the two-way ANOVA in R for the ToothGrowth data that matches the hypotheses stated above. 

    - Don't forget to use **as.factor( )** for any columns that should be factors but are not currently. 

```{r}
toothANOVA <- aov(len ~ supp + as.factor(dose) + supp:as.factor(dose), data=ToothGrowth)

summary(toothANOVA)
```

***Note: The interaction term should have degrees of freedom equal to the number of combinations possible in the interaction term (minus 1, minus the degrees of freedom for the first factor, minus the degrees of freedom for the second factor).**

<br>

***


The Two-Way ANOVA test shows the supplement type **does have** a significant effect on tooth growth (**p-value = 0.000231**), dosage level **has** a significant effect on tooth growth (**p-value = 4.046e-18, sometimes reported as "<2e-16"**), and the interaction dosage level and supplement type **has** a significant effect on tooth growth (**p-value = 0.021860**). 

<br> 

***


***APPROPRIATENESS OF THE TWO-WAY ANOVA***

<br>

- Check the appropriateness of your two-way ANOVA on the ToothGrowth data by creating the two required diagnostic plots.

```{r}
par(mfrow = c(1,2))

plot(toothANOVA, which = 1:2)
```

- Looking at these two diagnostic plots, the following ANOVA assumptions are satisfied: 

(1) Constant Variance of the Error Terms

(2) Normally Distributed Error Terms

<br> 

- when both assumptions are fulfilled, we can have confidence in the results of this analysis!

<br>

***


***GRAPHICAL AND NUMERICAL SUMMARIES AND CONCLUSIONS***

<br>

- We will make three different graphical and numerical summaries to support the three hypotheses

<br>

(1) The effect of **supplement type** (which is significant as shown by the p-value in the ANOVA above the supp term) is demonstrated in the following plot and numerical table.

    - Run this code in R to obtain the plot: 

    - xyplot(len ~ supp, data=ToothGrowth, type=c("p","a"), main="Orange Juice Showing Longer Teeth on Average in Guinea Pigs", ylab="Length of Odontoblasts", xlab="Vitamin C Delivery Method")
    
```{r}
xyplot(len ~ supp, data=ToothGrowth, type=c("p","a"), main="Orange Juice Showing Longer Teeth on Average in Guinea Pigs", ylab="Length of Odontoblasts", xlab="Vitamin C Delivery Method")
```

    - Run this code in R studio to obtain the table: 
    
        - ToothGrowth %>%
            group_by(supp) %>%
            rename(`Supplement Type` = supp) %>%
            summarise('Mean Length of Odontoblats`=mean(ToothGrowth), .groups="drop") %>%
            pander(caption="Mean Tooth Length According to Supplement Type")
            
```{r}
ToothGrowth %>%
  group_by(supp)%>%
  rename(`Supplement Type` = supp) %>%
  summarise(`Mean Length of Odontoblasts`= mean(len), .groups ="drop")%>%
  pander(caption="Mean Tooth Length According to Supplement Type")
```

<br>

***


(2) The effect of **Dosage Level** is demonstrated in the following plot. 

    - Run this code in R: 
    
        > xyplot( len ~ dose, data=ToothGrowth, type=c("p","a"), main="Higher Doses Showing Longer Teeth on Average in Guinea Pigs", ylab="Length of Odontoblasts", xlab="Dosage Level in mg/day")
        
```{r}
xyplot( len ~ dose, data=ToothGrowth, type=c("p","a"), main="Higher Doses Showing Longer Teeth on Average in Guinea Pigs", ylab="Length of Odontoblasts", xlab="Dosage Level in mg/day")
```

<br>

- Perform the R code necessary to compute the numerical summary

    - Run this code: 
    
        > ToothGrowth %>%
  group_by(as.factor(dose))%>%
  rename(`Dosage Level` = dose) %>%
  summarise(`Mean Length of Odontoblasts`= mean(len), .groups ="drop")%>%
  pander(caption="Mean Tooth Length According to Dosage Level")

```{r}
ToothGrowth %>%
  group_by(as.factor(dose))%>%
  rename(`Dosage Level` = dose) %>%
  summarise(`Mean Length of Odontoblasts`= mean(len), .groups ="drop")%>%
  pander(caption="Mean Tooth Length According to Dosage Level")
```

<br>

***


(3) The effect of **the interaction of supplement type and dosage level** is demonstrated in the following plot. 

    - Run the following code: 
    
        > xyplot( len ~ dose, data=ToothGrowth, groups=supp, type=c("p","a"), auto.key=TRUE, main="Delivery Methods are Equal at High Doses of VC", ylab="Length of Odontoblasts in Guinea Pigs", xlab="Dosage Level of Vitamin C in mg/day")
        
```{r}
xyplot( len ~ dose, data=ToothGrowth, groups=supp, type=c("p","a"), auto.key=TRUE, main="Delivery Methods are Equal at High Doses of VC", ylab="Length of Odontoblasts in Guinea Pigs", xlab="Dosage Level of Vitamin C in mg/day")
```

<br>

- Perform the R code necessary to compute the numerical summary

    - Run this code: 
    
        > 
        
```{r}
ToothGrowth %>%
  group_by(supp, dose) %>%
  summarise(ave=mean(len), .groups="drop") %>%
  spread(dose, ave) %>%
  pander(caption="Mean Tooth Length by Supplement Method and Dosage Level")
```

***


***CONCLUSION***

- Here is an example of the kind of conclusion that should be at the end of an ANOVA analysis. 

*For longer teeth in Guinea Pigs, we recommend delivering 2.0 mg/day of Vitamin C with either deliver method (Orange Juice or Ascorbic Acid). However, if that level of Vitamin C doses are too expensive to achieve, then a dosage of 1.0 mg/day of Vitamin C delivered with Orange Juice is recommended. This will result in a mean length gain of 22.7, which is quite close to the length gain of 26.1 that is achieved at a 2.0 mg/day dosage level. We recommend avoiding the 0.5 mg/day dosage level completely as that resulted in tooth growth gains of only 13.2 and 7.98 when delivered through orange juice or ascorbic acid, respectively.*

***


#### Assessment Quiz - ANOVA


<br>

(1) In a certain student's ANOVA analysis the "Between groups variance" was 18.52 while the "Within groups variance" was 4.9. What was the value of the test statistic of their ANOVA test?
 
    - **ANSWER : F = 3.78**

<br>
    
- As shown in the Explanation tab of the ANOVA page of the Math 325 Notebook, the ANOVA test statistic is an F statistic. It is calculated by taking the "Between groups variance" and dividing this by the "Within groups variance". Thus 18.52/4.9 = 3.779592 which rounds to 3.78.

<br>

***


(2) Make three Two-Way ANOVA graphs looking at the length, domhand, and sex columns of the KidsFeet dataset.

    - Based on your p-values and these graphs, which of the following is a correct conclusion to reach?


<div style="display: flex; justify-content: space-around;">
<div>

```{r}
xyplot(length ~ domhand, data=KidsFeet, type=c("p","a"), main="4th Grade Students", col='dodgerblue', xlab="Child's Dominant Hand", ylab="Length of Child's Longest Foot") 
```

</div>
 <div>
 
```{r}
xyplot(length ~ sex, data=KidsFeet, type=c("p","a"), main="4th Grade Students", col='dodgerblue', xlab="Child's Gender", ylab="Length of Child's Longest Foot") 
```

 </div>
</div>

```{r}
xyplot(length ~ sex, data=KidsFeet, groups=domhand, type=c("p","a"), main="4th Grade Students", auto.key=list(corner=c(1,1)))
```

    - **ANSWER : Among fourth graders, right handed boys have longer feet on average than left handed boys while the opposite is true for girls.**
    
    
***


(3) In the KidsFeet dataset, what is the average length of feet for fourth grade boys that are left handed?

    - **ANSWER : 24.1 CM**
    
        - found through the interaction table of the gender and dominant hand 


```{r}
KidsFeet %>%
  group_by(sex,domhand) %>%
  summarise(ave=mean(length), .groups="drop") %>%
  spread(domhand, ave) %>%
  pander(caption="Mean Foot Length According to Gender and Dominant Hand")
```


<br>

#### Class Activity - One- Way ANOVA

- What does ANOVA stand for?
    - Analysis Of Variance! 
    - tests several means simultaneously
    
<br>

- Rejecting a null -> I know that there is a different

- Fail to reject the null -> I don't know if they differ!

<br>


- If you conduct a ton of tests on one data set, the probability of a Type 1 Error increases!
    - we would expect that one of the tests
    - ex. conduct two tests, nearly doubles the Type 1 Error
    
- Thankfully, ANOVA protects against those potential errors
    
    
<br>
    
    
***

    

<u>Hypothesis of ANOVA</u>


- Null hypothesis = all means are equal

- Alternative hypothesis = at least one mean is different
    - problem(s): doesn't tell us which one is different or how many are different!
    
<br>


- Pr(>)F
    
    
***


<u>Explain the words "factor" and "level"</u>

- Factor 
    - gender (column)
    - use as.factor() for your x value! 

- Levels 
    - male & female (the values that happen in each column)
    
<br>
    
    
***

<u>Explain the difference between a one-way and a two-way ANOVA</u>

<br>

Degrees of freedom = Number of Groups - 1

Residual Degrees of freedom is the left over of the total data participants

<br>

(1) One-Way 
    - One Factor
    - Can only test one idea (ex. gender )
    - ignore deep questions




(2) Two-Way
    - Two Factors
    - Can test a different ideas (ex. gender with age, hardwork and faith) that create powerful interactions
    - Can does what two one way ANOVAs can do and THEN can see if those two factors can dance!!
        - ex. Wool -> 1 DF (2 Groups)
        - Tension -> 2 DF (3 Groups)
        - Wool:Tension -> 2 DF (Would be 7 Groups, BUT the separate columns pay 5 of it so you only need two!)

        

***


<u>Reading a Residuals vs. Fitted plot</u>

- Fitted values = means

- the vertical stretches of dots shows the number of means -> looking for a consistent vertical spread between each mean (constant variability)

- Pinky and Thumb (vertical line?) test
    - if you have to stretch or double your fingers, you are getting into the danger zone! 
    
    
***


#### Class Activity - Two- Way ANOVA


- Looking at the `DayCare.RmD` file! 


(1) How many Day Care Centers were included in the study?

    - 10


(2) How many Centers were included in the treatment group? The Control Group?

    - 6 and 4


(3) During which weeks did the researchers simply observe the Day Care Centers?

    - first four



(4) During which weeks was the fine applied to the treatment group?

    - w 5-16


(5) During which weeks was the fine removed from the treatment group?

    - w 17


(6) What was the main response variable of interest that the researchers recorded each week on the Day Care Centers?


    - The number of late children (does the fine change behavior?)
    
    
***


<u>How to change from wide to long data?</u>

**Use pivot_longer/wider funtion!**

Late <- pivot_longer(late, #Start with wide data set late
               cols = starts_with("Week"), #Find columns to gather from long data
               names_to = "Week", #New column name of Weeks in long data
               names_prefix = "Week", #Remove prefix of Week from Week1, Week2, ...
               names_transform = list(Week= ~as.numeric(.x)), #Make Week a numeric column of data
               values_to = "NumberofLateChildren") #Name of data column in long data
               
***


***How to change the dataset format with "pivot"***

- First off, load up the dataset

with 

```{r}
late <- read_csv("C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/late.csv")

Late <- pivot_longer(late,
               cols = starts_with("Week"),
               names_to = "Week",
               names_prefix = "Week",
               names_transform = list(Week= ~as.numeric(.x)),values_to = "NumberofLateChildren")
```

(1) Change to "long" data for use in R:

    **Late <- pivot_longer(late,**

(2) Start with wide data set late

    **cols = starts_with("Week"),**
               

(3) Find columns to gather from long data
               
               
    **names_to = "Week",**

(4) New column name of Weeks in long data 

    **names_prefix = "Week",**
               
(5) Remove prefix of Week from Week1, Week2, ... etc!

    **names_transform=list(Week=~as.numeric(.x)),**

(6) Make Week a numeric column of data
              
    **values_to = "NumberofLateChildren") #Name of data column in long data**



***


- Use the Late data set to add a new column to the data set that translates Weeks (a numeric column) into a column that is a factor called "Period". This factor Period should have three levels: "Before the Fine", "Fine Applied", and "Fine Removed".

```{r}
Late <- Late %>%
  mutate(
    Period = case_when(
      Week %in% c(1:4) ~ "Before the Fine",
      Week %in% c(5:16) ~ "Fine Applied",
      Week %in% c(17:20) ~ "Fine Removed"))

table(Late$Period) %>%
  pander()
```

***


- Practice writing hypotheses! As well as, fill in the blanks for the third set of hypotheses and check your work with your peers, a TA, or the teacher.

***


(1) <u>Factor 1 Hypothesis</u>

$$
H_0: \mu_\text{Control Group} = \mu_\text{Fine Group}
$$

$$
H_a: \text{The average number of late children differs for at least one Group}
$$

***


(2) <u>Factor 2 Hypothesis</u>

$$
H_0: \mu_ \text{Before the Fine}= \mu_ \text{Fine Applied}
= \mu_ \text{Fine Removed}
$$

$$
H_a: \text{The average number of late children differs fro at least one Period}
$$

***


(3) <u>Interaction Hypothesis (the dance!)</u>

$$
H_0: \text{The effect of the Fine on the average number of late children picked up does not depend on the period of the study.}
$$
$$
H_a: \text{The effect of the fine on the average number of late children changes in at least one Periods of the study.}
$$

***


Two Way ANOVA time! 


- Create an ANOVA table with the data using 
    
    - aov( Quanitative data ~ Factor 1 + Factor 2 + Facotr 1:Factor 2, data= ____ )
    

```{r}
myaov <- aov(NumberofLateChildren ~ Treatment + Period + Treatment:Period, data=Late) 

summary(myaov)
```

- Lots of Residuals -> good! (lots of power)

- Not a lof of Residuals -> no good! (not too much power)


If you can't get the right p-value, there is something wrong with your factors! 

- use as.factor()

***



Plot time! 

- Three different plots to make

    1. Factor 1
    
    2. Factor 2
    
    3. The Interaction (the visual dance!)

```{r}
xyplot(NumberofLateChildren ~ as.factor(Treatment), data=Late, type= c("p","a"))
xyplot(NumberofLateChildren ~ as.factor(Period), data=Late,type= c("p","a"))

xyplot(NumberofLateChildren ~ as.factor(Period), data=Late,auto.key = list(c(1,1)))
```


- Control Group did better and Fine Group did worse!!

    - incentivizing behavior ruins that ethical bond with others
    
***


Diagnostic Plots Time! 

```{r, fig.height=3}
par(mfrow=c(1,2))
plot(myaov, which=1:2)
```

- Remember pinky thumb test! (Constance variance)


***


### Week 7 | The Kruskall-Wallis Test

<br>

Studying from the [Kruskal-Wallis Test](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/Kruskal.html) page and its the second nonparametric test that we see thats quantitative to multiple categorical!

</div>


<br>

***


#### Skills Quiz - Kruskal-Wallis Test


- Use the `SaratogaHouses` data set for the following **Kruskal-Wallis Test**! 

```{r}
table(SaratogaHouses$fuel)
```

<br>

***


Say a homeowner in Saratoga County, New York is curious about whether upgrading their home from an oil heating fuel system to either a gas or electric fuel system would increase the resale value of their home. 

Use the `SaratogaHouses dataset` in R to answer the question, **"which heating system results in the highest distribution of home resale values (price)?**

- Null Hypothesis : The **price of homes** with either gas, oil, or electric heating fuel systems **all come from the same distribution**

- Alternative Hypothesis : At least one type of fuel systems results in a **different distribution of prices of homes**

```{r message=FALSE, warning=FALSE}
kruskal.test(price ~ fuel, data=SaratogaHouses)
```


The **Kruskal-Wallis Rank Sum Test** results in a p-value of 2.2e-16, meaning there is **sufficent** evidence to **conclude the hypothesis** the distribution of **prices of homes** is different for at least one of the **fuel system types**. 

<br>

***


- The graphic that best depicts the results of the Kruskall-Wallis Test...

    - **A Boxplot!!**
    
```{r}
boxplot(price ~ fuel, data=SaratogaHouses)
```

<br>

Questions regarding the previous graph: 

(1) Which **fuel type** results in the highest median price of homes?

    - **Gas**
    
(2) What is the **highest median price**?

    - This value cannot be accurately determined from a plot! So we need the summary table! 
    
```{r}
SaratogaHouses %>%
  group_by(fuel) %>%
  summarise(`Median Price`=median(price), .groups="drop") %>%
  pander(caption="Median Prices by Fuel Type")
```


***


- Study the `wages` and `pigweights` analyses examples and analyze them for the following information! 

<br>

    - BYU-Idaho Student wages Example
    
        1. Test Statistic (**Chi-squared**) = **2.3262**
        
        2. Degrees of Freedom = **3**
        
        3. P-value = **0.5075**
        
        4. Conclusion? : This is **insufficent** evidence to reject the null hypothesis that the samples are from the same distribution.  
        
        
<br>

    - Pigweights Example
    
        1. P-value = **0.009663**
        
        2. Conclusion? : There is **sufficent** evidence to conclude that at least one group is from a different population than the others. 
        
***


- Read through the `Kruskal-Wallis` Explanation Tab

    1. How many steps are there to calculating a p-value for the Kruskal-Wallis Test?
    
        - **6 Steps**
        
<br>
        
    2. Match the following letters with their meaning in the Kruskal-Wallis Test.
    
        - N : The total number of observations from all samples.
        
        - C : The number of samples being compared.
        
        - \bar{R_i} : The mean of the ranks for each sample  i
        
        - R_i : The sum of the ranks belonging to sample  i. 
        
        - H : The test statistic of the Kruskal-Wallis Test.
        
        - n_i : The size of sample i. 

<br>
        
    3. How is the p-value found?
    
        - The p-value of the Kruskal-Wallis Test is found using the **chi squared distribution** with **C - 1 degrees of freedom** as an approximation to the actual nonparametric distribution of the test! 
        
<br>

    4. What does the bottle-cap data tell us?
    
        - The p-value of **0.059** says that we have **insufficent evidence** to conclude that nay of the groups are from different distributions. In other words, we will continue to assume that hte groups all come from the same distribution. 
        
        
***


#### Assessement Quiz - Kruskal-Wallis


1. Use the Salaries dataset in R, library(car), to test the hypotheses and report the test statisic and the conlusion of your test


$$ H_0 : \text{The distribution of salary is the same for Associate, Assistant, and Full Professors.} $$
$$ H_a : \text{The distribution of salary is different for at least one type of Professor.} $$

```{r}
kruskal.test(salary ~ rank, data=Salaries)
```

```{r}
boxplot(salary ~ rank, data=Salaries)
```


```{r}
Salaries %>%
  group_by(rank) %>%
  summarise(`Median Salary`=median(salary), .groups="drop") %>%
  pander(caption="Median Salary by Rank")
```

    - Test statistic is 194 from a chi-squared distribution with 2 degrees of freedom, which has a very small p-value. It appears that Full Professors typically earn the most and Assistant Professors typically earn the least.
    
        - The boxplot shows Full Professors have the highest distribution, Associate Professors the next highest, and Assistant Professors have the lowest distribution
        
<br> 

2. A waiter at a local restaurant collects data on how much they earn in tips (per person) when groups of people each pay separately for their meal versus when one member of the group pays for everyone. Their very small sample of data is as follows.

<br>

Group ID	Number of People in Group	Type of Payment	Total Tip Amount	Per Person Amount
1	5	Each Paid	$16	$3.20
2	3	One Paid for All	$10	$3.33
3	4	One Paid for All	$9	$2.25
4	4	Each Paid	$15	$3.75
5	6	Each Paid	$21	$3.50

<br>
 

Which hypothesis test would be most appropriate for deciding if the average per person tip amount is higher when people that eat in groups each pay individually for their meal?

    - While techincally you could use a Kruskal-Wallis Test, when there are only *two groups in the data, it is equivalent to performing a Wilcoxon Rank Sum Test*. So **the best decision would be to use the Wilcoxon Rank Sum Test**. 
        - An Independent Samples t Test might also be useful if the data can be *assumed to be normal*. However, given the *small sample size* and the fact that there are *no repeated values*, the Wilcoxon Test is the safer option.
        
<br>
        
3. What is the correct way to describe the test statistic, H, of the Kruskal-Wallis Test?

    - It is **a combined measurement of how much the average ranks differ between each group**p-0.
    
        - ANOVA uses the F = (Between groups variance)/(Within groups variance) test statistic.
        
        - The One Sample t Test measures how far the sample mean is from mu for its test statistic.
        
        - The Wilcoxon Rank Sum test sums the ranks from one of the groups for its test statistic.


***


#### Class Activity - Idea Approval for the Week 11 "Consulting Opportunity or Research Project" 


***Option 1***

<br>

Find a data analysis opportunity for a client* where you use data the client provides to answer research questions they have.

- *A client could be a small business owner (maybe your parents, an aunt, uncle, or someone else you personally know),
a teacher you TA for,
your boss you currently work for or that you worked for in the past,
an old high school teacher,
or even a roommate.
The client could potentially even be yourself if say you were trying to buy a new car or 
some other important item. However, this analysis has the greatest chance of contributing to your resume if it is performed for someone else rather than just yourself. But if you did something for yourself that was say "blog worthy" and that other people would also find interesting, then that could be meaningful as well.

<br>

In the space below, state which option you are choosing, and provide details on how you are going to make this work. Over the next couple of weeks your teacher will follow up with you to see how you are doing with implementing your idea. If you have any concerns about whether your idea is a good idea or not, discuss that in person with your teacher right away. Make sure you only submit an idea that you are fairly confident will work well in the space below. 

<br>

- Make sure it powerful and correct!!

- START WITH THE QUESTION OR GRAPH

- hide technical details

- use two way thinkin

<br>

"We are offering a free one time data consultation under the supervision of my stats professor who has a P.H.D., the Biostatistics Program Director."

    - Don't make it like you need their help, make it to where they need your help!

look at how early assignments in the course can predict future success? 



***


#### Class Activity - Kruskal-Wallis Test


- nonparametric equivalent of a *One-Way ANOVA* -> rank based test 

    - (not caring about data values! )
    
- will have to use as.numeric() function to make columns into mumeric values

- for groups (C) use mutate(new column name = interaction(group 1, group 2))

- 


<br>

Any **similarities** this test has to the ***Wilcoxon Rank Sum test***? Discuss any **differences**?

- Similarities

    - both are nonparametric tests! 
    
    - don't get a lot of powerful insights

- Differences

    - Testing Groups
    
        - Wilcoxon = 2 groups
        - Kruskal-Wallis = 3 groups! Can dump all the groups at once
  
    - Test Statisitic
        
        - Wilcoxon (F)
        - Kruskal-Wallis (H)

            - C -> number of samples/groups (need 3 or more!)
            - N -> Total of all sample sizes
            - n_i -> each individual sample (n_i = n_1 + n_2 ...)
            - R bar _i -> the mean rank for each sample

***



What can you conclude when the p-value of the Kruskal-Wallis Test is significant?

- Significant : bet your money on that choice!

- Not Significant: using p-value to say "maybe" / suggesting that this is more likely to be the 

    - Tell them where to go from what the data does or doesn't give (what's the next step?)


***


What are the hypotheses of the Kruskal-Wallis Test?

- Null -> nothing different from each other

- Alternative -> at least one of these are different 

<br>

***


A question to ask when analyzing data: 

***What am I using this data for? What will this answer?***

<br>

***


Use the Friendly dataset from library(car) in R to perform a Kruskal-Wallis Test.

<br>

```{r}
kruskal.test(correct ~ condition, data=Friendly)
```

<br>

- Make a box plot! (boxplot or ggplot)

```{r}
boxplot(correct ~ condition, data=Friendly)
```

<br>

- Make a table! 

```{r}
favstats(correct ~ condition, data=Friendly) %>% 
  pander()
```

***THESE ARE THE THREE THINGS YOU WILL NEED FOR A KRUSKAL-WALLIS TEST!!***


***


### Week 8 | Simple Linear Regression

<br>


Studying from the [Linear Regression](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/LinearRegression.html) page and its 2 quantitative to 2 quantitative test! 
</div>


<br>

***


#### Skill Quiz - Simple Linear Regression



<u>There are five assupmtions of the simple linear regression model:</u>

1. Constant Variance

2. Independent Errors

3. Normal Errors

4. Fixed X Values

5. Linear Relation

<br>

- Questions about the assumptions: 

1. Which regression assumption(s) does the **residuals versus fitted values plot** diagnose?

  - Linear Relation 
    
  - Constance Variance

2. Which regression assumption(s) does the **Q-Q Plot of the residuals** diagnose?

  - Normal Errors
  
3. Which regression assumption(s) does the **Residuals versus Order plot** diagnose? *(Remember, this plot can only be created when the data was collected in a specific order.)*

  - Independent Errors


- Perform the following simple linear regression in R: 

```{r}
plot(Height ~ Volume, data = trees)

trees.lm <- lm(Height ~ Volume, data = trees)

abline(trees.lm)
```


<br>

- Check the assumptions!

```{r}
par(mfrow=c(1,2))

plot(trees.lm, which=1:2)

par(mfrow=c(1,1))
```

- Simple Linear Regression does not appear appropriate for these data. Which of the regression assumptions appear to be violated?

    - ***Constant Variance***
    
***


- Consider the Old Faithful Guyser (Yellowstone National Park) dataset in R!

```{r}
datatable(faithful,list(option=c(3,10,30)))
```


1. Perform a simple linear regression to determine if the expected waiting time to the next eruption can be modeled by the length of the previous eruption.

```{r}
plzwork <- lm(waiting ~ eruptions, data=faithful)

pander(summary(plzwork))
```

$$\hat{Y_i} = \underbrace{33.4744}_\text{Intercept} + \underbrace{10.7296}_\text{Slope}X_i$$

- Even though both the slope and intercept terms are **significant**, *the intercept term is not of interest*

  - This is because we are not interested in eruptions that last for zero minutes!
  
- The *slope term is of interest*

  - The slope tells us the increase in teh expected waiting time to the next erruption for every 1 minute increase in the length of the current erruption.
  
  - Specifically, ***for every one minute increase in length of current eruption, we would expect to wait <u>10.7296 minutes longer</u> (on average) until the next eruption!***
  
<br>

***


2. Verify that this linear regression is appropriate by making the residuals versus fitted-values and normal Q-Q Plot of the residuals in R.

```{r,fig.height=3}
par(mfrow=c(1,3))

plot(plzwork,which=1:2)

plot(plzwork$residuals)
```

- What are some statements that describe what the plots show us:

  - **It appears safe to assume that the errors are normally distributed.**
  
  - **It appears safe to assume that variance of the errors is constant.**
  
  - **The regression model appears to be appropriate to use for interpretation.**
  
<br>
  
***


3. Produce a scatterplot in R of this linear regression.

```{r}
plot(waiting~eruptions, data=faithful)
abline(plzwork)
```

- Say the most recent eruption of the Old Faithful Guyser lasted for 3.5 minutes. Vistors should expect to wait **70 minutes** until the next erutption, on average. 

<br>

***


#### Assessment Quiz - Simple Linear Regression


1. Use the KidsFeet dataset from library(mosaic) to perform a regression of the length of a child's foot (Y) on the width of a child's foot (X).

- What is the estimated slope and intercept of the least squares regression line for this data?

```{r}
chillin <- lm(length ~ width, data = KidsFeet)

pander(summary(chillin))
```

  - **Intercept: 9.82**
  
  - **Slope: 1.66**

<br>
  
***


2. Continue using the KidsFeet regression that you performed in the last problem. In other words, the regression of the length of a child's foot (Y) on the width of the child's foot (X).

- Plot the regression in R. 
```{r}
plot(length ~ width, data= KidsFeet)

abline(chillin)
```

<br>

***


3. The residual plot from the regression of the length of a child's foot on the width of the child's foot (using the KidsFeet dataset from library(mosaic)) shows the following residual plots.

- Write a statement that most correctly interprets these plots.

```{r}
par(mfrow=c(1,3))

plot(chillin,which=1:2)

plot(chillin$residuals)
```

  - **The "Residuals vs Fitted" plot shows the data to be linear and to have constant variance. However, the "Normal Q-Q" plot shows some evidence of non-normality of the residuals.**

***


#### Class Activity - Simple Linear Regression

- Here is the Regression Applet! 

    - [https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_en.html](https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_en.html)
    
    - Dots are unique and beautiful! They **represent individuals and help to show us a trend**

***


<u>The Mathematical Model of Regression (including the equation of the best-fit line).</u>

$$
  \underbrace{Y_i}_\text{Some Label about the Y-axis} = \overbrace{\beta_0}^\text{y-int} + \overbrace{\beta_1}^\text{slope} \underbrace{X_i}_\text{Some Label about the X-axis} + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0, \sigma^2)
$$
- True Regression Relation = the law (the trend)

- Error Term = the individual in accordance to the law

- Error Term Normally Distributed = the difference between the predicted values from the actual values (the errors) follow a normal distribution

<br>

- THE LINE IS THE AVERAGE!!!!!!! (best fit line)

    - Y hat is the line (the prediction)
    
        - ex. what the weather might be tommorrow
    
    - Y is the individiual point
        
        - ex. what the weather is today

***


<u>How to interpret the slope and intercept in a Simple Linear Regression.</u>

- **Slope** : how much the average is changing/ the ***change in average y - value***

    - its is **NOT** the ***average change in y***
    
    - individual results ***will*** vary! 
    
    - Zero slope = no trend/ relationship!
    
<br>

- **Intercept** : the average y-value when x is zero/ our starting place 

    - Beta (big b) -> the truth
    
    - lil b -> the estimate


***


<u>What a residual is in a regression?</u>

- The distance you are from the line! 

    - Positive -> over priced

    - Negative -> under priced


***


<u>How to measure the correlation of a regression.</u>



***


<u>How to diagnose the appropriateness of a regression for a given data set.</u>

- Three plots are needed :

    1. Residuals vs Fitted
    
        - trying to fit with a straight line (looking to see if the data is bent)
        
        - there is more to the story! 
    
    2. Normal Q-Q Plot
    
    3. Residuals vs. Order 
    
        - love the sight of chaos!



***


<u>How to perform a regression in RStudio and locate the slope and intercept from the output summary.</u>

- go to [Linear Regression](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/LinearRegression.html) page and the R instructions let you know!!

<br>

***


#### Class Activity - Simple Linear Regression (Part 2)

***


Review the following: 

1. Slope of a line, including the statistics symbol for it.

2. Y-intercept of a line, including the statistics symbol for it.


$$Y_i = \beta_0 + \beta_1X_i + \epsilon_i \text{ where } \epsilon_i ~ N(0,\sigma^2)$$

- $\beta_0$ : True y-intercept

- $\beta_1$ : True Slope 

  - $H_0 : \beta_1 = 0$
    
  - $H_a : \beta_1 \neq 0$
    
<br>

$$\hat{Y}_i = b_0 + b_1X_i$$
 
- $\hat{Y}_i$ : Estimated line

- $b_0$ : Sample y-intercept

- $b_1$ : Sample Slope


<br>

$$E\{Y_i\} = \beta_0 + \beta_1X_i$$

- $E\{Y_i\}$ : True Line

<br>

3. The "error" term.

$$
  \epsilon_i = \underbrace{Y_i}_{\substack{\text{Observed} \\ \text{Y-value}}} - \underbrace{E\{Y_i\}}_{\substack{\text{True Mean} \\ \text{Y-value}}} \quad \text{(error)}
$$

- the distance between the dot $Y_i$ and the true line $E\{Y_i\}$ 

<br>


4. Least squares.

<br>
    
5. The five simple linear regression regression assumptions.


<div style="padding-left:60px;color:darkgray;font-size:.8em;">
Each assumption is labeled in the regression equation below.
</div>

<span style="color:darkgray;">Regression Equation</span>
$$
  Y_i = \underbrace{\beta_0 + \beta_1 \overbrace{X_i}^\text{#4}}_{\text{#1}} + \epsilon_i \quad \text{where} \ \overbrace{\epsilon_i \sim}^\text{#5} \overbrace{N(0}^\text{#2}, \overbrace{\sigma^2}^\text{#3})
$$

1. The regression relation between $Y$ and $X$ is linear.
2. The error terms are normally distributed with $E\{\epsilon_i\}=0$.
3. The variance of the error terms is constant over all $X$ values.
4. The $X$ values can be considered fixed and measured without error.
5. The error terms are independent.

***


Compare and contrast the different variables: 

- $\epsilon_i$ : the truth/ hidden thing we can't see

    - errors "created" the data
  
- $r_i$ : residual for individual

    - residuals are compusted after using the data to "recreate" the line


<div style="padding-left:30px;">

| Residual $r_i$ | Error $\epsilon_i$ |
|----------------|--------------------|
| Distance between the dot $Y_i$ and the estimated line $\hat{Y}_i$ | Distance between the dot $Y_i$ and the true line $E\{Y_i\}$. |
| $r_i = Y_i - \hat{Y}_i$ | $\epsilon_i = Y_i - E\{Y_i\}$ |
| Known | Typically Unknown |

</div>

- The residuals are known values and they estimate the unknown (but true) error terms

  

<br>

- $\beta_0$ : True y-intercept

- $b_0$ : Estimated y-intercept


<br>

- $\beta_1$ : True Slope

- $b_1$ : Estimated Slope

<br>

- $E\{Y_i\}$ : true line/ law

- $\hat{Y}_i$ : estimate of truth (average, predicted, etc.)

- $Y_i$ : you! (based on true but unique!), i = individual (an individual's value)

<br>

```{r, echo=FALSE}
set.seed(6)
beta0 <- 5
beta1 <- 0.8
N <- 100
epsilon <- rnorm(N,sd=0.5)
x <- rbeta(N,5,5)*10 
y <- beta0 + beta1*x + epsilon
plot(x,y, pch=20, xlab="", ylab="", main="Regression Relation Diagram", xaxt='n', yaxt='n', xlim=c(1,8), ylim=c(5,13))
tmp <- legend("topleft", lty=c(2,0,1), legend=c(expression(paste(E, group("{",Y,"}"), " is the true regression relation (usually unknown)")), expression(paste(Y[i], " is the observed data")), expression(paste(hat(Y), " is the estimated regression relation"))), bty='n', cex=0.8, y.intersp=1.3)
points(.5*tmp$text$x[1]+.5*tmp$rect$left[1], tmp$text$y[2], pch=20)
abline(beta0,beta1, lty=2)
xylm <- lm(y ~ x)
abline(xylm)
```

Something to ponder: The true line, when coupled with the error terms, "creates" the data. The estimated (or fitted) line uses the sampled data to try to "re-create" the true line.

We could loosely call this the "order of creation" as shown by the following diagram.

```{r, fig.height=3}

par(mfrow=c(1,3), mai=c(.2,.2,.4,.1))
plot(y ~ x, col="white",  main="A Law is Given", yaxt='n', xaxt='n')
curve(beta0 + beta1*x, add=TRUE, lty=2)
plot(y ~ x, pch=16, main="Data is Created", xaxt='n', yaxt='n')
curve(beta0 + beta1*x, add=TRUE, lty=2)
plot(y ~ x, pch=16, xaxt='n', yaxt='n', main="The Law is Estimated")
curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt='n', xaxt='n')
curve(beta0 + beta1*x, add=TRUE, lty=2)
```

| A Law is Given     | Data is Created     | The Law is Estimated     |
|--------------------|---------------------|--------------------------|
| $E\{Y_i\} = \beta_0 + \beta_1 X_i$ | $Y_i = E\{Y_i\} + \epsilon_i$ | $\hat{Y}_i = b_0 + b_1 X_i$ |
| The true line is the "law". | The $Y_i$ are created by adding $\epsilon_i$ to $E\{Y_i\}$ where $E\{Y_i\} = \beta_0 + \beta_1 X_i$. | The law is estimated with $\hat{Y}_i$ which is given with `lm(...)`. |

<br>

***


<div style="padding-left:30px;color:darkgray;">
Expand each element below to learn more.
</div>

<span style="color:steelblue;font-size:.8em;padding-left:160px;">Regression Cheat Sheet</span> <a href="javascript:showhide('regressioncheatsheet')" style="font-size:.6em;color:skyblue;">(Expand)</a>

<div id="regressioncheatsheet" style="display:none;font-size:.7em;">

| Term | Pronunciation | Meaning | Math  | R Code | 
|------|----------------|-------|--------|---------|
| <span class="tooltipr">$Y_i$<span class="tooltiprtext"> `$Y_i$`</span> </span><span class="tooltipr">| "why-eye"      | The data | <span class="tooltipr"> $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0, \sigma^2)$<span class="tooltiprtext"> `$Y_i = \beta_0 + \beta_1 X_i +` `\epsilon_i \quad \text{where} \` `\epsilon_i \sim N(0, \sigma^2)$`</span> </span><span class="tooltipr"> |`YourDataSet$YourYvariable` |
| <span class="tooltipr">$\hat{Y}_i$<span class="tooltiprtext"> `$\hat{Y}_i$`</span> </span><span class="tooltipr"> | "why-hat-eye" | The fitted line |<span class="tooltipr"> $\hat{Y}_i = b_0 + b_1 X_i$<span class="tooltiprtext"> `$\hat{Y}_i = b_0 + b_1 X_i$`</span> | `lmObject$fitted.values` |
| <span class="tooltipr">$E\{Y_i\}$<span class="tooltiprtext"> `$E\{Y_i\}$`</span> </span><span class="tooltipr"> | "expected value of why-eye" | True mean y-value | <span class="tooltipr">$E\{Y_i\} = \beta_0 + \beta_1 X_i$<span class="tooltiprtext"> `$E\{Y_i\} = \beta_0 + \beta_1 X_i$`</span> | `<none>` |
| <span class="tooltipr">$\beta_0$<span class="tooltiprtext"> `$\beta_0$`</span> </span><span class="tooltipr"> | "beta-zero" | True y-intercept | `<none>` | `<none>` |
| <span class="tooltipr">$\beta_1$<span class="tooltiprtext"> `$\beta_1$`</span> </span><span class="tooltipr"> | "beta-one" | True slope | `<none>` | `<none>` |
| <span class="tooltipr">$b_0$<span class="tooltiprtext"> `$b_0$`</span> </span><span class="tooltipr"> | "b-zero" | Estimated y-intercept | <span class="tooltipr">$b_0 = \bar{Y} - b_1\bar{X}$<span class="tooltiprtext"> `$b_0 = \bar{Y} - b_1\bar{X}`</span> | `b_0 <- mean(Y) - b_1*mean(X)$` |
| <span class="tooltipr">$b_1$<span class="tooltiprtext"> `$b_1$`</span> </span><span class="tooltipr"> | "b-one" | Estimated slope |<span class="tooltipr">$b_1 = \frac{\sum X_i(Y_i - \bar{Y})}{\sum(X_i - \bar{X})^2}$<span class="tooltiprtext"> `$b_1 = \frac{\sum X_i(Y_i - \bar{Y})}` `{\sum(X_i - \bar{X})^2}$`</span> | `b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 )` |
| <span class="tooltipr">$\epsilon_i$<span class="tooltiprtext"> `$\epsilon_i$`</span> </span><span class="tooltipr"> | "epsilon-eye" | Distance of dot to true line | <span class="tooltipr">$\epsilon_i = Y_i - E\{Y_i\}$<span class="tooltiprtext"> `$\epsilon_i = Y_i - E\{Y_i\}$`</span> | `<none>` |
| <span class="tooltipr">$r_i$<span class="tooltiprtext"> `$r_i$`</span> </span><span class="tooltipr"> | "r-eye" or "residual-eye" | Distance of dot to estimated line | <span class="tooltipr">$r_i = Y_i - \hat{Y}_i$<span class="tooltiprtext"> `$r_i = Y_i - \hat{Y}_i$`</span> | `lmObject$residuals` |
| <span class="tooltipr">$\sigma^2$<span class="tooltiprtext"> `$\sigma^2$`</span> </span><span class="tooltipr"> | "sigma-squared" | Variance of the $\epsilon_i$ |<span class="tooltipr">$Var\{\epsilon_i\} = \sigma^2$<span class="tooltiprtext">`$Var\{\epsilon_i\} = \sigma^2$`</span> | `<none>` |
| <span class="tooltipr">$MSE$<span class="tooltiprtext"> `$MSE$`</span> </span><span class="tooltipr"> | "mean squared error" | Estimate of $\sigma^2$ | <span class="tooltipr">$MSE = \frac{SSE}{n-p}$<span class="tooltiprtext">`$MSE = \frac{SSE}{n-p}$`</span> | `sum( lmObject$res^2 ) / (n - p)` |
| <span class="tooltipr">$SSE$<span class="tooltiprtext"> `$SSE$`</span> </span><span class="tooltipr"> | "sum of squared error" (residuals) | Measure of dot's total deviation from the line |<span class="tooltipr">$SSE = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2$<span class="tooltiprtext">`$SSE = \sum_{i=1}^n` `(Y_i - \hat{Y}_i)^2$`</span> | `sum( lmObject$res^2 )` |
| <span class="tooltipr">$SSR$<span class="tooltiprtext"> `$SSR$`</span> </span><span class="tooltipr"> | "sum of squared regression error" | Measure of line's deviation from y-bar | <span class="tooltipr"> $SSR = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2$<span class="tooltiprtext">`$SSR = \sum_{i=1}^n` `(\hat{Y}_i - \bar{Y})^2$`</span> | `sum( (lmObject$fit - mean(YourData$Y))^2 )` |
| <span class="tooltipr">$SSTO$<span class="tooltiprtext"> `$SSTO$`</span> </span><span class="tooltipr"> | "total sum of squares" | Measure of total variation in Y | <span class="tooltipr">$SSR + SSE = SSTO = \sum_{i=1}^n (Y_i - \bar{Y})^2$<span class="tooltiprtext">`$SSR + SSE = SSTO = \sum_{i=1}^n` `(Y_i - \bar{Y})^2$`</span> | `sum( (YourData$Y - mean(YourData$Y))^2 )` |
| <span class="tooltipr">$R^2$<span class="tooltiprtext"> `$R^2$`</span> </span><span class="tooltipr"> | "R-squared" | Proportion of variation in Y explained by the regression | <span class="tooltipr">$R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}$<span class="tooltiprtext">`$R^2 = \frac{SSR}{SSTO} = 1` `- \frac{SSE}{SSTO}$`</span> | `SSR/SSTO` |
| <span class="tooltipr">$r$<span class="tooltiprtext"> `$r$`</span> </span><span class="tooltipr"> | "r" | Correlation between X and Y. | <span class="tooltipr">$r = \sqrt{R^2}$<span class="tooltiprtext">`$r = \sqrt{R^2}$`</span> | `sqrt(R^2)` |
| <span class="tooltipr">$\hat{Y}_h$<span class="tooltiprtext"> `$\hat{Y}_h$`</span></span> | "why-hat-aitch" | Estimated mean y-value for some x-value called $X_h$ | <span class="tooltipr">$\hat{Y}_h = b_0 + b_1 X_h$<span class="tooltiprtext">`$\hat{Y}_h = b_0 + b_1 X_h$`</span></span> | `predict(lmObject, data.frame(XvarName=#))` |
| <span class="tooltipr">$X_h$<span class="tooltiprtext"> `$X_h$`</span> </span> | "ex-aitch" | Some x-value, not necessarily one of the $X_i$ values used in the regression | <span class="tooltipr">$X_h =$ some number<span class="tooltiprtext">`$X_h = $`</span></span> | `Xh = #` |
| Confidence Interval | "confidence interval" | Estimated bounds at a certain level of confidence for a parameter | <span class="tooltipr">$b_0 \pm t^* \cdot s_{b_0}$<span class="tooltiprtext">`b_0 \pm t^* \cdot s_{b_0}`</span></span> or <span class="tooltipr">$b_1 \pm t^* \cdot s_{b_1}$<span class="tooltiprtext">`b_1 \pm t^* \cdot s_{b_1}`</span></span> | `confint(mylm, level = someConfidenceLevel)` |


| Parameter | Estimate |
|-----------|----------|
| $\beta_0$ | $b_0$    |
| $\beta_1$ | $b_1$    |
| $\epsilon_i$ | $r_i$ |
| $\sigma^2$ | $MSE$ |
| $\sigma$ | $\sqrt{MSE}$, the Residual standard error |



</div>


<br>

***


*Note: make r chunk height : 3*

<u>Explain how to use each of the following diagnostic plots.</u>


- Residuals vs. Fitted-values Plot

    - shows the relationship between variab\les and the spread of the data is consistant (want chaos)


- Q-Q Plot of the Residuals

    - checks if the model's error terms follow a normal distribution


- Order Plot of the Residuals

    - Good : looking for chaos! -> you've caught everything (life is boring)
    
    - Bad : obvious trends, decline in data, groupings -> theres more to the story/ information! 
    
<br>
    
***


1. What are the required elements to a well done simple linear regression analysis?
    
    - math equations, hypotheses, significance levels, scatter/ diagnostic plots, interperet the slope (how much the average is being changed by x)
    
2. What is the difference between a graphic and a diagnostic plot?

    - 

3. How do you use "math chunks" to write out the simple linear regression model and its corresponding set of hypotheses?

    - 
    
4. Why is it important to include the simple linear regression model along with your hypotheses about that model in your analysis?

    - putting a belief system on the data, putting the model onto the data (I believe this, lets see if its right, lets see if this is worth trying)
    
5. How do you interpret the values for the slope and intercept of a simple linear regression?

    - 

<br>  
    
***


### Week 9 | Multiple Linear Regression

<br>

Studying from the [Linear Regression](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/LinearRegression.html) page! It's got multiple groups that crazy!!!
</div>

***


#### Skills Quiz - Mulitple Linear Regression



<br>

As explained in the help file (?SaratogaHouses), the SaratogaHouses data set contains data about many houses from Saratoga County, New York in the year 2006. 

<u>Suppose a family is in search of a home that was:</u>

- **newly** constructed
- has **three** bedrooms
- trying to decide how big of a **livingArea** they can afford 
- whether or not the price of the home is significantly impacted by adding a **fireplace** 

<br>

Use the **SH2** data set (the filtered down version of the SaratogaHouses data set) and a **"two-lines" multiple regression model** to describe the **price** of a house according to the **size of the livingArea** of the house and whether or not the house has a **fireplace** (fireplaces is only 0 or 1 for the SH2 data).

```{r}
SH2 <- filter(SaratogaHouses, bedrooms == 3, newConstruction =="Yes")
```

***


The two-lines regression model for this situation would be most appropriately labeled as:

$$\underbrace{Y_i}_\text{Price} = \beta_0 + \beta_1 \underbrace{X_{2i}}_\text{livingArea} + \beta_2 \underbrace{X_{2i}}_\text{fireplaces} + \beta_3X_{1i}X_{2i}+\epsilon_i$$

$$X_{2i} = \left\{\begin{array}{ll} 1, & \text{Fireplace} \\ 0, & \text{No Fireplace} \end{array}\right.$$


- $\beta_0$ = 	
The average price of a home with no fireplace and a living area of zero square feet. Since this is unrealistic, this parameter doesn't actually carry any meaning for this particular regression model.

- $\beta_1$ = The change in the average price of a home without a fireplace as the living area increases by 1 additional square foot.

- $\beta_2$ = The difference in the average price of a home with a fireplace as compared to a home without a fireplace for homes with zero square feet of living area. 

- $\beta_3$ = The change in the effect of 1 additional square foot in the living area on the average price of homes with a fireplace as compared to homes without a fireplace.

<br>

***


***Perform the above regression in R.***

```{r}
sh2.lm <- lm(price ~ livingArea + fireplaces + livingArea:fireplaces, data=SH2)

summary(sh2.lm)
```


<u>Statistical results from a regression analysis, *a way to understand how different factors affect something we're interested in *(in this case, home prices).</u>

-  There are **four p-values** in the results, *one for each factor being studied*. 

<br>

P-values help us determine if a factor has a *significant effect or if the result could just be by chance*.
- These p-values come from something called a t-test, which is a common statistical method.
- One important finding is about how *the size of the living area affects home prices*:
  - The p-value for this factor is **0.0055**, which is considered **statistically significant**. This means *we can be pretty confident that the size of the living area really does affect home prices*.
  - For homes *without a fireplace*, each additional square foot of living space *increases the average price by* **$103.08**. This applies to ***three-bedroom, newly constructed homes***.

<br>

In simple terms, this analysis shows that ***bigger homes tend to cost more, and we can estimate by how much for certain types of homes.***

<br>

***


Now we ask the important question as to ***whether the average price of (three bedroom, newly constructed homes) is changed at all by the presence of a fireplace***. To do this, we test the following sets of hypotheses.

<br>

What is the p-value for the test of the hypotheses that: 

$$H_0: \beta_2 = 0 $$

$$H_a: \beta_2 \neq 0 $$
p - value = **0.1357**

<br>

What is the p-value for the test of the hypotheses that: 

$$H_0: \beta_3 = 0 $$

$$H_a: \beta_3 \neq 0 $$
p - value = **0.0654**

<br>

This shows that there is **no significant** difference in the average price of (three bedroom, newly constructed) homes that have a fireplace compared to those that do not have a fireplace. 

<br>

***


It is important to visualize a regression whenever possible so that the reader can connect with the "truth" about the situation that we are trying to show them.

```{r}
palette(c("lightblue", "orange"))

plot(price ~ livingArea, data=SH2, pch=16, col=as.factor(fireplaces))

h <- coef(sh2.lm)

curve(h[1]+h[2]*x, add=TRUE, col="lightblue")

curve(h[1]+h[3]+(h[2]+h[4])*x, add=TRUE, col="orange")


legend("topleft", legend=c("no","yes"), pch=1, col=palette(), title="Fireplaces", bty="n")
```

<br>

***


Before we ever fully trust the results and interpretation of a regression model, it is important to diagnose the appropriateness of the model.

```{r}
par(mfrow=c(1,3))

plot(sh2.lm, which=1)

qqPlot(sh2.lm, id=FALSE, main= "Q-Q plot")

plot(sh2.lm$residuals, main="Residuals vs Order")
```

These diagnostic plots all look quite good, so ***we can feel comfortable using this regression to reach the conclusions we reached above**!

<br>

***


While the "two-lines" model used in the previous question is a very good way to begin your journey into multiple linear regression, there is a vast number of other possible models that we could use, infinitely many actually. 

This problem will allow you to practice what is called a **"quadratic model."**

<br>

It is well known that in North America summers are warm, and the spring and fall seasons are cooler. **Use the airquality data set and a quadratic regression model to identify the warmest month of the year in New York City, NY (as measured at the La Guardia airport in 1973, see ?airquality).**

<br>

The mathematical model for this regression is given by: 

$$\underbrace{Y_i}_\text{Temp}=\beta_0 + \beta_1 \underbrace{X_i}_\text{Month} + \beta_2 \underbrace{X^2_i}_\text{Month Squared}+ \epsilon_i$$

<br>

***


```{r}
lm.airquality <- lm(Temp ~ Month + I(Month^2), data= airquality)

summary(lm.airquality)
```

The estimates shown in the summary output table above approximate the $\beta$’s in the regression model:

- $\beta_0$ is estimated by the (Intercept) value of -95.73
- $\beta_1$ is estimated by the Month value of 48.72
- $\beta_2$ is estimated by the I(Month^2) value of -3.283

$$\hat{Y_i} = \overbrace{-95.73}^\text{y-int}+\overbrace{48.72}^\text{slope term}X_i + \overbrace{-3.283}^\text{Quadratic Term} X^2_i$$

As shown in the summary output of this regression, ***the P-values for both Month and I(Month^2) are highly significant.***

<br>

***

**Check the appropriateness of this regression model by creating the Residuals vs. fitted-values, normal Q-Q plot, and residuals vs order plot**. The only plot that shows some problems is the Q-Q Plot of the residuals. 

```{r}
par(mfrow=c(1,3))

plot(lm.airquality, which=1)

qqPlot(lm.airquality, id=FALSE, main= "Q-Q plot", labels=FALSE)

plot(lm.airquality$residuals, main="Residuals vs Order")
```

- The **linear relation, constant variance, and independent error** assumptions for this model look like they are **well satisfied**. The only problem is a possible **difficulty with normality**.

<br>

***

*Continuing with the regression despite the possible difficulty*, use your math skills or R skills to predict the average high temperature in New York City, NY on July 4th. 


You will need to use:
- Month = 7.129032 to do this as July is month 7
- the 4th day would be 4/31 of that month: (7 + 4/31 = 7.129032)

Step 1: Use Regression Model Equation

$$\hat{Y_i} = \overbrace{-95.73}^\text{y-int}+\overbrace{48.72}^\text{slope term}X_i + \overbrace{-3.283}^\text{Quadratic Term} X^2_i$$

Step 2: Insert Month Term ($X_i$)

$$\hat{Y_i} = \overbrace{-95.73}^\text{y-int}+\overbrace{48.72}^\text{slope term}*\underbrace{(7.129032)}_\text{July 4th} + \overbrace{-3.283}^\text{Quadratic Term} * \underbrace{(7.129032)^2}_\text{July 4th} $$

Step 3: Use code thingys or compute by hand! 

- Used the code below to get the following answer!

```{r}
intercept <- coef(lm.airquality)[1]
slope <- coef(lm.airquality)[2]
beepboop <- coef(lm.airquality)[3]

month <- 7.129032

predicted_temperature <- intercept + slope * month + beepboop * month^2
predicted_temperature
```
***The predicted average temperature of July 4th in New York City from this regression model is <u> 84.74588</u> degrees Fahrenheit***

<br>

***

The graph of this regression, with the predicted temp for July 4th included, looks as follows.

```{r}
plot(Temp ~ Month, data=airquality, col="skyblue2", pch=21, bg="gray83", main="Quadratic Model using airquality data set", cex.main=1)

b <- coef(lm.airquality)

curve(b[1] + b[2]*x + b[3]*x^2, col="skyblue2", lwd=2, add=TRUE)

xpre <-7.129032
ypre <- b[1] + b[2]*xpre + b[3]*xpre^2

lines(c(xpre, xpre), c(0,ypre), lty=2, col="firebrick")

lines(c(0, xpre), c(ypre, ypre), lty=2, col="firebrick")

points(xpre, ypre, cex=1.2, col="red")

text(xpre + 0.2, ypre, "Predicted Temp", cex=0.5, pos=3)
```

Finally, while the quadratic model is not very useful for interpretation, the **vertex of the parabola has a valuable interpretation**. 

In this case, ***the vertex gives the warmest average temperature of the year, as well as the "Month" of the year where this occurs***. <u>Use the equation to find the x position of the vertex fo the parabola:</u>

$$\frac{-\beta_1}{2 * \beta_2}$$

```{r}
xvertex <- -slope / ( 2 * beepboop)

xvertex
```
<br>

<u>Then, input out x position back into the mathmatical equation:</u>

```{r}
yvertex <- intercept + slope * xvertex + beepboop * xvertex^2

yvertex
```
<br>

The vertex of this quadratic model occurs at an x-value of **7.420204**, which translates to a date of July 13th. And the maximum predicted (y-value) temperature corresponding to this data is **85.0242** degrees Fahrenheit! 

<br>

***

In building regression regression models, ***it is sometimes meaningful to mutate the data prior to running the regression***. Let's practice this using the RailTrail data set in library(mosaic) in R.

- The **Volume** column of the `RailTrail` data set shows **the number of people who use this particular trail on a given day**
  - It seems possible that the number of people on the trail on any particular day could be impacted by many things in this data set. ***Let's explore how the high temperature (hightemp) and whether or not it rains (this will be a new column we make) relates to the average number of users on the trail.***
  
***

To make the new "rain" column, mutate the precip column in the data set. 

- The precip column measures the amount of precipitation in inches that occurred that day.    - We need to change it into a column that is **a 1 if there was any precipitation and a 0 if there was not any precipitation**. 
  - That way, we have a column stating if it rained (1) or not (0). 
  
<br>

<u>To create the new column:</u>

1. Use mutate command, **mutate(**

2. Insert new column name we want to add in, **mutate(rain**

3. Add a case_when, **mutate(rain = case_when(**

4. Add the column we are mutating, **mutate(rain=case_when(precip **

5. Insert the criteria, **mutate(rain=case_when(precip == 0 ~ "No Rain"**


<br>

*The finished code is shown below with a scatter plot!*



```{r}
RainTrail <- RailTrail %>%
  mutate(rain = case_when(
    precip == 0 ~ "No Rain",
    precip > 0 ~ "Rain"
  ))

  palette(c("orange","skyblue"))

  plot(volume ~ hightemp, data=RainTrail, col=as.factor(rain), main="RailTrail Data Set")

  legend("topleft", legend=c("No Rain", "Rain"), col=palette(), pch=16)
```


Based on the data shown in the graph above, the ***Two-Lines Regression*** model would be the most appropriate to implement to see **if the average number of users on the trail is effected by the high temp and whether or not it rained**!

<br>

***

***We will not preform the Two-Lines Regression Model!***

```{r}
rain.lm <- lm(volume ~ hightemp + rain + hightemp:rain, data=RainTrail)

summary(rain.lm)
```

The estimated value of the hightemp variable's effect on the average volume of users on the trail: $\beta_1 \approx 5.2651$

<br>

<u>There's a few things we have to address</u>

1. The results show that most factors in our model don't have a significant effect on trail usage!
    - This seems odd, especially for rain. We'd expect fewer people on the trail when it's raining.
    
2. In cases like this, statisticians often try a trick:
    - If we have two related factors (like "rain" and "rain combined with temperature") and neither seems important,
    - We can remove the more complex one ("rain combined with temperature") and see what happens.
      - Sometimes, by doing this, we find that the simpler factor ("rain") becomes important on its own.
      
3. The next step is to try this simpler model and see if it helps us understand the effect of rain better.

***This approach helps us find clearer answers when our first attempt doesn't give us the results we expect***

<br>

$$\underbrace{Y_i}_\text{volume} = \beta_0 + \beta_1\underbrace{X_{1i}}_\text{hightemp} + \beta_2\underbrace{X_{2i}}_\text{rain} + \epsilon_i$$
$$where X_{2i} = \text{0 when there is no rain and} X_{2i} =  \text{1 when there is rain}$$

<br>

The regression below is our modified equation! 
- This particular regression **no longer allows for a change in slope** because the interaction term of $X_{1i}X_{2i}$ has been removed

```{r}
shorttrail.lm <- lm(volume ~ hightemp + rain, data=RainTrail)

summary(shorttrail.lm)
```

Summary: 

- What is the p-value for the rain term? 
  - **0.000333**
  
- This now shows that rain has **a significant** effect on the average number of users of the trail reducing the number of users by **83.3190** people when it rains. 
  - Note: do not enter the negative sign of the number because we stated that it was "reducing the average" already in the sentence
  
- Further, each 1 degree increase in the hightemp (in degrees Fahrenheit) increases the average number of people on the trail by **5.1655** <u>whether is is raining or not raining</u>

<br>

***

***Graph your regression!***

- use the numbers from the regression summary to create the lines! 

```{r}
palette(c("orange", "skyblue"))

plot(volume ~ hightemp, data=RainTrail, col=as.factor(rain), main="RailTrail Data Set")

legend("topleft", legend=c("No Rain", "Rain"), col=palette(), pch=16)

curve(46.6863 + 5.1655*x, add=TRUE, col="orange")
curve(-36.6327 + 5.1655*x, add=TRUE, col="skyblue")
```

<br>

***

***Finally, create the three necessary diagnostic plots to determine if this regression analysis was appropriate for these data!***

```{r}
par(mfrow=c(1,3))

plot(shorttrail.lm, which=1)

qqPlot(shorttrail.lm, id=FALSE, main= "Q-Q plot", labels=FALSE)

plot(shorttrail.lm$residuals, main="Residuals vs Order")
```

- For the most part, linearity, normality, and independence of the error terms looks good! 
  - But according to the **residuals vs. fitted-values plot** there is a possible problem witht eh **non-constant variance**
  - the vertical variability of the residuals is **increasing** for higher fitted values
  
- So, we will go ahead and consider our regression meaningful for interpretation, *even though everything is not perfect with these diagnostics plots*

<br>

***

#### Assessment Quiz - Multiple Linear Regression


<br>

1. What is the estimate of $\beta_1$ in the regression model below?

$$\underbrace{Y_i}_\text{length of foot} = \beta_0 + \beta_1 \underbrace{X_{1i}}_\text{width fo foot} + \beta_2 \underbrace{X_{2i}}_\text{0 if boy, 1 if girl} + \beta_3X_{1i}X_{2i} + \epsilon_i$$

```{r}
KidsFeet.lm <- lm(length ~ width + sex + width:sex, data=KidsFeet)

pander(summary(KidsFeet.lm))
```

***Answer***: **1.5423**, the model above is based of the **two lines model**. Then, lookign at the **width** row, that represents $\beta_1$/ slope, it will give us **1.5423**.

<br>

2. State the p-value of the hypothesis test in the regression model (both shown below). 

$$\underbrace{Y_i}_\text{stopping distance} = \beta_0 + \beta_1 \underbrace{X_{1i}}_\text{speed of car} + \beta_2 \underbrace{X_{1i}^2}_\text{(speed)^2} + \epsilon_i \text{ where } \epsilon_i ~ N(0,\sigma^2)$$

$$H_0: \beta_2 = 0$$

$$H_a: \beta_2 \neq 0$$

```{r}
lm.car <- lm(dist ~ speed + I(speed^2), data= cars)

pander(summary(lm.car))
```

***Answer***: **0.136**, the regression above is modedled after the **quadratic model**, and looking at the p-value of the **I(speed^2)** row, which represents $\beta_2$, the value is **0.136**

<br>

3. Consider both the regression summary output and graph shown below. What is the value of the y-intercept of the red line shown in the graph?

![](C:/Users/paige/OneDrive/Pictures/Screenshots 1/Screenshot 2024-11-16 144036.png)

***Answer***: The red line shown in the graph is the Mississippi line. Based on the regression summary output, we know the Quebec (black) line must be the baseline and that Mississippi must be the "changed line". So we have to add together the (Intercept) from the black line (baseline) which is 32.615181 and the change in intercept achieved by the Mississippi line -17.407002. That gives a new value of 15.20818, which rounds to ***15.2***. 


***

#### Class Activity - Mulitple Linear Regression (Part 1)


- Very wild west type of test! A space we can't even depict on a graph

- Two lines model -> only Brother Saunders class will know whatcha mean

<br>

$$\overbrace{Y_i}^\text{Dependent Variable} = \underbrace{\beta_0}_\text{Intercept} + \underbrace{\beta_1}_\text{Slope} \overbrace{X_i}^\text{Independent Variable}+\underbrace{\beta_2}_\text{Change in Intercept}X_{2i} + \underbrace{\beta_3}_\text{Change in Slope}X_iX_{2i}+ \underbrace{\epsilon_i}_\text{Errors Quantified (variation left over)}$$


- intercept + slope -> visible (estimates)

- $\beta_2$ and beta 3 -> not visible, depicts change

  - beta 3 = 0 -> parallel
  
  - beta 2 = 0 -> same y intercept

<br>

**Important Elements of a Multiple Linear Regression Analysis:**

- Use the mtcars dataset in R to reproduce the graphic shown below, or at least a graphic that shows the same information. 
    - Then perform a multiple linear regression that would give the equations for the two lines shown on the graphic. 
    - Check your work with your peers, and check off the items below that you are able to complete.
    
<br>

1. Recreate the **scatter plot** of the mtcars dataset

```{r}
mylmcarz <-lm(mpg ~ qsec + am + qsec:am, data=mtcars)

palette(c("dodgerblue","red"))

plot(mpg ~ qsec, data=mtcars, col=as.factor(am), pch = 16)

c <- coef(mylmcarz)

curve(c[1] + c[2]*x, add=TRUE, col= "dodgerblue")
curve(c[1]+c[3] + (c[2]+c[4])*x, add=TRUE, col="red")

```

<br>

2. State your **linear regression model in mathematical notation**

$$\overbrace{Y_i}^\text{mpg} = \underbrace{\beta_0}_\text{Intercept} + \underbrace{\beta_1}_\text{Slope} \overbrace{X_i}^\text{qsec}+\underbrace{\beta_2}_\text{Change in Intercept}\overbrace{X_{2i}}^\text{am} + \underbrace{\beta_3}_\text{Change in Slope}\overbrace{X_i}^\text{qsec}\overbrace{X_{2i}}^\text{am}+ \epsilon_i$$

<br>

3. Provide the estimated values of the slope and intercept of each of your drawn lines

```{r}
pander(summary(mylmcarz))
```
- Slope -> Describes starting points 

  - Automatic Transmission Vehicles **Slope** *(Intercept)*: 1.439
  
  - Manual Transmission Vehicles **Slope** *(Intercept + am)* : 2.76

<br>

- Intercept -> Indicates how the y variable changes with the x variable + the interaction between x variable and the two groups

  - Automatic Transmission Vehicles **Intercept** *(qsec)* : -9.01
  
  - Manual Transmission Vehicles **Intercept** *(qsec + qsec:am)* : -23.52

<br>


- Pr(>|t|) -> P-Value

  - non significance is a bit sticky 
  
4. Diagnose how appropriate it was to perform this regression

```{r}
par(mfrow=c(1,3))

plot(mylmcarz, which=1:2)

plot(mylmcarz$residuals)
```

5. Determine if the slopes or intercepts (or both) of the two lines actually show significant evidence of being different

- model believes the lines should be the same

<br>

***

#### Class Activity - Mulitple Linear Regression (Part 2)



Consider the multiple linear regression model given by: 

$$Y_i = \beta_0X_{1i} + \beta_2X_{2i} + \beta_3X_{1i}X_{2i}+ \epsilon_i $$
- $X_{2i}$ = 1 : Group B

- $X_{2i}$ = 0 : Group A


<br>

![](C:/Users/paige/OneDrive/Pictures/Screenshots 1/Screenshot 2024-11-13 185947.png)

<br>

| Part | What it does |
| --- | --- |
| $\beta_0$ | **Sets the starting point of the line** on the graph. |
| $\beta_1$ | **Determines how steep the first line** (for Group 0) is. |
| $\beta_2$ | **Changes where the second line starts** compared to the first line. |
| $\beta_3$ | **Changes how steep** the second line is compared to the first line. |

<br>

The "two-lines" model is a simple way to compare two groups in statistics. It uses two main parts:

1. A regular number ($X_{1i}$) that can be any value.
2. A special number ($X_{2i}$) that's either 0 or 1.

<br>

This special number (also called a "dummy variable" or "indicator variable") helps turn information about groups (like "Group A" or "Group B") into numbers we can use in math.

By using these two parts, we can create two separate lines on a graph:

- One line for Group A
- One line for Group B

<br>

***

<u>Practicing in R with the three following problems:</u>

<br>

***Problem 1: Equal Slopes Model***

```{r}
palette(c("skyblue","firebrick"))

plot(mpg ~ qsec, data=mtcars, col=as.factor(am), xlim=c(0,30), ylim=c(-30,40), main="1974 Motor Trend Cars", pch=16)

someNameForYourslopeLM <- lm(mpg ~ qsec + am + qsec:am, data=mtcars)

summary(someNameForYourslopeLM)

abline(someNameForYourslopeLM$coef[1], someNameForYourslopeLM$coef[2], col=palette()[1])

abline( someNameForYourslopeLM$coef[1]+someNameForYourslopeLM$coef[3],someNameForYourslopeLM$coef[2], col=palette()[2])

legend("topleft", legend=c("automatic","manual"), pch=1, col=palette(), title="Transmission (am)", bty="n")
```


<br>


***Problem 2: Equal Intercepts Model (but different slopes***)

```{r}
palette(c("skyblue","firebrick"))

plot(mpg ~ qsec, data=mtcars, col=as.factor(am), xlim=c(0,30), ylim=c(-30,40), main="1974 Motor Trend Cars", pch=16)

someNameForYourintLM <- lm(mpg ~ qsec + am + qsec:am, data=mtcars)

summary(someNameForYourintLM)

abline(someNameForYourintLM$coef[1], someNameForYourintLM$coef[2], col=palette()[1])

abline( someNameForYourintLM$coef[1],someNameForYourintLM$coef[2]+someNameForYourintLM$coef[4], col=palette()[2])

legend("topleft", legend=c("automatic","manual"), pch=1, col=palette(), title="Transmission (am)", bty="n")
```


<br>

***Problem 3: Full Model (different slopes & different intercepts)***


```{r message=FALSE, warning=FALSE}
palette(c("skyblue","firebrick"))

plot(mpg ~ qsec, data=mtcars, col=as.factor(mtcars$am), xlim=c(0,30), ylim=c(-30,40), main="1974 Motor Trend Cars", pch=16)

someNameForYourLM <- lm(mpg ~ qsec + am + qsec:am, data=mtcars)

summary(someNameForYourLM)

abline(someNameForYourLM$coef[1], someNameForYourLM$coef[2], col=palette()[1])

abline(someNameForYourLM$coef[1] + someNameForYourLM$coef[3],someNameForYourLM$coef[2] +someNameForYourLM$coef[4],col=palette()[2])

legend("topleft", legend=c("automatic","manual"), pch=1, col=palette(), title="Transmission (am)", bty="n")
```

<br>


***

***Summarize your findings!!***

```{r}
pander(summary(someNameForYourLM))
```


<br>

- What have you learned?


<u>Are all p-values significant in your lm(...) for the full model?</u>

  - The p-value for qsec shows the overall effect of qsec on mpg.
  - The p-value for am indicates if there’s a significant difference in intercepts between automatic and manual transmissions when slopes are allowed to vary.
  - The p-value for the interaction term (qsec:am) tells us whether the effect of qsec on mpg depends on the transmission type.
  

If the interaction term's p-value is low, it indicates that the relationship between qsec and mpg (the slope) differs between automatic and manual transmissions. A low p-value for am would suggest a difference in intercepts between the two transmission types.
  
<br>
  
<u>How about for the equal intercepts model?</u>
  
  - The p-value for qsec tells us if there's a statistically significant relationship between qsec and mpg overall, regardless of transmission type.
  - The p-value for am indicates if there’s a significant difference in the intercepts of mpg between automatic and manual transmissions, assuming the slope remains constant.
    
    
If the p-value for am is low (e.g., below 0.05), it suggests that there's a statistically significant difference in mpg between automatic and manual transmissions after accounting for qsec, even if the slope is the same.

<br>
  
<u>How about for the equal slopes model?</u>

  - The p-value for qsec tells us if there is a significant overall relationship between qsec and mpg.
  - The p-value for the interaction term (qsec:am) shows whether the slope for qsec is different for automatic and manual transmissions.


If the interaction term’s p-value is low, it indicates that the relationship between qsec and mpg differs by transmission type, suggesting that automatic and manual cars have significantly different slopes.

***

<u>Model Comparison Summary</u>

- Equal Slope Model: Tests if there’s a difference in intercept between transmissions while assuming the same slope.

- Equal Intercept Model: Tests if there’s a difference in slope between transmissions while assuming the same intercept.

- Full Model: Tests for differences in both intercept and slope between transmission types.


Comparing these models helps determine if transmission type affects the intercept, slope, or both in the relationship between qsec and mpg. Lower p-values indicate significant differences, suggesting that am (transmission type) plays a role in predicting mpg.

<br>

<u>Here are the key points:</u>

- The full model cannot be used due to insufficient evidence in the data.
- Two potential models are suggested: equal intercepts or equal slopes.
- The choice between these models depends on which is more useful for the specific situation.
- More data collection, especially for qsec times close to zero, would help determine if the intercepts are truly the same.
- The equal intercepts model (with different slopes) is recommended because it has:
    - A higher R2 value
    - Significant p-values for each term
    
<br>

This summary emphasizes the need for data-driven model selection in statistical analysis, highlighting the process of choosing between different models based on available evidence and statistical indicators.

<br>

***

### Week 10 | Simple Logistic Regression

<br>

Studying from the [Logistic Regression](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/LogisticRegression.html) page! Yes or no questions only!
</div>

***

#### Skill Quiz - Logisitic Regression


<u>What does $P(Y_i = 1|X_i)$ represent in logistic regression?</u>

<br>

- The **probability of a success** $Y_i = 1$ given the information contained in the explanatory variable(s).

- The symbol used to represent the notation in logistic regression is **$\pi_i$**

<br>

<u>The mathematical model for simple logistic regression is given by:</u>

$$P(Y_i = 1) = \frac{e^{stuff}}{1+e^{stuff}}= \pi_i$$

- The content for "stuff" being **a simple linear regression of the form $\beta_0 + \beta_1x_i$**

<br>

<u>With some algebra, the simple logistic regression model can be reorganized as:</u>

$$\frac{\pi_i}{1-\pi_i}= e^{\beta_0}e^{\beta_1x_i}$$

We can assume that: 

- $\frac{\pi_i}{1-\pi_i}$ is the **odds of success**, 
  - i.e. the odds that $Y_i = 1$
  
- The **odds of success** ***equal*** $e^{\beta_0}$ when $x_i = 0$

- The **odds of success** ***increase*** by the factor $e^{\beta_1}$ for ***every one unit increase*** in $x_i$

<br>

***

***Let's look at the [challenger simple logistic regression example](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/Analyses/Logistic%20Regression/Examples/challengerLogisticReg.html)!***

<u>The estimated value of $\beta_1$ is $b_1 = -0.232$. How is this value **interpreted**?</u>

- A **one** unit **increase** is outside temperature results in a **0.79** change in odds of an o-ring failure.
  - **Why one unit?** because looking at the graph, the odds decreases by every 1°F 
  - **Why increasing?** because we are looking at the temperature as it **increases** to compare the lower temperatures to the higher temperatures.
  - **Where did you get 0.79?** By calcuated the odds with $e^{b_1} = e^{-0.232} \approx 0.79$
  
<br>

Using the graphic of the challenger example, if the decision was made to only launch when there was less than 20% risk of an o-ring failure, **what would the minimum "safe launch temperature" be?**

- ***70°F***

<br>

***

<u>When is it appropriate to use the Hosmer-Lemeshow goodness of fit test for a logistic regression model?</u>

- When there are **very few or no repeated x-values** in the data

<br>

***Use the following code to answer the next few questions:***

```{r}
infert.glm <- glm((spontaneous>0)~age, data=infert, family=binomial)

pander(summary(infert.glm))
```

<br>

1. What values should be used in place of b0 and b1 in the following command to place the logistic curve on the plot you now have in R?

- curve(exp(**b0** + **b1***x)/(1 + exp(**b0** + **b1***x)), add=TRUE)
  - b0 = **1.487** (from the `intercept estimate` column)
  - b1 = **-0.05616** (from the `age estimate` column)
  
```{r}
plot((spontaneous>0)~age, data=infert)


curve( exp(1.487  + -0.05616*x)/(1 + exp(1.487  + -0.05616*x)), add=TRUE)
```

- This shows that **the logistic regression is not very useful for these data because the curve does not approach 0 or 1 within the range of the data**. 
  - a nice logistic curve going from 1 down to 0 witnessing that logistic regression is very useful for that data
  
<br>

2. Perform the code that shows **the appropriate goodness of fit test** for the following data:

```{r}
table(infert$age)
```

- ***Answer***: pchisq(**residual deviance**, **df for residual deviance**, lower.tail=FALSE)
  - Where the summary(infert.glm) output contains the values for residual deviance and df
  
<br>

3. Perform the correct goodness of fit test in R to test to see **if there is evidence that the above logistic regression model is not a good fit** for the data.

```{r}
pchisq(334.0, 246, lower.tail=FALSE)
```
- The p-value of the test is **0.0001573505**

- Note that since **the p-value is significant**, the conclusion of the test is that **the logistic model is not a good fit for these data.
  - In order words, the **null hypothesis** was that **the logisitc regression was a good fit**, but we have **sufficent evidence to reject** that assumption and conclude that **logistic regression is not very useful for these data**
  - We already saw this in the plot above, this just reaffirms our previous conclusion!
  
<br>

***

***Use the `Galton` dataset found within the mosaic package to answer the following questions.***

<br>

We know that in general **men are taller than women**. Thus, it seems *logical* that **the height of a person could be used to predict their gender**. Use the `Galton` dataset to produce a logistic regression model that allows us to ***predict the probability that the gender of an individual is male (sex == "M") given the height of the individual***.

<br>

***

1. The logisitc regression model for these data is given by: 

$$P(Y_i = \text{Male}) = \frac{e^{\beta_0 + \beta_1\text{height}_i}}{1 + e^{\beta_0 + \beta_1\text{height}_i}}$$

The estimated values for the **coefficents** of this equation are given in R as: 

```{r}
glm.Galton <- glm((sex == "M")~ height, data=Galton, family=binomial)

pander(summary(glm.Galton))
```

<br>

2. Create a graphic for this logistic regression.

```{r}
plot((sex == "M") ~ height, data=Galton)

curve(exp(-52.98 + 0.7968*x)/(1+ exp(-52.98 + 0.7968*x)), add=TRUE)
```

***Answer:***

- $\beta_0 \approx$ **-52.98**

- $\beta_1 \approx$ **0.7968**

<br>

- Additionally, this plot shows that the logistic regression is a **good fit** for these data!**

<br>

3. What are some things we can say about this logistic regression?

- For every **one inch increase in the height of an individual**, the *odds* that they will be a male **increases by a factor of 2.2**. 
  - In other words, **the odds of the person being a male more than doubles for every one inch increase in height**.

- The **probability** that an individual is male if they are *70 inches tall* is **0.942**.

- There is about a **50% chance** that the individual is Male (or Female) if they are **66.5 inches tall**.

<br>

4. Use your model to **predict the probability that a person who is 65 inches tall is male**. 
```{r}
predict(glm.Galton, newdata=data.frame(height=65), type="response") %>%
  pander()
```

<br>

5. Conduct **Hosmer and Lemeshow's Goodness of Fit test with g=10** to confirm that **this simple logistic model is appropriate to use height to predict sex**. 

```{r}
hoslem.test(glm.Galton$y, glm.Galton$fitted, g=10) %>%
  pander()
```

- The p-value of the test is **0.9896**
  - This shows that there is **insufficent** evidence to *reject the null hypothesis* that **the logistic regression is a good fit for these data**
  - In other words, the model looks **really good**!

***

#### Assessment Quiz - Logistic Regression


1. Use an appropriate test to determine if the birth weight in ounces of a baby (wt) can be used to predict the probability that the mother smoked at all (smoke>0, or, if your Gestation data set has words in the "smoke" column, use smoke!="never") during or prior to the pregnancy. The graphic of the correct analysis is shown below.

***What are the values of b0 and b1 for the curve shown in the graphic below?***

```{r}
Gestation.glm <- glm((smoke == "never")~wt, data=Gestation, family=binomial)

pander(summary(Gestation.glm))
```

<br>

2. Here is the output of a logistic regression performed in R. 

![](C:/Users/paige/OneDrive/Pictures/Screenshots 1/Screenshot 2024-11-24 210605.png)

***What is the predicted value for $P(Y_i = 1 | x_i = 25)$?***

<br>

There are **TWO** different ways you can get this answer: 

1. Type it all out

- it would look like this: 

exp(-6.6035 + 0.3070*25) / (1 + exp(-6.6035 + 0.3070*25)) = **0.7448821**

2. Use the predict function! 

- it would look like thiss: 

predict(glmObject, data.frame(mpg=25), type="response")

- only thing is, **we don't have a glmObject** so writing it out by hand is best for this question! 

<br>

3. Consider again the logisitc regression output shown below. 

![](C:/Users/paige/OneDrive/Pictures/Screenshots 1/Screenshot 2024-11-24 210605.png)

***How can we correctly interpret these results?***

- To intepret the effect of mpg (the x-variable) on the odds, we compute **exp(0.3070)**, which gives the value of eb1
  - Since **exp(0.3070) = 1.359341**, we find that the odds of a success, i.e., that **Yi =1 increases by 35.9341%, which rounds to 36% for every 1 unit increase in x**.
  - Notices that the **35.9341 came from 1.359341**. If you have 1.36 of what you originally had, then you increased by 0.36 or 36%.


***Answer***:  The odds that **Yi=1 increase by 36% for every 1 unit change in x**. 

<br>

***

#### Class Activity -Simple Logistic Regression (Part 1)



<u>Main question template:</u>

Whats the probability that **this thing happens**, given **the things that are happening*?**

- ex. What's the probability that the length of the foot we have belongs to a girl or a boy?

- ex. WILL THIS HAPPEN OR NOT BASED ON THIS KNOWN INFORMATION?? 


- a yes or no question! 

1. Explain each symbol of the simple logistic regression model, and what it represents

$$P(Y_i = 1|\, x_i) = \frac{e^{\beta_0 + \beta_1 x_i}}{1+e^{\beta_0 + \beta_1 x_i}} = \pi_i$$

- $\pi_i$ = P (pi is a letter )

- e = a number! 

- $e^{\beta_0 +\beta_1X_i}$ = serves as a bounds to go from 0 (impossibility) to 1 (certainty)
  - e to stuff / 1 + e to stuff

<br>

2. Explain how the simple logistic regression model is being used in the Challenger analysis. (What is the explanatory variable of the analysis, and what is it allowing us to predict?)

- success (0) or failures (1) -> probability of failure based on the given temperature. 

<br>

3. Explain the R code needed to perform a simple logistic regression.

[name of glm] <- **glm**( [yterm] >0 ~ [xterm], data=[data set name], **family=binomial** )

pander(summary([name of glm]))

4. Explain how to make predictions with the simple logistic regression model, either by hand, or using the predict() function in R.

- Write it into a predict statement! 



<br>

***

#### Class Activity - Simple Logistic Regression (Part 2)


$$$log(odds) = \beta_0 + \beta_iX_i$$

- the log of the odds are linear

<br>

- Probability = $\frac{Successes}{Total}$
  - doesn't change in a consistent way
  - What we **graph**!
  - depicts to a "moment/time of decision"

- Odds = $ \frac{Success} {Failure}$

  - "the times as likely to succeed"
  - What we **interpret**!
    - nothing to infinite! (never really graph the odds)
  - multiplication likeliness, **slope effects the odds**! 
  
$$\frac{\pi_i}{1 - \pi_i} = e^{\beta_0 + \beta_1X_i}= \underbrace{e^{\beta_0}}_\text{baseline odds}\underbrace{(e^{\beta_1})^{X_i}}}_\text{multiplicative change to the odds}$$
  
<br>

When graphing, the **BIG CHANGE IN THE CURVE LINE** shows that you found something crazy and pretty cool!

<br>

***Recreate the graphic shown below in R using the `KidsFeet` dataset. Then answer the following questions!***

<br>

```{r}
Feet.glm <- glm(sex== "B" ~ length, data=KidsFeet, family=binomial)

pander(summary(Feet.glm))
```

```{r}
plot(sex == "B" ~ length, data=KidsFeet)

curve(exp(-12.4860+0.5074*x)/(1+exp(-12.4860+0.5074*x)), add = TRUE)
```

***

Answer the following questions:

1. The predicted probability for a child being male if their foot length is 25 cm is: 

```{r}
predict(Feet.glm, newdata=data.frame(length = 25), type="response")
```

<br>

2. The estimated value of $e^{\beta_1}$ is ____ which is useful for interpreting the effect of X on the odds. 

```{r}
exp(0.5074)
```
<br>

3. How would you explain the proper interpretation of the estimated value for $\beta_1$ to someone using the phrase "the odds"?

- For every **0.5074 cm increase in foot length**, results in a **1.661 change in the odds** of the sex of the child being a male. 

<br>

***TWO ways to check the goodness of fit of a logistic regression model:***

1. Hosmer-Lemeshow Goodness-of-Fit Test ( **few to none replicated x-values)

- hoslem.test([yourglmname]$y, [yourglmname]$fitted, g=10)

2. Deviance Goodness-of-fit Test (**>50 repeated x-values**)

- pchisq([residual deviance],[df for residual deviance], lower.tail=FALSE)

<br>

***

### Week 11 | Consulting Opportunity or Research Project

<br>

***

#### Skill Quiz - Practice Final Exam


1. Use an appropriate test and the starwars dataset in R to determine if, on average, the species of Wookiees, Gungans, or Kaminoans are taller.

 
```{r}

```


<br>

***

#### Class Activity - Presenting your Findings


1. What are the five rubric  categories of an analysis in this class?

- 1) Hypothesis & Questions
- 2) Analysis
- 3) Graphical Summary
- 4) Interpretation
- 5) Presentation


<br>

2. Why is each of these elements important in the analysis?

<br>

3. How does a p-value help inform your conclusions for a particular analysis?

- Significant: 

- Non significant: We can gamble on this, but there's a sense that something is hiding here. There is a suspicious that that there is more here, but to find whats missing we can look deeper into this! 

<br>

***

***What do I do when my data doesn't pass the normality tests???***

1. Transform it! 

- There are several ways to transforming your y value by doing: 

  - log(yvalue)
  - (y-value)^2
  
- The way to check transformation corrected your distribution would be to do the code: **hist(datasetname$yvalue)**

- From there, you have to work with the transformed data in the ***testing space*** and then write your ***interpretation*** based off the non-transformed data. 

<br>

2. Suck it up! 

- If you transformed it and it doesn't create a normal distribution, no matter how much you change and check it, you just have to explain the following

  - We have to interpret the results with ***CAUTION***
  - We ***CANNOT*** promise that our results are definitive

<br>

***

### Week 12 | Chi Squared Test

<br>

***

#### Skills Quiz - Chi Squared Test


<u>Using the `Titanic` data set, answer the following:</u>

- Refers to the (Explanation file of the Chi Squared Tests)[file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/ChiSquaredTests.html] in the Statistics Notebook! 


1. What do the rows and columns factors called in the following table?

- Rows : **Class**

- Columns : **Survival**

![](C:/Users/paige/OneDrive/Pictures/Screenshots 1/Screenshot 2024-12-08 210615.png)

<br>

2. How would you state the null and alternative hypotheses for a chi squared test of independence?

$$H_0 : \text{The row variable and column variable are independent.}$$

$$H_a : \text{The row variable and column variable are associated.}$$

<br>

3. The two things need to obtain a p-value for the chi-squared test of independence are: 

  1. The **Test Statistic**
  
$$\chi^2 = \sum_{i=1}^m \frac{(O_i - E_i)^2}{E_i}$$

  2. The **distribution** of the test statistic that is caluclated under the assumption that the null hypothesis is true
  
<br>

The $E_i$ in the $X^2$ test statistic formula are called the **expected counts**. They are obtained by: 

$$E_i = \frac{\text{(row total)*(column total)}}{\text{(total total)}}$$

These values show us **what values we would expect to observe if the null hypothesis was true.**

- In other words, they provide the counts we would expect **if the row variable column variable were independent.**

<br>

4. The $X^2$ test statistic can be assumed to follow a: **chi-squared distribution**

- with degrees of freedom : $p = (r-1)*(c-1)$

- The chi-squared distribution is a **parametric** distribution because it has a single parameter known as the degrees of freedom, p.

<br>

**Person Residuals** are used for interpreting the results of a chi-squared test when the **alternative hypothesis** can be concluded to be the truth! 

- They show a relative measurement of how much the **observed counts** differ from the **expected counts**. 

<br><br>

***

<u>Use the `HairEyeColor` data set to answer the following questions:</u>

```{r}
 glasses <- cbind( Males = c(Glasses = 5, Contacts = 12, None = 18), Females = c(Glasses = 4, Contacts = 14, None = 22))

pander(glasses)
```

<br>

1. What can we interpret from these bar plots from the data?

- The gender of an individual **does not seem to be associated** with whether a person wears glasses, contacts, or no eye correction because **the pattern of the bars is essentially the same for male and females**.

- The data shows that **the** ***most common*** **results for either males or females is to** ***not wear glasses or contacts***. It also shows that **the** ***least common*** **results for both genders is** ***to wear glasses***. 

  - In other words, the pattern for both genders is the same.

```{r}
barplot(glasses, beside=TRUE, legend.text=TRUE, args.legend=list(x = "topright", cex=0.6, bty="n"), xlim= c(0, 10))
```

<br>

2. What are the null and alternative hypotheses for this data?

$$H_0 : \text{Corrective eye wearing and gender are independent.}$$

$$H_a : \text{Corrective eye wearing and gender are associated.}$$

<br>

3. Determine if a chi-squared test of independence is appropriate for the glasses data. 

- **Yes**, the requirements are met because **the average of the expected counts is greater than 5 and all expected counts are greater than 1**, even though some expected counts are less than 5.

```{r}
chis.glasses <- chisq.test(glasses)

pander(chis.glasses$expected)
```

<br>

4. View the results of our chi-squared test of the data.

```{r}
pander(chis.glasses)
```


<br>

5. What are the interpretations we can get from these results?

- This confirms our original suspicion that we saw in the graphic!

  - There is **insufficient evidence** to conclude the corrective eye wearing gender are **associated**.
  
- We will continue to assume the null hypothesis that whether someone wears glasses, contacts, or no corrective eye wear is **independent** of their gender.

<br>

6. What about the **Pearson Residuals**?

- Since we failed to reject the null **there is no interpretation to make for these data** so we are **not interested** in the Pearson Residuals. 

  -  However, so that you get the opportunity to see what the Pearson Residuals look like when we fail to reject, run the following code in R and notice that none of the residuals stand out as being exceptionally large in magnitude. 
  
```{r}
pander(chis.glasses$residuals)
```

  
This is ***always*** the case when we fail to reject the null hypothesis in a chi-squared test of independence!

<br><br>

***

Asia has become a major competitor with the U.S and Western Europe in education. The following table presents the counts of university degrees awarded to students in engineering and science (natural and social sciences) for the three regions.


<br>

The data (observed values) are depicted below:

```{r}
education <- cbind( `United States` = c(Engineering = 61941, `Natural Science` = 111158, `Social Science` = 182166), `Western Europe` = c(Engineering = 158931, `Natural Science` = 140126, `Social Science` = 116353), Asia = c(280772, 242879, 236018))

pander(education)
```

<br>

***Questions to ask***: 

- Are there any **differences** in the numbers of degrees awarded to each field for the different regions? 
  - In other words, are field and region associated? 

- Or can we assume that all three countries are **similar** in their patterns of degrees being awarded?

<br>

These questions can be stated in a hypothesis: 

$$H_0 : \text{Field and Region are independent/ not associated.}$$

$$H_a : \text{Field and Region are not independent/ associated}$$

<br>

<u>Conducting the Chi-squared test</u>

```{r}
chi.ed <- chisq.test(education)

chi.ed
```

***This shows sufficient evidence to conclude that field and region are associated!***

<br>

<u>Checking requirements:</u>

```{r}
edexpect <- chi.ed$expected

pander(edexpect)
```

<br>

<u>Create an appropriate graphic in R for this analysis:</u>

- use `ylim=` and `xlim` to change the stretching

- use `cex=` in `args.legend=` function to shrink the size of the legend

```{r}
barplot(education, beside=TRUE, legend.text=TRUE, args.legend= list(x="topleft", bty="n",cex=0.59), main="College Degrees Awarded by Region", ylim=c(0, 300000))
```
<br>

<u>Obtain the residuals</u>

```{r}
edresid <- chi.ed$residuals

pander(edresid)
```

**Questions that are answered by residuals:**

1. Which region differs the most from expected when is comes to the number of **Engineering degrees** awarded?

- ***United States***

<br>

2. Which region differs the most from expected when it comes to the number of **Social Science** degrees awarded?

- ***United States***

<br>

***Review time!!***

Consider the `InsectSprays` dataset in R.

- Test the hypothesis that the mean number of bugs killed by insecticides A, B, C, D, E, and F are all the same against the alternative that at least one mean is different. 

```{r}
datatable(InsectSprays, options(list=c(3,10,30)))
```


<br>

1. What hypothesis test should we use?

- **ANOVA!!** 

```{r}
bug.ova <- aov(count ~ spray, data = InsectSprays)

summary(bug.ova)
```

- The p-value of the test is **very small** because the test statistic is **34.7** showing that there is **sufficient** evidence to conclude that **at least one bug spray results in a different** average number of bugs per agricultural experimental unit than the other sprays. 

<br>

2. What is the appropriateness of this test?

```{r}
par(mfrow=c(1,2))
plot(bug.ova, which=1:2, pch=16)
```
- The appropriateness of the test is **questionable** because the residual plot shows that the **constant (equal) variance** is questionable (the first graph) and the Q-Q Plot suggests the errors are **likely not normally distributed**. 

  - Thus, we should not make any conclusion about these insect sprays until next week when we can use the non-parametric version of ANOVA called the "Kruskal-Wallis Test." In other words, this test is inconclusive due to difficulties with the requirements not being satisfied.
  
<br>

--

#### Assessement Quiz - Chi Squared


1. ***Perform an appropriate test in R to determine if the dominant hand of children is related to the season of the year they are born in.***

- Select the answer showing the P-value of your test. (Note that this test is not quite appropriate for these data. Due to the small sample sizes, the average expected count is only 4.875, but all expected counts are above 1.)

***First part:*** Turn the data into a **table**!

```{r}
Kids2 <- KidsFeet %>%     
  mutate(        
    season = case_when(            
      birthmonth %in% c(12,1,2) ~ "Winter",            
      birthmonth %in% c(3,4,5) ~ "Spring",            
      birthmonth %in% c(6,7,8) ~ "Summer",            
      birthmonth %in% c(9,10,11) ~ "Fall"        
   )    
)

kids.tab <- table(Kids2$domhand, Kids2$season)

pander(kids.tab)
```

<br>

***Part 2***: Use the **chi squared test**!

```{r}
chisq.test(kids.tab)
```

***Answer***: P-value = 0.6943

<br>

***

2. When performing a chi-squared test, there are always two ways to display the barplot of the table of observed counts.
***Which of the plots listed below depicts the same set of observed counts that are depicted in this plot?***

- The graphics can be found ***two different ways!***

<div style="display:flex; justify-content: space-around;">
  <div>
```{r}
barplot(t(kids.tab), beside=TRUE, legend.text = TRUE, col=c("darkgray","snow","skyblue2","skyblue4"), main="Children in 4th Grade", xlab="Dominant Hand", args.legend=list(title="Dominant Hand"))
```

  </div> 
  <div>
  ***OR***
  
  </div> 
  <div>
```{r}
barplot(kids.tab, beside=TRUE, legend.text = TRUE, col=c("skyblue","skyblue4"), main="Children in 4th Grade", args.legend=list(title="Dominant Hand", bty="n"), xlab="Birth Season")
```

  </div>
  </div>

<br>

***

3. **Perform an appropriate test to determine if wind speeds of over 12 mph are more or less likely to occur on hot days (days over 80 degrees F) than on other days.** Use the data below. 

```{r}
air2 <- airquality %>%
    mutate(Wind12 = case_when(Wind > 12 ~ "Windy", TRUE ~ "Calm"),
           Temp80 = case_when(Temp > 80 ~ "Hot", TRUE ~ "Fair"))

table.air <- table(air2$Wind12, air2$Temp80)

pander(table.air)
```

<br>

***Part 1***: Use the **chi- squared test!**

```{r}
chi.air <- chisq.test(table.air)

pander(chi.air)
```
<br>

***Part 2***: Find the **residuals** of the counts! 

```{r}
resid.air <- chi.air$residuals

pander(resid.air)
```

- The residual of -2.0865661 for "Hot/Windy" days shows that **Windy days are much less likely to occur on Hot days than on Fair days**.

<br>

***Answer***: There is sufficient evidence to conclude that windy days (wind speeds over 12 mph) are far less likely to occur on hot days (days over 80 degrees F) than on fair days (days under 80 degrees).

<br>

***

#### Class Activity - Chi Squared Test


- Study the example codes provided for the [combine function c( ... )](file:///C:/Users/paige/OneDrive/Documents/Fall%20Semester%202024/MATH%20325/Statistics-Notebook-master/RCommands.html#c(_)_The_Combine_Function)

<br>

Answer the following: 

1. Explain to your neighbor what the following code does using the **"row-bind"** function: 

**rbind**( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) )

```{r}
rbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) )
```


- creates whats called a "vector" or a list/ set of values (has to be the same length)
  - names the numbers as *Good, Bruised, or Rotten*, while assigning those assignments as vectors *Apples and Oranges*
  
<br>

2. How do the results change if you use the **"column-bind"** function instead? 

**cbind**( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) )

```{r}
cbind( Apples = c(Good = 80, Bruised = 20, Rotten = 15), Oranges = c(Good = 75, Bruised = 25, Rotten = 10) )
```


- Presents it in a different way! 

  - you can also put ***t([data set name])*** to transpose the data set 
  
<br>

***

***Table vs. Dataset***

<br>

- **Table**: Count things up/ frequency of when someone meets that criteria (Counts, counts, counts!)

  - 2D : the map of the floor of one building (with all the rooms)
  
  - 4D : the map of campus (multiple buildings and their floors, hallways etc.)
  
  - **This is what we need for a Chi Squared test**

- **Data set**: long and involved because it shows the information about each individual

<br>

***

When making a table: 

- first is **rows**, second is **columns**

  - rows = legend
  
  - columns = x axis/ clusters 
  
- just get real fancy with your colorings! 

  - changing legend colors and changing the color of the x axis clusters names
  
  - quick cheat : use ***xlim=(#,#)*** to move legend off to the side
  
- **CAUTION**: transposing the data can tell you a different story! 

  - dual perspectives -> transpose and non-transposed


<br>

***

#### Class Activity - Chi Squared Survey Results



<u> Step 1: Fetching the data</u>

*To use SAS data, use this code and change the pathing:*

gss2021 <- read_sas("Data/gss2021.sas7bdat", NULL)


<br>

*Pick the columns that you are interested in!*

- Use [GSS column title meaning](https://gssdataexplorer.norc.org/variables/vfilter)

- Columns I thought were interesting: 

- SPANKING : Favor of spanking to discipline child

  - 1 - 4 
  
    - 1 : Strongly Agree
    
    - 4: Strongly Disagree

- POLMURDR : Citizen questioned as a murder suspect

  - 1 (Yes) or 2 (No)

<br>

<u>Step 2: Create a table</u>

***Run the following codes:***

```{r}
gss2021 <- read_sas("Data/gss2021.sas7bdat", NULL)

table(gss2021$SPANKING)

table(gss2021$POLMURDR)

table(gss2021$SPANKING,gss2021$POLMURDR)
```

```{r}


table(gss2021$SPANKING)

table(gss2021$POLMURDR)

table(gss2021$SPANKING,gss2021$POLMURDR)

mitable <- table(gss2021$SPANKING, gss2021$POLMURDR)

rownames(mitable)

colnames(mitable)

rownames(mitable) <- c("Strongly Agree", "Agree", "Disagree", "Strongly Disagree") #these replace the 1, 2, and 3

colnames(mitable) <- c("Yes", "No") #these replace the 1 and 2

mitable %>%
  pander(caption = "2021 Potential Murderers", split.table=Inf)

```

<u>Step 3: Create a Bar Plot</u>

```{r}
barplot(mitable, beside = TRUE, legend.text= TRUE, args.legend = list(title="Should we discipline our children?", x="topright", cex=0.6, bty='n'), col = c("blue", "lightblue", "pink", "red"), xlim= c(0, 18), xlab="Have you been accused of murder?")
```

<br>

<u>Step 4: Conduct a Chi-Squared Test</u>

- Run a chi-squared test on your table with both variables in it to see **if they are independent or associated with each other**.

  - High p-value -> fail to reject, no association/ independent 
  
  - **Low p-value** -> reject the null, association/ not independent

```{r}
chichi <- chisq.test(mitable)

pander(chichi)
```

<br>

<u>Step 5: Check Requirements (expected vs. observed counts + residuals)</u>

- Check the requirements of your chi-squared test. *Are the results of the test valid?*

<br>

***

- Expected Counts

  - All expected counts must be **greater than 5**!

```{r}
pander(chichi$expected)
```

- Observed Counts

  - Used to compare against the expected counts! (If they are really different [result of low p-value] or really close)

```{r}
pander(mitable)
```
<br><br>

***

***Pearson Residuals***

- Allows us a quick understanding of which observed counts are responsible for the $X^2$ statistic being large!

  - Additionally, show the direction in which the observed counts different from the expected counts

$$X^2 = \frac{\sum(O_i - E_i)^2}{E_i}$$

$$X^2 = \sqrt{\frac{(O_i - E_i)^2}{E_i}} = \frac{(O_i - E_i)}{\sqrt{E_i}}$$

$$X^2 = \frac{(\text{Observed - Expected})}{\sqrt{\text{Expected}}}$$

***Where do our departure of the residuals happen?***

- independent (small difference) : close to 0

- not independent(big difference) : further away from 0 

```{r}
pander(chichi$residuals)
```


<br>

***

### Week 13 | Randomizing Testing

<br>

***

#### Skills Quiz - Randomization Testing


**The idea behind permutation testing is that the** ***null hypothesis*** **can be reworded to state that "any pattern that has been witnessed in the sampled data is simple due to** ***random chance*** **."**

- Permutation tests can be applied to any hypothesis testing scenario in order to compute the ***p-value*** of the test in away that does not require any assumption s of the data! 

The most difficult part of any permutation test : **figuring out how to permute the data!**
- which is performed differently for each hypothesis test

Thus, being able to identify the hypothesis test from the stated hypotheses is an important skill you have hopefully started to develop. To see how you are doing with this skill, *match the following null hypotheses with their appropriate test.*

<br>

- Chi-Squared Test

$H_0 : \text{the two variables are not associated}$

- Wilcoxon Signed-Rank Test

$H_0 : \text{median of differences = 0}$

- Paired Samples t Test

$H_0 : \mu_d = 0$

- Wilcoxon Rank Sum Test

$H_0 : difference in medians = 0$

- Independent Samples t Test

$H_0 : \mu_1 = \mu_2$

- ANOVA

$H_0 : \mu_1 = \mu_2 = \mu_3 = \mu_4$

<br>

```{r}
set.seed(1140411) #allows us each to get the same set of random values

   sample1 <- rnorm(30, 69, 2.5) #get a random sample of n=30 normally distributed values with mu=69 and sigma=2.5

   sample2 <- rnorm(30, 69, 2.5) #get another random sample of n=30 normally distributed values with mu=69 and sigma=2.5

   theData <- data.frame(values = c(sample1,sample2), group = rep(c(1,2), each=30)) #load the random samples into a data set

datatable(theData)

   boxplot(values ~ group, data = theData)
```

- Both sample 1 and sample 2 in the above code are samples of size **n = 30** from a **normal distribution** with **mean 69 and standard deviation 2.5**.
  - Thus they are each sample from the **same distribution**!
  - The hypothesis would be : $H_0 : \mu_1 = \mu_2$ for both of these data
  
<br>

Suppose we just had theData dataset without knowledge of the population this data came from. The permutation test of the stated null hypothesis would then be coded in R as:

myTest <- `t.test(values ~ group, data = theData, mu =0)`

observedTestStat <- myTest$`statistic`  
  
observedTestStat

 

N <- 2000      
permutedTestStats <- rep(NA, N)
for  (i in 1:N ) {
   permutedTest <- `t.test(sample(values) ~ group, data = theData, mu = 0` 

   permutedTestStats[i]  <-  permutedTest$`statisitc`
  
}
hist(permutedTestStats)
abline(v=observedTestStat)
sum(permutedTestStats >= observedTestStat)/N
sum(permutedTestStats <= observedTestStat)/N

```{r}
myTest4 <- t.test(values ~ group, data = theData, mu =0)

observedTestStat4 <- myTest4$statistic  
  
print(observedTestStat4)

 

N <- 2000      
permutedTestStats4 <- rep(NA, N)
for(i in 1:N ){
  permutedTest4 <- t.test(sample(values) ~ group, data = theData, mu = 0)
   permutedTestStats4[i]  <-  permutedTest4$statistic
   }

hist(permutedTestStats4)
abline(v=observedTestStat4)

greatvalue <- sum(permutedTestStats4 >= observedTestStat4)/N

print(greatvalue)

lessvalue <- sum(permutedTestStats4 <= observedTestStat4)/N

print(lessvalue)

twovalue <- 2*sum(permutedTestStats4 <= observedTestStat4)/N

print(twovalue)
```


***

#### Assessment Quiz - Randomization Testing


<br>

***

#### Class Activity - Permutation Testing (Part 1)


1. How does the picture of Legos in your Permutation Tesing Overview page demonstrate the idea of permutation testing?

![](C:/Users/paige/OneDrive/Pictures/Screenshots 1/Screenshot 2024-12-17 214147.png)

<div style="display:flex; justify-content: space-around;">
  <div>
This image represents: 
- clear structure/pattern
  - blocks were certainly organized and not sorted by random chance
- suggests **real** pattern that is not random
  - if there is **structure** in the data, then *"mixing up the data and dumping it out again"* will show **very different patterns** from the original
  
  </div> 
  <div>
  
This image represents: 
- a pile of toy blocks in a random pattern
  - as if the toy blocks were put in a bag, shaken up, and dumped out
- ***This is the idea of the permutation test***
  - if the data was just **random** to begin with, then we should see a **very similar pattern** by *"mixing up the data and dumping it out again."*

  </div>
  </div>

<br>

2. How is the distribution of the test statistic created using permutation testing?

<u>The distribution is created by:</u>

- Randomly shuffling (permuting) the data labels many times
- Calculating the test statistic for each permutation
- Building a distribution from all these permuted test statistics

<br>

3. How does this distribution differ from a parametric distribution?

Unlike parametric distributions (like normal or t-distributions) which follow theoretical mathematical formulas, permutation distributions:

- Are created directly from the actual data
- Don't assume any particular shape or underlying distribution
- Better reflect the true sampling distribution for the specific dataset

<br>

4. How is the p-value calculated from a permutation test?

<u>The p-value is calculated by:</u>

- Finding how many permuted test statistics are as extreme or more extreme than the observed test statistic
- Dividing this count by the total number of permutations 

<br>

5. What is a for loop? What does it allow you to do?

**A for loop is a programming construct that:**

- Allows you to repeat a set of instructions multiple times
- In permutation testing, it's used to automate the process of repeatedly shuffling data and calculating test statistics

<br>

***

Here are some R codes to implement in the premutation testing concepts described: 

<u>Function to perform permutation test</u>

permutation_test <- function(data, labels, n_permutations = 1000) {
  observed_stat <- calculate_test_statistic(data, labels)
  
<u>Store permuted statistics</u>

  perm_stats <- numeric(n_permutations)
  
<u>For loop to perform permutations</u>

  for(i in 1:n_permutations) {
  
    ***Randomly shuffle labels***
    shuffled_labels <- sample(labels)
    
    ***Calculate and store test statistic***
    perm_stats[i] <- calculate_test_statistic(data, shuffled_labels)
  }
  
<u>Calculate p-value</u>
  p_value <- mean(perm_stats >= observed_stat)
  
<u>Create distribution plot</u>
  hist(perm_stats, 
       main = "Permutation Distribution",
       xlab = "Test Statistic")
  abline(v = observed_stat, col = "red")
  
  return(p_value)
}

***This code implements the key concepts we see in the selection:***

- Uses a for loop to repeat the permutation process
- Randomly shuffles the data labels
- Calculates test statistics for each permutation
- Creates a distribution from the permuted statistics
- Calculates the p-value by comparing observed vs permuted statistics

<br>

***

#### Class Activity - Premutation Testing (Part 2)


1. Perform an ***independent two-sample t test*** using a permutation test.  Use the `mtcars` dataset and test **whether the average weight of the four cylinder cars differs from the average weight of eight cylinder cars**.  Use the following psuedo code as a starting spot.

<br>

<u>Step 1</u>
myTest <- **…perform the initial test…**
observedTestStat <- **…get the test statistic…**

<u>Step 2</u>
N <- 2000  
  permutedTestStats <- rep(NA, N)
for (i in  1:N){
  permutedTest <- **…perform test with permutedData…**
  permutedTestStats[i] <- **…get test statistic…**
}
hist(permutedTestStats)
abline(v=observedTestStat)

<u>Step 3</u>
sum(permutedTestStats >= observedTestStat)/N
sum(permutedTestStats <= observedTestStat)/N


```{r}
vroom <- mtcars %>%
  dplyr::select(cyl, wt) %>%
  dplyr::filter(cyl %in% c(4, 8))


#Step 1
myTest <- t.test(wt ~ cyl, data = vroom, mu = 0)
observedTestStat <- myTest$statistic

#Step 2
N <- 2000      
permutedTestStats <- rep(NA, N)
for (i in  1:N){
  permutedTest <- t.test(sample(wt) ~ cyl, data = vroom, mu=0)
  permutedTestStats[i] <- permutedTest$statistic
}
hist(permutedTestStats, main= "Permutation Test Distribution", xlab = "Test Statistic")
abline(v=observedTestStat, col= "red", lwd =2)

#Step 3
grea <- sum(permutedTestStats >= observedTestStat)/N
sum(permutedTestStats <= observedTestStat)/N

print(grea)

print(observedTestStat)
```

The ***observed test statistic*** (observedTestStat) for the previous test is [answer]. (round to two decimal places) : **-6.44**

<br>

***

2. Perform a **One-way ANOVA using a permutation test**.  Use the `diamonds` dataset and test **whether the average price of the diamonds depends on the clarity**.  Use the following psuedo code as a starting spot.  *(Hint: because the diamonds dataset has 53,000 rows we start with a small N value)*

<br>

<u>Step 1</u>
myTest <- **…perform the initial test…**
observedTestStat <- **summary(myTest)[[1]]$`F value`[1]**

<u>Step 2</u>
N <- 100      
permutedTestStats <- rep(NA, N)
for (i in  1:N){
  permutedTest <- **…perform test with permutedData…**
  permutedTestStats[i] <- **…get test statistic…**
}
hist(permutedTestStats)
abline(v=observedTestStat)

<u>Step 3</u>
sum(permutedTestStats >= observedTestStat)/N
sum(permutedTestStats <= observedTestStat)/N

```{r}
#Step 1
shiny <- diamonds %>%
  dplyr::select(clarity, price)

myTest2 <- aov(price ~ clarity, data = shiny)
observedTestStat2 <- summary(myTest2)[[1]]$`F value`[1]

#Step 2
N <- 100  
permutedTestStats2 <- rep(NA, N)
for (i in  1:N){
  permutedTest2 <- aov(sample(price) ~ clarity, data=diamonds)
  permutedTestStats2[i] <- summary(permutedTest2) [[1]]$`F value`[1]
}
hist(permutedTestStats2)
abline(v=observedTestStat2)

#Step 3
sum(permutedTestStats2 >= observedTestStat2)/N
sum(permutedTestStats2 <= observedTestStat2)/N

print(observedTestStat2)
```

The ***observed test statistic*** (observedTestStat) for the previous test is [answer]. (round to two decimal places) : **215.0193**

<br>

***

3. Perform a **Logistic Regression Test using a permutation test**.  Use the `SAT` dataset (mosaic library) and test **whether the likelihood of scoring above 1000 (sat > 1000) on the SAT depends on the expenditure per pupil (expend)**.  Use the following psuedo code as a starting spot.

<br>

<u>Step 1</u>
myTest <- **…perform the initial test…**
observedTestStat <- summary(myTest)[[12]][2,3]

<u>Step 2</u>
N <- 100      
permutedTestStats <- rep(NA, N)
for (i in  1:N){
  permutedTest <- **…perform test with permutedData…**
  permutedTestStats[i] <- **…get test statistic…**
}
hist(permutedTestStats)
abline(v=observedTestStat)

<u>Step 3</u>
sum(permutedTestStats >= observedTestStat)/N
sum(permutedTestStats <= observedTestStat)/N

```{r}
SAT2 <- SAT %>% 
  mutate(sat2 = case_when(
    sat > 1000 ~ 1,
    TRUE ~ 0
  ))

#Step 1
myTest3 <- glm(sat2 ~ expend, data=SAT2, family=binomial)
observedTestStat3 <- summary(myTest3)[[12]][2,3]

#Step 2
N <- 100      
permutedTestStats3 <- rep(NA, N)
for (i in  1:N){
  permutedTest3 <- glm(sample(sat2) ~ expend, data=SAT2, family=binomial)
  permutedTestStats3[i] <- summary(permutedTest3)[[12]][2,3]
}
hist(permutedTestStats3)
abline(v=observedTestStat3)

#Step 3
sum(permutedTestStats3 >= observedTestStat3)/N
sum(permutedTestStats3 <= observedTestStat3)/N

print(observedTestStat3)
```

The ***observed test statistic*** (observedTestStat) for the previous test is [answer]. (round to two decimal places) : **-2.499955**


<br>

***

##  Favorite Visuals/ Tips 

<br>

***

### Analysis Tips

<br>

#### Visual making questions


<u>Ask yourself the following questions when making a visual:</u>


(1) Does the title present new information or does it restate what the axis state? 

(2) Is there any information that should or could be explained?

(3) Do the colors/ style add to the presentation of the information?

(4) Is it accessible? (do you need to clarify how to use the graphics?)

(5) Most importantly... did you spell everything right?[^1]



```{r, eval=FALSE}
# Using ```{r, eval=FALSE} turns off the chunk, but still shows it.
# Useful when you want to remember code, but not run it in this file.
```

***

#### Background Tips

- Don't try to write up a ton of stuff, try to get to the point! 

- More words $\neq$ "the better"

- Less is more, have them explore! 
    - use "tabset" commands

***

### Cool Graphics

#### Histograms


***


Great for showing the **distribution of data for a single quantitative variable** when the sample size is large. 


- Dotplots are a good alternative for smaller sample sizes. Gives a good feel for the [mean](NumericalSummaries.html#mean) and [standard deviation](NumericalSummaries.html#sd) of the data. 


***


<a href="javascript:showhide('ggplotcars')">
<div class="hoverchunk">
<span class="tooltipr">
ggplot
  <span class="tooltiprtext">An R function "ggplot" used to create a framework for a graphic that will have elements added to it with the `+` sign.</span>
</span><span class="tooltipr">
(
  <span class="tooltiprtext">Parenthesis to begin the function. Must touch the last letter of the function.</span>
</span><span class="tooltipr">
cars
  <span class="tooltiprtext">"cars" is a dataset. Type "View(cars)" in R to see it.</span>
</span><span class="tooltipr">
,&nbsp; 
  <span class="tooltiprtext">The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice.</span>
</span><span class="tooltipr">
aes(
  <span class="tooltiprtext">The `aes` or "aesthetics" function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become.</span>
</span><span class="tooltipr">
x=dist
  <span class="tooltiprtext">"x=" declares which variable will become the x-axis of the graphic.</span>
</span><span class="tooltipr">
)  
  <span class="tooltiprtext">Closing parenthsis for the aes function.</span>
</span><span class="tooltipr">
)  
  <span class="tooltiprtext">Closing parenthsis for the ggplot function.</span>
</span><span class="tooltipr">
&nbsp;+&nbsp;
  <span class="tooltiprtext">The addition symbol `+` is used to add further elements to the ggplot.</span>
</span><br/><span class="tooltipr">
&nbsp;&nbsp;  geom_histogram(
  <span class="tooltiprtext">The "geom_histogram()" function causes the ggplot to become a histogram. There are many other "geom_" functions that could be used.</span>
</span><span class="tooltipr">
binwidth=5,&nbsp;
  <span class="tooltiprtext">The "binwidth" command controls the width of the bars in the histogram.</span>
</span><span class="tooltipr">
fill="firebrick1",
  <span class="tooltiprtext">The "fill" command controls the color of the insides of each bar.</span>
</span><span class="tooltipr">
color="yellow"
  <span class="tooltiprtext">The "color" command controls the color of the edges of each bar.</span>
</span><span class="tooltipr">
)  
  <span class="tooltiprtext">Closing parenthsis for the geom_histogram function.</span>
</span><span class="tooltipr">
&nbsp;+&nbsp;
  <span class="tooltiprtext">The addition symbol `+` is used to add further elements to the ggplot.</span>
</span><br/><span class="tooltipr">
&nbsp;&nbsp;  ggtitle(
  <span class="tooltiprtext">The "ggtitle(" function is used to add a title to the plot.</span>
</span><span class="tooltipr">
"Races of Lightning McQueen",&nbsp;
  <span class="tooltiprtext">This is the cool sick title of the graph, always make it interesting and not re-explaining the axis labels.</span>
</span><span class="tooltipr">
&nbsp;+&nbsp;
  <span class="tooltiprtext">The addition symbol `+` is used to add further elements to the ggplot.</span>
</span><span class="tooltipr">
xlab("Speed (mph)")&nbsp;
  <span class="tooltiprtext">The "xlab(" command allows you name the x axis.</span>
</span><span class="tooltipr">
&nbsp;+&nbsp;
  <span class="tooltiprtext">The addition symbol `+` is used to add further elements to the ggplot.</span>
</span><span class="tooltipr">
ylab("Distance")
  <span class="tooltiprtext">The "ylab(" command allows you to label the y axis.</span>
</span><span class="tooltipr">
&nbsp;+&nbsp;
  <span class="tooltiprtext">The addition symbol `+` is used to add further elements to the ggplot.</span>
</span><span class="tooltipr">
theme( &nbsp; 
  <span class="tooltiprtext">The "theme(" function allows us to manipulate the title, axis labels, and tick size.</span>
</span><span class="tooltipr">
plot.title= &nbsp; 
  <span class="tooltiprtext">The "plot.title=" allows us to manipulate the title in a various amount of ways.</span>
</span><span class="tooltipr">
size=20,color="dodgerblue",face="bold",family="serif"),&nbsp; 
  <span class="tooltiprtext">These commands allow us to manipulate size, color, bold/italic, or font of the title.</span>
</span><span class="tooltipr">
axis.title.x = &nbsp; 
  <span class="tooltiprtext">The "axis.title.x =" allows us to modify the font size of the axis title (change the x to a y and it changes it for the y axis).</span>
</span><span class="tooltipr">
element_text(size=16),&nbsp; 
  <span class="tooltiprtext">The "element_text(size=" allows us to modify the font size.</span>
</span><span class="tooltipr">
axis.text.x =&nbsp; 
  <span class="tooltiprtext">The "axis.text.x =" allows us to modify the size of the values/labels along the x axis.</span>
</span><span class="tooltipr">
element_text(size=12),&nbsp; 
  <span class="tooltiprtext">The "element_text(size=" allows us to modify the font size.</span>
</span><span class="tooltipr">
axis.title.y = &nbsp; 
  <span class="tooltiprtext">The "axis.title.y =" allows us to modify the font size of the axis title. </span>
</span><span class="tooltipr">
element_text(size=16),&nbsp; 
  <span class="tooltiprtext">The "element_text(size=" allows us to modify the font size.</span>
</span><span class="tooltipr">
axis.text.y =&nbsp; 
  <span class="tooltiprtext">The "axis.text.y =" allows us to modify the size of the values/labels along the y axis.</span>
</span><span class="tooltipr">
element_text(size=12)&nbsp; 
  <span class="tooltiprtext">The "element_text(size=" allows us to modify the font size.</span>
</span><span class="tooltipr">
)  
  <span class="tooltiprtext">Closing parenthsis for the labs function.</span>
</span><span class="tooltipr">
&nbsp;&nbsp;&nbsp;&nbsp;  
  <span class="tooltiprtext">Press Enter to run the code.</span>
</span><span class="tooltipr" style="float:right;">
&nbsp;...&nbsp; 
  <span class="tooltiprtext">Click to View Output.</span>
</span>
</div>
</a>
<div id="ggplotcars" style="display:none;">

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(cars, aes(x=dist))+geom_histogram(binwidth=15, fill="firebrick1", color="yellow")+ggtitle("Races of Lightning McQueen")+ xlab("Speed (mph)")+ylab("Distance")+theme(plot.title=element_text(size=20,color="red",face="bold",family="serif"),axis.title.x = element_text(size=16), axis.title.y = element_text(size=16),axis.text.x = element_text(size=12),axis.text.y = element_text(size=12))
```
</div>
  
<br/>

***

#### Scatter Plots

***

Depicts the **actual values of the data points**, which are $(x,y)$ pairs.


- Works well for small or large sample sizes. Visualizes well the [correlation](NumericalSummaries.html#correlation) between the two variables. Should be used in linear regression contexts whenever possible.


***

```{r message=FALSE, warning=FALSE}
Rent <- read_csv("C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/Rent.csv")


Rent_filtered <- Rent %>%
  mutate(`Walking Minutes to the MC`= round((CrowFlyMetersToMC/1609)*16.67,0))%>%
  dplyr::select(Gender,Name,Address,`Walking Minutes to the MC`,Residents,AvgFloorPlanCost, Latitude, Longitude)%>%
  mutate(`Monthly Floor Plan Cost`=round(AvgFloorPlanCost/3.5,2))%>%
  filter(Gender == "F")%>%
  filter(`Monthly Floor Plan Cost` < 300)%>%
  rename(`Semester Floor Plan Cost` = AvgFloorPlanCost)%>%
  rename(`Apartment Complex`= Name)%>%
  arrange(`Monthly Floor Plan Cost`)
```


<a href="javascript:showhide('PopularBYUIFemaleStudentHousing')">
<div class="hoverchunk">
<span class="tooltipr">
plot_ly(
  <span class="tooltiprtext">
  the "plot_ly" function creates an interactive scatter plot
  </span></span><span class="tooltipr">
Rent_filtered,
  <span class="tooltiprtext">
  the filtered "Rent" data set
  </span></span>
<span class="tooltipr">
x=~`Monthly Floor Plan Cost`,
  <span class="tooltiprtext">
  the x variable
  </span></span>
  <span class="tooltipr">
y= ~Residents,
  <span class="tooltiprtext">
  the y variable
  </span></span>
  <span class="tooltipr">
type = 'scatter',
  <span class="tooltiprtext">
  specifies the type of plot
  </span></span>
  <span class="tooltipr">
mode = 'markers',
  <span class="tooltiprtext">
  defines the drawing mode for the scatter plot
  </span></span>
  <span class="tooltipr">
markers = list(size = 15),
  <span class="tooltiprtext">
  customizes the appearance of the markers
  </span></span>
  <span class="tooltipr">
color = ~ `Monthly Floor Plan Cost`,
  <span class="tooltiprtext">
  Determines the color of each marker based on a variable
  </span></span>
  <span class="tooltipr">
colors = 'Blues',
  <span class="tooltiprtext">
  specifies the color palette to use for the markers
  </span></span>
  <span class="tooltipr">
text = ~ paste (
  <span class="tooltiprtext">
  defines the test that appears when you hover over the marker
  </span></span>
  <span class="tooltipr">
`Apartment Complex`,
  <span class="tooltiprtext">
  shows the name of the apartment complex
  </span></span>
  <span class="tooltipr">
" \ n",
  <span class="tooltiprtext">
  adds a line break/indent (there is a backward slash in front of the n)
  </span></span>
  <span class="tooltipr">
"$",
  <span class="tooltiprtext">
  displays the dollar sign 
  </span></span>
  <span class="tooltipr">
`Monthly Floor Plan Cost`))
  <span class="tooltiprtext">
  shows the monthly floor plan cost of each apartment complex when you hover over the marker
  </span></span>
  <span class="tooltipr">
%>%
  <span class="tooltiprtext">
  the pipe operator
  </span></span>
  <span class="tooltipr">
layout(
  <span class="tooltiprtext">
  modifies the layout and appearance of the plot adding titles, annotaitons, and other layout options!
  </span></span>
  <span class="tooltipr">
title = "Popular BYU-Idaho Female Student Housing",
  <span class="tooltiprtext">
  sets the main title of the plot
  </span></span>
  <span class="tooltipr">
annotations = list(
  <span class="tooltiprtext">
  adds annotations to the plot
  </span></span>
  <span class="tooltipr">
list(x=0.7,y=1.03,
  <span class="tooltiprtext">
  plots where the annotation shows up
  </span></span>
  <span class="tooltipr">
text = "(Light Blue - Cheap, Dark Blue - Expensive)",
  <span class="tooltiprtext">
  The annotation itself!
  </span></span>
  <span class="tooltipr">
showarrow = FALSE, xref= 'paper', xanchor = 'center', yanchor = 'auto', 
  <span class="tooltiprtext">
  I don't know tbh
  </span></span>
  <span class="tooltipr">
font = list(
  <span class="tooltiprtext">
  allows the customization of the x and y axis titles
  </span></span>
  <span class="tooltipr">
size = 10, color = "gray"))),
  <span class="tooltiprtext">
  changes the size and color of the x and y axis titles
  </span></span>
  <span class="tooltipr">
xaxis = list(
  <span class="tooltiprtext">
  allows you to state the desired title of the x axis
  </span></span>
  <span class="tooltipr">
title = "Rent Cost (USD)"),
  <span class="tooltiprtext">
  the title of the x axis
  </span></span>
  <span class="tooltipr">
yaxis = list(
  <span class="tooltiprtext">
  allows you to state the desired title of the y axis
  </span></span>
  <span class="tooltipr">
title = "Residents"))
  <span class="tooltiprtext">
  the title of the y axis
  </span></span>
  <span class="tooltipr">
&nbsp;&nbsp;&nbsp;&nbsp;
<span class="tooltiprtext">Press Enter to run the code.</span>
</span><span class="tooltipr" style="float:right;">
&nbsp;...&nbsp; 
  <span class="tooltiprtext">Click to View Output.</span>
</span>
</div>
</a>
<div id="PopularBYUIFemaleStudentHousing"style="display:none;">

```{r message=FALSE, warning=FALSE}
plot_ly(Rent_filtered,
        x= ~`Monthly Floor Plan Cost`,
        y= ~Residents,
        type = 'scatter',
        mode = 'markers',
        markers = list(size = 15),
        color = ~`Monthly Floor Plan Cost`,
        colors = 'Blues',
        text = ~paste(`Apartment Complex`,"\n","$",`Monthly Floor Plan Cost`))%>%
  layout (title = "Popular BYU-Idaho Female Student Housing",annotations =list(list(x=0.7, y= 1.03,
                 text = "(Light Blue - Cheap, Dark Blue - Expensive)",
                 showarrow = FALSE,
                 xref= 'paper', yref = 'paper',
                 xanchor = 'center', yanchor = 'auto',
                 font = list(size = 10, color = "gray"))),
          xaxis = list(title = "Rent Cost (USD)"),
          yaxis = list(title = "Residents"))
```


</div>


***



```{r}
palette(c("skyblue","firebrick"))

  plot(mpg ~ qsec, data=mtcars, col=as.factor(am), pch=16, xlab="Quarter Mile Time (seconds)", ylab="Miles per Gallon", main="1974 Motor Trend Vehicles")
  legend("topright", pch=16, legend=c("automatic","manual"), title="Transmission", bty='n', col=palette())
```


***

<br>

- ggplot to plot_ly example[^2]

```{r message=FALSE, warning=FALSE}
ExamsTutor <- read_csv("C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/ExamsTutor.csv")

TTExams <- ggplot(ExamsTutor, aes(x=`Tutoring Final Score`, y= `Exams Final Score`)) +
  geom_point(size=1.5, color = "darkolivegreen", alpha =0.5) +
  geom_smooth(method="lm", formula= y~x, se=FALSE, size= 0.5, color="darkgreen")+
  labs(title="BYU-Idaho's 100B Students Tutoring Effects on Chapter Tests") +
  theme_minimal()

ggplotly(TTExams)
```


***

#### Bar Charts


***


Depicts **the number of occurrances for each category**, or *level*, of the qualitative variable. 

- Similar to a histogram, but there is no natural way to order the bars. Thus the white-space between each bar. It is called a *Pareto* chart if the bars are ordered from tallest to shortest. 

- **Clustered and stacked bar charts** are often used to display information for **two qualitative variables** simultaneously. 


***

```{r}
myplot <- ggplot(starwars, aes(x=hair_color, fill=gender)) + geom_bar()

ggplotly(myplot)
```

***

#### Boxplots

***


Graphical depiction of the [five-number summary](NumericalSummaries.html#quartiles). Great for **comparing the distributions of data across several groups or categories**. 

- Provides a quick visual understanding of the **location of the median** as well as the **range of the data**. Can be useful in showing **outliers**. 

- Sample size should be larger than at least five, or computing the *five*-number summary is not very meaningful. Side-by-side dotplots are a good alternative for smaller sample sizes. 


```{r}
plot_ly(KidsFeet, y=~length, x=~as.factor(sex),type="box",color=~sex, colors=c("lightblue","pink")) %>%
  layout( title="4th Grader's Average Foot Size",
          xaxis=list(title="Gender"),yaxis=list(title="Length (cm)"))

```

<br>

***

```{r}
food <- read_csv("C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/food.csv")

Foood <- food %>%
  dplyr::select(weight, breakfast, fries, drink, soup) %>%
  dplyr::filter(!is.na(suppressWarnings(as.numeric(weight))) + 
           is.na(drink)) %>%
  mutate(`Food Perception Score` = ifelse(breakfast == "Cereal", 0, 
                               ifelse(breakfast == "Donut", 1, 0)) +
                        ifelse(fries == "1", 1, ifelse(fries == "2", 0, 0)) + 
                        ifelse(drink == "orange juice", 0, ifelse(drink == "soda", 1, 0)) +
                        ifelse(soup == "1", 0, ifelse(soup == "2", 1, 0))) %>%
  mutate(weight = as.numeric(weight))

boxplot(weight ~ `Food Perception Score`, data=Foood, 
        col=c("darkseagreen1","palegreen","palegreen3","palegreen4","mediumseagreen"), xlab="Food Perception Score", main="Weight of College Students", ylab="Weight (lbs)")
stripchart(weight ~ `Food Perception Score`, data=Foood, pch=16,vertical=TRUE, add=TRUE, col="gray28")
```


***


#### Dot Plots


***

Depicts the **actual values of each data point**. Best for small sample sizes or for datasets where there are lots of repeated values. 

- Histograms or boxplots are better alternatives for large sample sizes when there are few repeated values. Great for **comparing the distribution of data across several groups or categories**. 



```{r}
ggplot(KidsFeet, aes(x=factor(sex),y=length, fill=sex)) +
  coord_flip() +
  geom_dotplot(binaxis="y",
               stackdir="up",
               position="dodge",
               dotsize=0.45,
               binwidth=0.5) +
  scale_fill_manual(values=c("B"="lightblue","G"="pink")) +
  labs(title= "Kennedy 4th Grader's Foot Size", x="Length (cm)", y="Gender")
```

***

#### Tables

Always remember to pipe in a pander when you can!<br><br>


***


```{r message=FALSE, warning=FALSE}
datatable(Rent_filtered, options=list(lengthMenu =c(3,10,30)), extensions="Responsive")
```


***

| &nbsp; | $H_0$ True | $H_0$ False |
|--------|------------|-------------|
| **Reject** $H_0$ | Type I Error | Correct Decision |
| **Accept** $H_0$ | Correct Decision | Type II Error |

<br>

***

```{r message=FALSE, warning=FALSE}
favstats(height~eye_color, data=starwars)%>%
  pander()
```

***

```{r message=FALSE, warning=FALSE}
HSS <- read_csv("C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/HighSchoolSeniors.csv")

HSS.GJ <- dplyr::select(HSS, c(Reaction_time,Outdoor_Activities_Hours,Video_Games_Hours)) %>%
  filter_all(all_vars(!is.na(.))) %>%
  filter(Outdoor_Activities_Hours != Video_Games_Hours) %>%
  filter(abs(Outdoor_Activities_Hours - Video_Games_Hours) >= 10) %>%
  filter(Reaction_time > 0.1 & Reaction_time < 0.4) %>%
  mutate(More_Activity = if_else(Video_Games_Hours > Outdoor_Activities_Hours, "Gamer", "Jock")) %>%
  rename(`Reaction Time (Seconds)` = Reaction_time) %>%
  rename(`Time Spent Outside (Hours)` = Outdoor_Activities_Hours) %>%
  rename(`Time Spent Playing Video Games (Hours)` = Video_Games_Hours) %>%
  rename(`Gamer or Jock?` = More_Activity)


HSS.G <- dplyr::select(HSS.GJ, c(`Reaction Time (Seconds)`,`Gamer or Jock?`)) %>%
  filter(`Gamer or Jock?` == "Gamer")


HSS.J <- dplyr::select(HSS.GJ, c(`Reaction Time (Seconds)`,`Gamer or Jock?`)) %>%
  filter(`Gamer or Jock?` == "Jock")


datatable(HSS.GJ, options=list(lengthMenu =c(3,10,30)), extensions="Responsive")
```




- We added a classification column in this one! It's pretty cool!!

<br>

***

```{r message=FALSE, warning=FALSE}
datatable(Foood, options=list(pageLength = 10)) %>%
  formatStyle('Food Perception Score', backgroundColor = 'lightgreen')
```

<br>

***

- kable function is pretty cool tooooooo!!

```{r message=FALSE, warning=FALSE}
questions <- c("Which one of these pictures do you associate the word 'breakfast'?",
               "Which picture do you associate with the word 'drink'?",
               "Which of these pictures you associate with word 'fries'?",
               "Which of the two pictures you associate with the word 'soup'?")

foodoptions <- c("Cereal /  Donut",
                 "1- Orange Juice / 2- Soda",
                 "1- McDonald's Fries / 2- Home Fries",
                 "1- Veggie Soup / 2- Creamy Soup")

collegestusurvey <- data.frame( Question = questions, Options = foodoptions, stringsAsFactors = FALSE)

kable(collegestusurvey, col.names = c("Questions", "Food Options"), caption ="Food Survey")
```

```{r}
Foodie <- favstats(weight ~ `Food Perception Score`, data = Foood)[,-10]

Foodie2 <- as.data.frame(Foodie)

kable(Foodie2)
```

***

#### Maps

***

Helps to show where things are in reference to something!



***


```{r}
mc_icon <- makeAwesomeIcon(icon= "university", iconColor = "white", markerColor = "lightblue", library = "fa")

leaflet(data = Rent_filtered)%>%
  addTiles() %>%
  setView(lng=-111.7833595717528, lat=43.82217013118815,zoom =14.511)%>%
  addAwesomeMarkers(lng=-111.7827756599601, lat=43.81821783971177,
                    icon= mc_icon, popup= "Manwarding Center(MC)")%>%
  # addMarkers(lng=-111.7826513, lat= 43.82271564, popup= "HILL'S COLLEGE AVE APTS<br>5 Min. Walk")%>%
  # addMarkers(lng=-111.7876455, lat= 43.82246794, popup= "PINES, WOMEN<br>6 Min. Walk")%>%
  # addMarkers(lng=-111.7882195, lat=43.81930189, popup= "DAVENPORT APARTMENTS<br>5 Min. Walk")%>%
  # addMarkers(lng= -111.7754656, lat=43.8174285, popup= "BUENA VISTA<br>6 Min. Walk")%>%
  # addMarkers(lng=-111.7805055, lat=43.82015341, popup= "RIVIERA APARTMENTS<br>3 Min. Walk")%>%
  # addMarkers(lng=-111.7798457, lat=43.82065271, popup= "BAYSIDE MANOR<br>3 Min. Walk")%>%
  # addMarkers(lng=-111.7890135, lat=43.81866711, popup= "BROOKLYN APARTMENTS<br>5 Min. Walk")%>%
  # addMarkers(lng=-111.7871091, lat=43.82469723, popup= "COTTONWOOD - WOMEN<br>8 Min. Walk")%>%
  # addMarkers(lng=-111.7806664, lat=43.82124102, popup= "CRESTWOOD COTTAGE<br>4 Min. Walk")%>%
  # addMarkers(lng=-111.7877153, lat=43.81913933, popup= "ROYAL CREST<br>4 Min. Walk")%>%
  # addMarkers(lng=-111.7792019, lat=43.82347422, popup= "BLUE DOOR, WOMEN<br>6 Min. Walk")%>%
  # addMarkers(lng=-111.7883, lat=43.8200102, popup= "SUNSET HALL<br>5 Min. Walk")%>%
  addMarkers(lng=~Longitude, lat=~Latitude, popup=~paste(`Apartment Complex`,"<br>","Walk Mins.",`Walking Minutes to the MC`)) %>%
  addLegend(position="topright",
            colors= c("lightblue","cornflowerblue"),
            labels = c("Manwarding Center (MC)","Apartment Complexes"),
            title= "Building Locations")%>%
  addControl("<strong>Click the marker to see additional information</strong>",position = "topright", className = "map-caption")
```

***

### Styling and Formats

***


Wow! I typed some words. Some <span style="color:blue;">blue words</span>, some <span style="background-color:yellow;">highlighted words</span>, some <span style="font-size:2em;">big words</span>.


Make a link to a meaningful page of the Statistics-Notebook: [Customization Help](RMarkdownHints.html) <- Read through this page!

Or to a page on the internet: [R Colors](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf)

***

#### Word Stylez

***

##### Colors

< span style="color:[INSERT COLOR];">[INSERT WORDS] < /span >

<br><br>


##### Highlighting

< span style="background-color:[INSERT COLOR];">[INSERT WORDS] < /span >

<br><br>


##### Font Size

< span style="font-size:[INSERT SIZE]em;">[INSERT WORDS] < /span >

<br><br>


##### Underlining

< u>[INSERT WORDS] < /u >

<br><br>


##### Bolding, Italicize, or both!

One * -> Italicize

Two ** -> Bold

Three *** -> Combo

<br><br>

##### Shrinking font-size 

< div style = "font-size:.8em;" > 

<br><br>

##### Changing Words to Gray Text

< div style = "color:gray;" >

<br><br>


***


#### Formatting

##### Section making

{ .tabset .tabset-pills OR fade }
 
<br><br>

##### Making a Caption

< div style ="font-size:.8em;color:#888888;" >
[INSERT THE WORDS YOU WANT TO SAY HERE!]
< / div >

<br><br>

##### Inserting a Picture

! [  _  ] ( [COPY AND PASTE PICTURE INTO THE PARANTHESES] )

<br>

<u>Practice header is 3 stars and in all caps like this:</u>

***PRACTICE***<br><br>

*3 dashes* makes line in between texts (make sure they aren't touching any text and that they are spaced away from things)<br><br>

< br > < br > (makes bigger gap inbetween text, the more you add the wider the gap!)<br><br>

- one dash makes a bullet point
    - pressing space 4 times make a sub bullet point!
        - will this make another one?
        
<br>

##### Pathing Data Set Code
        
<u>Need to path to data set? Use this code:</u>

[Data set name] <- read_csv("C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/[Data set name]")

***or***

do read_csv(../../Data/[Data Set Name].csv)

<br>

Also, to create a csv. file in R, use this code!

write.csv([data set name],"C:/Users/paige/OneDrive/Documents/Fall Semester 2024/MATH 325/Statistics-Notebook-master/Data/[data set name].csv", row.names = FALSE)

<br>

##### Themes

<u>Want a different theme?</u>

Here are some options! 

- "default"
- "bootstrap" -> dark blue buttons
- "cerulean" -> light blue buttons
- "cosmo" -> light blue square button
- "darkly" -> dark blue button + black background
- "flatly" -> DARK blue button with green text!
- "journal" -> pink buttons
- "lumen" -> greyish blue buttons
- "paper" -> blue buttons, lame text
- "readable" -> blue buttons, text is big and bubbly
- "sandstone" -> tan buttons
- "simplex" -> red buttons
- "spacelab" -> darker blue gray buttons, softer shape
- "united" -> redish orange buttons
- "yeti" -> greyish more squared buttons
- "architect" (from prettydoc package)
- "cayman"
- "hpstr"
- "leonids"
- "tactile"
- "html_clean" (from the rmdformats package)
- "html_docco"
- "material"
- "readthedown"

<br>

##### Text Side by Side

<u>Want to make something side by side</u>

< div style="display: flex; justify-content: space-around;" >
  < div >
  [Insert words]
  < /div > 
  < div >
  [more words right here]
  < /div >
  < / div >
  
<br>

##### Hyper linking

<u>Want to make hyper link your stuff from one part to go back and forth between where you are and the source?</u>

(did that even make sense??)

1. Put a **[ ^ 1 ]** next to the source you are wanting to link the sentence to.

    - There is an example to show you in the visual making questions
    
<br>

2. In the sources, put **[ ^ 1 ] : [ The name for your hyper link in brackets ]** ***( the link of the video in parentheses )***

    - [^1] : [It would look like this!](https://youtu.be/dQw4w9WgXcQ?si=6RuIKLyQ5-Z5wL3a)
    
##### ggplot into plot_ly

1. Assignment Operator (<-) your ggplot a name

2. use ggplotly([Insert name you <- to]) to change the ggplot to a plot_ly!

  - example is in the [^2]:scatter plot section!
    
***

## Final Exam Practice

1. Perform an appropriate hypothesis test in R to decide if the medians displayed in the graph are significantly different or not. (Uses `KidsFeet` dataset)

```{r}
t.test(width ~ sex, data= KidsFeet, mu=0, alternative= "two.sided", conf.level = 0.95)
```
2. What is the correct conclusion to make in a logistic regression when the goodness of fit test (either one) gives a p-value of 0.352?

		
a) This shows the logistic regression provides significantly useful results.

	
b) Continue to believe the logistic regression was appropriate to perform on the given data.

	
c) Realize that there was insufficient evidence to conclude that the logistic regression was a good fit for the data.

	
d) The logistic regression simply isn't a good fit for the data in that case.

3. A permutation test for a Kruskal-Wallis Test Statistic is performed with the following result: 

![](C:/Users/paige/OneDrive/Pictures/Screenshots 1/Screenshot 2024-12-18 223902.png)

Note that the numbers above the bars state the frequency of observations contained in that bar.

 

What is the p-value of this permutation test?

 

	
a) p-value = 0.0658

	
b) p-value = 0.1042

	
c) p-value = 0.0351

	
d) p-value = 0.0835

4. Run the following code in R.

  Util <- Utilities

  Util$Season <- cut(Util$month%%12, c(0,2,5,8,11), c("Winter","Spring","Summer","Fall"))

  boxplot(elecbill ~ Season, data=Util, main="A Residence in Minnesota", ylab="Monthly Electricity Bill (US Dollars)", xlab="Season of the Year", col=c("skyblue","darkseagreen3","coral","goldenrod"))

 

Perform an appropriate test in R to determine if at least one season shows evidence of yielding stochastically different values of electric bills from the other seasons.

Report the test statistic, degrees of freedom, and p-value of the test.

 

	
a) F-statistic: 7.234 on 3 and 103 DF,  p-value: 0.0001879

	
b) chi-squared = 321, df = 318, p-value = 0.4424

	
c) chi-squared = 18.677, df = 3, p-value = 0.0003189

	
d) F-statistic: 3.457 on 3 and 318 DF,  p-value: 0.0004425

5. What does each row of the Utilities dataset represent?

a) A house.

	
b) A month.

	
c) A different measurement on gas or electric usage, or even the weather.

	
d) A billing period.

6. Run the following codes in R.

  util.lm <- lm(gasbill ~ temp + I(temp^2), data=Utilities)
  plot(gasbill ~ temp, data=Utilities)
  b <- util.lm$coefficients
  curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE)
  summary(util.lm)

 

Which of the following correctly states the mathematical model for the above regression?

 
a) $\underbrace{Y_i}_{temp} = \beta_0 + \beta_1 \underbrace{X_i}_{gasbill} + \beta_2 X_i^2 + \epsilon_i$
	

b) $\underbrace{Y_i}_{temp} = \beta_0 + \beta_1 \underbrace{X_{1i}}_{gasbill} + \beta_2 \underbrace{X_{2i}}_{elecbill} + \epsilon_i$

c) $\underbrace{Y_i}_{gasbill} = \beta_0 + \beta_1 \underbrace{X_{1i}}_{temp} + \beta_2 \underbrace{X_{2i}}_{month} + \epsilon_i$


d) $\underbrace{Y_i}_{gasbill} = \beta_0 + \beta_1 \underbrace{X_i}_{temp} + \beta_2 X_i^2 + \epsilon_i$

7. Run the following codes in R.

  U2 <- subset(Utilities, month %in% c(3,6))
  plot(gasbill ~ elecbill, data=U2, col=month)
  
```{r}
U2 <- subset(Utilities, month %in% c(3,6))
  plot(gasbill ~ elecbill, data=U2, col=month)
  
whatlm <- lm(gasbill ~ elecbill, data = U2)

summary(whatlm)
```


Perform an analysis that allows you to place two separate lines on this scatterplot.

Report the p-value of the test of whether these lines have significantly different slopes or not.

a) The slopes differ significantly, p-value = 7.14e-06

	
b) No significant difference in slopes, p-value = 0.654

	
c) No significant difference in slopes, p-value = 0.8883.

	
d) The slopes differ significantly, p-value = 6.54e-06

8. A group of Math 221 students performed a reseearch project for their class studying the parking abilities of men and women at BYU-Idaho. Essentially, they watched people park and recorded the final parking as "well done", "crooked", or "failed". They also recorded the gender of the person parking the vehicle.

What type of analyses would best help them determine if men or women are better at parking?

 

	
a) Wilcoxon Rank Sum Test

	
b) Chi Squared Test

	
c) Two-way ANOVA

	
d) Kruskal-Wallis Test

9. Suppose the homeowner of the home where the Utilities dataset was collected from approached you with the question: "How much should I plan on my electric bill costing me this July?"

Use the Utilities dataset in R to perform some useful calculations in order to answer their question.


```{r}
uti <- Utilities %>%
  dplyr::select(elecbill, month)

whatthelm <- lm(elecbill ~ month, data = uti)

summary(whatthelm)
```


Then, select the best answer from the list below.

 

	
a) Based on your past data, it looks like you have spent as little as $8.08 and as much as $69.82 on electricity in July. However, your average is around $22.82, so $25 or so is probably a safe bet.

	
b) Based on your past data, it looks like you have spent as little as $29.32 and as much as $114.90 on electricity in July. However, 3 out of every 4 bills in July was between $91.55 abd $114.90. So planning on at least $90 is probably a safe bet.

	
c) Looking at your past spending, it seems your electric bills in July are all over the place. I would recommend using less electricty and keeping your bill closer to your minimum value of $29.32 rather than on your high end of $114.90.

	
d) Looking at your past spending, it seems your electric bills in July are all over the place. I would recommend using less electricty and keeping your bill closer to your minimum value of $8.08 rather than on the high end of $69.82

10. Use the Utilities dataset in R to perform a linear regression that would place a simple linear regression line on this graph.

Which of the following correctly interprets the slope of the line from this regression? (You may need to scroll down to see the answer options.)

![](C:/Users/paige/OneDrive/Pictures/Screenshots 1/Screenshot 2024-12-18 225554.png)
```{r}
huhlm <- lm(elecbill ~ kwh, data=Utilities)

summary(huhlm)
```

	
a) For every 1 hour increase in kilowatt hours of electricity used, this homeowner pays an extra 10.9 cents on their bill.

	
b) For every 1 hour increase in kilowatt hours of electricity used, this homeowner pays an extra 13.2 cents on their bill.

	
c) This homeowner is being charged an average of 13.2 cents per kilowatt hour of electricity used.

	
d) This homeowner is being charged an average of 10.9 cents per kilowatt hour of electricity used.

11. Consider the output of a chi squared test performed in R that is shown below. What is the p-value of the test?

    Pearson's Chi-squared test

data:  table(gender, groupId))
X-squared = 4.9575, df = 2, p-value = ________

 

	
a) 0.378

	
b) 0.897

	
c) 0.0838

	
d) 0.000485

12. Use the KidsFeet dataset in R to calculate the predicted probability that the birthyear of a child is '88 when the length of the foot (measured in October of 1997) is 26 cm long. (Scroll down to select your answer.)

![](C:/Users/paige/OneDrive/Pictures/Screenshots 1/Screenshot 2024-12-18 225943.png)


	
a) 0.705

	
b) 0.711

	
c) 0.707

	
d) 0.709

13. Use the KidsFeet dataset in R to perform a test to determine if  β1
  is significantly different from zero in the model shown below.

Report the p-value of your test.

$$\underbrace{Y_i}_\text{width} = \beta_0 + \beta_1 \underbrace{X_i}_\text{birthmonth} + \epsilon_i$$

a) P-value = 0.0248

	
b) P-value = 0.000163

	
c) P-value = 0.551

	
d) P-value = 0.995

13. Select the best interpretation of the slope estimate from the logistic regression summary(...) output shown below.

![](C:/Users/paige/OneDrive/Pictures/Screenshots 1/Screenshot 2024-12-18 230215.png)

	
a) Every 1 unit increase in X results in a 1.36% increase in the odds that  Yi=1
  .

	
b) Every 1 unit increase in X results in a 1.37% increase in the odds that  Yi=1
 .

	
c) Every 1 unit increase in X results in a 1.46% drop in the odds that  Yi=1
  .

	
d) Every 1 unit increase in X results in a 1.47% drop in the odds that  Yi=1

14. A student tried to perform the following t Test in R, but got an error message. Go ahead and run the code yourself to see the error message.

    t.test(mpg ~ cyl, data=mtcars)

Which of the following would provide the student with a solution to this error message?

```{r}
kruskal.test(mpg ~ cyl, data=mtcars)
```


	
a) Try using kruskal.test(...) instead of t.test(...).

	
b) Try using wilcox.test(...) instead of t.test(...)

	
c) Try using as.factor(cyl) instead of just cyl in your t.test(...) code.

	
d) Try using paired=TRUE inside your t.test(...) code.

15. Run the following codes in R.

  View(mtcars)

  ?mtcars

If a researcher were to use only the hp and vs columns, which type of analysis would NOT make sense to try and perform with this data as it is currently shown?

	
a) A logistic regression.

	
b) A Wilcoxon Rank Sum Test.

	
c) A chi-squared Test.

	
d) An Independent Samples t Test.

16. A certain hospital collected data from mothers who recently gave birth. They asked the mother if she smoked at all during the pregnancy and recorded her answer as either "yes" or "no". They also recorded the birth weight of the newborn child in grams.

If a logistic regression were used with this data, which of the following would properly word the question for the analysis?

 

 

	
a) Does the probability of the child's birth weight depend on whether or not the mother smoked during the pregnancy?

	
b) Does smoking during a pregnancy effect the birth weight of the child?

	
c) Are children born to mothers that smoke lighter on average than children born to mothers who don't smoke?

	
d) Can the birth weight of the child predict whether or not the mother smoked during the pregnancy?

17. Use the KidsFeet dataset in R to calculate the average foot length of the boys in the dataset who were born in January.

 
```{r}
bigfeet <- KidsFeet %>%
  dplyr::select(length, sex) %>%
  filter(sex == "B")

mean(bigfeet$length)
```

	
a) 25.2

	
b) 25.1

	
c) 24.7

	
d) 23.4



## Final Exam 

<br>


```{r}
YoungAdults <- read_csv("https://raw.githubusercontent.com/saundersg/Statistics-Notebook/master/Data/YoungAdults.csv") 

```

```{r}
oolm <- lm(Weight ~ Height, data=YoungAdults)

predict(oolm, data.frame(Height = 183))
```

```{r}
ayogm <- glm(as.factor(YoungAdults$Lying) ~ Religiosity_score, data=YoungAdults, family=binomial)

summary(ayogm)
```

```{r}
wilcox.test(Religiosity_score ~ Dominant_hand, data=YoungAdults)

ggplot(data=YoungAdults, aes(x=as.factor(Dominant_hand), y=Religiosity_score)) + 
  geom_boxplot()
```


```{r}
prunedudes <- YoungAdults %>%
  filter(Number_of_siblings == 0) %>%
  filter(Smoking == "never smoked")%>%
  filter(Lying == "never")
```

```{r}
t.test(Fear_score ~ Only_child, data = YoungAdults, mu = 0, alternative= "two.sided", conf.level = 0.95)
```

```{r}
YoungAdults.hwg <- YoungAdults %>%
  dplyr::select(Height, Weight, Gender) %>%
  na.omit()
ggplot(YoungAdults.hwg, aes(x=Height, y=Weight, color=Gender))+
  geom_point() +
  geom_smooth(method = "lm", se=F)
```


```{r}
YoungAdults_biggerfam <- YoungAdults %>%
  filter(Number_of_siblings > 3)

plot(Religiosity_score ~ Fear_score, data= YoungAdults_biggerfam)

relig_vs_fear.lm <- lm(Religiosity_score ~ Fear_score, data=YoungAdults_biggerfam)
abline(relig_vs_fear.lm)
observedTestStat <-  summary(relig_vs_fear.lm)[[4]][2,3]

```

```{r}
N <- 2000
premutedTestStats <-rep(NA,N)
set.seed(121)
for(i in 1:N){
  permutedData <- 
    sample(YoungAdults_biggerfam$Religiosity_score)
permutedTest <- lm(sample(Religiosity_score) ~ Fear_score, data=YoungAdults_biggerfam)
permutedTestStats[i] <- summary(permutedTest)[[4]][2,3]
}

```


```{r}
healthyanova <- aov(Weight ~ Gender + I_live_a_healthy_lifestyle + Gender:I_live_a_healthy_lifestyle, data = YoungAdults)

summary(healthyanova)

ggplot(healthyanova, aes(x=I_live_a_healthy_lifestyle, y = Weight, group=Gender, color=Gender)) + geom_point() + stat_summary(fun="mean", geom="line")
```


```{r}
boyyoungin <- YoungAdults %>%
  dplyr::select(Gender, I_live_a_healthy_lifestyle, Weight) %>%
  dplyr::filter(Gender == "male" & I_live_a_healthy_lifestyle == "Agree")
  
mean(boyyoungin$Weight, na.rm=TRUE)

```


```{r}
kruskal.test(Music_diversity_score ~ Education, data= YoungAdults)
```


```{r}
YoungAdults %>%
  filter(I_like_music !="#N/A", !is.na(Gender), !is.na(Punctuality))%>%
  mutate(I_like_music = factor(I_like_music, levels=c("Strongly Disagree", "Disagree", "Neutral", "Agree", "Strongly Agree"), ordered = TRUE)) %>%
  ggplot(aes(x=I_like_music, y=Happiness_score)) +
  geom_boxplot() +
  facet_wrap(~Punctuality) +
  coord_flip()
```

***

